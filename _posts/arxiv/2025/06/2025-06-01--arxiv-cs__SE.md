---
title: cs.SE @ 2025-06-01
date: 2025-06-01
layout: post
---

- [00](#article-0) | 05-29 | How to Elicit Explainability Requirements? A Comparison of Interviews,   Focus Groups, and Surveys | Wie zu Elicit Erklärbarkeit Anforderungen? Ein Vergleich von Interviews, Fokusgruppen und Umfragen | 如何制定明确的解释要求?访谈、焦点小组和调查的比较 | [2505.23684v1](http://arxiv.org/abs/2505.23684v1)
- [01](#article-1) | 05-29 | Quantum-Based Software Engineering | Quantenbasierte Software-Engineering | 基于量子的软件工程 | [2505.23674v1](http://arxiv.org/abs/2505.23674v1)
- [02](#article-2) | 05-29 | GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents | GSO: Herausfordernde Software-Optimierungsaufgaben zur Bewertung von SWE-Agenten | GSO:评估SWE-Agentics的有挑战的软件优化任务 | [2505.23671v1](http://arxiv.org/abs/2505.23671v1)
- [03](#article-3) | 05-29 | Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software   Engineering | Satori-SWE: Evolutionäre Test-Zeit-Skalierung für probeneffiziente Software-Engineering | Satori-SWE:样本高效软件工程的进化测试-时间尺度 | [2505.23604v1](http://arxiv.org/abs/2505.23604v1)
- [04](#article-4) | 05-29 | LLM Performance for Code Generation on Noisy Tasks | LLM-Performance für Code-Generierung bei lauten Aufgaben | LLM 噪音任务代码生成的LLM性能 | [2505.23598v1](http://arxiv.org/abs/2505.23598v1)
- [05](#article-5) | 05-29 | LLM-based Property-based Test Generation for Guardrailing Cyber-Physical   Systems | LLM-basierte property-based Test Generation for Guardrailing Cyber-Physical Systems | 以LLM为基础的保护网络-物理系统基于财产的 | [2505.23549v1](http://arxiv.org/abs/2505.23549v1)
- [06](#article-6) | 05-29 | The CASE Framework -- A New Architecture for Participatory Research and   Digital Health Surveillance | Der CASE Framework - Eine neue Architektur für partizipative Forschung und digitale Gesundheitsüberwachung | CASE框架 -- -- 参与性研究和数字健康监测的新架构 | [2505.23516v1](http://arxiv.org/abs/2505.23516v1)
- [07](#article-7) | 05-29 | Identity resolution of software metadata using Large Language Models | Identitätsauflösung von Software-Metadaten mit großen Sprachmodellen | 使用大语言模式的软件元数据的识别分辨率 | [2505.23500v1](http://arxiv.org/abs/2505.23500v1)
- [08](#article-8) | 05-29 | Synthesizing Performance Constraints for Evaluating and Improving Code   Efficiency | Synthese von Leistungsbeschränkungen zur Bewertung und Verbesserung der Code-Effizienz | 综合评估和提高《守则》效率的绩效制约因素 | [2505.23471v1](http://arxiv.org/abs/2505.23471v1)
- [09](#article-9) | 05-29 | What About Emotions? Guiding Fine-Grained Emotion Extraction from Mobile   App Reviews | Was ist mit Emotionen? Guiding Fine-Grained Emotion Extraction aus Mobile App Bewertungen | 情感呢?指导从移动应用程序评论中抽取精美情感的导师 | [2505.23452v1](http://arxiv.org/abs/2505.23452v1)
- [10](#article-10) | 05-29 | From Knowledge to Noise: CTIM-Rover and the Pitfalls of Episodic Memory   in Software Engineering Agents | Vom Wissen zum Lärm: CTIM-Rover und die Pitfalls des episodischen Gedächtnisses in Software Engineering Agents | 从知识到噪音:CTIM-Rover和软件工程代理器中电离内存的空洞 | [2505.23422v1](http://arxiv.org/abs/2505.23422v1)
- [11](#article-11) | 05-29 | SWE-bench Goes Live! | SWE-Bench geht live! | SWE -BECHE GOES 现场直播! | [2505.23419v1](http://arxiv.org/abs/2505.23419v1)
- [12](#article-12) | 05-29 | Toward Effective AI Governance: A Review of Principles | Auf dem Weg zu einer effektiven KI-Governance: Eine Überprüfung der Grundsätze | 实现有效的独立大赦国际治理:原则审查 | [2505.23417v1](http://arxiv.org/abs/2505.23417v1)
- [13](#article-13) | 05-29 | BugRepro: Enhancing Android Bug Reproduction with Domain-Specific   Knowledge Integration | BugRepro: Verbesserung der Android Bug Reproduction mit Domain-spezifischer Wissensintegration | Bugrepro: 利用特定域知识集成增强Android虫复制 | [2505.14528v2](http://arxiv.org/abs/2505.14528v2)
- [14](#article-14) | 05-29 | Afterburner: Reinforcement Learning Facilitates Self-Improving Code   Efficiency Optimization | Nachbrenner: Verstärktes Lernen erleichtert selbstverbessernde Code-Effizienz-Optimierung | 事后焚烧:强化学习促进自我改进法规效率优化 | [2505.23387v1](http://arxiv.org/abs/2505.23387v1)
- [15](#article-15) | 05-29 | Personality-Guided Code Generation Using Large Language Models | Personalitätsgeführte Code-Generierung mit großen Sprachmodellen | 使用大语言模式的 个人 使用大语言模式的 人 性 指导 代码 生成 | [2411.00006v2](http://arxiv.org/abs/2411.00006v2)
- [16](#article-16) | 05-29 | OSS-UAgent: An Agent-based Usability Evaluation Framework for Open   Source Software | OSS-UAgent: Ein Agent-basiertes Usability Evaluation Framework für Open Source Software | OSS-UUA代理:基于代理的开放源码软件使用性评价框架 | [2505.23239v1](http://arxiv.org/abs/2505.23239v1)
- [17](#article-17) | 05-29 | Artemis: Toward Accurate Detection of Server-Side Request Forgeries   through LLM-Assisted Inter-Procedural Path-Sensitive Taint Analysis | Artemis: Auf dem Weg zur genauen Erkennung von Server-Side Request Forgeries durch LLM-Assisted Inter-Procedural Path-Sensitive Taint Analysis | 人工制品:通过LLM协助的跨程序间路由感知性图解分析,力求准确探测服务器-Side请求的伪造情况 | [2502.21026v3](http://arxiv.org/abs/2502.21026v3)
- [18](#article-18) | 05-29 | Two Is Better Than One: Rotations Scale LoRAs | Zwei ist besser als eins: Rotationsskala LoRAs | 二比一好:轮作规模LORAs | [2505.23184v1](http://arxiv.org/abs/2505.23184v1)
- [19](#article-19) | 05-29 | An open-source Modular Online Psychophysics Platform (MOPP) | Eine Open-Source-Plattform für modulare Online-Psychophysik (MOPP) | 开放源码模块在线心理物理学平台(MOPP) | [2505.23137v1](http://arxiv.org/abs/2505.23137v1)
- [20](#article-20) | 05-29 | VERINA: Benchmarking Verifiable Code Generation | VERINA: Benchmarking der überprüfbaren Code-Generierung | VERINA:可核实代码生成基准 | [2505.23135v1](http://arxiv.org/abs/2505.23135v1)
- [21](#article-21) | 05-29 | Can LLMs Reason About Program Semantics? A Comprehensive Evaluation of   LLMs on Formal Specification Inference | Kann LLMs Grund über Programm Semantik? Eine umfassende Bewertung von LLMs auf formale Spezifikation Inferenz | CLLMs 方案语义学理由:全面评价关于正式具体推断的LLMs | [2503.04779v4](http://arxiv.org/abs/2503.04779v4)
- [22](#article-22) | 05-29 | DINGO: Constrained Inference for Diffusion LLMs | DINGO: Beschränkte Schlussfolgerung für Diffusion LLMs | DINGO: 扩散长效LMM的连续推论 | [2505.23061v1](http://arxiv.org/abs/2505.23061v1)
- [23](#article-23) | 05-29 | HACMony: Automatically Detecting Hopping-related Audio-stream Conflict   Issues on HarmonyOS | HACMony: Automatische Erkennung von Hopping-bezogenen Audio-Stream-Konflikten auf HarmonyOS | HACMonny:自动检测与Happing有关的和谐OS音频流冲突问题 | [2504.07472v2](http://arxiv.org/abs/2504.07472v2)
- [24](#article-24) | 05-29 | Chain of Grounded Objectives: Bridging Process and Goal-oriented   Prompting for Code Generation | Kette der geerdeten Ziele: Überbrückungsprozess und zielorientiertes Prompting für die Codegenerierung | 基本目标链链:搭桥进程和以目标为导向的促进代码生成 | [2501.13978v2](http://arxiv.org/abs/2501.13978v2)
- [25](#article-25) | 05-29 | Structural Abstraction and Selective Refinement for Formal Verification | Strukturelle Abstraktion und selektive Verfeinerung für formale Verifizierung | 正式核查的结构性抽象和选择性改进 | [2505.22982v1](http://arxiv.org/abs/2505.22982v1)
- [26](#article-26) | 05-29 | CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance | CodeSteer: Symbolisch-Augmentierte Sprachmodelle über Code/Text Anleitung | 代码器:通过编码/文本指导的代码/文本指导的代码器:代号辅助语言模式 | [2502.04350v2](http://arxiv.org/abs/2502.04350v2)
- [27](#article-27) | 05-29 | BYOS: Knowledge-driven Large Language Models Bring Your Own Operating   System More Excellent | BYOS: Wissensgetriebene große Sprachmodelle bringen Ihr eigenes Betriebssystem hervorragender | BYOS: 知识驱动的大型语言模式使自己的操作系统更加出色 | [2503.09663v2](http://arxiv.org/abs/2503.09663v2)
- [28](#article-28) | 05-28 | Unlocking Mental Health: Exploring College Students' Well-being through   Smartphone Behaviors | Entsperren der psychischen Gesundheit: Erforschen des Wohlbefindens der Studenten durch Smartphone-Verhalten | 解锁心理健康:通过智能手机行为探索大学生福祉 | [2502.08766v2](http://arxiv.org/abs/2502.08766v2)
- [29](#article-29) | 05-28 | Evolution analysis of software quality metrics in an open-source java   project: A case study on TestNG | Evolutionsanalyse von Software-Qualitätsmetriken in einem Open-Source-Java-Projekt: Eine Fallstudie zu TestNG | 开放源码 Java项目软件质量衡量标准演变分析:测试NG案例研究 | [2505.22884v1](http://arxiv.org/abs/2505.22884v1)
- [30](#article-30) | 05-28 | Visualizing Cloud-native Applications with KubeDiagrams | Cloud-native Anwendungen mit KubeDiagrammen visualisieren | 带有KubeDiagrams 的可视化云源应用 | [2505.22879v1](http://arxiv.org/abs/2505.22879v1)
- [31](#article-31) | 05-28 | RocqStar: Leveraging Similarity-driven Retrieval and Agentic Systems for   Rocq generation | RocqStar: Leveraging-ähnliche Retrieval- und Agentiksysteme für die Rocq-Generation | RocqStar:利用利用相似度驱动回收系统和干系统来生成Rocq | [2505.22846v1](http://arxiv.org/abs/2505.22846v1)
- [32](#article-32) | 05-28 | A Tool for Generating Exceptional Behavior Tests With Large Language   Models | Ein Tool zur Generierung außergewöhnlicher Verhaltenstests mit großen Sprachmodellen | 生成使用大语言模式的特殊行为测试工具 | [2505.22818v1](http://arxiv.org/abs/2505.22818v1)
- [33](#article-33) | 05-28 | What Needs Attention? Prioritizing Drivers of Developers' Trust and   Adoption of Generative AI | Was braucht Aufmerksamkeit? Priorisieren von Treibern des Entwicklervertrauens und der Annahme Generativer KI | 需要注意什么?将开发者信任的驱动因素列为优先事项,并采用创新的AI | [2505.17418v2](http://arxiv.org/abs/2505.17418v2)
- [34](#article-34) | 05-28 | LabUtopia: High-Fidelity Simulation and Hierarchical Benchmark for   Scientific Embodied Agents | LabUtopia: High-Fidelity-Simulation und hierarchischer Benchmark für wissenschaftliche körpereigene Wirkstoffe | LabUtopia:科学渗透剂的高纤维模拟和等级基准 | [2505.22634v1](http://arxiv.org/abs/2505.22634v1)
- [35](#article-35) | 05-28 | Smart Contracts for SMEs and Large Companies | Intelligente Verträge für KMU und Großunternehmen | 中小企业和大公司的智能合同 | [2505.22619v1](http://arxiv.org/abs/2505.22619v1)
- [36](#article-36) | 05-28 | BPMN to Smart Contract by Business Analyst | BPMN auf Smart Contract von Business Analyst | 商业分析员将BPMN改为智能合同 | [2505.22612v1](http://arxiv.org/abs/2505.22612v1)
- [37](#article-37) | 05-28 | GitGoodBench: A Novel Benchmark For Evaluating Agentic Performance On   Git | GitGoodBench: Ein neuartiger Benchmark für die Bewertung Agentischer Performance auf Git | GitGoodbunch:评估基特生物表现的新基准 | [2505.22583v1](http://arxiv.org/abs/2505.22583v1)
- [38](#article-38) | 05-28 | LAMBDA: A Large Model Based Data Agent | LAMBDA: Ein großer modellbasierter Datenagent | LAMBDA:一个大型模型数据代理 | [2407.17535v3](http://arxiv.org/abs/2407.17535v3)
- [39](#article-39) | 05-28 | Advancing Expert Specialization for Better MoE | Advancing Experten-Spezialisierung für bessere MoE | 推进专家专业专业促进改善教育部 | [2505.22323v1](http://arxiv.org/abs/2505.22323v1)
- [40](#article-40) | 05-28 | Evolution of repositories and privacy laws: commit activities in the   GDPR and CCPA era | Entwicklung von Repositorys und Datenschutzgesetzen: Aktivitäten in der DSGVO und CCPA-Ära verpflichten | 保管库和隐私法的演变演变:在GDPR和CCPA时代开展活动 | [2505.22234v1](http://arxiv.org/abs/2505.22234v1)
- [41](#article-41) | 05-28 | Thermal Modeling and Optimal Allocation of Avionics Safety-critical   Tasks on Heterogeneous MPSoCs | Thermische Modellierung und optimale Allokation von Avionik Sicherheitskritische Aufgaben auf heterogenen MPSoCs | 热建模和最佳分配航空气象安全关键任务 | [2505.22214v1](http://arxiv.org/abs/2505.22214v1)
- [42](#article-42) | 05-28 | Towards Conversational Development Environments: Using Theory-of-Mind   and Multi-Agent Architectures for Requirements Refinement | Hin zu konversatorischen Entwicklungsumgebungen: Verwendung von Theorie-von-Mind- und Multi-Agent-Architekturen für Anforderungen Verfeinerung | 走向对话型发展环境:利用理论和多机构架构改进要求 | [2505.20973v2](http://arxiv.org/abs/2505.20973v2)
- [43](#article-43) | 05-28 | Towards Practical Defect-Focused Automated Code Review | Auf dem Weg zu einer praktischen fehlerorientierten automatisierten Code-Überprüfung | 走向实际失效-受污染的自动编码审查 | [2505.17928v2](http://arxiv.org/abs/2505.17928v2)
- [44](#article-44) | 05-28 | SVA-ICL: Improving LLM-based Software Vulnerability Assessment via   In-Context Learning and Information Fusion | SVA-ICL: Verbesserung der LLM-basierten Software Vulnerability Assessment durch In-Context Learning und Information Fusion | SVA-ICL:通过文内学习和信息融合改进基于LLM的软件脆弱性评估 | [2505.10008v2](http://arxiv.org/abs/2505.10008v2)
- [45](#article-45) | 05-28 | Jailbreak Distillation: Renewable Safety Benchmarking | Jailbreak Destillation: Benchmarking für erneuerbare Sicherheit | 蒸馏:可再生能源安全基准 | [2505.22037v1](http://arxiv.org/abs/2505.22037v1)
- [46](#article-46) | 05-28 | Securing the Software Package Supply Chain for Critical Systems | Sicherung der Softwarepaket-Lieferkette für kritische Systeme | 保障关键系统软件包供应链 | [2505.22023v1](http://arxiv.org/abs/2505.22023v1)
- [47](#article-47) | 05-28 | How Do Experts Make Sense of Integrated Process Models? | Wie verstehen Experten integrierte Prozessmodelle? | 专家如何看待综合进程模式? | [2505.20667v2](http://arxiv.org/abs/2505.20667v2)
- [48](#article-48) | 05-28 | System-driven Cloud Architecture Design Support with Structured State   Management and Guided Decision Assistance | Systemgesteuerte Cloud-Architektur-Design-Unterstützung mit strukturiertem Staatsmanagement und beratender Entscheidungshilfe | 提供结构化国家管理和指导决策援助的系统驱动云层结构设计支持 | [2505.20701v2](http://arxiv.org/abs/2505.20701v2)
- [49](#article-49) | 05-28 | Larger Is Not Always Better: Exploring Small Open-source Language Models   in Logging Statement Generation | Größere ist nicht immer besser: Erforschen von kleinen Open-Source-Sprachenmodellen bei der Erstellung von Protokollierungsanweisungen | 大并非总是更好:探索记录报表生成中的小型开放源语言模式 | [2505.16590v2](http://arxiv.org/abs/2505.16590v2)
- [50](#article-50) | 05-28 | Co-Saving: Resource Aware Multi-Agent Collaboration for Software   Development | Co-Saving: Ressourcenschonende Multi-Agenten-Kollaboration für Software-Entwicklung | 共同节省:为开发软件进行有意识的资源、多机构协作 | [2505.21898v1](http://arxiv.org/abs/2505.21898v1)
- [51](#article-51) | 05-27 | Augmenting Software Bills of Materials with Software Vulnerability   Description: A Preliminary Study on GitHub | Augmenting Software Bills of Materials with Software Vulnerability Beschreibung: Eine Vorstudie zu GitHub | 增加具有软件脆弱性说明的软件材料账单:关于GitHub的初步研究 | [2503.13998v2](http://arxiv.org/abs/2503.13998v2)
- [52](#article-52) | 05-27 | Leveraging XP and CRISP-DM for Agile Data Science Projects | Nutzung von XP und CRISP-DM für agile Data Science Projekte | 利用XP和CRISP-DM为敏感数据科学项目发挥杠杆作用 | [2505.21603v1](http://arxiv.org/abs/2505.21603v1)
- [53](#article-53) | 05-27 | JITScope: Interactive Visualization of JIT Compiler IR Transformations | JITScope: Interaktive Visualisierung von JIT Compiler IR-Transformationen | JIT编辑器 IR 转换的交互式视觉化 | [2505.21599v1](http://arxiv.org/abs/2505.21599v1)
- [54](#article-54) | 05-27 | GUARD:Dual-Agent based Backdoor Defense on Chain-of-Thought in Neural   Code Generation | GUARD:Dual-Agent-basierte Backdoor-Verteidigung auf Ketten-of-Thought in Neural Code Generation | GUARD: 在神经代码生成过程中寻求的连锁研究中,基于 " 以企业为基地 " 的后门防御 | [2505.21425v1](http://arxiv.org/abs/2505.21425v1)
- [55](#article-55) | 05-27 | A first look at ROS~2 applications written in asynchronous Rust | Ein erster Blick auf ROS~2 Anwendungen geschrieben in asynchronen Rust | 首先看一看ROS~2的申请,这些申请是以非同步鲁斯特书写的。 | [2505.21323v1](http://arxiv.org/abs/2505.21323v1)
- [56](#article-56) | 05-27 | Computational Reproducibility of R Code Supplements on OSF | Berechnung der Reproduzierbarkeit von R-Code-Ergänzungen auf OSF | OSF的R代码补编的计算可复制性 | [2505.21590v1](http://arxiv.org/abs/2505.21590v1)
- [57](#article-57) | 05-27 | ColorGo: Directed Concolic Execution | ColorGo: Direkte konkolische Ausführung | 颜色 Go : 指向排列执行 | [2505.21130v1](http://arxiv.org/abs/2505.21130v1)
- [58](#article-58) | 05-27 | CXXCrafter: An LLM-Based Agent for Automated C/C++ Open Source Software   Building | CXXCrafter: Ein LLM-basierter Agent für automatisiertes C/C++ Open Source Software Building | CXXCFFF: 一个基于LLM的自动 C/C++ 开放源码软件大楼LLM代理 | [2505.21069v1](http://arxiv.org/abs/2505.21069v1)
- [59](#article-59) | 05-27 | Thinking Before Running! Efficient Code Generation with Thorough   Exploration and Optimal Refinement | Vor dem Laufen denken! Effiziente Codegenerierung mit gründlicher Exploration und optimaler Verfeinerung | 在运行前思考! 高效的代码生成, 彻底探索和优化精炼 | [2502.17442v2](http://arxiv.org/abs/2502.17442v2)
- [60](#article-60) | 05-27 | Optimizing Case-Based Reasoning System for Functional Test Script   Generation with Large Language Models | Optimierung des Case-Based-Reasoning-Systems für die Generierung funktionaler Testskripte mit großen Sprachmodellen | 为具有大语言模型的功能测试脚本生成优化基于个案的理由说明系统 | [2503.20576v3](http://arxiv.org/abs/2503.20576v3)
- [61](#article-61) | 05-27 | RepoMaster: Autonomous Exploration and Understanding of GitHub   Repositories for Complex Task Solving | RepoMaster: Autonome Exploration und Verständnis von GitHub-Lagerstätten für komplexe Aufgabenlösung | RepoMaster:为复杂任务解决而自主探索和了解GitHub储存库 | [2505.21577v1](http://arxiv.org/abs/2505.21577v1)
- [62](#article-62) | 05-27 | An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE   Tasks | Ein LLM-as-Judge Metric zur Überwindung der Lücke mit menschlicher Bewertung in SE-Aufgaben | 消除社会经济任务中与人的评价差距的法学硕士法官 | [2505.20854v1](http://arxiv.org/abs/2505.20854v1)
- [63](#article-63) | 05-27 | Why do Machine Learning Notebooks Crash? An Empirical Study on Public   Python Jupyter Notebooks | Warum zerfallen Machine-Learning-Notebooks? Eine empirische Studie über öffentliche Python-Jupyter-Notebooks | 为什么机器学习笔记本崩溃? | [2411.16795v3](http://arxiv.org/abs/2411.16795v3)
- [64](#article-64) | 05-27 | Can Agents Fix Agent Issues? | Können Agenten Probleme mit Agenten beheben? | 特工能解决代理问题吗? | [2505.20749v1](http://arxiv.org/abs/2505.20749v1)
- [65](#article-65) | 05-27 | Enhancing Code LLMs with Reinforcement Learning in Code Generation: A   Survey | Verbesserung von Code LLMs mit Verstärkungslernen in der Codegenerierung: Eine Umfrage | 增强法典制定中强化学习的加强守则LLMS 代码生成:调查 | [2412.20367v3](http://arxiv.org/abs/2412.20367v3)
- [66](#article-66) | 05-27 | SV-TrustEval-C: Evaluating Structure and Semantic Reasoning in Large   Language Models for Source Code Vulnerability Analysis | SV-TrustEval-C: Bewertung von Struktur und semantischer Vernunft in großen Sprachmodellen für die Analyse von Quellencode-Anfälligkeiten | SV-信任值-C:在源码脆弱性分析大语言模型中评估结构和语义理由 | [2505.20630v1](http://arxiv.org/abs/2505.20630v1)
- [67](#article-67) | 05-26 | Smart Contract Vulnerabilities, Tools, and Benchmarks: An Updated   Systematic Literature Review | Smart Contract Vulnerabilitys, Tools und Benchmarks: Ein aktualisierter systematischer Literaturbericht | 智能合同脆弱性、工具和基准:更新的系统文献审查 | [2412.01719v2](http://arxiv.org/abs/2412.01719v2)
- [68](#article-68) | 05-26 | Large Language Models for IT Automation Tasks: Are We There Yet? | Große Sprachmodelle für IT-Automatisierungsaufgaben: Sind wir noch da? | 信息技术自动化任务大语言模型:我们是否还存在? | [2505.20505v1](http://arxiv.org/abs/2505.20505v1)
- [69](#article-69) | 05-26 | Modeling and Analysis of the Landing Gear System with the Generalized   Contracts | Modellierung und Analyse des Landing Gear Systems mit den Generalized Contracts | 通用合同着陆器系统的建模和分析 | [2111.10426v3](http://arxiv.org/abs/2111.10426v3)
- [70](#article-70) | 05-26 | SWE-rebench: An Automated Pipeline for Task Collection and   Decontaminated Evaluation of Software Engineering Agents | SWE-Rebench: Eine automatisierte Pipeline für die Task Collection und die dekontaminierte Evaluation von Software Engineering Agents | SWE-rebench:软件工程剂任务收集和除污评价自动管道 | [2505.20411v1](http://arxiv.org/abs/2505.20411v1)
- [71](#article-71) | 05-26 | GPUMC: A Stateless Model Checker for GPU Weak Memory Concurrency | GPUMC: Ein staatenloser Modellprüfer für GPU-Schwachspeicherkonkurrenz | GPUMC: GPU 弱内存调制货币的无国籍模式检查器 | [2505.20207v1](http://arxiv.org/abs/2505.20207v1)
- [72](#article-72) | 05-26 | Evaluating Large Language Models for Code Review | Bewertung großer Sprachmodelle für die Code-Überprüfung | 评价用于守则审查的大语言模式 | [2505.20206v1](http://arxiv.org/abs/2505.20206v1)
- [73](#article-73) | 05-26 | Exposing Go's Hidden Bugs: A Novel Concolic Framework | Aufdecken der versteckten Bugs von Go: Ein neuartiges konkolisches Rahmenwerk | 展露 Go 隐藏的臭虫: 新分类框架 | [2505.20183v1](http://arxiv.org/abs/2505.20183v1)
- [74](#article-74) | 05-26 | An Empirical Study on Strong-Weak Model Collaboration for Repo-level   Code Generation | Eine empirische Studie zur stark schwachen Modellkooperation für die Codegenerierung auf Repo-Ebene | 关于回收层代码生成的 " 强弱 " 示范协作经验研究 | [2505.20182v1](http://arxiv.org/abs/2505.20182v1)
- [75](#article-75) | 05-26 | Evaluating Software Plagiarism Detection in the Age of AI: Automated   Obfuscation and Lessons for Academic Integrity | Bewertung von Software Plagiaterkennung im Zeitalter der KI: Automatisierte Verschleierung und Lehren für akademische Integrität | 评价AI时代软件高射率检测:学术廉正方面的自动读写和教益 | [2505.20158v1](http://arxiv.org/abs/2505.20158v1)
- [76](#article-76) | 05-26 | The CodeInverter Suite: Control-Flow and Data-Mapping Augmented Binary   Decompilation with LLMs | Die CodeInverter Suite: Control-Flow und Data-Mapping Augmented Binary Decompilation mit LLMs | 代码输入器套件:控制-光和数据-制表增强的二进制解析与LLMS | [2503.07215v2](http://arxiv.org/abs/2503.07215v2)
- [77](#article-77) | 05-26 | StructEval: Benchmarking LLMs' Capabilities to Generate Structural   Outputs | StructEval: Benchmarking der Kapazitäten von LLM zur Erzeugung struktureller Outputs | DructEval:将LLMs的能力与产生结构性产出挂钩 | [2505.20139v1](http://arxiv.org/abs/2505.20139v1)
- [78](#article-78) | 05-26 | Engineering Trustworthy Machine-Learning Operations with Zero-Knowledge   Proofs | Engineering Vertrauenswürdige Maschinen-Learning-Operationen mit Null-Wissens-Proofs | 具有零知识证明的工程可信赖的机械学习操作 | [2505.20136v1](http://arxiv.org/abs/2505.20136v1)
- [79](#article-79) | 05-26 | Grammars of Formal Uncertainty: When to Trust LLMs in Automated   Reasoning Tasks | Grammatik der formalen Unsicherheit: Wann man LLMs bei automatisierten Aufgaben zur Begründung vertraut | 正式不确定性的语法:在自动说明理由任务中何时信任LLMs | [2505.20047v1](http://arxiv.org/abs/2505.20047v1)
- [80](#article-80) | 05-26 | A Survey on the Safety and Security Threats of Computer-Using Agents:   JARVIS or Ultron? | Eine Umfrage über die Sicherheitsbedrohungen von Computer-Verwendern: JARVIS oder Ultron? | JARVIS还是ULTRON? 调查计算机用户的安全和安保威胁:JARVIS还是ULTRON? | [2505.10924v2](http://arxiv.org/abs/2505.10924v2)
- [81](#article-81) | 05-26 | Ontology- and LLM-based Data Harmonization for Federated Learning in   Healthcare | Ontologie- und LLM-basierte Datenharmonisierung für das Federated Learning in Healthcare | 以本体学和LLM为基础的保健方面联邦学习数据统一 | [2505.20020v1](http://arxiv.org/abs/2505.20020v1)
- [82](#article-82) | 05-26 | Requirements Coverage-Guided Minimization for Natural Language Test   Cases | Anforderungen Abdeckungsgeführte Minimierung für natürliche Sprachtests | 以涵盖范围为指导的尽量减少自然语言测试案件 | [2505.20004v1](http://arxiv.org/abs/2505.20004v1)
- [83](#article-83) | 05-26 | The Invisible Hand: Unveiling Provider Bias in Large Language Models for   Code Generation | Die unsichtbare Hand: Enthüllen von Provider-Bias in großen Sprachmodellen für die Codegenerierung | 无形手:守则生成大语言模式中的 " 无形手 " : " 不可忽视的提供者 " 。 | [2501.07849v2](http://arxiv.org/abs/2501.07849v2)
- [84](#article-84) | 05-26 | Systems of Twinned Systems: A Systematic Literature Review | Systeme von Zwillingssystemen: Ein Systematischer Literaturbericht | 结对系统系统系统:系统文献审查 | [2505.19916v1](http://arxiv.org/abs/2505.19916v1)
- [85](#article-85) | 05-26 | Deconstructing Obfuscation: A four-dimensional framework for evaluating   Large Language Models assembly code deobfuscation capabilities | Dekonstruieren von Obfuscation: Ein vierdimensionaler Rahmen für die Auswertung von Großsprachenmodellen Assembly Code Deobfuscation Fähigkeiten | 解构腐蚀:四维框架,用于评价大语言模型组装编码脱腐能力 | [2505.19887v1](http://arxiv.org/abs/2505.19887v1)
- [86](#article-86) | 05-26 | SecVulEval: Benchmarking LLMs for Real-World C/C++ Vulnerability   Detection | SecVulEval: Benchmarking LLMs für real-World C/C++ Sicherheitserkennung | SecVulEval:确定真实世界C/C+++脆弱性检测LLMs基准 | [2505.19828v1](http://arxiv.org/abs/2505.19828v1)
- [87](#article-87) | 05-26 | A Python workflow definition for computational materials design | Eine Python-Workflow-Definition für die Berechnung von Materialien | 计算材料设计中的 Python 工作流程定义 | [2505.20366v1](http://arxiv.org/abs/2505.20366v1)
- [88](#article-88) | 05-26 | CIDRe: A Reference-Free Multi-Aspect Criterion for Code Comment Quality   Measurement | CIDRe: Ein referenzfreies Multi-Aspekt-Kriterium für die Qualitätsmessung von Code Comment | CIDRe: 守则评论质量衡量的无参考性、无参考性、多特征的多标准标准 | [2505.19757v1](http://arxiv.org/abs/2505.19757v1)
- [89](#article-89) | 05-26 | RDFGraphGen: An RDF Graph Generator based on SHACL Shapes | RDFGraphGen: Ein RDF Graph Generator auf Basis von SHACL Shapes | RDFGraphGen:基于 SHACL 形状的 RDF 图形生成器 | [2407.17941v2](http://arxiv.org/abs/2407.17941v2)
- [90](#article-90) | 05-26 | SETBVE: Quality-Diversity Driven Exploration of Software Boundary   Behaviors | SETBVE: Qualität-Diversität treibt die Erforschung von Software-Grenzverhalten an | SETVE: 软件边界行为的质量-多样性驱动探索 | [2505.19736v1](http://arxiv.org/abs/2505.19736v1)
- [91](#article-91) | 05-26 | Large Language Models in Code Co-generation for Safe Autonomous Vehicles | Große Sprachmodelle in der Kogeneration Code für sichere autonome Fahrzeuge | 安全自治车辆代码共同生成大语言模式 | [2505.19658v1](http://arxiv.org/abs/2505.19658v1)
- [92](#article-92) | 05-26 | Software Engineering for Self-Adaptive Robotics: A Research Agenda | Software-Engineering für selbstadaptive Robotik: Eine Forschungsagenda | 自我适应机器人学软件工程:研究议程 | [2505.19629v1](http://arxiv.org/abs/2505.19629v1)
- [93](#article-93) | 05-26 | Search-Based Software Engineering in the Landscape of AI Foundation   Models | Search-Based Software Engineering in der Landschaft der AI-Stiftung Modelle | AI基金会模型景观中的搜索软件工程 | [2505.19625v1](http://arxiv.org/abs/2505.19625v1)
- [94](#article-94) | 05-26 | LEGO-Compiler: Enhancing Neural Compilation Through Translation   Composability | LEGO-Compiler: Neurale Kompilierung durch Übersetzungskompatibilität verbessern | LEGO-Compuper:通过翻译集成加强神经汇编 | [2505.20356v1](http://arxiv.org/abs/2505.20356v1)
- [95](#article-95) | 05-26 | CODE-DITING: A Reasoning-Based Metric for Functional Alignment in Code   Evaluation | CODE-DITING: Ein auf Vernunft basierendes Metric für die funktionelle Ausrichtung in der Code-Evaluation | 代码化:守则评价中功能一致性的基于理由的计量标准 | [2505.19502v1](http://arxiv.org/abs/2505.19502v1)
- [96](#article-96) | 05-26 | Benchmarking and Enhancing LLM Agents in Localizing Linux Kernel Bugs | Benchmarking und Verbesserung von LLM-Agenten bei der Lokalisierung von Linux-Kernel-Fehlern | 确定和加强Linux内核虫本地化的Linux Kernel 虫的基准和加强LLM代理物 | [2505.19489v1](http://arxiv.org/abs/2505.19489v1)
- [97](#article-97) | 05-26 | Regulating Algorithmic Management: A Multi-Stakeholder Study of   Challenges in Aligning Software and the Law for Workplace Scheduling | Regulierung des algorithmischen Managements: Eine Multi-Stakeholder-Studie über Herausforderungen bei der Ausrichtung von Software und dem Gesetz für die Arbeitsplanung | 规范工资管理:多方利益攸关方研究软件和工作场所时间安排法在调整软件和工作场所时间安排法方面面临的挑战 | [2505.02329v2](http://arxiv.org/abs/2505.02329v2)
- [98](#article-98) | 05-26 | Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications   of Agentic AI | Vibe Coding vs. Agentic Coding: Grundlagen und praktische Implikationen von Agentic AI | Vibe 编码与 Agentic 编码:Agent AI 的基本要素和实际影响 | [2505.19443v1](http://arxiv.org/abs/2505.19443v1)
- [99](#article-99) | 05-26 | Simple and Effective Baselines for Code Summarisation Evaluation | Einfache und effektive Grundlagen für die Code-Summarisation-Bewertung | 用于代码摘要评价的简单有效基线 | [2505.19392v1](http://arxiv.org/abs/2505.19392v1)
- [100](#article-100) | 05-25 | Architectures of Error: A Philosophical Inquiry into AI and Human Code   Generation | Architekturen des Irrtums: Eine philosophische Untersuchung der KI- und menschlichen Code-Generation | 错误结构结构:对大赦国际和人类代码生成的哲学调查 | [2505.19353v1](http://arxiv.org/abs/2505.19353v1)
- [101](#article-101) | 05-25 | Retrieval-Augmented Generation for Service Discovery: Chunking   Strategies and Benchmarking | Retrieval-Augmented Generation for Service Discovery: Chunking Strategien und Benchmarking | 服务发现回收-启动型服务生成:启动战略和基准制定 | [2505.19310v1](http://arxiv.org/abs/2505.19310v1)
- [102](#article-102) | 05-25 | VerifyThisBench: Generating Code, Specifications, and Proofs All at Once | VerifyThisBench: Code, Spezifikationen und Beweise auf einmal generieren | 校验时间: 生成代码、规格和证明 | [2505.19271v1](http://arxiv.org/abs/2505.19271v1)
- [103](#article-103) | 05-25 | CLEVER: A Curated Benchmark for Formally Verified Code Generation | CLEVER: Ein kuratierter Benchmark für die formal verifizierte Codegenerierung | 正式核实的代码生成基准 | [2505.13938v3](http://arxiv.org/abs/2505.13938v3)
- [104](#article-104) | 05-25 | An Empirical Study of Vulnerability Handling Times in CPython | Eine empirische Studie über die Zeiten des Umgangs mit Gefährlichkeit in CPython | CPython 脆弱性处理时间经验研究 | [2411.00447v2](http://arxiv.org/abs/2411.00447v2)
- [105](#article-105) | 05-25 | An Initial Exploration of Fine-tuning Small Language Models for Smart   Contract Reentrancy Vulnerability Detection | Eine erste Erkundung von Feinsteuerungs-Kleinsprachenmodellen für intelligente Vertragsrepentrancy Sicherheitserkennung | 初步探索智能合同留置率易变性探测智能合同微调小型语言模型 | [2505.19059v1](http://arxiv.org/abs/2505.19059v1)
- [106](#article-106) | 05-25 | AIGCodeSet: A New Annotated Dataset for AI Generated Code Detection | AIGCodeSet: Ein neuer kommentierter Datensatz für KI Generated Code Detection | AIGCodeSet:AI 生成代码探测新附加说明数据集 | [2412.16594v3](http://arxiv.org/abs/2412.16594v3)
- [107](#article-107) | 05-25 | On-Demand Scenario Generation for Testing Automated Driving Systems | On-Demand-Szenario-Generierung für die Prüfung automatisierter Fahrsysteme | 自动驾驶系统测试的 " 现场需求 " 情景生成 | [2505.14053v2](http://arxiv.org/abs/2505.14053v2)
- [108](#article-108) | 05-25 | Automated Trustworthiness Oracle Generation for Machine Learning Text   Classifiers | Automatisierte Vertrauenswürdigkeit Oracle Generation für Machine Learning Text Klassifikatoren | 机械学习文字分类的自动可信赖性甲骨文生成 | [2410.22663v4](http://arxiv.org/abs/2410.22663v4)
- [109](#article-109) | 05-25 | Co-PatcheR: Collaborative Software Patching with Component(s)-specific   Small Reasoning Models | Co-PatcheR: Kollaborative Software-Patching mit Komponenten-spezifischen Small-Reasoning-Modellen | 共同配给R:与特定组成部分的小型理由模型合作的软件补补补 | [2505.18955v1](http://arxiv.org/abs/2505.18955v1)
- [110](#article-110) | 05-24 | From Output to Evaluation: Does Raw Instruction-Tuned Code LLMs Output   Suffice for Fill-in-the-Middle Code Generation? | Von der Ausgabe bis zur Auswertung: Reicht die Ausgabe von LLMs mit rohem Instruktionscode für die Generierung von Fill-in-the-Middle Code aus? | 从输出到评价:原始指令-指令代码LLMs 输出足量是否用于中代代号的填充? | [2505.18789v1](http://arxiv.org/abs/2505.18789v1)
- [111](#article-111) | 05-24 | ARMS: A Vision for Actor Reputation Metric Systems in the Open-Source   Software Supply Chain | ARMS: Vision für Actor Reputation Metric Systems in der Open Source Software Supply Chain | ARMS:开放源码软件供应链中行为名声计量系统展望 | [2505.18760v1](http://arxiv.org/abs/2505.18760v1)
- [112](#article-112) | 05-24 | AutoP2C: An LLM-Based Agent Framework for Code Repository Generation   from Multimodal Content in Academic Papers | AutoP2C: Ein LLM-basiertes Agent-Framework für die Code-Repository-Generierung aus multimodalen Inhalten in wissenschaftlichen Papieren | 自动P2C: 学术论文中多种形式内容的法规存储器生成基于LLM的LLM代理框架 | [2504.20115v2](http://arxiv.org/abs/2504.20115v2)
- [113](#article-113) | 05-24 | Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair | Beheben von 7.400 Fehlern für 1$: Günstige Crash-Site-Programm-Reparatur | 为1美元固定7 400个臭虫:低廉的撞车-点火方案维修 | [2505.13103v2](http://arxiv.org/abs/2505.13103v2)
- [114](#article-114) | 05-24 | SEW: Self-Evolving Agentic Workflows for Automated Code Generation | SEW: Selbst-evolvierende Agentische Workflows für die automatisierte Codegenerierung | SEW:自动代码生成的自演动态制剂工作流程 | [2505.18646v1](http://arxiv.org/abs/2505.18646v1)
- [115](#article-115) | 05-24 | ACECODER: Acing Coder RL via Automated Test-Case Synthesis | ACECODER: Acing Coder RL über automatisierte Test-Case-Synthese | 通过自动测试-案件综合合成检索编码器 RL | [2502.01718v4](http://arxiv.org/abs/2502.01718v4)
- [116](#article-116) | 05-24 | On the Structure and Semantics of Identifier Names Containing Closed   Syntactic Category Words | Über die Struktur und Semantik von Identifier-Namen, die geschlossene syntaktische Kategorie Wörter enthalten | 关于含有闭合同步词类的标识名称的结构和语义 | [2505.18444v1](http://arxiv.org/abs/2505.18444v1)

## Article 0
### Title@2025-05-29: How to Elicit Explainability Requirements? A Comparison of Interviews,   Focus Groups, and Surveys
**Title**: How to Elicit Explainability Requirements? A Comparison of Interviews,   Focus Groups, and Surveys | Wie zu Elicit Erklärbarkeit Anforderungen? Ein Vergleich von Interviews, Fokusgruppen und Umfragen | 如何制定明确的解释要求?访谈、焦点小组和调查的比较 [2505.23684v1](http://arxiv.org/abs/2505.23684v1)

**Authors**: Martin Obaidi, Jakob Droste, Hannah Deters, Marc Herrmann, Raymond Ochsner, Jil Klünder, Kurt Schneider

As software systems grow increasingly complex, explainability has become a crucial non-functional requirement for transparency, user trust, and regulatory compliance. Eliciting explainability requirements is challenging, as different methods capture varying levels of detail and structure. This study examines the efficiency and effectiveness of three commonly used elicitation methods - focus groups, interviews, and online surveys - while also assessing the role of taxonomy usage in structuring and improving the elicitation process. We conducted a case study at a large German IT consulting company, utilizing a web-based personnel management software. A total of two focus groups, 18 interviews, and an online survey with 188 participants were analyzed. The results show that interviews were the most efficient, capturing the highest number of distinct needs per participant per time spent. Surveys collected the most explanation needs overall but had high redundancy. Delayed taxonomy introduction resulted in a greater number and diversity of needs, suggesting that a two-phase approach is beneficial. Based on our findings, we recommend a hybrid approach combining surveys and interviews to balance efficiency and coverage. Future research should explore how automation can support elicitation and how taxonomies can be better integrated into different methods.

---

## Article 1
### Title@2025-05-29: Quantum-Based Software Engineering
**Title**: Quantum-Based Software Engineering | Quantenbasierte Software-Engineering | 基于量子的软件工程 [2505.23674v1](http://arxiv.org/abs/2505.23674v1)

**Authors**: Jianjun Zhao

Quantum computing has demonstrated potential for solving computationally intensive problems more efficiently than classical methods. Many software engineering tasks, such as test case selection, static analysis, code clone detection, and defect prediction, involve complex optimization, search, or classification, making them candidates for quantum enhancement. In this paper, we propose Quantum-Based Software Engineering (QBSE), a potential research direction for applying quantum computing to classical software engineering problems. We outline its scope, clarify its distinction from quantum software engineering (QSE), and identify key problem types that may benefit from quantum optimization, search, and learning techniques. We also summarize existing research efforts that remain fragmented. Finally, we sketch a preliminary research agenda that may help guide the future development of QBSE as a structured and meaningful direction within software engineering.

---

## Article 2
### Title@2025-05-29: GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents
**Title**: GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents | GSO: Herausfordernde Software-Optimierungsaufgaben zur Bewertung von SWE-Agenten | GSO:评估SWE-Agentics的有挑战的软件优化任务 [2505.23671v1](http://arxiv.org/abs/2505.23671v1)

**Authors**: Manish Shetty, Naman Jain, Jinjian Liu, Vijay Kethanaboyina, Koushik Sen, Ion Stoica

Developing high-performance software is a complex task that requires specialized expertise. We introduce GSO, a benchmark for evaluating language models' capabilities in developing high-performance software. We develop an automated pipeline that generates and executes performance tests to analyze repository commit histories to identify 102 challenging optimization tasks across 10 codebases, spanning diverse domains and programming languages. An agent is provided with a codebase and performance test as a precise specification, and tasked to improve the runtime efficiency, which is measured against the expert developer optimization. Our quantitative evaluation reveals that leading SWE-Agents struggle significantly, achieving less than 5% success rate, with limited improvements even with inference-time scaling. Our qualitative analysis identifies key failure modes, including difficulties with low-level languages, practicing lazy optimization strategies, and challenges in accurately localizing bottlenecks. We release the code and artifacts of our benchmark along with agent trajectories to enable future research.

---

## Article 3
### Title@2025-05-29: Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software   Engineering
**Title**: Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software   Engineering | Satori-SWE: Evolutionäre Test-Zeit-Skalierung für probeneffiziente Software-Engineering | Satori-SWE:样本高效软件工程的进化测试-时间尺度 [2505.23604v1](http://arxiv.org/abs/2505.23604v1)

**Authors**: Guangtao Zeng, Maohao Shen, Delin Chen, Zhenting Qi, Subhro Das, Dan Gutfreund, David Cox, Gregory Wornell, Wei Lu, Zhang-Wei Hong, Chuang Gan

Language models (LMs) perform well on standardized coding benchmarks but struggle with real-world software engineering tasks such as resolving GitHub issues in SWE-Bench, especially when model parameters are less than 100B. While smaller models are preferable in practice due to their lower computational cost, improving their performance remains challenging. Existing approaches primarily rely on supervised fine-tuning (SFT) with high-quality data, which is expensive to curate at scale. An alternative is test-time scaling: generating multiple outputs, scoring them using a verifier, and selecting the best one. Although effective, this strategy often requires excessive sampling and costly scoring, limiting its practical application. We propose Evolutionary Test-Time Scaling (EvoScale), a sample-efficient method that treats generation as an evolutionary process. By iteratively refining outputs via selection and mutation, EvoScale shifts the output distribution toward higher-scoring regions, reducing the number of samples needed to find correct solutions. To reduce the overhead from repeatedly sampling and selection, we train the model to self-evolve using reinforcement learning (RL). Rather than relying on external verifiers at inference time, the model learns to self-improve the scores of its own generations across iterations. Evaluated on SWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or exceed the performance of models with over 100B parameters while using a few samples. Code, data, and models will be fully open-sourced.

---

## Article 4
### Title@2025-05-29: LLM Performance for Code Generation on Noisy Tasks
**Title**: LLM Performance for Code Generation on Noisy Tasks | LLM-Performance für Code-Generierung bei lauten Aufgaben | LLM 噪音任务代码生成的LLM性能 [2505.23598v1](http://arxiv.org/abs/2505.23598v1)

**Authors**: Radzim Sendyka, Christian Cabrera, Andrei Paleyes, Diana Robinson, Neil Lawrence

This paper investigates the ability of large language models (LLMs) to recognise and solve tasks which have been obfuscated beyond recognition. Focusing on competitive programming and benchmark tasks (LeetCode and MATH), we compare performance across multiple models and obfuscation methods, such as noise and redaction. We demonstrate that all evaluated LLMs can solve tasks obfuscated to a level where the text would be unintelligible to human readers, and does not contain key pieces of instruction or context. We introduce the concept of eager pattern matching to describe this behaviour, which is not observed in tasks published after the models' knowledge cutoff date, indicating strong memorisation or overfitting to training data, rather than legitimate reasoning about the presented problem. We report empirical evidence of distinct performance decay patterns between contaminated and unseen datasets. We discuss the implications for benchmarking and evaluations of model behaviour, arguing for caution when designing experiments using standard datasets. We also propose measuring the decay of performance under obfuscation as a possible strategy for detecting dataset contamination and highlighting potential safety risks and interpretability issues for automated software systems.

---

## Article 5
### Title@2025-05-29: LLM-based Property-based Test Generation for Guardrailing Cyber-Physical   Systems
**Title**: LLM-based Property-based Test Generation for Guardrailing Cyber-Physical   Systems | LLM-basierte property-based Test Generation for Guardrailing Cyber-Physical Systems | 以LLM为基础的保护网络-物理系统基于财产的 [2505.23549v1](http://arxiv.org/abs/2505.23549v1)

**Authors**: Khashayar Etemadi, Marjan Sirjani, Mahshid Helali Moghadam, Per Strandberg, Paul Pettersson

Cyber-physical systems (CPSs) are complex systems that integrate physical, computational, and communication subsystems. The heterogeneous nature of these systems makes their safety assurance challenging. In this paper, we propose a novel automated approach for guardrailing cyber-physical systems using property-based tests (PBTs) generated by Large Language Models (LLMs). Our approach employs an LLM to extract properties from the code and documentation of CPSs. Next, we use the LLM to generate PBTs that verify the extracted properties on the CPS. The generated PBTs have two uses. First, they are used to test the CPS before it is deployed, i.e., at design time. Secondly, these PBTs can be used after deployment, i.e., at run time, to monitor the behavior of the system and guardrail it against unsafe states. We implement our approach in ChekProp and conduct preliminary experiments to evaluate the generated PBTs in terms of their relevance (how well they match manually crafted properties), executability (how many run with minimal manual modification), and effectiveness (coverage of the input space partitions). The results of our experiments and evaluation demonstrate a promising path forward for creating guardrails for CPSs using LLM-generated property-based tests.

---

## Article 6
### Title@2025-05-29: The CASE Framework -- A New Architecture for Participatory Research and   Digital Health Surveillance
**Title**: The CASE Framework -- A New Architecture for Participatory Research and   Digital Health Surveillance | Der CASE Framework - Eine neue Architektur für partizipative Forschung und digitale Gesundheitsüberwachung | CASE框架 -- -- 参与性研究和数字健康监测的新架构 [2505.23516v1](http://arxiv.org/abs/2505.23516v1)

**Authors**: Marco Hirsch, Peter Hevesi, Paul Lukowicz

We present the CASE framework, an open-source platform for adaptive, context-aware participatory research, and pandemic preparedness. CASE implements an event-driven architecture that enables dynamic survey workflows, allowing real-time adaptation based on participant responses, external data, temporal conditions, and evolving user states. The framework supports a broad range of research needs, from simple one-time questionnaires to complex longitudinal studies with advanced conditional logic. Built on over a decade of practical experience, CASE underwent a major architectural rework in 2024, transitioning from a microservice-based design to a streamlined monolithic architecture. This evolution significantly improved maintainability, flexibility, and accessibility to deployment, particularly for institutions with limited technical capacity. CASE has been successfully deployed across diverse domains, powering national disease surveillance platforms, supporting post-COVID cohort studies, and enabling real-time sentiment analysis during political events. These applications, involving tens of thousands of participants, demonstrate the framework's scalability, versatility, and practical value. This paper describes the foundations of CASE, details its architectural evolution, and presents lessons learned from real-world deployments. We establish CASE as a mature and reusable research infrastructure that balances sophisticated functionality with practical implementation, addressing the critical global need for sustainable and institutionally controlled data collection systems.

---

## Article 7
### Title@2025-05-29: Identity resolution of software metadata using Large Language Models
**Title**: Identity resolution of software metadata using Large Language Models | Identitätsauflösung von Software-Metadaten mit großen Sprachmodellen | 使用大语言模式的软件元数据的识别分辨率 [2505.23500v1](http://arxiv.org/abs/2505.23500v1)

**Authors**: Eva Martín del Pico, Josep Lluís Gelpí, Salvador Capella-Gutiérrez

Software is an essential component of research. However, little attention has been paid to it compared with that paid to research data. Recently, there has been an increase in efforts to acknowledge and highlight the importance of software in research activities.   Structured metadata from platforms like bio.tools, Bioconductor, and Galaxy ToolShed offers valuable insights into research software in the Life Sciences. Although originally intended to support discovery and integration, this metadata can be repurposed for large-scale analysis of software practices. However, its quality and completeness vary across platforms, reflecting diverse documentation practices.   To gain a comprehensive view of software development and sustainability, consolidating this metadata is necessary, but requires robust mechanisms to address its heterogeneity and scale.   This article presents an evaluation of instruction-tuned large language models for the task of software metadata identity resolution, a critical step in assembling a cohesive collection of research software. Such a collection is the reference component for the Software Observatory at OpenEBench, a platform that aggregates metadata to monitor the FAIRness of research software in the Life Sciences.   We benchmarked multiple models against a human-annotated gold standard, examined their behavior on ambiguous cases, and introduced an agreement-based proxy for high-confidence automated decisions. The proxy achieved high precision and statistical robustness, while also highlighting the limitations of current models and the broader challenges of automating semantic judgment in FAIR-aligned software metadata across registries and repositories.

---

## Article 8
### Title@2025-05-29: Synthesizing Performance Constraints for Evaluating and Improving Code   Efficiency
**Title**: Synthesizing Performance Constraints for Evaluating and Improving Code   Efficiency | Synthese von Leistungsbeschränkungen zur Bewertung und Verbesserung der Code-Effizienz | 综合评估和提高《守则》效率的绩效制约因素 [2505.23471v1](http://arxiv.org/abs/2505.23471v1)

**Authors**: Jun Yang, Cheng-Chi Wang, Bogdan Alexandru Stoica, Kexin Pei

Large Language Models (LLMs) have been increasingly used to optimize code efficiency. Evaluating their effectiveness and further suggesting optimization opportunities often rely on high-quality tests to demonstrate the performance bottlenecks presented in the program. However, existing approaches rely on a limited set of hand-curated inputs or LLM-generated uninteresting length-stressing tests, failing to reveal more nuanced optimization opportunities. We present WEDGE, a framework for generating performance-stressing input given the program under test. WEDGE synthesizes explicit performance-characterizing constraints in the form of branch conditions to partition the programs' execution space into performance-specific regions. When integrated with the coverage-guided fuzzer, reaching different regions introduces explicit rewards for test generation to explore inefficient implementations. Our evaluation shows that WEDGE introduces a significant slowdown compared to the tests in CodeContests and those claimed to be optimized by existing approaches. From the utility perspective, integrating our tests substantially improves the existing code optimization approaches that rely on test-driven execution feedback. We release PERFFORGE, the performance tests generated by WEDGE, to benchmark future approaches for efficient code generation at https://github.com/UChiSeclab/perfforge.

---

## Article 9
### Title@2025-05-29: What About Emotions? Guiding Fine-Grained Emotion Extraction from Mobile   App Reviews
**Title**: What About Emotions? Guiding Fine-Grained Emotion Extraction from Mobile   App Reviews | Was ist mit Emotionen? Guiding Fine-Grained Emotion Extraction aus Mobile App Bewertungen | 情感呢?指导从移动应用程序评论中抽取精美情感的导师 [2505.23452v1](http://arxiv.org/abs/2505.23452v1)

**Authors**: Quim Motger, Marc Oriol, Max Tiessler, Xavier Franch, Jordi Marco

Opinion mining plays a vital role in analysing user feedback and extracting insights from textual data. While most research focuses on sentiment polarity (e.g., positive, negative, neutral), fine-grained emotion classification in app reviews remains underexplored. This paper addresses this gap by identifying and addressing the challenges and limitations in fine-grained emotion analysis in the context of app reviews. Our study adapts Plutchik's emotion taxonomy to app reviews by developing a structured annotation framework and dataset. Through an iterative human annotation process, we define clear annotation guidelines and document key challenges in emotion classification. Additionally, we evaluate the feasibility of automating emotion annotation using large language models, assessing their cost-effectiveness and agreement with human-labelled data. Our findings reveal that while large language models significantly reduce manual effort and maintain substantial agreement with human annotators, full automation remains challenging due to the complexity of emotional interpretation. This work contributes to opinion mining by providing structured guidelines, an annotated dataset, and insights for developing automated pipelines to capture the complexity of emotions in app reviews.

---

## Article 10
### Title@2025-05-29: From Knowledge to Noise: CTIM-Rover and the Pitfalls of Episodic Memory   in Software Engineering Agents
**Title**: From Knowledge to Noise: CTIM-Rover and the Pitfalls of Episodic Memory   in Software Engineering Agents | Vom Wissen zum Lärm: CTIM-Rover und die Pitfalls des episodischen Gedächtnisses in Software Engineering Agents | 从知识到噪音:CTIM-Rover和软件工程代理器中电离内存的空洞 [2505.23422v1](http://arxiv.org/abs/2505.23422v1)

**Authors**: Tobias Lindenbauer, Georg Groh, Hinrich Schütze

We introduce CTIM-Rover, an AI agent for Software Engineering (SE) built on top of AutoCodeRover (Zhang et al., 2024) that extends agentic reasoning frameworks with an episodic memory, more specifically, a general and repository-level Cross-Task-Instance Memory (CTIM). While existing open-source SE agents mostly rely on ReAct (Yao et al., 2023b), Reflexion (Shinn et al., 2023), or Code-Act (Wang et al., 2024), all of these reasoning and planning frameworks inefficiently discard their long-term memory after a single task instance. As repository-level understanding is pivotal for identifying all locations requiring a patch for fixing a bug, we hypothesize that SE is particularly well positioned to benefit from CTIM. For this, we build on the Experiential Learning (EL) approach ExpeL (Zhao et al., 2024), proposing a Mixture-Of-Experts (MoEs) inspired approach to create both a general-purpose and repository-level CTIM. We find that CTIM-Rover does not outperform AutoCodeRover in any configuration and thus conclude that neither ExpeL nor DoT-Bank (Lingam et al., 2024) scale to real-world SE problems. Our analysis indicates noise introduced by distracting CTIM items or exemplar trajectories as the likely source of the performance degradation.

---

## Article 11
### Title@2025-05-29: SWE-bench Goes Live!
**Title**: SWE-bench Goes Live! | SWE-Bench geht live! | SWE -BECHE GOES 现场直播! [2505.23419v1](http://arxiv.org/abs/2505.23419v1)

**Authors**: Linghao Zhang, Shilin He, Chaoyun Zhang, Yu Kang, Bowen Li, Chengxing Xie, Junhao Wang, Maoquan Wang, Yufan Huang, Shengyu Fu, Elsie Nallipogu, Qingwei Lin, Yingnong Dang, Saravan Rajmohan, Dongmei Zhang

The issue-resolving task, where a model generates patches to fix real-world bugs, has emerged as a critical benchmark for evaluating the capabilities of large language models (LLMs). While SWE-bench and its variants have become standard in this domain, they suffer from key limitations: they have not been updated since their initial releases, cover a narrow set of repositories, and depend heavily on manual effort for instance construction and environment setup. These factors hinder scalability and introduce risks of overfitting and data contamination. In this work, we present \textbf{SWE-bench-Live}, a \textit{live-updatable} benchmark designed to overcome these challenges. Our initial release consists of 1,319 tasks derived from real GitHub issues created since 2024, spanning 93 repositories. Each task is accompanied by a dedicated Docker image to ensure reproducible execution. Central to our benchmark is \method, an automated curation pipeline that streamlines the entire process from instance creation to environment setup, removing manual bottlenecks and enabling scalability and continuous updates. We evaluate a range of state-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a substantial performance gap compared to static benchmarks like SWE-bench, even under controlled evaluation conditions. To better understand this discrepancy, we perform detailed analyses across repository origin, issue recency, and task difficulty. By providing a fresh, diverse, and executable benchmark grounded in live repository activity, SWE-bench-Live facilitates rigorous, contamination-resistant evaluation of LLMs and agents in dynamic, real-world software development settings.

---

## Article 12
### Title@2025-05-29: Toward Effective AI Governance: A Review of Principles
**Title**: Toward Effective AI Governance: A Review of Principles | Auf dem Weg zu einer effektiven KI-Governance: Eine Überprüfung der Grundsätze | 实现有效的独立大赦国际治理:原则审查 [2505.23417v1](http://arxiv.org/abs/2505.23417v1)

**Authors**: Danilo Ribeiro, Thayssa Rocha, Gustavo Pinto, Bruno Cartaxo, Marcelo Amaral, Nicole Davila, Ana Camargo

Artificial Intelligence (AI) governance is the practice of establishing frameworks, policies, and procedures to ensure the responsible, ethical, and safe development and deployment of AI systems. Although AI governance is a core pillar of Responsible AI, current literature still lacks synthesis across such governance frameworks and practices. Objective: To identify which frameworks, principles, mechanisms, and stakeholder roles are emphasized in secondary literature on AI governance. Method: We conducted a rapid tertiary review of nine peer-reviewed secondary studies from IEEE and ACM (20202024), using structured inclusion criteria and thematic semantic synthesis. Results: The most cited frameworks include the EU AI Act and NIST RMF; transparency and accountability are the most common principles. Few reviews detail actionable governance mechanisms or stakeholder strategies. Conclusion: The review consolidates key directions in AI governance and highlights gaps in empirical validation and inclusivity. Findings inform both academic inquiry and practical adoption in organizations.

---

## Article 13
### Title@2025-05-29: BugRepro: Enhancing Android Bug Reproduction with Domain-Specific   Knowledge Integration
**Title**: BugRepro: Enhancing Android Bug Reproduction with Domain-Specific   Knowledge Integration | BugRepro: Verbesserung der Android Bug Reproduction mit Domain-spezifischer Wissensintegration | Bugrepro: 利用特定域知识集成增强Android虫复制 [2505.14528v2](http://arxiv.org/abs/2505.14528v2)

**Authors**: Hongrong Yin, Jinhong Huang, Yao Li, Yunwei Dong, Tao Zhang

Mobile application development is a fast-paced process where maintaining high-quality user experiences is crucial. Bug reproduction, a key aspect of maintaining app quality, often faces significant challenges. Specifically, when descriptions in bug reports are ambiguous or difficult to comprehend, current approaches fail to extract accurate information. Moreover, modern applications exhibit inherent complexity with multiple pages and diverse functionalities, making it challenging for existing methods to map the relevant information in bug reports to the corresponding UI elements that need to be manipulated. To address these challenges, we propose BugRepro, a novel technique that integrates domain-specific knowledge to enhance the accuracy and efficiency of bug reproduction. BugRepro adopts a Retrieval-Augmented Generation (RAG) approach. It retrieves similar bug reports along with their corresponding steps to reproduce (S2R) entities from an example-rich RAG document. In addition, BugRepro explores the graphical user interface (GUI) of the app and extracts transition graphs from the user interface to incorporate app-specific knowledge to guide large language models (LLMs) in their exploration process. Our experiments demonstrate that BugRepro significantly outperforms two state-of-the-art methods (ReCDroid and AdbGPT). For S2R entity extraction accuracy, it achieves a 7.57 to 28.89 percentage point increase over prior methods. For the bug reproduction success rate, the improvement reaches 74.55% and 152.63%. In reproduction efficiency, the gains are 0.72% and 76.68%.

---

## Article 14
### Title@2025-05-29: Afterburner: Reinforcement Learning Facilitates Self-Improving Code   Efficiency Optimization
**Title**: Afterburner: Reinforcement Learning Facilitates Self-Improving Code   Efficiency Optimization | Nachbrenner: Verstärktes Lernen erleichtert selbstverbessernde Code-Effizienz-Optimierung | 事后焚烧:强化学习促进自我改进法规效率优化 [2505.23387v1](http://arxiv.org/abs/2505.23387v1)

**Authors**: Mingzhe Du, Luu Tuan Tuan, Yue Liu, Yuhao Qing, Dong Huang, Xinyi He, Qian Liu, Zejun Ma, See-kiong Ng

Large Language Models (LLMs) generate functionally correct solutions but often fall short in code efficiency, a critical bottleneck for real-world deployment. In this paper, we introduce a novel test-time iterative optimization framework to address this, employing a closed-loop system where LLMs iteratively refine code based on empirical performance feedback from an execution sandbox. We explore three training strategies: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Group Relative Policy Optimization~(GRPO). Experiments on our Venus dataset and the APPS benchmark show that SFT and DPO rapidly saturate in efficiency gains. In contrast, GRPO, using reinforcement learning (RL) with execution feedback, continuously optimizes code performance, significantly boosting both pass@1 (from 47% to 62%) and the likelihood of outperforming human submissions in efficiency (from 31% to 45%). Our work demonstrates effective test-time code efficiency improvement and critically reveals the power of RL in teaching LLMs to truly self-improve code efficiency.

---

## Article 15
### Title@2025-05-29: Personality-Guided Code Generation Using Large Language Models
**Title**: Personality-Guided Code Generation Using Large Language Models | Personalitätsgeführte Code-Generierung mit großen Sprachmodellen | 使用大语言模式的 个人 使用大语言模式的 人 性 指导 代码 生成 [2411.00006v2](http://arxiv.org/abs/2411.00006v2)

**Authors**: Yaoqi Guo, Zhenpeng Chen, Jie M. Zhang, Yang Liu, Yun Ma

Code generation, the automatic creation of source code from natural language descriptions, has garnered significant attention due to its potential to streamline software development. Inspired by research that links task-personality alignment with improved development outcomes, we conduct an empirical study on personality-guided code generation using large language models (LLMs). Specifically, we investigate how emulating personality traits appropriate to the coding tasks affects LLM performance. We extensively evaluate this approach using seven widely adopted LLMs across four representative datasets. Our results show that personality guidance significantly enhances code generation accuracy, with improved pass rates in 23 out of 28 LLM-dataset combinations. Notably, in 11 cases, the improvement exceeds 5%, and in 5 instances, it surpasses 10%, with the highest gain reaching 12.9%. Additionally, personality guidance can be easily integrated with other prompting strategies to further boost performance. We open-source our code and data at https://github.com/IanWalls/Persona-Code.

---

## Article 16
### Title@2025-05-29: OSS-UAgent: An Agent-based Usability Evaluation Framework for Open   Source Software
**Title**: OSS-UAgent: An Agent-based Usability Evaluation Framework for Open   Source Software | OSS-UAgent: Ein Agent-basiertes Usability Evaluation Framework für Open Source Software | OSS-UUA代理:基于代理的开放源码软件使用性评价框架 [2505.23239v1](http://arxiv.org/abs/2505.23239v1)

**Authors**: Lingkai Meng, Yu Shao, Long Yuan, Longbin Lai, Peng Cheng, Wenyuan Yu, Wenjie Zhang, Xuemin Lin, Jingren Zhou

Usability evaluation is critical to the impact and adoption of open source software (OSS), yet traditional methods relying on human evaluators suffer from high costs and limited scalability. To address these limitations, we introduce OSS-UAgent, an automated, configurable, and interactive agent-based usability evaluation framework specifically designed for open source software. Our framework employs intelligent agents powered by large language models (LLMs) to simulate developers performing programming tasks across various experience levels (from Junior to Expert). By dynamically constructing platform-specific knowledge bases, OSS-UAgent ensures accurate and context-aware code generation. The generated code is automatically evaluated across multiple dimensions, including compliance, correctness, and readability, providing a comprehensive measure of the software's usability. Additionally, our demonstration showcases OSS-UAgent's practical application in evaluating graph analytics platforms, highlighting its effectiveness in automating usability evaluation.

---

## Article 17
### Title@2025-05-29: Artemis: Toward Accurate Detection of Server-Side Request Forgeries   through LLM-Assisted Inter-Procedural Path-Sensitive Taint Analysis
**Title**: Artemis: Toward Accurate Detection of Server-Side Request Forgeries   through LLM-Assisted Inter-Procedural Path-Sensitive Taint Analysis | Artemis: Auf dem Weg zur genauen Erkennung von Server-Side Request Forgeries durch LLM-Assisted Inter-Procedural Path-Sensitive Taint Analysis | 人工制品:通过LLM协助的跨程序间路由感知性图解分析,力求准确探测服务器-Side请求的伪造情况 [2502.21026v3](http://arxiv.org/abs/2502.21026v3)

**Authors**: Yuchen Ji, Ting Dai, Zhichao Zhou, Yutian Tang, Jingzhu He

Server-side request forgery (SSRF) vulnerabilities are inevitable in PHP web applications. Existing static tools in detecting vulnerabilities in PHP web applications neither contain SSRF-related features to enhance detection accuracy nor consider PHP's dynamic type features. In this paper, we present Artemis, a static taint analysis tool for detecting SSRF vulnerabilities in PHP web applications. First, Artemis extracts both PHP built-in and third-party functions as candidate source and sink functions. Second, Artemis constructs both explicit and implicit call graphs to infer functions' relationships. Third, Artemis performs taint analysis based on a set of rules that prevent over-tainting and pauses when SSRF exploitation is impossible. Fourth, Artemis analyzes the compatibility of path conditions to prune false positives. We have implemented a prototype of Artemis and evaluated it on 250 PHP web applications. Artemis reports 207 true vulnerable paths (106 true SSRFs) with 15 false positives. Of the 106 detected SSRFs, 35 are newly found and reported to developers, with 24 confirmed and assigned CVE IDs.

---

## Article 18
### Title@2025-05-29: Two Is Better Than One: Rotations Scale LoRAs
**Title**: Two Is Better Than One: Rotations Scale LoRAs | Zwei ist besser als eins: Rotationsskala LoRAs | 二比一好:轮作规模LORAs [2505.23184v1](http://arxiv.org/abs/2505.23184v1)

**Authors**: Hongcan Guo, Guoshun Nan, Yuan Yang, Diyang Zhang, Haotian Li, Zhican Chen, Qinchuan Zhou, Yuhan Ran, Xinye Cao, Sicong Leng, Xiaofeng Tao, Xudong Jiang

Scaling Low-Rank Adaptation (LoRA)-based Mixture-of-Experts (MoE) facilitates large language models (LLMs) to efficiently adapt to diverse tasks. However, traditional gating mechanisms that route inputs to the best experts may fundamentally hinder LLMs' scalability, leading to poor generalization and underfitting issues. We identify that the root cause lies in the restricted expressiveness of existing weighted-sum mechanisms, both within and outside the convex cone of LoRA representations. This motivates us to propose RadarGate, a novel geometrically inspired gating method that introduces rotational operations of LoRAs representations to boost the expressiveness and facilitate richer feature interactions among multiple LoRAs for scalable LLMs. Specifically, we first fuse each LoRA representation to other LoRAs using a learnable component and then feed the output to a rotation matrix. This matrix involves learnable parameters that define the relative angular relationship between LoRA representations. Such a simple yet effective mechanism provides an extra degree of freedom, facilitating the learning of cross-LoRA synergies and properly tracking the challenging poor generalization and underfitting issues as the number of LoRA grows. Extensive experiments on 6 public benchmarks across 21 tasks show the effectiveness of our RadarGate for scaling LoRAs. We also provide valuable insights, revealing that the rotations to each pair of representations are contrastive, encouraging closer alignment of semantically similar representations during geometrical transformation while pushing distance ones further apart. We will release our code to the community.

---

## Article 19
### Title@2025-05-29: An open-source Modular Online Psychophysics Platform (MOPP)
**Title**: An open-source Modular Online Psychophysics Platform (MOPP) | Eine Open-Source-Plattform für modulare Online-Psychophysik (MOPP) | 开放源码模块在线心理物理学平台(MOPP) [2505.23137v1](http://arxiv.org/abs/2505.23137v1)

**Authors**: Yuval Samoilov-Kats, Matan Noach, Noam Beer, Yuval Efrati, Adam Zaidel

In recent years, there is a growing need and opportunity to use online platforms for psychophysics research. Online experiments make it possible to evaluate large and diverse populations remotely and quickly, complementing laboratory-based research. However, developing and running online psychophysics experiments poses several challenges: i) a high barrier-to-entry for researchers who often need to learn complex code-based platforms, ii) an uncontrolled experimental environment, and iii) questionable credibility of the participants. Here, we introduce an open-source Modular Online Psychophysics Platform (MOPP) to address these challenges. Through the simple web-based interface of MOPP, researchers can build modular experiments, share them with others, and copy or modify tasks from each others environments. MOPP provides built-in features to calibrate for viewing distance and to measure visual acuity. It also includes email-based and IP-based authentication, and reCAPTCHA verification. We developed five example psychophysics tasks, that come preloaded in the environment, and ran a pilot experiment which was hosted on the AWS (Amazon Web Services) cloud. Pilot data collected for these tasks yielded similar results to those reported in laboratory settings. MOPP can thus help researchers collect large psychophysics datasets online, with reduced turnaround time, and in a standardized manner.

---

## Article 20
### Title@2025-05-29: VERINA: Benchmarking Verifiable Code Generation
**Title**: VERINA: Benchmarking Verifiable Code Generation | VERINA: Benchmarking der überprüfbaren Code-Generierung | VERINA:可核实代码生成基准 [2505.23135v1](http://arxiv.org/abs/2505.23135v1)

**Authors**: Zhe Ye, Zhengxu Yan, Jingxuan He, Timothe Kasriel, Kaiyu Yang, Dawn Song

Large language models (LLMs) are increasingly integrated in software development, but ensuring correctness in LLM-generated code remains challenging and often requires costly manual review. Verifiable code generation -- jointly generating code, specifications, and proofs of code-specification alignment -- offers a promising path to address this limitation and further unleash LLMs' benefits in coding. Yet, there exists a significant gap in evaluation: current benchmarks often lack support for end-to-end verifiable code generation. In this paper, we introduce Verina (Verifiable Code Generation Arena), a high-quality benchmark enabling a comprehensive and modular evaluation of code, specification, and proof generation as well as their compositions. Verina consists of 189 manually curated coding tasks in Lean, with detailed problem descriptions, reference implementations, formal specifications, and extensive test suites. Our extensive evaluation of state-of-the-art LLMs reveals significant challenges in verifiable code generation, especially in proof generation, underscoring the need for improving LLM-based theorem provers in verification domains. The best model, OpenAI o4-mini, generates only 61.4% correct code, 51.0% sound and complete specifications, and 3.6% successful proofs, with one trial per task. We hope Verina will catalyze progress in verifiable code generation by providing a rigorous and comprehensive benchmark. We release our dataset on https://huggingface.co/datasets/sunblaze-ucb/verina and our evaluation code on https://github.com/sunblaze-ucb/verina.

---

## Article 21
### Title@2025-05-29: Can LLMs Reason About Program Semantics? A Comprehensive Evaluation of   LLMs on Formal Specification Inference
**Title**: Can LLMs Reason About Program Semantics? A Comprehensive Evaluation of   LLMs on Formal Specification Inference | Kann LLMs Grund über Programm Semantik? Eine umfassende Bewertung von LLMs auf formale Spezifikation Inferenz | CLLMs 方案语义学理由:全面评价关于正式具体推断的LLMs [2503.04779v4](http://arxiv.org/abs/2503.04779v4)

**Authors**: Thanh Le-Cong, Bach Le, Toby Murray

Large Language Models (LLMs) are increasingly being used to automate programming tasks. Yet, LLMs' capabilities in reasoning about program semantics are still inadequately studied, leaving significant potential for further exploration. This paper introduces FormalBench, a comprehensive benchmark designed to evaluate LLMs' reasoning abilities on program semantics, particularly via the task of synthesizing formal program specifications to assist verifying program correctness. This task requires both comprehensive reasoning over all possible program executions and the generation of precise, syntactically correct expressions that adhere to formal syntax and semantics. Using this benchmark, we evaluated the ability of LLMs in synthesizing consistent and complete specifications. Our findings show that LLMs perform well with simple control flows but struggle with more complex structures, especially loops, even with advanced prompting. Additionally, LLMs exhibit limited robustness against semantic-preserving transformations. We also highlight common failure patterns and design self-repair prompts, improving success rates by 25%.

---

## Article 22
### Title@2025-05-29: DINGO: Constrained Inference for Diffusion LLMs
**Title**: DINGO: Constrained Inference for Diffusion LLMs | DINGO: Beschränkte Schlussfolgerung für Diffusion LLMs | DINGO: 扩散长效LMM的连续推论 [2505.23061v1](http://arxiv.org/abs/2505.23061v1)

**Authors**: Tarun Suresh, Debangshu Banerjee, Shubham Ugare, Sasa Misailovic, Gagandeep Singh

Diffusion LLMs have emerged as a promising alternative to conventional autoregressive LLMs, offering significant potential for improved runtime efficiency. However, existing diffusion models lack the ability to provably enforce user-specified formal constraints, such as regular expressions, which makes them unreliable for tasks that require structured outputs, such as fixed-schema JSON generation. Unlike autoregressive models that generate tokens sequentially, diffusion LLMs predict a block of tokens in parallel. This parallelism makes traditional constrained decoding algorithms, which are designed for sequential token prediction, ineffective at preserving the true output distribution. To address this limitation, we propose DINGO, a dynamic programming-based constrained decoding strategy that is both efficient and provably distribution-preserving. DINGO enables sampling of output strings with the highest probability under the model's predicted distribution, while strictly satisfying any user-specified regular expression. On standard symbolic math and JSON generation benchmarks, DINGO achieves up to a 68 percentage point improvement over unconstrained inference

---

## Article 23
### Title@2025-05-29: HACMony: Automatically Detecting Hopping-related Audio-stream Conflict   Issues on HarmonyOS
**Title**: HACMony: Automatically Detecting Hopping-related Audio-stream Conflict   Issues on HarmonyOS | HACMony: Automatische Erkennung von Hopping-bezogenen Audio-Stream-Konflikten auf HarmonyOS | HACMonny:自动检测与Happing有关的和谐OS音频流冲突问题 [2504.07472v2](http://arxiv.org/abs/2504.07472v2)

**Authors**: Jinlong He, Binru Huang, Changwei Xia, Hengqin Yang, Jiwei Yan, Jun Yan

HarmonyOS is emerging as a popular distributed operating system for diverse mobile devices. One of its standout features is app-hopping, which allows users to seamlessly transition apps across different HarmonyOS devices. However, when apps playing audio streams hop between devices, they can easily trigger Hopping-related Audio-stream Conflict (HAC) scenarios. Improper resolution of HAC will lead to significant HAC issues, which are harder to detect compared to single-device audio-stream conflicts, due to the unclear semantics of HarmonyOS's app-hopping mechanism and the lack of effective multi-app hopping testing methods. To fill the gap, this paper introduces an automated and efficient approach to detecting HAC issues. We formalized the operational semantics of HarmonyOS's app-hopping mechanism for audio streams for the first time. Leveraging this formalization, we designed an Audio Service Transition Graph (ASTG) to model the behaviors of audio-API-related services and proposed a model-based approach to detect HAC issues automatically. Our techniques were implemented in a tool, HACMony, and evaluated on 20 real-world HarmonyOS apps. Experimental results reveal that 11 of the 20 apps exhibit HAC issues. Additionally, we summarized the detected issues into two typical types, namely MoD and MoR, and analyzed their characteristics to assist and guide both app and OS developers.

---

## Article 24
### Title@2025-05-29: Chain of Grounded Objectives: Bridging Process and Goal-oriented   Prompting for Code Generation
**Title**: Chain of Grounded Objectives: Bridging Process and Goal-oriented   Prompting for Code Generation | Kette der geerdeten Ziele: Überbrückungsprozess und zielorientiertes Prompting für die Codegenerierung | 基本目标链链:搭桥进程和以目标为导向的促进代码生成 [2501.13978v2](http://arxiv.org/abs/2501.13978v2)

**Authors**: Sangyeop Yeo, Seung-won Hwang, Yu-Seung Ma

The use of Large Language Models (LLMs) for code generation has gained significant attention in recent years. Existing methods often aim to improve the quality of generated code by incorporating additional contextual information or guidance into input prompts. Many of these approaches adopt sequential reasoning strategies, mimicking human-like step-by-step thinking. However, such strategies may constrain flexibility, as they do not always align with the structured characteristics of programming languages. This paper introduces the Chain of Grounded Objectives (CGO), a method that embeds functional objectives into input prompts to enhance code generation. By leveraging appropriately structured objectives as input and avoiding explicit sequential procedures, CGO adapts effectively to the structured nature of programming tasks. Empirical evaluations demonstrate that CGO effectively enhances code generation, addressing limitations of existing approaches.

---

## Article 25
### Title@2025-05-29: Structural Abstraction and Selective Refinement for Formal Verification
**Title**: Structural Abstraction and Selective Refinement for Formal Verification | Strukturelle Abstraktion und selektive Verfeinerung für formale Verifizierung | 正式核查的结构性抽象和选择性改进 [2505.22982v1](http://arxiv.org/abs/2505.22982v1)

**Authors**: Christoph Luckeneder, Ralph Hoch, Hermann Kaindl

Safety verification of robot applications is extremely challenging due to the complexity of the environment that a robot typically operates in. Formal verification with model-checking provides guarantees but it may often take too long or even fail for complex models of the environment. A usual solution approach is abstraction, more precisely behavioral abstraction. Our new approach introduces structural abstraction instead, which we investigated in the context of voxel representation of the robot environment. This kind of abstraction leads to abstract voxels. We also propose a complete and automated verification workflow, which is based on an already existing methodology for robot applications, and inspired by the key ideas behind counterexample-guided abstraction refinement (CEGAR) - performing an initial abstraction and successively introducing refinements based on counterexamples, intertwined with model-checker runs. Hence, our approach uses selective refinement of structural abstractions to improve the runtime efficiency of model-checking. A fully-automated implementation of our approach showed its feasibility, since counterexamples have been found for a realistic scenario with a fairly high (maximal) resolution in a few minutes, while direct model-checker runs led to a crash after a couple of days.

---

## Article 26
### Title@2025-05-29: CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance
**Title**: CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance | CodeSteer: Symbolisch-Augmentierte Sprachmodelle über Code/Text Anleitung | 代码器:通过编码/文本指导的代码/文本指导的代码器:代号辅助语言模式 [2502.04350v2](http://arxiv.org/abs/2502.04350v2)

**Authors**: Yongchao Chen, Yilun Hao, Yueying Liu, Yang Zhang, Chuchu Fan

Existing methods fail to effectively steer Large Language Models (LLMs) between textual reasoning and code generation, leaving symbolic computing capabilities underutilized. We introduce CodeSteer, an effective method for guiding LLM code/text generation. We construct a comprehensive benchmark SymBench comprising 37 symbolic tasks with adjustable complexity and also synthesize datasets of 12k multi-turn guidance/generation trajectories and 5.5k guidance comparison pairs. We fine-tune the Llama-3-8B model with a newly designed multi-turn supervised fine-tuning (SFT) and direct preference optimization (DPO). The resulting model, CodeSteerLLM, augmented with the proposed symbolic and self-answer checkers, effectively guides the code/text generation of larger models. Augmenting GPT-4o with CodeSteer raises its average performance score from 53.3 to 86.4, even outperforming the existing best LLM OpenAI o1 (82.7), o1-preview (74.8), and DeepSeek R1 (76.8) across all 37 tasks (28 seen, 9 unseen). Trained for GPT-4o, CodeSteer demonstrates superior generalizability, providing an average 41.8 performance boost on Claude, Mistral, and GPT-3.5. CodeSteer-guided LLMs fully harness symbolic computing to maintain strong performance on highly complex tasks. Models, Datasets, and Codes are available at https://github.com/yongchao98/CodeSteer-v1.0 and https://huggingface.co/yongchao98.

---

## Article 27
### Title@2025-05-29: BYOS: Knowledge-driven Large Language Models Bring Your Own Operating   System More Excellent
**Title**: BYOS: Knowledge-driven Large Language Models Bring Your Own Operating   System More Excellent | BYOS: Wissensgetriebene große Sprachmodelle bringen Ihr eigenes Betriebssystem hervorragender | BYOS: 知识驱动的大型语言模式使自己的操作系统更加出色 [2503.09663v2](http://arxiv.org/abs/2503.09663v2)

**Authors**: Hongyu Lin, Yuchen Li, Haoran Luo, Kaichun Yao, Libo Zhang, Mingjie Xing, Yanjun Wu

Operating System (OS) kernel tuning involves systematically adjusting kernel configurations to optimize system performance. Despite recent advancements in large language models (LLMs), kernel tuning remains a critical challenge due to: (1) the semantic gap between abstract tuning objective and concrete config options, (2) insufficient environmental interaction induces LLM hallucinations, and (3) the rapid evolution of kernel versions. To address these challenges, we propose BYOS, a LLM-powered framework that automates kernel tuning through three key innovations: structured knowledge construction and mapping, knowledge-driven configuration generation, and continuous knowledge maintenance. Extensive experiments show that BYOS achieves 7.1%-155.4% performance improvements over default configurations across standard OS benchmarks and real-world applications, demonstrating structured knowledge representation can overcome key limitations of pure LLM solutions for system optimization. Our code is available at https://github.com/LHY-24/BYOS.

---

## Article 28
### Title@2025-05-28: Unlocking Mental Health: Exploring College Students' Well-being through   Smartphone Behaviors
**Title**: Unlocking Mental Health: Exploring College Students' Well-being through   Smartphone Behaviors | Entsperren der psychischen Gesundheit: Erforschen des Wohlbefindens der Studenten durch Smartphone-Verhalten | 解锁心理健康:通过智能手机行为探索大学生福祉 [2502.08766v2](http://arxiv.org/abs/2502.08766v2)

**Authors**: Wei Xuan, Meghna Roy Chowdhury, Yi Ding, Yixue Zhao

The global mental health crisis is a pressing concern, with college students particularly vulnerable to rising mental health disorders. The widespread use of smartphones among young adults, while offering numerous benefits, has also been linked to negative outcomes such as addiction and regret, significantly impacting well-being. Leveraging the longest longitudinal dataset collected over four college years through passive mobile sensing, this study is the first to examine the relationship between students' smartphone unlocking behaviors and their mental health at scale in real-world settings. We provide the first evidence demonstrating the predictability of phone unlocking behaviors for mental health outcomes based on a large dataset, highlighting the potential of these novel features for future predictive models. Our findings reveal important variations in smartphone usage across genders and locations, offering a deeper understanding of the interplay between digital behaviors and mental health. We highlight future research directions aimed at mitigating adverse effects and promoting digital well-being in this population.

---

## Article 29
### Title@2025-05-28: Evolution analysis of software quality metrics in an open-source java   project: A case study on TestNG
**Title**: Evolution analysis of software quality metrics in an open-source java   project: A case study on TestNG | Evolutionsanalyse von Software-Qualitätsmetriken in einem Open-Source-Java-Projekt: Eine Fallstudie zu TestNG | 开放源码 Java项目软件质量衡量标准演变分析:测试NG案例研究 [2505.22884v1](http://arxiv.org/abs/2505.22884v1)

**Authors**: Venkata Sai Sravya Sambaturu

Software quality is critical in modern software engineering, especially in large and evolving codebases. This study analyzes the evolution of software quality metrics in five successive versions of the open-source Java testing framework TestNG. Using the static analysis tool Understand, eleven key object-oriented metrics, including cyclomatic complexity, class coupling, and lines of code, were extracted for each version. Statistical and visual analyses reveal structural trends over time. The results indicate that TestNG has matured into a more stable and maintainable framework, reflecting ongoing development, refactoring, and architectural improvements. This study provides insights into design evolution and offers recommendations for maintaining code quality in similar projects.

---

## Article 30
### Title@2025-05-28: Visualizing Cloud-native Applications with KubeDiagrams
**Title**: Visualizing Cloud-native Applications with KubeDiagrams | Cloud-native Anwendungen mit KubeDiagrammen visualisieren | 带有KubeDiagrams 的可视化云源应用 [2505.22879v1](http://arxiv.org/abs/2505.22879v1)

**Authors**: Philippe Merle, Fabio Petrillo

Modern distributed applications increasingly rely on cloud-native platforms to abstract the complexity of deployment and scalability. As the de facto orchestration standard, Kubernetes enables this abstraction, but its declarative configuration model makes the architectural understanding difficult. Developers, operators, and architects struggle to form accurate mental models from raw manifests, Helm charts, or cluster state descriptions. We introduce KubeDiagrams, an open-source tool that transforms Kubernetes manifests into architecture diagrams. By grounding our design in a user-centered study of real-world visualization practices, we identify the specific challenges Kubernetes users face and map these to concrete design requirements. KubeDiagrams integrates seamlessly with standard Kubernetes artifacts, preserves semantic fidelity to core concepts, and supports extensibility and automation. We detail the tool's architecture, visual encoding strategies, and extensibility mechanisms. Three case studies illustrate how KubeDiagrams enhances system comprehension and supports architectural reasoning in distributed cloud-native systems. KubeDiagrams addresses concrete pain points in Kubernetes-based DevOps practices and is valued for its automation, clarity, and low-friction integration into real-world tooling environments.

---

## Article 31
### Title@2025-05-28: RocqStar: Leveraging Similarity-driven Retrieval and Agentic Systems for   Rocq generation
**Title**: RocqStar: Leveraging Similarity-driven Retrieval and Agentic Systems for   Rocq generation | RocqStar: Leveraging-ähnliche Retrieval- und Agentiksysteme für die Rocq-Generation | RocqStar:利用利用相似度驱动回收系统和干系统来生成Rocq [2505.22846v1](http://arxiv.org/abs/2505.22846v1)

**Authors**: Nikita Khramov, Andrei Kozyrev, Gleb Solovev, Anton Podkopaev

Interactive Theorem Proving was repeatedly shown to be fruitful combined with Generative Artificial Intelligence. This paper assesses multiple approaches to Rocq generation and illuminates potential avenues for improvement. We highlight the importance of thorough premise selection for generating Rocq proofs and propose a novel approach, leveraging retrieval via a self-attentive embedder model. The evaluation of the designed approach shows up to 28% relative increase of the generator's performance. We tackle the problem of writing Rocq proofs using a multi-stage agentic system, tailored for formal verification, and demonstrate its high effectiveness. We conduct an ablation study and show the use of multi-agent debate on the planning stage of proof synthesis.

---

## Article 32
### Title@2025-05-28: A Tool for Generating Exceptional Behavior Tests With Large Language   Models
**Title**: A Tool for Generating Exceptional Behavior Tests With Large Language   Models | Ein Tool zur Generierung außergewöhnlicher Verhaltenstests mit großen Sprachmodellen | 生成使用大语言模式的特殊行为测试工具 [2505.22818v1](http://arxiv.org/abs/2505.22818v1)

**Authors**: Linghan Zhong, Samuel Yuan, Jiyang Zhang, Yu Liu, Pengyu Nie, Junyi Jessy Li, Milos Gligoric

Exceptional behavior tests (EBTs) are crucial in software development for verifying that code correctly handles unwanted events and throws appropriate exceptions. However, prior research has shown that developers often prioritize testing "happy paths", e.g., paths without unwanted events over exceptional scenarios. We present exLong, a framework that automatically generates EBTs to address this gap. exLong leverages a large language model (LLM) fine-tuned from CodeLlama and incorporates reasoning about exception-throwing traces, conditional expressions that guard throw statements, and non-exceptional behavior tests that execute similar traces. Our demonstration video illustrates how exLong can effectively assist developers in creating comprehensive EBTs for their project (available at https://youtu.be/Jro8kMgplZk).

---

## Article 33
### Title@2025-05-28: What Needs Attention? Prioritizing Drivers of Developers' Trust and   Adoption of Generative AI
**Title**: What Needs Attention? Prioritizing Drivers of Developers' Trust and   Adoption of Generative AI | Was braucht Aufmerksamkeit? Priorisieren von Treibern des Entwicklervertrauens und der Annahme Generativer KI | 需要注意什么?将开发者信任的驱动因素列为优先事项,并采用创新的AI [2505.17418v2](http://arxiv.org/abs/2505.17418v2)

**Authors**: Rudrajit Choudhuri, Bianca Trinkenreich, Rahul Pandita, Eirini Kalliamvakou, Igor Steinmacher, Marco Gerosa, Christopher Sanchez, Anita Sarma

Generative AI (genAI) tools are advertised as productivity aids. Yet, issues related to miscalibrated trust and usage friction continue to hinder their adoption. Additionally, AI can be exclusionary, failing to support diverse users adequately, further exacerbating these concerns. One such aspect of diversity is cognitive diversity -- variations in users' cognitive styles -- that leads to divergence in interaction styles. When an individual's cognitive styles are unsupported, it creates additional barriers to technology adoption. Thus, to design tools that developers trust, we must first understand what factors affect their trust and intentions to use these tools in practice?   We developed a theoretical model of factors influencing trust and adoption intentions towards genAI through a large-scale survey with developers (N=238) at GitHub and Microsoft. Using Partial Least Squares-Structural Equation Modeling (PLS-SEM), we found that genAI's system/output quality, functional value, and goal maintenance significantly influence developers' trust, which along with their cognitive styles, affects their intentions to use these tools in work. An Importance-Performance Matrix Analysis (IPMA) identified factors that, despite their strong influence, underperform, revealing specific genAI aspects that need design prioritization. We bolster these findings by qualitatively analyzing developers' perceived challenges and risks of genAI usage to uncover why these gaps persist in development contexts. For genAI to indeed be a true productivity aid rather than a disguised productivity sink, it must align with developers' goals, maintain contextual transparency, reduce cognitive burden, and provide equitable interaction support. We provide practical suggestions to guide future genAI tool design for effective, trustworthy, and inclusive human-genAI interactions.

---

## Article 34
### Title@2025-05-28: LabUtopia: High-Fidelity Simulation and Hierarchical Benchmark for   Scientific Embodied Agents
**Title**: LabUtopia: High-Fidelity Simulation and Hierarchical Benchmark for   Scientific Embodied Agents | LabUtopia: High-Fidelity-Simulation und hierarchischer Benchmark für wissenschaftliche körpereigene Wirkstoffe | LabUtopia:科学渗透剂的高纤维模拟和等级基准 [2505.22634v1](http://arxiv.org/abs/2505.22634v1)

**Authors**: Rui Li, Zixuan Hu, Wenxi Qu, Jinouwen Zhang, Zhenfei Yin, Sha Zhang, Xuantuo Huang, Hanqing Wang, Tai Wang, Jiangmiao Pang, Wanli Ouyang, Lei Bai, Wangmeng Zuo, Ling-Yu Duan, Dongzhan Zhou, Shixiang Tang

Scientific embodied agents play a crucial role in modern laboratories by automating complex experimental workflows. Compared to typical household environments, laboratory settings impose significantly higher demands on perception of physical-chemical transformations and long-horizon planning, making them an ideal testbed for advancing embodied intelligence. However, its development has been long hampered by the lack of suitable simulator and benchmarks. In this paper, we address this gap by introducing LabUtopia, a comprehensive simulation and benchmarking suite designed to facilitate the development of generalizable, reasoning-capable embodied agents in laboratory settings. Specifically, it integrates i) LabSim, a high-fidelity simulator supporting multi-physics and chemically meaningful interactions; ii) LabScene, a scalable procedural generator for diverse scientific scenes; and iii) LabBench, a hierarchical benchmark spanning five levels of complexity from atomic actions to long-horizon mobile manipulation. LabUtopia supports 30 distinct tasks and includes more than 200 scene and instrument assets, enabling large-scale training and principled evaluation in high-complexity environments. We demonstrate that LabUtopia offers a powerful platform for advancing the integration of perception, planning, and control in scientific-purpose agents and provides a rigorous testbed for exploring the practical capabilities and generalization limits of embodied intelligence in future research.

---

## Article 35
### Title@2025-05-28: Smart Contracts for SMEs and Large Companies
**Title**: Smart Contracts for SMEs and Large Companies | Intelligente Verträge für KMU und Großunternehmen | 中小企业和大公司的智能合同 [2505.22619v1](http://arxiv.org/abs/2505.22619v1)

**Authors**: C. G. Liu, P. Bodorik, D. Jutla

Research on blockchains addresses multiple issues, with one being writing smart contracts. In our previous research we described methodology and a tool to generate, in automated fashion, smart contracts from BPMN models. The generated smart contracts provide support for multi-step transactions that facilitate repair/upgrade of smart contracts. In this paper we show how the approach is used to support collaborations via smart contracts for companies ranging from SMEs with little IT capabilities to companies with IT using blockchain smart contracts. Furthermore, we also show how the approach is used for certain applications to generate smart contracts by a BPMN modeler who does not need any knowledge of blockchain technology or smart contract development - thus we are hoping to facilitate democratization of smart contracts and blockchain technology.

---

## Article 36
### Title@2025-05-28: BPMN to Smart Contract by Business Analyst
**Title**: BPMN to Smart Contract by Business Analyst | BPMN auf Smart Contract von Business Analyst | 商业分析员将BPMN改为智能合同 [2505.22612v1](http://arxiv.org/abs/2505.22612v1)

**Authors**: C. G. Liu, P. Bodorik, D. Jutla

This paper addresses the challenge of creating smart contracts for applications represented using Business Process Management and Notation (BPMN) models. In our prior work we presented a methodology that automates the generation of smart contracts from BPMN models. This approach abstracts the BPMN flow control, making it independent of the underlying blockchain infrastructure, with only the BPMN task elements requiring coding. In subsequent research, we enhanced our approach by adding support for nested transactions and enabling a smart contract repair and/or upgrade. To empower Business Analysts (BAs) to generate smart contracts without relying on software developers, we tackled the challenge of generating smart contracts from BPMN models without assistance of a software developer. We exploit the Decision Model and Notation (DMN) standard to represent the decisions and the business logic of the BPMN task elements and amended our methodology for transformation of BPMN models into smart contracts to support also the generation script to represent the business logic represented by the DMN models. To support such transformation, we describe how the BA documents, using the BPMN elements, the flow of information along with the flow of execution. Thus, if the BA is successful in representing the blockchain application requirements using BPMN and DMN models, our methodology and the tool, called TABS, that we developed as a proof of concept, is used to generate the smart contracts directly from those models without developer assistance.

---

## Article 37
### Title@2025-05-28: GitGoodBench: A Novel Benchmark For Evaluating Agentic Performance On   Git
**Title**: GitGoodBench: A Novel Benchmark For Evaluating Agentic Performance On   Git | GitGoodBench: Ein neuartiger Benchmark für die Bewertung Agentischer Performance auf Git | GitGoodbunch:评估基特生物表现的新基准 [2505.22583v1](http://arxiv.org/abs/2505.22583v1)

**Authors**: Tobias Lindenbauer, Egor Bogomolov, Yaroslav Zharov

Benchmarks for Software Engineering (SE) AI agents, most notably SWE-bench, have catalyzed progress in programming capabilities of AI agents. However, they overlook critical developer workflows such as Version Control System (VCS) operations. To address this issue, we present GitGoodBench, a novel benchmark for evaluating AI agent performance on VCS tasks. GitGoodBench covers three core Git scenarios extracted from permissive open-source Python, Java, and Kotlin repositories. Our benchmark provides three datasets: a comprehensive evaluation suite (900 samples), a rapid prototyping version (120 samples), and a training corpus (17,469 samples). We establish baseline performance on the prototyping version of our benchmark using GPT-4o equipped with custom tools, achieving a 21.11% solve rate overall. We expect GitGoodBench to serve as a crucial stepping stone toward truly comprehensive SE agents that go beyond mere programming.

---

## Article 38
### Title@2025-05-28: LAMBDA: A Large Model Based Data Agent
**Title**: LAMBDA: A Large Model Based Data Agent | LAMBDA: Ein großer modellbasierter Datenagent | LAMBDA:一个大型模型数据代理 [2407.17535v3](http://arxiv.org/abs/2407.17535v3)

**Authors**: Maojun Sun, Ruijian Han, Binyan Jiang, Houduo Qi, Defeng Sun, Yancheng Yuan, Jian Huang

We introduce LArge Model Based Data Agent (LAMBDA), a novel open-source, code-free multi-agent data analysis system that leverages the power of large language models. LAMBDA is designed to address data analysis challenges in data-driven applications through innovatively designed data agents using natural language. At the core of LAMBDA are two key agent roles: the programmer and the inspector, which are engineered to work together seamlessly. Specifically, the programmer generates code based on the user's instructions and domain-specific knowledge, while the inspector debugs the code when necessary. To ensure robustness and handle adverse scenarios, LAMBDA features a user interface that allows direct user intervention. Moreover, LAMBDA can flexibly integrate external models and algorithms through our proposed Knowledge Integration Mechanism, catering to the needs of customized data analysis. LAMBDA has demonstrated strong performance on various data analysis tasks. It has the potential to enhance data analysis paradigms by seamlessly integrating human and artificial intelligence, making it more accessible, effective, and efficient for users from diverse backgrounds. The strong performance of LAMBDA in solving data analysis problems is demonstrated using real-world data examples. The code for LAMBDA is available at https://github.com/AMA-CMFAI/LAMBDA and videos of three case studies can be viewed at https://www.polyu.edu.hk/ama/cmfai/lambda.html.

---

## Article 39
### Title@2025-05-28: Advancing Expert Specialization for Better MoE
**Title**: Advancing Expert Specialization for Better MoE | Advancing Experten-Spezialisierung für bessere MoE | 推进专家专业专业促进改善教育部 [2505.22323v1](http://arxiv.org/abs/2505.22323v1)

**Authors**: Hongcan Guo, Haolang Lu, Guoshun Nan, Bolun Chu, Jialin Zhuang, Yuan Yang, Wenhao Che, Sicong Leng, Qimei Cui, Xudong Jiang

Mixture-of-Experts (MoE) models enable efficient scaling of large language models (LLMs) by activating only a subset of experts per input. However, we observe that the commonly used auxiliary load balancing loss often leads to expert overlap and overly uniform routing, which hinders expert specialization and degrades overall performance during post-training. To address this, we propose a simple yet effective solution that introduces two complementary objectives: (1) an orthogonality loss to encourage experts to process distinct types of tokens, and (2) a variance loss to encourage more discriminative routing decisions. Gradient-level analysis demonstrates that these objectives are compatible with the existing auxiliary loss and contribute to optimizing the training process. Experimental results over various model architectures and across multiple benchmarks show that our method significantly enhances expert specialization. Notably, our method improves classic MoE baselines with auxiliary loss by up to 23.79%, while also maintaining load balancing in downstream tasks, without any architectural modifications or additional components. We will release our code to contribute to the community.

---

## Article 40
### Title@2025-05-28: Evolution of repositories and privacy laws: commit activities in the   GDPR and CCPA era
**Title**: Evolution of repositories and privacy laws: commit activities in the   GDPR and CCPA era | Entwicklung von Repositorys und Datenschutzgesetzen: Aktivitäten in der DSGVO und CCPA-Ära verpflichten | 保管库和隐私法的演变演变:在GDPR和CCPA时代开展活动 [2505.22234v1](http://arxiv.org/abs/2505.22234v1)

**Authors**: Georgia M. Kapitsaki, Maria Papoutsoglou

Free and open source software has gained a lot of momentum in the industry and the research community. The latest advances in privacy legislation, including the EU General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA), have forced the community to pay special attention to users' data privacy. The main aim of this work is to examine software repositories that are acting on privacy laws. We have collected commit data from GitHub repositories in order to understand indications on main data privacy laws (GDPR, CCPA, CPRA, UK DPA) in the last years. Via an automated process, we analyzed 37,213 commits from 12,391 repositories since 2016, whereas 594 commits from the 70 most popular repositories of the dataset were manually analyzed. We observe that most commits were performed on the year the law came into effect and privacy relevant terms appear in the commit messages, whereas reference to specific data privacy user rights is scarce. The study showed that more educational activities on data privacy user rights are needed, as well as tools for privacy recommendations, whereas verifying actual compliance via source code execution is a useful direction for software engineering researchers.

---

## Article 41
### Title@2025-05-28: Thermal Modeling and Optimal Allocation of Avionics Safety-critical   Tasks on Heterogeneous MPSoCs
**Title**: Thermal Modeling and Optimal Allocation of Avionics Safety-critical   Tasks on Heterogeneous MPSoCs | Thermische Modellierung und optimale Allokation von Avionik Sicherheitskritische Aufgaben auf heterogenen MPSoCs | 热建模和最佳分配航空气象安全关键任务 [2505.22214v1](http://arxiv.org/abs/2505.22214v1)

**Authors**: Ondřej Benedikt, Michal Sojka, Přemysl Šůcha, Pavel Zaykov, Zdeněk Hanzálek

Multi-Processor Systems-on-Chip (MPSoC) can deliver high performance needed in many industrial domains, including aerospace. However, their high power consumption, combined with avionics safety standards, brings new thermal management challenges. This paper investigates techniques for offline thermal-aware allocation of periodic tasks on heterogeneous MPSoCs running at a fixed clock frequency, as required in avionics. The goal is to find the assignment of tasks to (i) cores and (ii) temporal isolation windows while minimizing the MPSoC temperature. To achieve that, we propose and analyze three power models, and integrate them within several novel optimization approaches based on heuristics, a black-box optimizer, and Integer Linear Programming (ILP). We perform the experimental evaluation on three popular MPSoC platforms (NXP i.MX8QM MEK, NXP i.MX8QM Ixora, NVIDIA TX2) and observe a difference of up to 5.5{\deg}C among the tested methods (corresponding to a 22% reduction w.r.t. the ambient temperature). We also show that our method, integrating the empirical power model with the ILP, outperforms the other methods on all tested platforms.

---

## Article 42
### Title@2025-05-28: Towards Conversational Development Environments: Using Theory-of-Mind   and Multi-Agent Architectures for Requirements Refinement
**Title**: Towards Conversational Development Environments: Using Theory-of-Mind   and Multi-Agent Architectures for Requirements Refinement | Hin zu konversatorischen Entwicklungsumgebungen: Verwendung von Theorie-von-Mind- und Multi-Agent-Architekturen für Anforderungen Verfeinerung | 走向对话型发展环境:利用理论和多机构架构改进要求 [2505.20973v2](http://arxiv.org/abs/2505.20973v2)

**Authors**: Keheliya Gallaba, Ali Arabat, Dayi Lin, Mohammed Sayagh, Ahmed E. Hassan

Foundation Models (FMs) have shown remarkable capabilities in various natural language tasks. However, their ability to accurately capture stakeholder requirements remains a significant challenge for using FMs for software development. This paper introduces a novel approach that leverages an FM-powered multi-agent system called AlignMind to address this issue. By having a cognitive architecture that enhances FMs with Theory-of-Mind capabilities, our approach considers the mental states and perspectives of software makers. This allows our solution to iteratively clarify the beliefs, desires, and intentions of stakeholders, translating these into a set of refined requirements and a corresponding actionable natural language workflow in the often-overlooked requirements refinement phase of software engineering, which is crucial after initial elicitation. Through a multifaceted evaluation covering 150 diverse use cases, we demonstrate that our approach can accurately capture the intents and requirements of stakeholders, articulating them as both specifications and a step-by-step plan of action. Our findings suggest that the potential for significant improvements in the software development process justifies these investments. Our work lays the groundwork for future innovation in building intent-first development environments, where software makers can seamlessly collaborate with AIs to create software that truly meets their needs.

---

## Article 43
### Title@2025-05-28: Towards Practical Defect-Focused Automated Code Review
**Title**: Towards Practical Defect-Focused Automated Code Review | Auf dem Weg zu einer praktischen fehlerorientierten automatisierten Code-Überprüfung | 走向实际失效-受污染的自动编码审查 [2505.17928v2](http://arxiv.org/abs/2505.17928v2)

**Authors**: Junyi Lu, Lili Jiang, Xiaojia Li, Jianbing Fang, Fengjun Zhang, Li Yang, Chun Zuo

The complexity of code reviews has driven efforts to automate review comments, but prior approaches oversimplify this task by treating it as snippet-level code-to-text generation and relying on text similarity metrics like BLEU for evaluation. These methods overlook repository context, real-world merge request evaluation, and defect detection, limiting their practicality. To address these issues, we explore the full automation pipeline within the online recommendation service of a company with nearly 400 million daily active users, analyzing industry-grade C++ codebases comprising hundreds of thousands of lines of code. We identify four key challenges: 1) capturing relevant context, 2) improving key bug inclusion (KBI), 3) reducing false alarm rates (FAR), and 4) integrating human workflows. To tackle these, we propose 1) code slicing algorithms for context extraction, 2) a multi-role LLM framework for KBI, 3) a filtering mechanism for FAR reduction, and 4) a novel prompt design for better human interaction. Our approach, validated on real-world merge requests from historical fault reports, achieves a 2x improvement over standard LLMs and a 10x gain over previous baselines. While the presented results focus on C++, the underlying framework design leverages language-agnostic principles (e.g., AST-based analysis), suggesting potential for broader applicability.

---

## Article 44
### Title@2025-05-28: SVA-ICL: Improving LLM-based Software Vulnerability Assessment via   In-Context Learning and Information Fusion
**Title**: SVA-ICL: Improving LLM-based Software Vulnerability Assessment via   In-Context Learning and Information Fusion | SVA-ICL: Verbesserung der LLM-basierten Software Vulnerability Assessment durch In-Context Learning und Information Fusion | SVA-ICL:通过文内学习和信息融合改进基于LLM的软件脆弱性评估 [2505.10008v2](http://arxiv.org/abs/2505.10008v2)

**Authors**: Chaoyang Gao, Xiang Chen, Guangbei Zhang

Context: Software vulnerability assessment (SVA) is critical for identifying, evaluating, and prioritizing security weaknesses in software applications. Objective: Despite the increasing application of large language models (LLMs) in various software engineering tasks, their effectiveness in SVA remains underexplored. Method: To address this gap, we introduce a novel approach SVA-ICL, which leverages in-context learning (ICL) to enhance LLM performance. Our approach involves the selection of high-quality demonstrations for ICL through information fusion, incorporating both source code and vulnerability descriptions. For source code, we consider semantic, lexical, and syntactic similarities, while for vulnerability descriptions, we focus on textual similarity. Based on the selected demonstrations, we construct context prompts and consider DeepSeek-V2 as the LLM for SVA-ICL. Results: We evaluate the effectiveness of SVA-ICL using a large-scale dataset comprising 12,071 C/C++ vulnerabilities. Experimental results demonstrate that SVA-ICL outperforms state-of-the-art SVA baselines in terms of Accuracy, F1-score, and MCC measures. Furthermore, ablation studies highlight the significance of component customization in SVA-ICL, such as the number of demonstrations, the demonstration ordering strategy, and the optimal fusion ratio of different modalities. Conclusion: Our findings suggest that leveraging ICL with information fusion can effectively improve the effectiveness of LLM-based SVA, warranting further research in this direction.

---

## Article 45
### Title@2025-05-28: Jailbreak Distillation: Renewable Safety Benchmarking
**Title**: Jailbreak Distillation: Renewable Safety Benchmarking | Jailbreak Destillation: Benchmarking für erneuerbare Sicherheit | 蒸馏:可再生能源安全基准 [2505.22037v1](http://arxiv.org/abs/2505.22037v1)

**Authors**: Jingyu Zhang, Ahmed Elgohary, Xiawei Wang, A S M Iftekhar, Ahmed Magooda, Benjamin Van Durme, Daniel Khashabi, Kyle Jackson

Large language models (LLMs) are rapidly deployed in critical applications, raising urgent needs for robust safety benchmarking. We propose Jailbreak Distillation (JBDistill), a novel benchmark construction framework that "distills" jailbreak attacks into high-quality and easily-updatable safety benchmarks. JBDistill utilizes a small set of development models and existing jailbreak attack algorithms to create a candidate prompt pool, then employs prompt selection algorithms to identify an effective subset of prompts as safety benchmarks. JBDistill addresses challenges in existing safety evaluation: the use of consistent evaluation prompts across models ensures fair comparisons and reproducibility. It requires minimal human effort to rerun the JBDistill pipeline and produce updated benchmarks, alleviating concerns on saturation and contamination. Extensive experiments demonstrate our benchmarks generalize robustly to 13 diverse evaluation models held out from benchmark construction, including proprietary, specialized, and newer-generation LLMs, significantly outperforming existing safety benchmarks in effectiveness while maintaining high separability and diversity. Our framework thus provides an effective, sustainable, and adaptable solution for streamlining safety evaluation.

---

## Article 46
### Title@2025-05-28: Securing the Software Package Supply Chain for Critical Systems
**Title**: Securing the Software Package Supply Chain for Critical Systems | Sicherung der Softwarepaket-Lieferkette für kritische Systeme | 保障关键系统软件包供应链 [2505.22023v1](http://arxiv.org/abs/2505.22023v1)

**Authors**: Ritwik Murali, Akash Ravi

Software systems have grown as an indispensable commodity used across various industries, and almost all essential services depend on them for effective operation. The software is no longer an independent or stand-alone piece of code written by a developer but rather a collection of packages designed by multiple developers across the globe. Ensuring the reliability and resilience of these systems is crucial since emerging threats target software supply chains, as demonstrated by the widespread SolarWinds hack in late 2020. These supply chains extend beyond patches and updates, involving distribution networks throughout the software lifecycle. Industries like smart grids, manufacturing, healthcare, and finance rely on interconnected software systems and their dependencies for effective functioning. To secure software modules and add-ons, robust distribution architectures are essential. The proposed chapter enhances the existing delivery frameworks by including a permissioned ledger with Proof of Authority consensus and multi-party signatures. The proposed system aims to prevent attacks while permitting every stakeholder to verify the same. Critical systems can interface with the secure pipeline without disrupting existing functionalities, thus preventing the cascading effect of an attack at any point in the supply chain.

---

## Article 47
### Title@2025-05-28: How Do Experts Make Sense of Integrated Process Models?
**Title**: How Do Experts Make Sense of Integrated Process Models? | Wie verstehen Experten integrierte Prozessmodelle? | 专家如何看待综合进程模式? [2505.20667v2](http://arxiv.org/abs/2505.20667v2)

**Authors**: Tianwa Chen, Barbara Weber, Graeme Shanks, Gianluca Demartini, Marta Indulska, Shazia Sadiq

A range of integrated modeling approaches have been developed to enable a holistic representation of business process logic together with all relevant business rules. These approaches address inherent problems with separate documentation of business process models and business rules. In this study, we explore how expert process workers make sense of the information provided through such integrated modeling approaches. To do so, we complement verbal protocol analysis with eye-tracking metrics to reveal nuanced user behaviours involved in the main phases of sensemaking, namely information foraging and information processing. By studying expert process workers engaged in tasks based on integrated modeling of business processes and rules, we provide insights that pave the way for a better understanding of sensemaking practices and improved development of business process and business rule integration approaches. Our research underscores the importance of offering personalized support mechanisms that increase the efficacy and efficiency of sensemaking practices for process knowledge workers.

---

## Article 48
### Title@2025-05-28: System-driven Cloud Architecture Design Support with Structured State   Management and Guided Decision Assistance
**Title**: System-driven Cloud Architecture Design Support with Structured State   Management and Guided Decision Assistance | Systemgesteuerte Cloud-Architektur-Design-Unterstützung mit strukturiertem Staatsmanagement und beratender Entscheidungshilfe | 提供结构化国家管理和指导决策援助的系统驱动云层结构设计支持 [2505.20701v2](http://arxiv.org/abs/2505.20701v2)

**Authors**: Ryosuke Kohita, Akira Kasuga

Cloud architecture design is a complex process requiring both technical expertise and architectural knowledge to develop solutions from frequently ambiguous requirements. We present CloudArchitectBuddy, a system-driven cloud architecture design support application with two key mechanisms: (1) structured state management that enhances design understanding through explicit representation of requirements and architectural decisions, and (2) guided decision assistance that facilitates design progress through proactive verification and requirement refinement. Our study with 16 industry practitioners showed that while our approach achieved comparable design quality to a chat interface, participants rated our system higher for usability and appreciated its ability to help understand architectural relationships and identify missing requirements. However, participants also expressed a need for user-initiated interactions where they could freely provide design instructions and engage in detailed discussions with LLMs. These results suggest that integrating a chat interface into our structured and guided workflow approach would create a more practical solution, balancing systematic design support with conversational flexibility for comprehensive cloud architecture development.

---

## Article 49
### Title@2025-05-28: Larger Is Not Always Better: Exploring Small Open-source Language Models   in Logging Statement Generation
**Title**: Larger Is Not Always Better: Exploring Small Open-source Language Models   in Logging Statement Generation | Größere ist nicht immer besser: Erforschen von kleinen Open-Source-Sprachenmodellen bei der Erstellung von Protokollierungsanweisungen | 大并非总是更好:探索记录报表生成中的小型开放源语言模式 [2505.16590v2](http://arxiv.org/abs/2505.16590v2)

**Authors**: Renyi Zhong, Yichen Li, Guangba Yu, Wenwei Gu, Jinxi Kuang, Yintong Huo, Michael R. Lyu

Developers use logging statements to create logs that document system behavior and aid in software maintenance. As such, high-quality logging is essential for effective maintenance; however, manual logging often leads to errors and inconsistency. Recent methods emphasize using large language models (LLMs) for automated logging statement generation, but these present privacy and resource issues, hindering their suitability for enterprise use. This paper presents the first large-scale empirical study evaluating small open-source language models (SOLMs) for automated logging statement generation. We evaluate four prominent SOLMs using various prompt strategies and parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA) and Retrieval-Augmented Generation (RAG). Our results show that fine-tuned SOLMs with LoRA and RAG prompts, particularly Qwen2.5-coder-14B, outperform existing tools and LLM baselines in predicting logging locations and generating high-quality statements, with robust generalization across diverse repositories. These findings highlight SOLMs as a privacy-preserving, efficient alternative for automated logging.

---

## Article 50
### Title@2025-05-28: Co-Saving: Resource Aware Multi-Agent Collaboration for Software   Development
**Title**: Co-Saving: Resource Aware Multi-Agent Collaboration for Software   Development | Co-Saving: Ressourcenschonende Multi-Agenten-Kollaboration für Software-Entwicklung | 共同节省:为开发软件进行有意识的资源、多机构协作 [2505.21898v1](http://arxiv.org/abs/2505.21898v1)

**Authors**: Rennai Qiu, Chen Qian, Ran Li, Yufan Dang, Weize Chen, Cheng Yang, Yingli Zhang, Ye Tian, Xuantang Xiong, Lei Han, Zhiyuan Liu, Maosong Sun

Recent advancements in Large Language Models (LLMs) and autonomous agents have demonstrated remarkable capabilities across various domains. However, standalone agents frequently encounter limitations when handling complex tasks that demand extensive interactions and substantial computational resources. Although Multi-Agent Systems (MAS) alleviate some of these limitations through collaborative mechanisms like task decomposition, iterative communication, and role specialization, they typically remain resource-unaware, incurring significant inefficiencies due to high token consumption and excessive execution time. To address these limitations, we propose a resource-aware multi-agent system -- Co-Saving (meaning that multiple agents collaboratively engage in resource-saving activities), which leverages experiential knowledge to enhance operational efficiency and solution quality. Our key innovation is the introduction of "shortcuts" -- instructional transitions learned from historically successful trajectories -- which allows to bypass redundant reasoning agents and expedite the collective problem-solving process. Experiments for software development tasks demonstrate significant advantages over existing methods. Specifically, compared to the state-of-the-art MAS ChatDev, our method achieves an average reduction of 50.85% in token usage, and improves the overall code quality by 10.06%.

---

## Article 51
### Title@2025-05-27: Augmenting Software Bills of Materials with Software Vulnerability   Description: A Preliminary Study on GitHub
**Title**: Augmenting Software Bills of Materials with Software Vulnerability   Description: A Preliminary Study on GitHub | Augmenting Software Bills of Materials with Software Vulnerability Beschreibung: Eine Vorstudie zu GitHub | 增加具有软件脆弱性说明的软件材料账单:关于GitHub的初步研究 [2503.13998v2](http://arxiv.org/abs/2503.13998v2)

**Authors**: Davide Fucci, Massimiliano Di Penta, Simone Romano, Giuseppe Scanniello

Software Bills of Material (SBOMs) are becoming a consolidated, often enforced by governmental regulations, way to describe software composition. However, based on recent studies, SBOMs suffer from limited support for their consumption and lack information beyond simple dependencies, especially regarding software vulnerabilities. This paper reports the results of a preliminary study in which we augmented SBOMs of 40 open-source projects with information about Common Vulnerabilities and Exposures (CVE) exposed by project dependencies. Our augmented SBOMs have been evaluated by submitting pull requests and by asking project owners to answer a survey. Although, in most cases, augmented SBOMs were not directly accepted because owners required a continuous SBOM update, the received feedback shows the usefulness of the suggested SBOM augmentation.

---

## Article 52
### Title@2025-05-27: Leveraging XP and CRISP-DM for Agile Data Science Projects
**Title**: Leveraging XP and CRISP-DM for Agile Data Science Projects | Nutzung von XP und CRISP-DM für agile Data Science Projekte | 利用XP和CRISP-DM为敏感数据科学项目发挥杠杆作用 [2505.21603v1](http://arxiv.org/abs/2505.21603v1)

**Authors**: Andre Massahiro Shimaoka, Renato Cordeiro Ferreira, Alfredo Goldman

This study explores the integration of eXtreme Programming (XP) and the Cross-Industry Standard Process for Data Mining (CRISP-DM) in agile Data Science projects. We conducted a case study at the e-commerce company Elo7 to answer the research question: How can the agility of the XP method be integrated with CRISP-DM in Data Science projects? Data was collected through interviews and questionnaires with a Data Science team consisting of data scientists, ML engineers, and data product managers. The results show that 86% of the team frequently or always applies CRISP-DM, while 71% adopt XP practices in their projects. Furthermore, the study demonstrates that it is possible to combine CRISP-DM with XP in Data Science projects, providing a structured and collaborative approach. Finally, the study generated improvement recommendations for the company.

---

## Article 53
### Title@2025-05-27: JITScope: Interactive Visualization of JIT Compiler IR Transformations
**Title**: JITScope: Interactive Visualization of JIT Compiler IR Transformations | JITScope: Interaktive Visualisierung von JIT Compiler IR-Transformationen | JIT编辑器 IR 转换的交互式视觉化 [2505.21599v1](http://arxiv.org/abs/2505.21599v1)

**Authors**: Kyra Dalbo, Yumna Ahmed, HeuiChan Lim

The complexity of modern Just-In-Time (JIT) compiler optimization poses significant challenges for developers seeking to understand and debug intermediate representation (IR) behavior. This work introduces JITScope, an interactive visualization framework that illustrates how IR nodes and instructions evolve across compilation phases. The system features a full-stack architecture: a Python-based backend transforms raw JSON-formatted IR data-representing an abstract model of the JIT compiler IR-into a normalized SQLite database; a controller layer serves processed CSV data; and a D3.js-powered frontend renders an interactive, phase-aware graph of IR node transformations. The design emphasizes modularity, traceability, and flexibility. Our roadmap explores intuitive visual representations of phase-level changes in IR node connectivity, values, and access patterns. Ultimately, JITScope lays a foundation for future tooling that enables visual exploration of IR evolution, including phase filtering, value tracking, and function-access mapping-offering a new lens into the behaviors and impacts of compiler optimizations.

---

## Article 54
### Title@2025-05-27: GUARD:Dual-Agent based Backdoor Defense on Chain-of-Thought in Neural   Code Generation
**Title**: GUARD:Dual-Agent based Backdoor Defense on Chain-of-Thought in Neural   Code Generation | GUARD:Dual-Agent-basierte Backdoor-Verteidigung auf Ketten-of-Thought in Neural Code Generation | GUARD: 在神经代码生成过程中寻求的连锁研究中,基于 " 以企业为基地 " 的后门防御 [2505.21425v1](http://arxiv.org/abs/2505.21425v1)

**Authors**: Naizhu Jin, Zhong Li, Tian Zhang, Qingkai Zeng

With the widespread application of large language models in code generation, recent studies demonstrate that employing additional Chain-of-Thought generation models can significantly enhance code generation performance by providing explicit reasoning steps. However, as external components, CoT models are particularly vulnerable to backdoor attacks, which existing defense mechanisms often fail to detect effectively. To address this challenge, we propose GUARD, a novel dual-agent defense framework specifically designed to counter CoT backdoor attacks in neural code generation. GUARD integrates two core components: GUARD-Judge, which identifies suspicious CoT steps and potential triggers through comprehensive analysis, and GUARD-Repair, which employs a retrieval-augmented generation approach to regenerate secure CoT steps for identified anomalies. Experimental results show that GUARD effectively mitigates attacks while maintaining generation quality, advancing secure code generation systems.

---

## Article 55
### Title@2025-05-27: A first look at ROS~2 applications written in asynchronous Rust
**Title**: A first look at ROS~2 applications written in asynchronous Rust | Ein erster Blick auf ROS~2 Anwendungen geschrieben in asynchronen Rust | 首先看一看ROS~2的申请,这些申请是以非同步鲁斯特书写的。 [2505.21323v1](http://arxiv.org/abs/2505.21323v1)

**Authors**: Martin Škoudlil, Michal Sojka, Zdeněk Hanzálek

The increasing popularity of the Rust programming language in building robotic applications using the Robot Operating System (ROS~2) raises questions about its real-time execution capabilities, particularly when employing asynchronous programming. Existing real-time scheduling and response-time analysis techniques for ROS~2 focus on applications written in C++ and do not address the unique execution models and challenges presented by Rust's asynchronous programming paradigm. In this paper, we analyze the execution model of R2R -- an asynchronous Rust ROS~2 bindings and various asynchronous Rust runtimes, comparing them with the execution model of C++ ROS~2 applications. We propose a structured approach for R2R applications aimed at deterministic real-time operation involving thread prioritization and callback-to-thread mapping schemes. Our experimental evaluation based on measuring end-to-end latencies of a synthetic application shows that the proposed approach is effective and outperforms other evaluated configurations. A more complex autonomous driving case study demonstrates its practical applicability. Overall, the experimental results indicate that our proposed structure achieves bounded response times for time-critical tasks. This paves the way for future work to adapt existing or develop new response-time analysis techniques for R2R applications using our structure.

---

## Article 56
### Title@2025-05-27: Computational Reproducibility of R Code Supplements on OSF
**Title**: Computational Reproducibility of R Code Supplements on OSF | Berechnung der Reproduzierbarkeit von R-Code-Ergänzungen auf OSF | OSF的R代码补编的计算可复制性 [2505.21590v1](http://arxiv.org/abs/2505.21590v1)

**Authors**: Lorraine Saju, Tobias Holtdirk, Meetkumar Pravinbhai Mangroliya, Arnim Bleier

Computational reproducibility is fundamental to scientific research, yet many published code supplements lack the necessary documentation to recreate their computational environments. While researchers increasingly share code alongside publications, the actual reproducibility of these materials remains poorly understood.   In this work, we assess the computational reproducibility of 296 R projects using the StatCodeSearch dataset. Of these, only 264 were still retrievable, and 98.8% lacked formal dependency descriptions required for successful execution. To address this, we developed an automated pipeline that reconstructs computational environments directly from project source code. Applying this pipeline, we executed the R scripts within custom Docker containers and found that 25.87% completed successfully without error.   We conducted a detailed analysis of execution failures, identifying reproducibility barriers such as undeclared dependencies, invalid file paths, and system-level issues. Our findings show that automated dependency inference and containerisation can support scalable verification of computational reproducibility and help identify practical obstacles to code reuse and transparency in scientific research.

---

## Article 57
### Title@2025-05-27: ColorGo: Directed Concolic Execution
**Title**: ColorGo: Directed Concolic Execution | ColorGo: Direkte konkolische Ausführung | 颜色 Go : 指向排列执行 [2505.21130v1](http://arxiv.org/abs/2505.21130v1)

**Authors**: Jia Li, Jiacheng Shen, Yuxin Su, Michael R. Lyu

Directed fuzzing is a critical technique in cybersecurity, targeting specific sections of a program. This approach is essential in various security-related domains such as crash reproduction, patch testing, and vulnerability detection. Despite its importance, current directed fuzzing methods exhibit a trade-off between efficiency and effectiveness. For instance, directed grey-box fuzzing, while efficient in generating fuzzing inputs, lacks sufficient precision. The low precision causes time wasted on executing code that cannot help reach the target site. Conversely, interpreter- or observer-based directed symbolic execution can produce high-quality inputs while incurring non-negligible runtime overhead. These limitations undermine the feasibility of directed fuzzers in real-world scenarios. To kill the birds of efficiency and effectiveness with one stone, in this paper, we involve compilation-based concolic execution into directed fuzzing and present ColorGo, achieving high scalability while preserving the high precision from symbolic execution. ColorGo is a new directed whitebox fuzzer that concretely executes the instrumented program with constraint-solving capability on generated input. It guides the exploration by \textit{incremental coloration}, including static reachability analysis and dynamic feasibility analysis. We evaluated ColorGo on diverse real-world programs and demonstrated that ColorGo outperforms AFLGo by up to \textbf{100x} in reaching target sites and reproducing target crashes.

---

## Article 58
### Title@2025-05-27: CXXCrafter: An LLM-Based Agent for Automated C/C++ Open Source Software   Building
**Title**: CXXCrafter: An LLM-Based Agent for Automated C/C++ Open Source Software   Building | CXXCrafter: Ein LLM-basierter Agent für automatisiertes C/C++ Open Source Software Building | CXXCFFF: 一个基于LLM的自动 C/C++ 开放源码软件大楼LLM代理 [2505.21069v1](http://arxiv.org/abs/2505.21069v1)

**Authors**: Zhengmin Yu, Yuan Zhang, Ming Wen, Yinan Nie, Wenhui Zhang, Min Yang

Project building is pivotal to support various program analysis tasks, such as generating intermediate rep- resentation code for static analysis and preparing binary code for vulnerability reproduction. However, automating the building process for C/C++ projects is a highly complex endeavor, involving tremendous technical challenges, such as intricate dependency management, diverse build systems, varied toolchains, and multifaceted error handling mechanisms. Consequently, building C/C++ projects often proves to be difficult in practice, hindering the progress of downstream applications. Unfortunately, research on facilitating the building of C/C++ projects remains to be inadequate. The emergence of Large Language Models (LLMs) offers promising solutions to automated software building. Trained on extensive corpora, LLMs can help unify diverse build systems through their comprehension capabilities and address complex errors by leveraging tacit knowledge storage. Moreover, LLM-based agents can be systematically designed to dynamically interact with the environment, effectively managing dynamic building issues. Motivated by these opportunities, we first conduct an empirical study to systematically analyze the current challenges in the C/C++ project building process. Particularly, we observe that most popular C/C++ projects encounter an average of five errors when relying solely on the default build systems. Based on our study, we develop an automated build system called CXXCrafter to specifically address the above-mentioned challenges, such as dependency resolution. Our evaluation on open-source software demonstrates that CXXCrafter achieves a success rate of 78% in project building. Specifically, among the Top100 dataset, 72 projects are built successfully by both CXXCrafter and manual efforts, 3 by CXXCrafter only, and 14 manually only. ...

---

## Article 59
### Title@2025-05-27: Thinking Before Running! Efficient Code Generation with Thorough   Exploration and Optimal Refinement
**Title**: Thinking Before Running! Efficient Code Generation with Thorough   Exploration and Optimal Refinement | Vor dem Laufen denken! Effiziente Codegenerierung mit gründlicher Exploration und optimaler Verfeinerung | 在运行前思考! 高效的代码生成, 彻底探索和优化精炼 [2502.17442v2](http://arxiv.org/abs/2502.17442v2)

**Authors**: Xiaoqing Zhang, Yuhan Liu, Flood Sung, Xiuying Chen, Shuo Shang, Rui Yan

Code generation is crucial in software engineering for automating the coding process efficiently. While test-time computation methods show promise, they suffer from high latency due to multiple computation rounds. To overcome this, we introduce \textbf{ThinkCoder}, a framework that combines thorough exploration with optimal refinement. The exploration phase diversifies the solution space by searching for potential solutions, followed by a refinement phase that enhances precision. This approach allows us to select the best solution through careful consideration before taking action, avoiding excessive trial and error. To further minimize test-time computation overhead, we introduce preference-driven optimization with Reinforced Self-Training (ReST), which uses exploration trajectories from ThinkCoder to guide LLM's evolution. This approach enhances LLM's exploration efficiency via preference learning, cutting costs while maintaining accuracy. ThinkCoder boosts the performance with a single LLM, excelling on benchmarks like HumanEval and MBPP. Compared to SOTA models, it improves Pass@1 by 3.0\% over MapCoder with just 6.4\% of the computation cost. Against AgentCoder, ThinkCoder achieves a 0.5\% higher Pass@1 after 2 rounds, outperforming AgentCoder's 5 rounds. Additionally, ReST with success trajectories enhances efficiency, allowing models like LLaMA2-7B to achieve competitive results using only 20\% of the computational resources. These results highlight the framework's effectiveness and scalability.

---

## Article 60
### Title@2025-05-27: Optimizing Case-Based Reasoning System for Functional Test Script   Generation with Large Language Models
**Title**: Optimizing Case-Based Reasoning System for Functional Test Script   Generation with Large Language Models | Optimierung des Case-Based-Reasoning-Systems für die Generierung funktionaler Testskripte mit großen Sprachmodellen | 为具有大语言模型的功能测试脚本生成优化基于个案的理由说明系统 [2503.20576v3](http://arxiv.org/abs/2503.20576v3)

**Authors**: Siyuan Guo, Huiwu Liu, Xiaolong Chen, Yuming Xie, Liang Zhang, Tao Han, Hechang Chen, Yi Chang, Jun Wang

In this work, we explore the potential of large language models (LLMs) for generating functional test scripts, which necessitates understanding the dynamically evolving code structure of the target software. To achieve this, we propose a case-based reasoning (CBR) system utilizing a 4R cycle (i.e., retrieve, reuse, revise, and retain), which maintains and leverages a case bank of test intent descriptions and corresponding test scripts to facilitate LLMs for test script generation. To improve user experience further, we introduce Re4, an optimization method for the CBR system, comprising reranking-based retrieval finetuning and reinforced reuse finetuning. Specifically, we first identify positive examples with high semantic and script similarity, providing reliable pseudo-labels for finetuning the retriever model without costly labeling. Then, we apply supervised finetuning, followed by a reinforcement learning finetuning stage, to align LLMs with our production scenarios, ensuring the faithful reuse of retrieved cases. Extensive experimental results on two product development units from Huawei Datacom demonstrate the superiority of the proposed CBR+Re4. Notably, we also show that the proposed Re4 method can help alleviate the repetitive generation issues with LLMs.

---

## Article 61
### Title@2025-05-27: RepoMaster: Autonomous Exploration and Understanding of GitHub   Repositories for Complex Task Solving
**Title**: RepoMaster: Autonomous Exploration and Understanding of GitHub   Repositories for Complex Task Solving | RepoMaster: Autonome Exploration und Verständnis von GitHub-Lagerstätten für komplexe Aufgabenlösung | RepoMaster:为复杂任务解决而自主探索和了解GitHub储存库 [2505.21577v1](http://arxiv.org/abs/2505.21577v1)

**Authors**: Huacan Wang, Ziyi Ni, Shuo Zhang, Shuo Lu, Sen Hu, Ziyang He, Chen Hu, Jiaye Lin, Yifu Guo, Yuntao Du, Pin Lyu

The ultimate goal of code agents is to solve complex tasks autonomously. Although large language models (LLMs) have made substantial progress in code generation, real-world tasks typically demand full-fledged code repositories rather than simple scripts. Building such repositories from scratch remains a major challenge. Fortunately, GitHub hosts a vast, evolving collection of open-source repositories, which developers frequently reuse as modular components for complex tasks. Yet, existing frameworks like OpenHands and SWE-Agent still struggle to effectively leverage these valuable resources. Relying solely on README files provides insufficient guidance, and deeper exploration reveals two core obstacles: overwhelming information and tangled dependencies of repositories, both constrained by the limited context windows of current LLMs. To tackle these issues, we propose RepoMaster, an autonomous agent framework designed to explore and reuse GitHub repositories for solving complex tasks. For efficient understanding, RepoMaster constructs function-call graphs, module-dependency graphs, and hierarchical code trees to identify essential components, providing only identified core elements to the LLMs rather than the entire repository. During autonomous execution, it progressively explores related components using our exploration tools and prunes information to optimize context usage. Evaluated on the adjusted MLE-bench, RepoMaster achieves a 110% relative boost in valid submissions over the strongest baseline OpenHands. On our newly released GitTaskBench, RepoMaster lifts the task-pass rate from 24.1% to 62.9% while reducing token usage by 95%. Our code and demonstration materials are publicly available at https://github.com/wanghuacan/RepoMaster.

---

## Article 62
### Title@2025-05-27: An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE   Tasks
**Title**: An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE   Tasks | Ein LLM-as-Judge Metric zur Überwindung der Lücke mit menschlicher Bewertung in SE-Aufgaben | 消除社会经济任务中与人的评价差距的法学硕士法官 [2505.20854v1](http://arxiv.org/abs/2505.20854v1)

**Authors**: Xin Zhou, Kisub Kim, Ting Zhang, Martin Weyssow, Luis F. Gomes, Guang Yang, David Lo

Large Language Models (LLMs) and other automated techniques have been increasingly used to support software developers by generating software artifacts such as code snippets, patches, and comments. However, accurately assessing the correctness of these generated artifacts remains a significant challenge. On one hand, human evaluation provides high accuracy but is labor-intensive and lacks scalability. On the other hand, other existing automatic evaluation metrics are scalable and require minimal human effort, but they often fail to accurately reflect the actual correctness of generated software artifacts.   In this paper, we present SWE-Judge, the first evaluation metric for LLM-as-Ensemble-Judge specifically designed to accurately assess the correctness of generated software artifacts. SWE-Judge first defines five distinct evaluation strategies, each implemented as an independent judge. A dynamic team selection mechanism then identifies the most appropriate subset of judges to produce a final correctness score through ensembling. We evaluate SWE-Judge across a diverse set of software engineering (SE) benchmarks, including CoNaLa, Card2Code, HumanEval-X, APPS, APR-Assess, and Summary-Assess. These benchmarks span three SE tasks: code generation, automated program repair, and code summarization. Experimental results demonstrate that SWE-Judge consistently achieves a higher correlation with human judgments, with improvements ranging from 5.9% to 183.8% over existing automatic metrics. Furthermore, SWE-Judge reaches agreement levels with human annotators that are comparable to inter-annotator agreement in code generation and program repair tasks. These findings underscore SWE-Judge's potential as a scalable and reliable alternative to human evaluation.

---

## Article 63
### Title@2025-05-27: Why do Machine Learning Notebooks Crash? An Empirical Study on Public   Python Jupyter Notebooks
**Title**: Why do Machine Learning Notebooks Crash? An Empirical Study on Public   Python Jupyter Notebooks | Warum zerfallen Machine-Learning-Notebooks? Eine empirische Studie über öffentliche Python-Jupyter-Notebooks | 为什么机器学习笔记本崩溃? [2411.16795v3](http://arxiv.org/abs/2411.16795v3)

**Authors**: Yiran Wang, Willem Meijer, José Antonio Hernández López, Ulf Nilsson, Dániel Varró

Jupyter notebooks have become central in data science, integrating code, text and output in a flexible environment. With the rise of machine learning (ML), notebooks are increasingly used for prototyping and data analysis. However, due to their dependence on complex ML libraries and the flexible notebook semantics that allow cells to be run in any order, notebooks are susceptible to software bugs that may lead to program crashes. This paper presents a comprehensive empirical study focusing on crashes in publicly available Python ML notebooks. We collect 64,031 notebooks containing 92,542 crashes from GitHub and Kaggle, and manually analyze a sample of 746 crashes across various aspects, including crash types and root causes. Our analysis identifies unique ML-specific crash types, such as tensor shape mismatches and dataset value errors that violate API constraints. Additionally, we highlight unique root causes tied to notebook semantics, including out-of-order execution and residual errors from previous cells, which have been largely overlooked in prior research. Furthermore, we identify the most error-prone ML libraries, and analyze crash distribution across ML pipeline stages. We find that over 40% of crashes stem from API misuse and notebook-specific issues. Crashes frequently occur when using ML libraries like TensorFlow/Keras and Torch. Additionally, over 70% of the crashes occur during data preparation, model training, and evaluation or prediction stages of the ML pipeline, while data visualization errors tend to be unique to ML notebooks.

---

## Article 64
### Title@2025-05-27: Can Agents Fix Agent Issues?
**Title**: Can Agents Fix Agent Issues? | Können Agenten Probleme mit Agenten beheben? | 特工能解决代理问题吗? [2505.20749v1](http://arxiv.org/abs/2505.20749v1)

**Authors**: Alfin Wijaya Rahardja, Junwei Liu, Weitong Chen, Zhenpeng Chen, Yiling Lou

LLM-based agent systems are emerging as a new software paradigm and have been widely adopted across diverse domains such as medicine, robotics, and programming. However, maintaining these systems requires substantial effort, as they are inevitably prone to bugs and continually evolve to meet changing external requirements. Therefore, automatically resolving agent issues (i.e., bug reports or feature requests) is a crucial and challenging task. While recent software engineering (SE) agents (e.g., SWE-agent) have shown promise in addressing issues in traditional software systems, it remains unclear how effectively they can resolve real-world issues in agent systems, which differ significantly from traditional software. To fill this gap, we first manually analyze 201 real-world agent issues and identify common categories of agent issues. We then spend 500 person-hours constructing AGENTISSUE-BENCH, a reproducible benchmark comprising 50 agent issue resolution tasks (each with an executable environment and failure-triggering tests). We further evaluate state-of-the-art SE agents on AGENTISSUE-BENCH and reveal their limited effectiveness (i.e., with only 3.33% - 12.67% resolution rates). These results underscore the unique challenges of maintaining agent systems compared to traditional software, highlighting the need for further research to develop advanced SE agents for resolving agent issues. Data and code are available at https://alfin06.github.io/AgentIssue-Bench-Leaderboard/#/ .

---

## Article 65
### Title@2025-05-27: Enhancing Code LLMs with Reinforcement Learning in Code Generation: A   Survey
**Title**: Enhancing Code LLMs with Reinforcement Learning in Code Generation: A   Survey | Verbesserung von Code LLMs mit Verstärkungslernen in der Codegenerierung: Eine Umfrage | 增强法典制定中强化学习的加强守则LLMS 代码生成:调查 [2412.20367v3](http://arxiv.org/abs/2412.20367v3)

**Authors**: Junqiao Wang, Zeng Zhang, Yangfan He, Zihao Zhang, Yuyang Song, Tianyu Shi, Yuchen Li, Hengyuan Xu, Kunyu Wu, Xin Yi, Zhongwei Wan, Xinhang Yuan, Kuan Lu, Menghao Huo, Guangwu Qian, Keqin Li, Qiuwu Chen, Lewei He

Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing large language models (LLMs) in code generation and optimization. This survey systematically reviews RL-driven techniques across the code development lifecycle, from compiler-level optimizations and resource allocation strategies to end-to-end code synthesis frameworks. We first examine classical and modern RL algorithms -- spanning policy gradients, actor-critic methods, human-feedback alignment, and preference-based optimization -- and their adaptations to the unique challenges of code generation, such as sparse and delayed rewards. Next, we analyze key benchmarks, datasets, and evaluation metrics that drive progress in RL-augmented Code LLMs. Finally, we identify open problems, including the need for richer feedback sources, support for low-level and domain-specific languages, and methods to reduce computational overhead. By consolidating current insights and outlining future directions, this work aims to guide researchers and practitioners in leveraging RL to produce more robust, efficient, and human-aligned code generation systems.

---

## Article 66
### Title@2025-05-27: SV-TrustEval-C: Evaluating Structure and Semantic Reasoning in Large   Language Models for Source Code Vulnerability Analysis
**Title**: SV-TrustEval-C: Evaluating Structure and Semantic Reasoning in Large   Language Models for Source Code Vulnerability Analysis | SV-TrustEval-C: Bewertung von Struktur und semantischer Vernunft in großen Sprachmodellen für die Analyse von Quellencode-Anfälligkeiten | SV-信任值-C:在源码脆弱性分析大语言模型中评估结构和语义理由 [2505.20630v1](http://arxiv.org/abs/2505.20630v1)

**Authors**: Yansong Li, Paula Branco, Alexander M. Hoole, Manish Marwah, Hari Manassery Koduvely, Guy-Vincent Jourdan, Stephan Jou

As Large Language Models (LLMs) evolve in understanding and generating code, accurately evaluating their reliability in analyzing source code vulnerabilities becomes increasingly vital. While studies have examined LLM capabilities in tasks like vulnerability detection and repair, they often overlook the importance of both structure and semantic reasoning crucial for trustworthy vulnerability analysis. To address this gap, we introduce SV-TrustEval-C, a benchmark designed to evaluate LLMs' abilities for vulnerability analysis of code written in the C programming language through two key dimensions: structure reasoning - assessing how models identify relationships between code elements under varying data and control flow complexities; and semantic reasoning - examining their logical consistency in scenarios where code is structurally and semantically perturbed. Our results show that current LLMs are far from satisfactory in understanding complex code relationships and that their vulnerability analyses rely more on pattern matching than on robust logical reasoning. These findings underscore the effectiveness of the SV-TrustEval-C benchmark and highlight critical areas for enhancing the reasoning capabilities and trustworthiness of LLMs in real-world vulnerability analysis tasks. Our initial benchmark dataset is publicly available.

---

## Article 67
### Title@2025-05-26: Smart Contract Vulnerabilities, Tools, and Benchmarks: An Updated   Systematic Literature Review
**Title**: Smart Contract Vulnerabilities, Tools, and Benchmarks: An Updated   Systematic Literature Review | Smart Contract Vulnerabilitys, Tools und Benchmarks: Ein aktualisierter systematischer Literaturbericht | 智能合同脆弱性、工具和基准:更新的系统文献审查 [2412.01719v2](http://arxiv.org/abs/2412.01719v2)

**Authors**: Gerardo Iuliano, Dario Di Nucci

Smart contracts are self-executing programs on blockchain platforms like Ethereum, which have revolutionized decentralized finance by enabling trustless transactions and the operation of decentralized applications. Despite their potential, the security of smart contracts remains a critical concern due to their immutability and transparency, which expose them to malicious actors. Numerous solutions for vulnerability detection have been proposed, but it is still unclear which one is the most effective. This paper presents a systematic literature review that explores vulnerabilities in Ethereum smart contracts, focusing on automated detection tools and benchmark evaluation. We reviewed 3,380 studies from five digital libraries and five major software engineering conferences, applying a structured selection process that resulted in 222 high-quality studies. The key results include a hierarchical taxonomy of 192 vulnerabilities grouped into 14 categories, a comprehensive list of 219 detection tools with corresponding functionalities, methods, and code transformation techniques, a mapping between our taxonomy and the list of tools, and a collection of 133 benchmarks used for tool evaluation. We conclude with a discussion about the insights into the current state of Ethereum smart contract security and directions for future research.

---

## Article 68
### Title@2025-05-26: Large Language Models for IT Automation Tasks: Are We There Yet?
**Title**: Large Language Models for IT Automation Tasks: Are We There Yet? | Große Sprachmodelle für IT-Automatisierungsaufgaben: Sind wir noch da? | 信息技术自动化任务大语言模型:我们是否还存在? [2505.20505v1](http://arxiv.org/abs/2505.20505v1)

**Authors**: Md Mahadi Hassan, John Salvador, Akond Rahman, Santu Karmaker

LLMs show promise in code generation, yet their effectiveness for IT automation tasks, particularly for tools like Ansible, remains understudied. Existing benchmarks rely primarily on synthetic tasks that fail to capture the needs of practitioners who use IT automation tools, such as Ansible. We present ITAB (IT Automation Task Benchmark), a benchmark of 126 diverse tasks (e.g., configuring servers, managing files) where each task accounts for state reconciliation: a property unique to IT automation tools. ITAB evaluates LLMs' ability to generate functional Ansible automation scripts via dynamic execution in controlled environments. We evaluate 14 open-source LLMs, none of which accomplish pass@10 at a rate beyond 12%. To explain these low scores, we analyze 1,411 execution failures across the evaluated LLMs and identify two main categories of prevalent semantic errors: failures in state reconciliation related reasoning (44.87% combined from variable (11.43%), host (11.84%), path(11.63%), and template (9.97%) issues) and deficiencies in module-specific execution knowledge (24.37% combined from Attribute and parameter (14.44%) and module (9.93%) errors). Our findings reveal key limitations in open-source LLMs' ability to track state changes and apply specialized module knowledge, indicating that reliable IT automation will require major advances in state reasoning and domain-specific execution understanding.

---

## Article 69
### Title@2025-05-26: Modeling and Analysis of the Landing Gear System with the Generalized   Contracts
**Title**: Modeling and Analysis of the Landing Gear System with the Generalized   Contracts | Modellierung und Analyse des Landing Gear Systems mit den Generalized Contracts | 通用合同着陆器系统的建模和分析 [2111.10426v3](http://arxiv.org/abs/2111.10426v3)

**Authors**: Abdelkader Khouass, christian attiogbé, mohamed messabihi

Nowadays, there are several complex systems in different sectors such as aviation, air traffic control ...etc. These systems do not have a precise perimeter, they are open and made of various specific components built with different languages and environments. The modeling, assembly and analysis of such open and complex heterogeneous systems are challenges in software engineering. This paper describes how the Minarets method decreases the difficulty of modeling, composition and analysis of the well known case study of the landing gear system. The method consists in: equipping individual components with generalized contracts that integrate various facets related to different concerns, composing these components according to their facets and verifying the resulting system with respect to the involved facets as well. The proposed method may be used or extended to cover more facets, and by strengthening assistance tool through proactive aspects in modeling, composing multi-facets contracts and finally the verification of the heterogeneous systems.

---

## Article 70
### Title@2025-05-26: SWE-rebench: An Automated Pipeline for Task Collection and   Decontaminated Evaluation of Software Engineering Agents
**Title**: SWE-rebench: An Automated Pipeline for Task Collection and   Decontaminated Evaluation of Software Engineering Agents | SWE-Rebench: Eine automatisierte Pipeline für die Task Collection und die dekontaminierte Evaluation von Software Engineering Agents | SWE-rebench:软件工程剂任务收集和除污评价自动管道 [2505.20411v1](http://arxiv.org/abs/2505.20411v1)

**Authors**: Ibragim Badertdinov, Alexander Golubev, Maksim Nekrashevich, Anton Shevtsov, Simon Karasik, Andrei Andriushchenko, Maria Trofimova, Daria Litvintseva, Boris Yangel

LLM-based agents have shown promising capabilities in a growing range of software engineering (SWE) tasks. However, advancing this field faces two critical challenges. First, high-quality training data is scarce, especially data that reflects real-world SWE scenarios, where agents must interact with development environments, execute code and adapt behavior based on the outcomes of their actions. Existing datasets are either limited to one-shot code generation or comprise small, manually curated collections of interactive tasks, lacking both scale and diversity. Second, the lack of fresh interactive SWE tasks affects evaluation of rapidly improving models, as static benchmarks quickly become outdated due to contamination issues. To address these limitations, we introduce a novel, automated, and scalable pipeline to continuously extract real-world interactive SWE tasks from diverse GitHub repositories. Using this pipeline, we construct SWE-rebench, a public dataset comprising over 21,000 interactive Python-based SWE tasks, suitable for reinforcement learning of SWE agents at scale. Additionally, we use continuous supply of fresh tasks collected using SWE-rebench methodology to build a contamination-free benchmark for agentic software engineering. We compare results of various LLMs on this benchmark to results on SWE-bench Verified and show that performance of some language models might be inflated due to contamination issues.

---

## Article 71
### Title@2025-05-26: GPUMC: A Stateless Model Checker for GPU Weak Memory Concurrency
**Title**: GPUMC: A Stateless Model Checker for GPU Weak Memory Concurrency | GPUMC: Ein staatenloser Modellprüfer für GPU-Schwachspeicherkonkurrenz | GPUMC: GPU 弱内存调制货币的无国籍模式检查器 [2505.20207v1](http://arxiv.org/abs/2505.20207v1)

**Authors**: Soham Chakraborty, S. Krishna, Andreas Pavlogiannis, Omkar Tuppe

GPU computing is embracing weak memory concurrency for performance improvement. However, compared to CPUs, modern GPUs provide more fine-grained concurrency features such as scopes, have additional properties like divergence, and thereby follow different weak memory consistency models. These features and properties make concurrent programming on GPUs more complex and error-prone. To this end, we present GPUMC, a stateless model checker to check the correctness of GPU shared-memory concurrent programs under scoped-RC11 weak memory concurrency model. GPUMC explores all possible executions in GPU programs to reveal various errors - races, barrier divergence, and assertion violations. In addition, GPUMC also automatically repairs these errors in the appropriate cases.   We evaluate GPUMC with benchmarks and real-life GPU programs. GPUMC is efficient both in time and memory in verifying large GPU programs where state-of-the-art tools are timed out. In addition, GPUMC identifies all known errors in these benchmarks compared to the state-of-the-art tools.

---

## Article 72
### Title@2025-05-26: Evaluating Large Language Models for Code Review
**Title**: Evaluating Large Language Models for Code Review | Bewertung großer Sprachmodelle für die Code-Überprüfung | 评价用于守则审查的大语言模式 [2505.20206v1](http://arxiv.org/abs/2505.20206v1)

**Authors**: Umut Cihan, Arda İçöz, Vahid Haratian, Eray Tüzün

Context: Code reviews are crucial for software quality. Recent AI advances have allowed large language models (LLMs) to review and fix code; now, there are tools that perform these reviews. However, their reliability and accuracy have not yet been systematically evaluated. Objective: This study compares different LLMs' performance in detecting code correctness and suggesting improvements. Method: We tested GPT4o and Gemini 2.0 Flash on 492 AI generated code blocks of varying correctness, along with 164 canonical code blocks from the HumanEval benchmark. To simulate the code review task objectively, we expected LLMs to assess code correctness and improve the code if needed. We ran experiments with different configurations and reported on the results. Results: With problem descriptions, GPT4o and Gemini 2.0 Flash correctly classified code correctness 68.50% and 63.89% of the time, respectively, and corrected the code 67.83% and 54.26% of the time for the 492 code blocks of varying correctness. Without problem descriptions, performance declined. The results for the 164 canonical code blocks differed, suggesting that performance depends on the type of code. Conclusion: LLM code reviews can help suggest improvements and assess correctness, but there is a risk of faulty outputs. We propose a process that involves humans, called the "Human in the loop LLM Code Review" to promote knowledge sharing while mitigating the risk of faulty outputs.

---

## Article 73
### Title@2025-05-26: Exposing Go's Hidden Bugs: A Novel Concolic Framework
**Title**: Exposing Go's Hidden Bugs: A Novel Concolic Framework | Aufdecken der versteckten Bugs von Go: Ein neuartiges konkolisches Rahmenwerk | 展露 Go 隐藏的臭虫: 新分类框架 [2505.20183v1](http://arxiv.org/abs/2505.20183v1)

**Authors**: Karolina Gorna, Nicolas Iooss, Yannick Seurin, Rida Khatoun

The widespread adoption of the Go programming language in infrastructure backends and blockchain projects has heightened the need for improved security measures. Established techniques such as unit testing, static analysis, and program fuzzing provide foundational protection mechanisms. Although symbolic execution tools have made significant contributions, opportunities remain to address the complexities of Go's runtime and concurrency model. In this work, we present Zorya, a novel methodology leveraging concrete and symbolic (concolic) execution to evaluate Go programs comprehensively. By systematically exploring execution paths to uncover vulnerabilities beyond conventional testing, symbolic execution offers distinct advantages, and coupling it with concrete execution mitigates the path explosion problem. Our solution employs Ghidra's P-Code as an intermediate representation (IR). This implementation detects runtime panics in the TinyGo compiler and supports both generic and custom invariants. Furthermore, P-Code's generic IR nature enables analysis of programs written in other languages such as C. Future enhancements may include intelligent classification of concolic execution logs to identify vulnerability patterns.

---

## Article 74
### Title@2025-05-26: An Empirical Study on Strong-Weak Model Collaboration for Repo-level   Code Generation
**Title**: An Empirical Study on Strong-Weak Model Collaboration for Repo-level   Code Generation | Eine empirische Studie zur stark schwachen Modellkooperation für die Codegenerierung auf Repo-Ebene | 关于回收层代码生成的 " 强弱 " 示范协作经验研究 [2505.20182v1](http://arxiv.org/abs/2505.20182v1)

**Authors**: Shubham Gandhi, Atharva Naik, Yiqing Xie, Carolyn Rose

We study cost-efficient collaboration between strong and weak language models for repository-level code generation, where the weak model handles simpler tasks at lower cost, and the most challenging tasks are delegated to the strong model. While many works propose architectures for this task, few analyze performance relative to cost. We evaluate a broad spectrum of collaboration strategies: context-based, pipeline-based, and dynamic, on GitHub issue resolution. Our most effective collaborative strategy achieves equivalent performance to the strong model while reducing the cost by 40%. Based on our findings, we offer actionable guidelines for choosing collaboration strategies under varying budget and performance constraints. Our results show that strong-weak collaboration substantially boosts the weak model's performance at a fraction of the cost, pipeline and context-based methods being most efficient. We release the code for our work at https://github.com/shubhamrgandhi/codegen-strong-weak-collab.

---

## Article 75
### Title@2025-05-26: Evaluating Software Plagiarism Detection in the Age of AI: Automated   Obfuscation and Lessons for Academic Integrity
**Title**: Evaluating Software Plagiarism Detection in the Age of AI: Automated   Obfuscation and Lessons for Academic Integrity | Bewertung von Software Plagiaterkennung im Zeitalter der KI: Automatisierte Verschleierung und Lehren für akademische Integrität | 评价AI时代软件高射率检测:学术廉正方面的自动读写和教益 [2505.20158v1](http://arxiv.org/abs/2505.20158v1)

**Authors**: Timur Sağlam, Larissa Schmid

Plagiarism in programming assignments is a persistent issue in computer science education, increasingly complicated by the emergence of automated obfuscation attacks. While software plagiarism detectors are widely used to identify suspicious similarities at scale and are resilient to simple obfuscation techniques, they are vulnerable to advanced obfuscation based on structural modification of program code that preserves the original program behavior. While different defense mechanisms have been proposed to increase resilience against these attacks, their current evaluation is limited to the scope of attacks used and lacks a comprehensive investigation regarding AI-based obfuscation. In this paper, we investigate the resilience of these defense mechanisms against a broad range of automated obfuscation attacks, including both algorithmic and AI-generated methods, and for a wide variety of real-world datasets. We evaluate the improvements of two defense mechanisms over the plagiarism detector JPlag across over four million pairwise program comparisons. Our results show significant improvements in detecting obfuscated plagiarism instances, and we observe an improved detection of AI-generated programs, even though the defense mechanisms are not designed for this use case. Based on our findings, we provide an in-depth discussion of their broader implications for academic integrity and the role of AI in education.

---

## Article 76
### Title@2025-05-26: The CodeInverter Suite: Control-Flow and Data-Mapping Augmented Binary   Decompilation with LLMs
**Title**: The CodeInverter Suite: Control-Flow and Data-Mapping Augmented Binary   Decompilation with LLMs | Die CodeInverter Suite: Control-Flow und Data-Mapping Augmented Binary Decompilation mit LLMs | 代码输入器套件:控制-光和数据-制表增强的二进制解析与LLMS [2503.07215v2](http://arxiv.org/abs/2503.07215v2)

**Authors**: Peipei Liu, Jian Sun, Rongkang Sun, Li Chen, Zhaoteng Yan, Peizheng Zhang, Dapeng Sun, Dawei Wang, Xiaoling Zhang, Dan Li

Binary decompilation plays a vital role in various cybersecurity and software engineering tasks. Recently, end-to-end decompilation methods powered by large language models (LLMs) have garnered significant attention due to their ability to generate highly readable source code with minimal human intervention. However, existing LLM-based approaches face several critical challenges, including limited capability in reconstructing code structure and logic, low accuracy in data recovery, concerns over data security and privacy, and high computational resource requirements. To address these issues, we develop the CodeInverter Suite, making three contributions: (1) the CodeInverter Workflow (CIW) is a novel prompt engineering workflow that incorporates control flow graphs (CFG) and explicit data mappings to improve LLM-based decompilation. (2) Using CIW on well-known source code datasets, we curate the CodeInverter Dataset (CID), a domain-specific dataset containing 8.69 million samples that contains CFGs and data mapping tables. (3) We train the CoderInverter Models (CIMs) on CID, generating two lightweight LLMs (with 1.3B and 6.7B parameters) intended for efficient inference in privacy-sensitive or resource-constrained environments. Extensive experiments on two benchmarks demonstrate that the CIW substantially enhances the performance of various LLMs across multiple metrics. Our CIM-6.7B can achieve state-of-the-art decompilation performance, outperforming existing LLMs even with over 100x more parameters in decompilation tasks, an average improvement of 11.03% in re-executability, 6.27% in edit similarity.

---

## Article 77
### Title@2025-05-26: StructEval: Benchmarking LLMs' Capabilities to Generate Structural   Outputs
**Title**: StructEval: Benchmarking LLMs' Capabilities to Generate Structural   Outputs | StructEval: Benchmarking der Kapazitäten von LLM zur Erzeugung struktureller Outputs | DructEval:将LLMs的能力与产生结构性产出挂钩 [2505.20139v1](http://arxiv.org/abs/2505.20139v1)

**Authors**: Jialin Yang, Dongfu Jiang, Lipeng He, Sherman Siu, Yuxuan Zhang, Disen Liao, Zhuofeng Li, Huaye Zeng, Yiming Jia, Haozhe Wang, Benjamin Schneider, Chi Ruan, Wentao Ma, Zhiheng Lyu, Yifei Wang, Yi Lu, Quy Duc Do, Ziyan Jiang, Ping Nie, Wenhu Chen

As Large Language Models (LLMs) become integral to software development workflows, their ability to generate structured outputs has become critically important. We introduce StructEval, a comprehensive benchmark for evaluating LLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and renderable (HTML, React, SVG) structured formats. Unlike prior benchmarks, StructEval systematically evaluates structural fidelity across diverse formats through two paradigms: 1) generation tasks, producing structured output from natural language prompts, and 2) conversion tasks, translating between structured formats. Our benchmark encompasses 18 formats and 44 types of task, with novel metrics for format adherence and structural correctness. Results reveal significant performance gaps, even state-of-the-art models like o1-mini achieve only 75.58 average score, with open-source alternatives lagging approximately 10 points behind. We find generation tasks more challenging than conversion tasks, and producing correct visual content more difficult than generating text-only structures.

---

## Article 78
### Title@2025-05-26: Engineering Trustworthy Machine-Learning Operations with Zero-Knowledge   Proofs
**Title**: Engineering Trustworthy Machine-Learning Operations with Zero-Knowledge   Proofs | Engineering Vertrauenswürdige Maschinen-Learning-Operationen mit Null-Wissens-Proofs | 具有零知识证明的工程可信赖的机械学习操作 [2505.20136v1](http://arxiv.org/abs/2505.20136v1)

**Authors**: Filippo Scaramuzza, Giovanni Quattrocchi, Damian A. Tamburri

As Artificial Intelligence (AI) systems, particularly those based on machine learning (ML), become integral to high-stakes applications, their probabilistic and opaque nature poses significant challenges to traditional verification and validation methods. These challenges are exacerbated in regulated sectors requiring tamper-proof, auditable evidence, as highlighted by apposite legal frameworks, e.g., the EU AI Act. Conversely, Zero-Knowledge Proofs (ZKPs) offer a cryptographic solution that enables provers to demonstrate, through verified computations, adherence to set requirements without revealing sensitive model details or data. Through a systematic survey of ZKP protocols, we identify five key properties (non-interactivity, transparent setup, standard representations, succinctness, and post-quantum security) critical for their application in AI validation and verification pipelines. Subsequently, we perform a follow-up systematic survey analyzing ZKP-enhanced ML applications across an adaptation of the Team Data Science Process (TDSP) model (Data & Preprocessing, Training & Offline Metrics, Inference, and Online Metrics), detailing verification objectives, ML models, and adopted protocols. Our findings indicate that current research on ZKP-Enhanced ML primarily focuses on inference verification, while the data preprocessing and training stages remain underexplored. Most notably, our analysis identifies a significant convergence within the research domain toward the development of a unified Zero-Knowledge Machine Learning Operations (ZKMLOps) framework. This emerging framework leverages ZKPs to provide robust cryptographic guarantees of correctness, integrity, and privacy, thereby promoting enhanced accountability, transparency, and compliance with Trustworthy AI principles.

---

## Article 79
### Title@2025-05-26: Grammars of Formal Uncertainty: When to Trust LLMs in Automated   Reasoning Tasks
**Title**: Grammars of Formal Uncertainty: When to Trust LLMs in Automated   Reasoning Tasks | Grammatik der formalen Unsicherheit: Wann man LLMs bei automatisierten Aufgaben zur Begründung vertraut | 正式不确定性的语法:在自动说明理由任务中何时信任LLMs [2505.20047v1](http://arxiv.org/abs/2505.20047v1)

**Authors**: Debargha Ganguly, Vikash Singh, Sreehari Sankar, Biyao Zhang, Xuecen Zhang, Srinivasan Iyengar, Xiaotian Han, Amit Sharma, Shivkumar Kalyanaraman, Vipin Chaudhary

Large language models (LLMs) show remarkable promise for democratizing automated reasoning by generating formal specifications. However, a fundamental tension exists: LLMs are probabilistic, while formal verification demands deterministic guarantees. This paper addresses this epistemological gap by comprehensively investigating failure modes and uncertainty quantification (UQ) in LLM-generated formal artifacts. Our systematic evaluation of five frontier LLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's domain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on factual ones), with known UQ techniques like the entropy of token probabilities failing to identify these errors. We introduce a probabilistic context-free grammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty taxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy for logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables selective verification, drastically reducing errors (14-100%) with minimal abstention, transforming LLM-driven formalization into a reliable engineering discipline.

---

## Article 80
### Title@2025-05-26: A Survey on the Safety and Security Threats of Computer-Using Agents:   JARVIS or Ultron?
**Title**: A Survey on the Safety and Security Threats of Computer-Using Agents:   JARVIS or Ultron? | Eine Umfrage über die Sicherheitsbedrohungen von Computer-Verwendern: JARVIS oder Ultron? | JARVIS还是ULTRON? 调查计算机用户的安全和安保威胁:JARVIS还是ULTRON? [2505.10924v2](http://arxiv.org/abs/2505.10924v2)

**Authors**: Ada Chen, Yongjiang Wu, Junyuan Zhang, Jingyu Xiao, Shu Yang, Jen-tse Huang, Kun Wang, Wenxuan Wang, Shuai Wang

Recently, AI-driven interactions with computing devices have advanced from basic prototype tools to sophisticated, LLM-based systems that emulate human-like operations in graphical user interfaces. We are now witnessing the emergence of \emph{Computer-Using Agents} (CUAs), capable of autonomously performing tasks such as navigating desktop applications, web pages, and mobile apps. However, as these agents grow in capability, they also introduce novel safety and security risks. Vulnerabilities in LLM-driven reasoning, with the added complexity of integrating multiple software components and multimodal inputs, further complicate the security landscape. In this paper, we present a systematization of knowledge on the safety and security threats of CUAs. We conduct a comprehensive literature review and distill our findings along four research objectives: \textit{\textbf{(i)}} define the CUA that suits safety analysis; \textit{\textbf{(ii)} } categorize current safety threats among CUAs; \textit{\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive strategies; \textit{\textbf{(iv)}} summarize prevailing benchmarks, datasets, and evaluation metrics used to assess the safety and performance of CUAs. Building on these insights, our work provides future researchers with a structured foundation for exploring unexplored vulnerabilities and offers practitioners actionable guidance in designing and deploying secure Computer-Using Agents.

---

## Article 81
### Title@2025-05-26: Ontology- and LLM-based Data Harmonization for Federated Learning in   Healthcare
**Title**: Ontology- and LLM-based Data Harmonization for Federated Learning in   Healthcare | Ontologie- und LLM-basierte Datenharmonisierung für das Federated Learning in Healthcare | 以本体学和LLM为基础的保健方面联邦学习数据统一 [2505.20020v1](http://arxiv.org/abs/2505.20020v1)

**Authors**: Natallia Kokash, Lei Wang, Thomas H. Gillespie, Adam Belloum, Paola Grosso, Sara Quinney, Lang Li, Bernard de Bono

The rise of electronic health records (EHRs) has unlocked new opportunities for medical research, but privacy regulations and data heterogeneity remain key barriers to large-scale machine learning. Federated learning (FL) enables collaborative modeling without sharing raw data, yet faces challenges in harmonizing diverse clinical datasets. This paper presents a two-step data alignment strategy integrating ontologies and large language models (LLMs) to support secure, privacy-preserving FL in healthcare, demonstrating its effectiveness in a real-world project involving semantic mapping of EHR data.

---

## Article 82
### Title@2025-05-26: Requirements Coverage-Guided Minimization for Natural Language Test   Cases
**Title**: Requirements Coverage-Guided Minimization for Natural Language Test   Cases | Anforderungen Abdeckungsgeführte Minimierung für natürliche Sprachtests | 以涵盖范围为指导的尽量减少自然语言测试案件 [2505.20004v1](http://arxiv.org/abs/2505.20004v1)

**Authors**: Rongqi Pan, Feifei Niu, Lionel C. Briand, Hanyang Hu

As software systems evolve, test suites tend to grow in size and often contain redundant test cases. Such redundancy increases testing effort, time, and cost. Test suite minimization (TSM) aims to eliminate such redundancy while preserving key properties such as requirement coverage and fault detection capability. In this paper, we propose RTM (Requirement coverage-guided Test suite Minimization), a novel TSM approach designed for requirement-based testing (validation), which can effectively reduce test suite redundancy while ensuring full requirement coverage and a high fault detection rate (FDR) under a fixed minimization budget. Based on common practice in critical systems where functional safety is important, we assume test cases are specified in natural language and traced to requirements before being implemented. RTM preprocesses test cases using three different preprocessing methods, and then converts them into vector representations using seven text embedding techniques. Similarity values between vectors are computed utilizing three distance functions. A Genetic Algorithm, whose population is initialized by coverage-preserving initialization strategies, is then employed to identify an optimized subset containing diverse test cases matching the set budget.   We evaluate RTM on an industrial automotive system dataset comprising $736$ system test cases and $54$ requirements. Experimental results show that RTM consistently outperforms baseline techniques in terms of FDR across different minimization budgets while maintaining full requirement coverage. Furthermore, we investigate the impact of test suite redundancy levels on the effectiveness of TSM, providing new insights into optimizing requirement-based test suites under practical constraints.

---

## Article 83
### Title@2025-05-26: The Invisible Hand: Unveiling Provider Bias in Large Language Models for   Code Generation
**Title**: The Invisible Hand: Unveiling Provider Bias in Large Language Models for   Code Generation | Die unsichtbare Hand: Enthüllen von Provider-Bias in großen Sprachmodellen für die Codegenerierung | 无形手:守则生成大语言模式中的 " 无形手 " : " 不可忽视的提供者 " 。 [2501.07849v2](http://arxiv.org/abs/2501.07849v2)

**Authors**: Xiaoyu Zhang, Juan Zhai, Shiqing Ma, Qingshuang Bao, Weipeng Jiang, Qian Wang, Chao Shen, Yang Liu

Large Language Models (LLMs) have emerged as the new recommendation engines, surpassing traditional methods in both capability and scope, particularly in code generation. In this paper, we reveal a novel provider bias in LLMs: without explicit directives, these models show systematic preferences for services from specific providers in their recommendations (e.g., favoring Google Cloud over Microsoft Azure). To systematically investigate this bias, we develop an automated pipeline to construct the dataset, incorporating 6 distinct coding task categories and 30 real-world application scenarios. Leveraging this dataset, we conduct the first comprehensive empirical study of provider bias in LLM code generation across seven state-of-the-art LLMs, utilizing approximately 500 million tokens (equivalent to $5,000+ in computational costs). Our findings reveal that LLMs exhibit significant provider preferences, predominantly favoring services from Google and Amazon, and can autonomously modify input code to incorporate their preferred providers without users' requests. Such a bias holds far-reaching implications for market dynamics and societal equilibrium, potentially contributing to digital monopolies. It may also deceive users and violate their expectations, leading to various consequences. We call on the academic community to recognize this emerging issue and develop effective evaluation and mitigation methods to uphold AI security and fairness.

---

## Article 84
### Title@2025-05-26: Systems of Twinned Systems: A Systematic Literature Review
**Title**: Systems of Twinned Systems: A Systematic Literature Review | Systeme von Zwillingssystemen: Ein Systematischer Literaturbericht | 结对系统系统系统:系统文献审查 [2505.19916v1](http://arxiv.org/abs/2505.19916v1)

**Authors**: Feyi Adesanya, Kanan Castro Silva, Valdemar V. Graciano Neto, Istvan David

Modern systems exhibit unprecedented complexity due to their increased scale, interconnectedness, and the heterogeneity of their digital and physical components. In response to scaling challenges, the system-of-systems (SoS) paradigm proposes flexible aggregations of subsystems into a larger whole, while maintaining the independence of subsystems to various degrees. In response to the cyber-physical convergence, the digital twin (DT) paradigm proposes a tight coupling between digital and physical components through computational reflection and precise control. As these two paradigms address distinct parts of the overall challenge, combining the two promises more comprehensive methods to engineer what we call systems of twinned systems (SoTS). The noticeably growing body of knowledge on SoTS calls for a review of the state of the art. In this work, we report on our systematic literature survey of SoTS. We screened over 2500 potential studies, of which we included 80 and investigated them in detail. To converge SoS and DT, we derive a classification framework for SoTS that is backward compatible with the currently accepted theories of SoS and DT.

---

## Article 85
### Title@2025-05-26: Deconstructing Obfuscation: A four-dimensional framework for evaluating   Large Language Models assembly code deobfuscation capabilities
**Title**: Deconstructing Obfuscation: A four-dimensional framework for evaluating   Large Language Models assembly code deobfuscation capabilities | Dekonstruieren von Obfuscation: Ein vierdimensionaler Rahmen für die Auswertung von Großsprachenmodellen Assembly Code Deobfuscation Fähigkeiten | 解构腐蚀:四维框架,用于评价大语言模型组装编码脱腐能力 [2505.19887v1](http://arxiv.org/abs/2505.19887v1)

**Authors**: Anton Tkachenko, Dmitrij Suskevic, Benjamin Adolphi

Large language models (LLMs) have shown promise in software engineering, yet their effectiveness for binary analysis remains unexplored. We present the first comprehensive evaluation of commercial LLMs for assembly code deobfuscation. Testing seven state-of-the-art models against four obfuscation scenarios (bogus control flow, instruction substitution, control flow flattening, and their combination), we found striking performance variations--from autonomous deobfuscation to complete failure. We propose a theoretical framework based on four dimensions: Reasoning Depth, Pattern Recognition, Noise Filtering, and Context Integration, explaining these variations. Our analysis identifies five error patterns: predicate misinterpretation, structural mapping errors, control flow misinterpretation, arithmetic transformation errors, and constant propagation errors, revealing fundamental limitations in LLM code processing.We establish a three-tier resistance model: bogus control flow (low resistance), control flow flattening (moderate resistance), and instruction substitution/combined techniques (high resistance). Universal failure against combined techniques demonstrates that sophisticated obfuscation remains effective against advanced LLMs. Our findings suggest a human-AI collaboration paradigm where LLMs reduce expertise barriers for certain reverse engineering tasks while requiring human guidance for complex deobfuscation. This work provides a foundation for evaluating emerging capabilities and developing resistant obfuscation techniques.x deobfuscation. This work provides a foundation for evaluating emerging capabilities and developing resistant obfuscation techniques.

---

## Article 86
### Title@2025-05-26: SecVulEval: Benchmarking LLMs for Real-World C/C++ Vulnerability   Detection
**Title**: SecVulEval: Benchmarking LLMs for Real-World C/C++ Vulnerability   Detection | SecVulEval: Benchmarking LLMs für real-World C/C++ Sicherheitserkennung | SecVulEval:确定真实世界C/C+++脆弱性检测LLMs基准 [2505.19828v1](http://arxiv.org/abs/2505.19828v1)

**Authors**: Md Basim Uddin Ahmed, Nima Shiri Harzevili, Jiho Shin, Hung Viet Pham, Song Wang

Large Language Models (LLMs) have shown promise in software engineering tasks, but evaluating their effectiveness in vulnerability detection is challenging due to the lack of high-quality datasets. Most existing datasets are limited to function-level labels, ignoring finer-grained vulnerability patterns and crucial contextual information. Also, poor data quality such as mislabeling, inconsistent annotations, and duplicates can lead to inflated performance and weak generalization. Moreover, by including only the functions, these datasets miss broader program context, like data/control dependencies and interprocedural interactions, that are essential for accurately understanding real-world security flaws. Without this context, detection models are evaluated under unrealistic assumptions.   To address these limitations, this paper introduces SecVulEval, a benchmark designed to support fine-grained evaluation of LLMs and other detection methods with rich contextual information. SecVulEval focuses on real-world C/C++ vulnerabilities at the statement level. This granularity enables more precise evaluation of a model's ability to localize vulnerabilities, beyond simple binary classification at the function level. By incorporating rich contextual information, SecVulEval sets a new standard for vulnerability detection benchmarks in realistic scenarios. This benchmark includes 25,440 function samples covering 5,867 unique CVEs in C/C++ projects from 1999 to 2024. We evaluated the SOTA LLMs with a multi-agent-based approach. The evaluation on our dataset shows that the models are still far from accurately predicting vulnerable statements in a given function. The best-performing Claude-3.7-Sonnet model achieves 23.83% F1-score for detecting vulnerable statements with correct reasoning. Finally, we analyze the LLM outputs and provide insights into their behavior in vulnerability detection for C/C++.

---

## Article 87
### Title@2025-05-26: A Python workflow definition for computational materials design
**Title**: A Python workflow definition for computational materials design | Eine Python-Workflow-Definition für die Berechnung von Materialien | 计算材料设计中的 Python 工作流程定义 [2505.20366v1](http://arxiv.org/abs/2505.20366v1)

**Authors**: Jan Janssen, Janine George, Julian Geiger, Marnik Bercx, Xing Wang, Christina Ertural, Joerg Schaarschmidt, Alex M. Ganose, Giovanni Pizzi, Tilmann Hickel, Joerg Neugebauer

Numerous Workflow Management Systems (WfMS) have been developed in the field of computational materials science with different workflow formats, hindering interoperability and reproducibility of workflows in the field. To address this challenge, we introduce here the Python Workflow Definition (PWD) as a workflow exchange format to share workflows between Python-based WfMS, currently AiiDA, jobflow, and pyiron. This development is motivated by the similarity of these three Python-based WfMS, that represent the different workflow steps and data transferred between them as nodes and edges in a graph. With the PWD, we aim at fostering the interoperability and reproducibility between the different WfMS in the context of Findable, Accessible, Interoperable, Reusable (FAIR) workflows. To separate the scientific from the technical complexity, the PWD consists of three components: (1) a conda environment that specifies the software dependencies, (2) a Python module that contains the Python functions represented as nodes in the workflow graph, and (3) a workflow graph stored in the JavaScript Object Notation (JSON). The first version of the PWD supports directed acyclic graph (DAG)-based workflows. Thus, any DAG-based workflow defined in one of the three WfMS can be exported to the PWD and afterwards imported from the PWD to one of the other WfMS. After the import, the input parameters of the workflow can be adjusted and computing resources can be assigned to the workflow, before it is executed with the selected WfMS. This import from and export to the PWD is enabled by the PWD Python library that implements the PWD in AiiDA, jobflow, and pyiron.

---

## Article 88
### Title@2025-05-26: CIDRe: A Reference-Free Multi-Aspect Criterion for Code Comment Quality   Measurement
**Title**: CIDRe: A Reference-Free Multi-Aspect Criterion for Code Comment Quality   Measurement | CIDRe: Ein referenzfreies Multi-Aspekt-Kriterium für die Qualitätsmessung von Code Comment | CIDRe: 守则评论质量衡量的无参考性、无参考性、多特征的多标准标准 [2505.19757v1](http://arxiv.org/abs/2505.19757v1)

**Authors**: Maria Dziuba, Valentin Malykh

Effective generation of structured code comments requires robust quality metrics for dataset curation, yet existing approaches (SIDE, MIDQ, STASIS) suffer from limited code-comment analysis. We propose CIDRe, a language-agnostic reference-free quality criterion combining four synergistic aspects: (1) relevance (code-comment semantic alignment), (2) informativeness (functional coverage), (3) completeness (presence of all structure sections), and (4) description length (detail sufficiency). We validate our criterion on a manually annotated dataset. Experiments demonstrate CIDRe's superiority over existing metrics, achieving improvement in cross-entropy evaluation. When applied to filter comments, the models finetuned on CIDRe-filtered data show statistically significant quality gains in GPT-4o-mini assessments.

---

## Article 89
### Title@2025-05-26: RDFGraphGen: An RDF Graph Generator based on SHACL Shapes
**Title**: RDFGraphGen: An RDF Graph Generator based on SHACL Shapes | RDFGraphGen: Ein RDF Graph Generator auf Basis von SHACL Shapes | RDFGraphGen:基于 SHACL 形状的 RDF 图形生成器 [2407.17941v2](http://arxiv.org/abs/2407.17941v2)

**Authors**: Milos Jovanovik, Marija Vecovska, Maxime Jakubowski, Katja Hose

Developing and testing modern RDF-based applications often requires access to RDF datasets with certain characteristics. Unfortunately, it is very difficult to publicly find domain-specific knowledge graphs that conform to a particular set of characteristics. Hence, in this paper we propose RDFGraphGen, an open-source RDF graph generator that uses characteristics provided in the form of SHACL (Shapes Constraint Language) shapes to generate synthetic RDF graphs. RDFGraphGen is domain-agnostic, with configurable graph structure, value constraints, and distributions. It also comes with a number of predefined values for popular schema.org classes and properties, for more realistic graphs. Our results show that RDFGraphGen is scalable and can generate small, medium, and large RDF graphs in any domain.

---

## Article 90
### Title@2025-05-26: SETBVE: Quality-Diversity Driven Exploration of Software Boundary   Behaviors
**Title**: SETBVE: Quality-Diversity Driven Exploration of Software Boundary   Behaviors | SETBVE: Qualität-Diversität treibt die Erforschung von Software-Grenzverhalten an | SETVE: 软件边界行为的质量-多样性驱动探索 [2505.19736v1](http://arxiv.org/abs/2505.19736v1)

**Authors**: Sabinakhon Akbarova, Felix Dobslaw, Francisco Gomes de Oliveira Neto, Robert Feldt

Software systems exhibit distinct behaviors based on input characteristics, and failures often occur at the boundaries between input domains. Traditional Boundary Value Analysis (BVA) relies on manual heuristics, while automated Boundary Value Exploration (BVE) methods typically optimize a single quality metric, risking a narrow and incomplete survey of boundary behaviors. We introduce SETBVE, a customizable, modular framework for automated black-box BVE that leverages Quality-Diversity (QD) optimization to systematically uncover and refine a broader spectrum of boundaries. SETBVE maintains an archive of boundary pairs organized by input- and output-based behavioral descriptors. It steers exploration toward underrepresented regions while preserving high-quality boundary pairs and applies local search to refine candidate boundaries. In experiments with ten integer-based functions, SETBVE outperforms the baseline in diversity, boosting archive coverage by 37 to 82 percentage points. A qualitative analysis reveals that SETBVE identifies boundary candidates the baseline misses. While the baseline method typically plateaus in both diversity and quality after 30 seconds, SETBVE continues to improve in 600-second runs, demonstrating better scalability. Even the simplest SETBVE configurations perform well in identifying diverse boundary behaviors. Our findings indicate that balancing quality with behavioral diversity can help identify more software edge-case behaviors than quality-focused approaches.

---

## Article 91
### Title@2025-05-26: Large Language Models in Code Co-generation for Safe Autonomous Vehicles
**Title**: Large Language Models in Code Co-generation for Safe Autonomous Vehicles | Große Sprachmodelle in der Kogeneration Code für sichere autonome Fahrzeuge | 安全自治车辆代码共同生成大语言模式 [2505.19658v1](http://arxiv.org/abs/2505.19658v1)

**Authors**: Ali Nouri, Beatriz Cabrero-Daniel, Zhennan Fei, Krishna Ronanki, Håkan Sivencrona, Christian Berger

Software engineers in various industrial domains are already using Large Language Models (LLMs) to accelerate the process of implementing parts of software systems. When considering its potential use for ADAS or AD systems in the automotive context, there is a need to systematically assess this new setup: LLMs entail a well-documented set of risks for safety-related systems' development due to their stochastic nature. To reduce the effort for code reviewers to evaluate LLM-generated code, we propose an evaluation pipeline to conduct sanity-checks on the generated code. We compare the performance of six state-of-the-art LLMs (CodeLlama, CodeGemma, DeepSeek-r1, DeepSeek-Coders, Mistral, and GPT-4) on four safety-related programming tasks. Additionally, we qualitatively analyse the most frequent faults generated by these LLMs, creating a failure-mode catalogue to support human reviewers. Finally, the limitations and capabilities of LLMs in code generation, and the use of the proposed pipeline in the existing process, are discussed.

---

## Article 92
### Title@2025-05-26: Software Engineering for Self-Adaptive Robotics: A Research Agenda
**Title**: Software Engineering for Self-Adaptive Robotics: A Research Agenda | Software-Engineering für selbstadaptive Robotik: Eine Forschungsagenda | 自我适应机器人学软件工程:研究议程 [2505.19629v1](http://arxiv.org/abs/2505.19629v1)

**Authors**: Shaukat Ali, Ana Cavalcanti, Cláudio Ângelo Gonçalves Gomes, Peter Gorm Larsen, Hassan Sartaj, Anastasios Tefas, Jim Woodcock, Houxiang Zhang

Self-adaptive robotic systems are designed to operate autonomously in dynamic and uncertain environments, requiring robust mechanisms to monitor, analyse, and adapt their behaviour in real-time. Unlike traditional robotic software, which follows predefined logic, self-adaptive robots leverage artificial intelligence, machine learning, and model-driven engineering to continuously adjust to changing operational conditions while ensuring reliability, safety, and performance. This paper presents a research agenda for software engineering in self-adaptive robotics, addressing critical challenges across two key dimensions: (1) the development phase, including requirements engineering, software design, co-simulation, and testing methodologies tailored to adaptive robotic systems, and (2) key enabling technologies, such as digital twins, model-driven engineering, and AI-driven adaptation, which facilitate runtime monitoring, fault detection, and automated decision-making. We discuss open research challenges, including verifying adaptive behaviours under uncertainty, balancing trade-offs between adaptability, performance, and safety, and integrating self-adaptation frameworks like MAPE-K. By providing a structured roadmap, this work aims to advance the software engineering foundations for self-adaptive robotic systems, ensuring they remain trustworthy, efficient, and capable of handling real-world complexities.

---

## Article 93
### Title@2025-05-26: Search-Based Software Engineering in the Landscape of AI Foundation   Models
**Title**: Search-Based Software Engineering in the Landscape of AI Foundation   Models | Search-Based Software Engineering in der Landschaft der AI-Stiftung Modelle | AI基金会模型景观中的搜索软件工程 [2505.19625v1](http://arxiv.org/abs/2505.19625v1)

**Authors**: Hassan Sartaj, Shaukat Ali

Search-based software engineering (SBSE), at the intersection of artificial intelligence (AI) and software engineering, has been an active area of research for about 25 years. It has been applied to solve numerous problems across the entire software engineering lifecycle and has demonstrated its versatility in multiple domains. With the recent advancements in AI, particularly the emergence of foundation models (FMs), the evolution of SBSE alongside FMs remains undetermined. In this window of opportunity, we propose a research roadmap that articulates the current landscape of SBSE in relation to foundation models (FMs), highlights open challenges, and outlines potential research directions for advancing SBSE through its interplay with FMs. This roadmap aims to establish a forward-thinking and innovative perspective for the future of SBSE in the era of FMs.

---

## Article 94
### Title@2025-05-26: LEGO-Compiler: Enhancing Neural Compilation Through Translation   Composability
**Title**: LEGO-Compiler: Enhancing Neural Compilation Through Translation   Composability | LEGO-Compiler: Neurale Kompilierung durch Übersetzungskompatibilität verbessern | LEGO-Compuper:通过翻译集成加强神经汇编 [2505.20356v1](http://arxiv.org/abs/2505.20356v1)

**Authors**: Shuoming Zhang, Jiacheng Zhao, Chunwei Xia, Zheng Wang, Yunji Chen, Xiaobing Feng, Huimin Cui

Large language models (LLMs) have the potential to revolutionize how we design and implement compilers and code translation tools. However, existing LLMs struggle to handle long and complex programs. We introduce LEGO-Compiler, a novel neural compilation system that leverages LLMs to translate high-level languages into assembly code. Our approach centers on three key innovations: LEGO translation, which decomposes the input program into manageable blocks; breaking down the complex compilation process into smaller, simpler verifiable steps by organizing it as a verifiable LLM workflow by external tests; and a feedback mechanism for self-correction. Supported by formal proofs of translation composability, LEGO-Compiler demonstrates high accuracy on multiple datasets, including over 99% on ExeBench and 97.9% on industrial-grade AnsiBench. Additionally, LEGO-Compiler has also acheived near one order-of-magnitude improvement on compilable code size scalability. This work opens new avenues for applying LLMs to system-level tasks, complementing traditional compiler technologies.

---

## Article 95
### Title@2025-05-26: CODE-DITING: A Reasoning-Based Metric for Functional Alignment in Code   Evaluation
**Title**: CODE-DITING: A Reasoning-Based Metric for Functional Alignment in Code   Evaluation | CODE-DITING: Ein auf Vernunft basierendes Metric für die funktionelle Ausrichtung in der Code-Evaluation | 代码化:守则评价中功能一致性的基于理由的计量标准 [2505.19502v1](http://arxiv.org/abs/2505.19502v1)

**Authors**: Guang Yang, Yu Zhou, Xiang Chen, Wei Zheng, Xing Hu, Xin Zhou, David Lo, Taolue Chen

Trustworthy evaluation methods for code snippets play a crucial role in neural code generation. Traditional methods, which either rely on reference solutions or require executable test cases, have inherent limitation in flexibility and scalability. The recent LLM-as-Judge methodology offers a promising alternative by directly evaluating functional consistency between the problem description and the generated code. To systematically understand the landscape of these LLM-as-Judge methods, we conduct a comprehensive empirical study across three diverse datasets. Our investigation reveals the pros and cons of two categories of LLM-as-Judge methods: the methods based on general foundation models can achieve good performance but require complex prompts and lack explainability, while the methods based on reasoning foundation models provide better explainability with simpler prompts but demand substantial computational resources due to their large parameter sizes. To address these limitations, we propose CODE-DITING, a novel code evaluation method that balances accuracy, efficiency and explainability. We develop a data distillation framework that effectively transfers reasoning capabilities from DeepSeek-R1671B to our CODE-DITING 1.5B and 7B models, significantly enhancing evaluation explainability and reducing the computational cost. With the majority vote strategy in the inference process, CODE-DITING 1.5B outperforms all models with the same magnitude of parameters and achieves performance which would normally exhibit in a model with 5 times of parameter scale. CODE-DITING 7B surpasses GPT-4o and DeepSeek-V3 671B, even though it only uses 1% of the parameter volume of these large models. Further experiments show that CODEDITING is robust to preference leakage and can serve as a promising alternative for code evaluation.

---

## Article 96
### Title@2025-05-26: Benchmarking and Enhancing LLM Agents in Localizing Linux Kernel Bugs
**Title**: Benchmarking and Enhancing LLM Agents in Localizing Linux Kernel Bugs | Benchmarking und Verbesserung von LLM-Agenten bei der Lokalisierung von Linux-Kernel-Fehlern | 确定和加强Linux内核虫本地化的Linux Kernel 虫的基准和加强LLM代理物 [2505.19489v1](http://arxiv.org/abs/2505.19489v1)

**Authors**: Zhenhao Zhou, Zhuochen Huang, Yike He, Chong Wang, Jiajun Wang, Yijian Wu, Xin Peng, Yiling Lou

The Linux kernel is a critical system, serving as the foundation for numerous systems. Bugs in the Linux kernel can cause serious consequences, affecting billions of users. Fault localization (FL), which aims at identifying the buggy code elements in software, plays an essential role in software quality assurance. While recent LLM agents have achieved promising accuracy in FL on recent benchmarks like SWE-bench, it remains unclear how well these methods perform in the Linux kernel, where FL is much more challenging due to the large-scale code base, limited observability, and diverse impact factors. In this paper, we introduce LinuxFLBench, a FL benchmark constructed from real-world Linux kernel bugs. We conduct an empirical study to assess the performance of state-of-the-art LLM agents on the Linux kernel. Our initial results reveal that existing agents struggle with this task, achieving a best top-1 accuracy of only 41.6% at file level. To address this challenge, we propose LinuxFL$^+$, an enhancement framework designed to improve FL effectiveness of LLM agents for the Linux kernel. LinuxFL$^+$ substantially improves the FL accuracy of all studied agents (e.g., 7.2% - 11.2% accuracy increase) with minimal costs. Data and code are available at https://github.com/FudanSELab/LinuxFLBench.

---

## Article 97
### Title@2025-05-26: Regulating Algorithmic Management: A Multi-Stakeholder Study of   Challenges in Aligning Software and the Law for Workplace Scheduling
**Title**: Regulating Algorithmic Management: A Multi-Stakeholder Study of   Challenges in Aligning Software and the Law for Workplace Scheduling | Regulierung des algorithmischen Managements: Eine Multi-Stakeholder-Studie über Herausforderungen bei der Ausrichtung von Software und dem Gesetz für die Arbeitsplanung | 规范工资管理:多方利益攸关方研究软件和工作场所时间安排法在调整软件和工作场所时间安排法方面面临的挑战 [2505.02329v2](http://arxiv.org/abs/2505.02329v2)

**Authors**: Jonathan Lynn, Rachel Y. Kim, Sicun Gao, Daniel Schneider, Sachin S. Pandya, Min Kyung Lee

Algorithmic management (AM)'s impact on worker well-being has led to calls for regulation. However, little is known about the effectiveness and challenges in real-world AM regulation across the regulatory process -- rule operationalization, software use, and enforcement. Our multi-stakeholder study addresses this gap within workplace scheduling, one of the few AM domains with implemented regulations. We interviewed 38 stakeholders across the regulatory process: regulators, defense attorneys, worker advocates, managers, and workers. Our findings suggest that the efficacy of AM regulation is influenced by: (i) institutional constraints that challenge efforts to encode law into AM software, (ii) on-the-ground use of AM software that shapes its ability to facilitate compliance, (iii) mismatches between software and regulatory contexts that hinder enforcement, and (iv) unique concerns that software introduces when used to regulate AM. These findings underscore the importance of a sociotechnical approach to AM regulation, which considers organizational and collaborative contexts alongside the inherent attributes of software. We offer future research directions and implications for technology policy and design.

---

## Article 98
### Title@2025-05-26: Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications   of Agentic AI
**Title**: Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications   of Agentic AI | Vibe Coding vs. Agentic Coding: Grundlagen und praktische Implikationen von Agentic AI | Vibe 编码与 Agentic 编码:Agent AI 的基本要素和实际影响 [2505.19443v1](http://arxiv.org/abs/2505.19443v1)

**Authors**: Ranjan Sapkota, Konstantinos I. Roumeliotis, Manoj Karkee

This review presents a comprehensive analysis of two emerging paradigms in AI-assisted software development: vibe coding and agentic coding. While both leverage large language models (LLMs), they differ fundamentally in autonomy, architectural design, and the role of the developer. Vibe coding emphasizes intuitive, human-in-the-loop interaction through prompt-based, conversational workflows that support ideation, experimentation, and creative exploration. In contrast, agentic coding enables autonomous software development through goal-driven agents capable of planning, executing, testing, and iterating tasks with minimal human intervention. We propose a detailed taxonomy spanning conceptual foundations, execution models, feedback loops, safety mechanisms, debugging strategies, and real-world tool ecosystems. Through comparative workflow analysis and 20 detailed use cases, we illustrate how vibe systems thrive in early-stage prototyping and education, while agentic systems excel in enterprise-grade automation, codebase refactoring, and CI/CD integration. We further examine emerging trends in hybrid architectures, where natural language interfaces are coupled with autonomous execution pipelines. Finally, we articulate a future roadmap for agentic AI, outlining the infrastructure needed for trustworthy, explainable, and collaborative systems. Our findings suggest that successful AI software engineering will rely not on choosing one paradigm, but on harmonizing their strengths within a unified, human-centered development lifecycle.

---

## Article 99
### Title@2025-05-26: Simple and Effective Baselines for Code Summarisation Evaluation
**Title**: Simple and Effective Baselines for Code Summarisation Evaluation | Einfache und effektive Grundlagen für die Code-Summarisation-Bewertung | 用于代码摘要评价的简单有效基线 [2505.19392v1](http://arxiv.org/abs/2505.19392v1)

**Authors**: Jade Robinson, Jonathan K. Kummerfeld

Code documentation is useful, but writing it is time-consuming. Different techniques for generating code summaries have emerged, but comparing them is difficult because human evaluation is expensive and automatic metrics are unreliable. In this paper, we introduce a simple new baseline in which we ask an LLM to give an overall score to a summary. Unlike n-gram and embedding-based baselines, our approach is able to consider the code when giving a score. This allows us to also make a variant that does not consider the reference summary at all, which could be used for other tasks, e.g., to evaluate the quality of documentation in code bases. We find that our method is as good or better than prior metrics, though we recommend using it in conjunction with embedding-based methods to avoid the risk of LLM-specific bias.

---

## Article 100
### Title@2025-05-25: Architectures of Error: A Philosophical Inquiry into AI and Human Code   Generation
**Title**: Architectures of Error: A Philosophical Inquiry into AI and Human Code   Generation | Architekturen des Irrtums: Eine philosophische Untersuchung der KI- und menschlichen Code-Generation | 错误结构结构:对大赦国际和人类代码生成的哲学调查 [2505.19353v1](http://arxiv.org/abs/2505.19353v1)

**Authors**: Camilo Chacón Sartori

With the rise of generative AI (GenAI), Large Language Models are increasingly employed for code generation, becoming active co-authors alongside human programmers. Focusing specifically on this application domain, this paper articulates distinct ``Architectures of Error'' to ground an epistemic distinction between human and machine code generation. Examined through their shared vulnerability to error, this distinction reveals fundamentally different causal origins: human-cognitive versus artificial-stochastic. To develop this framework and substantiate the distinction, the analysis draws critically upon Dennett's mechanistic functionalism and Rescher's methodological pragmatism. I argue that a systematic differentiation of these error profiles raises critical philosophical questions concerning semantic coherence, security robustness, epistemic limits, and control mechanisms in human-AI collaborative software development. The paper also utilizes Floridi's levels of abstraction to provide a nuanced understanding of how these error dimensions interact and may evolve with technological advancements. This analysis aims to offer philosophers a structured framework for understanding GenAI's unique epistemological challenges, shaped by these architectural foundations, while also providing software engineers a basis for more critically informed engagement.

---

## Article 101
### Title@2025-05-25: Retrieval-Augmented Generation for Service Discovery: Chunking   Strategies and Benchmarking
**Title**: Retrieval-Augmented Generation for Service Discovery: Chunking   Strategies and Benchmarking | Retrieval-Augmented Generation for Service Discovery: Chunking Strategien und Benchmarking | 服务发现回收-启动型服务生成:启动战略和基准制定 [2505.19310v1](http://arxiv.org/abs/2505.19310v1)

**Authors**: Robin D. Pesl, Jerin G. Mathew, Massimo Mecella, Marco Aiello

Integrating multiple (sub-)systems is essential to create advanced Information Systems. Difficulties mainly arise when integrating dynamic environments, e.g., the integration at design time of not yet existing services. This has been traditionally addressed using a registry that provides the API documentation of the endpoints. Large Language Models have shown to be capable of automatically creating system integrations (e.g., as service composition) based on this documentation but require concise input due to input oken limitations, especially regarding comprehensive API descriptions. Currently, it is unknown how best to preprocess these API descriptions. In the present work, we (i) analyze the usage of Retrieval Augmented Generation for endpoint discovery and the chunking, i.e., preprocessing, of state-of-practice OpenAPIs to reduce the input oken length while preserving the most relevant information. To further reduce the input token length for the composition prompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that only receives a summary of the most relevant endpoints nd retrieves specification details on demand. We evaluate RAG for endpoint discovery using (iii) a proposed novel service discovery benchmark SOCBench-D representing a general setting across numerous domains and the real-world RestBench enchmark, first, for the different chunking possibilities and parameters measuring the endpoint retrieval accuracy. Then, we assess the Discovery Agent using the same test data set. The prototype shows how to successfully employ RAG for endpoint discovery to reduce the token count. Our experiments show that endpoint-based approaches outperform naive chunking methods for preprocessing. Relying on an agent significantly improves precision while being prone to decrease recall, disclosing the need for further reasoning capabilities.

---

## Article 102
### Title@2025-05-25: VerifyThisBench: Generating Code, Specifications, and Proofs All at Once
**Title**: VerifyThisBench: Generating Code, Specifications, and Proofs All at Once | VerifyThisBench: Code, Spezifikationen und Beweise auf einmal generieren | 校验时间: 生成代码、规格和证明 [2505.19271v1](http://arxiv.org/abs/2505.19271v1)

**Authors**: Xun Deng, Sicheng Zhong, Andreas Veneris, Fan Long, Xujie Si

Large language models (LLMs) have demonstrated remarkable progress in code generation, but many existing benchmarks are approaching saturation and offer little guarantee on the trustworthiness of the generated programs, offering limited insight into deeper reasoning capabilities. We introduce VerifyThisBench, a new benchmark designed to evaluate LLMs on end-to-end program verification tasks that require interpreting natural language problem descriptions, formulating formal specifications, generating code, and constructing correctness proofs. Our evaluation reveals that even state-of-the-art (SOTA) models, such as o3-mini, achieve a pass rate of less than 4%, with many outputs failing to compile. To reduce task complexity, we further propose VerifyThisBenchXS, a variant in which partial implementations or proofs are provided. We systematically assess SOTA models on both benchmarks, uncovering key strengths and limitations in their formal reasoning and verification capabilities.

---

## Article 103
### Title@2025-05-25: CLEVER: A Curated Benchmark for Formally Verified Code Generation
**Title**: CLEVER: A Curated Benchmark for Formally Verified Code Generation | CLEVER: Ein kuratierter Benchmark für die formal verifizierte Codegenerierung | 正式核实的代码生成基准 [2505.13938v3](http://arxiv.org/abs/2505.13938v3)

**Authors**: Amitayush Thakur, Jasper Lee, George Tsoukalas, Meghana Sistla, Matthew Zhao, Stefan Zetzsche, Greg Durrett, Yisong Yue, Swarat Chaudhuri

We introduce ${\rm C{\small LEVER}}$, a high-quality, curated benchmark of 161 problems for end-to-end verified code generation in Lean. Each problem consists of (1) the task of generating a specification that matches a held-out ground-truth specification, and (2) the task of generating a Lean implementation that provably satisfies this specification. Unlike prior benchmarks, ${\rm C{\small LEVER}}$ avoids test-case supervision, LLM-generated annotations, and specifications that leak implementation logic or allow vacuous solutions. All outputs are verified post-hoc using Lean's type checker to ensure machine-checkable correctness. We use ${\rm C{\small LEVER}}$ to evaluate several few-shot and agentic approaches based on state-of-the-art language models. These methods all struggle to achieve full verification, establishing it as a challenging frontier benchmark for program synthesis and formal reasoning. Our benchmark can be found on GitHub(https://github.com/trishullab/clever) as well as HuggingFace(https://huggingface.co/datasets/amitayusht/clever). All our evaluation code is also available online(https://github.com/trishullab/clever-prover).

---

## Article 104
### Title@2025-05-25: An Empirical Study of Vulnerability Handling Times in CPython
**Title**: An Empirical Study of Vulnerability Handling Times in CPython | Eine empirische Studie über die Zeiten des Umgangs mit Gefährlichkeit in CPython | CPython 脆弱性处理时间经验研究 [2411.00447v2](http://arxiv.org/abs/2411.00447v2)

**Authors**: Jukka Ruohonen

The paper examines the handling times of software vulnerabilities in CPython, the reference implementation and interpreter for the today's likely most popular programming language, Python. The background comes from the so-called vulnerability life cycle analysis, the literature on bug fixing times, and the recent research on security of Python software. Based on regression analysis, the associated vulnerability fixing times can be explained very well merely by knowing who have reported the vulnerabilities. Severity, proof-of-concept code, commits made to a version control system, comments posted on a bug tracker, and references to other sources do not explain the vulnerability fixing times. With these results, the paper contributes to the recent effort to better understand security of the Python ecosystem.

---

## Article 105
### Title@2025-05-25: An Initial Exploration of Fine-tuning Small Language Models for Smart   Contract Reentrancy Vulnerability Detection
**Title**: An Initial Exploration of Fine-tuning Small Language Models for Smart   Contract Reentrancy Vulnerability Detection | Eine erste Erkundung von Feinsteuerungs-Kleinsprachenmodellen für intelligente Vertragsrepentrancy Sicherheitserkennung | 初步探索智能合同留置率易变性探测智能合同微调小型语言模型 [2505.19059v1](http://arxiv.org/abs/2505.19059v1)

**Authors**: Ignacio Mariano Andreozzi Pofcher, Joshua Ellul

Large Language Models (LLMs) are being used more and more for various coding tasks, including to help coders identify bugs and are a promising avenue to support coders in various tasks including vulnerability detection -- particularly given the flexibility of such generative AI models and tools. Yet for many tasks it may not be suitable to use LLMs, for which it may be more suitable to use smaller language models that can fit and easily execute and train on a developer's computer. In this paper we explore and evaluate whether smaller language models can be fine-tuned to achieve reasonable results for a niche area: vulnerability detection -- specifically focusing on detecting the reentrancy bug in Solidity smart contracts.

---

## Article 106
### Title@2025-05-25: AIGCodeSet: A New Annotated Dataset for AI Generated Code Detection
**Title**: AIGCodeSet: A New Annotated Dataset for AI Generated Code Detection | AIGCodeSet: Ein neuer kommentierter Datensatz für KI Generated Code Detection | AIGCodeSet:AI 生成代码探测新附加说明数据集 [2412.16594v3](http://arxiv.org/abs/2412.16594v3)

**Authors**: Basak Demirok, Mucahid Kutlu

While large language models provide significant convenience for software development, they can lead to ethical issues in job interviews and student assignments. Therefore, determining whether a piece of code is written by a human or generated by an artificial intelligence (AI) model is a critical issue. In this study, we present AIGCodeSet, which consists of 2.828 AI-generated and 4.755 human-written Python codes, created using CodeLlama 34B, Codestral 22B, and Gemini 1.5 Flash. In addition, we share the results of our experiments conducted with baseline detection methods. Our experiments show that a Bayesian classifier outperforms the other models.

---

## Article 107
### Title@2025-05-25: On-Demand Scenario Generation for Testing Automated Driving Systems
**Title**: On-Demand Scenario Generation for Testing Automated Driving Systems | On-Demand-Szenario-Generierung für die Prüfung automatisierter Fahrsysteme | 自动驾驶系统测试的 " 现场需求 " 情景生成 [2505.14053v2](http://arxiv.org/abs/2505.14053v2)

**Authors**: Songyang Yan, Xiaodong Zhang, Kunkun Hao, Haojie Xin, Yonggang Luo, Jucheng Yang, Ming Fan, Chao Yang, Jun Sun, Zijiang Yang

The safety and reliability of Automated Driving Systems (ADS) are paramount, necessitating rigorous testing methodologies to uncover potential failures before deployment. Traditional testing approaches often prioritize either natural scenario sampling or safety-critical scenario generation, resulting in overly simplistic or unrealistic hazardous tests. In practice, the demand for natural scenarios (e.g., when evaluating the ADS's reliability in real-world conditions), critical scenarios (e.g., when evaluating safety in critical situations), or somewhere in between (e.g., when testing the ADS in regions with less civilized drivers) varies depending on the testing objectives. To address this issue, we propose the On-demand Scenario Generation (OSG) Framework, which generates diverse scenarios with varying risk levels. Achieving the goal of OSG is challenging due to the complexity of quantifying the criticalness and naturalness stemming from intricate vehicle-environment interactions, as well as the need to maintain scenario diversity across various risk levels. OSG learns from real-world traffic datasets and employs a Risk Intensity Regulator to quantitatively control the risk level. It also leverages an improved heuristic search method to ensure scenario diversity. We evaluate OSG on the Carla simulators using various ADSs. We verify OSG's ability to generate scenarios with different risk levels and demonstrate its necessity by comparing accident types across risk levels. With the help of OSG, we are now able to systematically and objectively compare the performance of different ADSs based on different risk levels.

---

## Article 108
### Title@2025-05-25: Automated Trustworthiness Oracle Generation for Machine Learning Text   Classifiers
**Title**: Automated Trustworthiness Oracle Generation for Machine Learning Text   Classifiers | Automatisierte Vertrauenswürdigkeit Oracle Generation für Machine Learning Text Klassifikatoren | 机械学习文字分类的自动可信赖性甲骨文生成 [2410.22663v4](http://arxiv.org/abs/2410.22663v4)

**Authors**: Lam Nguyen Tung, Steven Cho, Xiaoning Du, Neelofar Neelofar, Valerio Terragni, Stefano Ruberto, Aldeida Aleti

Machine learning (ML) for text classification has been widely used in various domains. These applications can significantly impact ethics, economics, and human behavior, raising serious concerns about trusting ML decisions. Studies indicate that conventional metrics are insufficient to build human trust in ML models. These models often learn spurious correlations and predict based on them. In the real world, their performance can deteriorate significantly. To avoid this, a common practice is to test whether predictions are reasonable based on valid patterns in the data. Along with this, a challenge known as the trustworthiness oracle problem has been introduced. Due to the lack of automated trustworthiness oracles, the assessment requires manual validation of the decision process disclosed by explanation methods. However, this is time-consuming, error-prone, and unscalable.   We propose TOKI, the first automated trustworthiness oracle generation method for text classifiers. TOKI automatically checks whether the words contributing the most to a prediction are semantically related to the predicted class. Specifically, we leverage ML explanations to extract the decision-contributing words and measure their semantic relatedness with the class based on word embeddings. We also introduce a novel adversarial attack method that targets trustworthiness vulnerabilities identified by TOKI. To evaluate their alignment with human judgement, experiments are conducted. We compare TOKI with a naive baseline based solely on model confidence and TOKI-guided adversarial attack method with A2T, a SOTA adversarial attack method. Results show that relying on prediction uncertainty cannot effectively distinguish between trustworthy and untrustworthy predictions, TOKI achieves 142% higher accuracy than the naive baseline, and TOKI-guided attack method is more effective with fewer perturbations than A2T.

---

## Article 109
### Title@2025-05-25: Co-PatcheR: Collaborative Software Patching with Component(s)-specific   Small Reasoning Models
**Title**: Co-PatcheR: Collaborative Software Patching with Component(s)-specific   Small Reasoning Models | Co-PatcheR: Kollaborative Software-Patching mit Komponenten-spezifischen Small-Reasoning-Modellen | 共同配给R:与特定组成部分的小型理由模型合作的软件补补补 [2505.18955v1](http://arxiv.org/abs/2505.18955v1)

**Authors**: Yuheng Tang, Hongwei Li, Kaijie Zhu, Michael Yang, Yangruibo Ding, Wenbo Guo

Motivated by the success of general-purpose large language models (LLMs) in software patching, recent works started to train specialized patching models. Most works trained one model to handle the end-to-end patching pipeline (including issue localization, patch generation, and patch validation). However, it is hard for a small model to handle all tasks, as different sub-tasks have different workflows and require different expertise. As such, by using a 70 billion model, SOTA methods can only reach up to 41% resolved rate on SWE-bench-Verified. Motivated by the collaborative nature, we propose Co-PatcheR, the first collaborative patching system with small and specialized reasoning models for individual components. Our key technique novelties are the specific task designs and training recipes. First, we train a model for localization and patch generation. Our localization pinpoints the suspicious lines through a two-step procedure, and our generation combines patch generation and critique. We then propose a hybrid patch validation that includes two models for crafting issue-reproducing test cases with and without assertions and judging patch correctness, followed by a majority vote-based patch selection. Through extensive evaluation, we show that Co-PatcheR achieves 46% resolved rate on SWE-bench-Verified with only 3 x 14B models. This makes Co-PatcheR the best patcher with specialized models, requiring the least training resources and the smallest models. We conduct a comprehensive ablation study to validate our recipes, as well as our choice of training data number, model size, and testing-phase scaling strategy.

---

## Article 110
### Title@2025-05-24: From Output to Evaluation: Does Raw Instruction-Tuned Code LLMs Output   Suffice for Fill-in-the-Middle Code Generation?
**Title**: From Output to Evaluation: Does Raw Instruction-Tuned Code LLMs Output   Suffice for Fill-in-the-Middle Code Generation? | Von der Ausgabe bis zur Auswertung: Reicht die Ausgabe von LLMs mit rohem Instruktionscode für die Generierung von Fill-in-the-Middle Code aus? | 从输出到评价:原始指令-指令代码LLMs 输出足量是否用于中代代号的填充? [2505.18789v1](http://arxiv.org/abs/2505.18789v1)

**Authors**: Wasi Uddin Ahmad, Somshubra Majumdar, Boris Ginsburg

Post-processing is crucial for the automatic evaluation of LLMs in fill-in-the-middle (FIM) code generation due to the frequent presence of extraneous code in raw outputs. This extraneous generation suggests a lack of awareness regarding output boundaries, requiring truncation for effective evaluation. The determination of an optimal truncation strategy, however, often proves intricate, particularly when the scope includes several programming languages. This study investigates the necessity of post-processing instruction-tuned LLM outputs. Our findings reveal that supervised fine-tuning significantly enhances FIM code generation, enabling LLMs to generate code that seamlessly integrates with the surrounding context. Evaluating our fine-tuned \texttt{Qwen2.5-Coder} (base and instruct) models on HumanEval Infilling and SAFIM benchmarks demonstrates improved performances without post-processing, especially when the \emph{middle} consist of complete lines. However, post-processing of the LLM outputs remains necessary when the \emph{middle} is a random span of code.

---

## Article 111
### Title@2025-05-24: ARMS: A Vision for Actor Reputation Metric Systems in the Open-Source   Software Supply Chain
**Title**: ARMS: A Vision for Actor Reputation Metric Systems in the Open-Source   Software Supply Chain | ARMS: Vision für Actor Reputation Metric Systems in der Open Source Software Supply Chain | ARMS:开放源码软件供应链中行为名声计量系统展望 [2505.18760v1](http://arxiv.org/abs/2505.18760v1)

**Authors**: Kelechi G. Kalu, Sofia Okorafor, Betül Durak, Kim Laine, Radames C. Moreno, Santiago Torres-Arias, James C. Davis

Many critical information technology and cyber-physical systems rely on a supply chain of open-source software projects. OSS project maintainers often integrate contributions from external actors. While maintainers can assess the correctness of a change request, assessing a change request's cybersecurity implications is challenging. To help maintainers make this decision, we propose that the open-source ecosystem should incorporate Actor Reputation Metrics (ARMS). This capability would enable OSS maintainers to assess a prospective contributor's cybersecurity reputation. To support the future instantiation of ARMS, we identify seven generic security signals from industry standards; map concrete metrics from prior work and available security tools, describe study designs to refine and assess the utility of ARMS, and finally weigh its pros and cons.

---

## Article 112
### Title@2025-05-24: AutoP2C: An LLM-Based Agent Framework for Code Repository Generation   from Multimodal Content in Academic Papers
**Title**: AutoP2C: An LLM-Based Agent Framework for Code Repository Generation   from Multimodal Content in Academic Papers | AutoP2C: Ein LLM-basiertes Agent-Framework für die Code-Repository-Generierung aus multimodalen Inhalten in wissenschaftlichen Papieren | 自动P2C: 学术论文中多种形式内容的法规存储器生成基于LLM的LLM代理框架 [2504.20115v2](http://arxiv.org/abs/2504.20115v2)

**Authors**: Zijie Lin, Yiqing Shen, Qilin Cai, He Sun, Jinrui Zhou, Mingjun Xiao

Machine Learning (ML) research is spread through academic papers featuring rich multimodal content, including text, diagrams, and tabular results. However, translating these multimodal elements into executable code remains a challenging and time-consuming process that requires substantial ML expertise. We introduce ``Paper-to-Code'' (P2C), a novel task that transforms the multimodal content of scientific publications into fully executable code repositories, which extends beyond the existing formulation of code generation that merely converts textual descriptions into isolated code snippets. To automate the P2C process, we propose AutoP2C, a multi-agent framework based on large language models that processes both textual and visual content from research papers to generate complete code repositories. Specifically, AutoP2C contains four stages: (1) repository blueprint extraction from established codebases, (2) multimodal content parsing that integrates information from text, equations, and figures, (3) hierarchical task decomposition for structured code generation, and (4) iterative feedback-driven debugging to ensure functionality and performance. Evaluation on a benchmark of eight research papers demonstrates the effectiveness of AutoP2C, which can successfully generate executable code repositories for all eight papers, while OpenAI-o1 or DeepSeek-R1 can only produce runnable code for one paper. The code is available at https://github.com/shoushouyu/Automated-Paper-to-Code.

---

## Article 113
### Title@2025-05-24: Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair
**Title**: Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair | Beheben von 7.400 Fehlern für 1$: Günstige Crash-Site-Programm-Reparatur | 为1美元固定7 400个臭虫:低廉的撞车-点火方案维修 [2505.13103v2](http://arxiv.org/abs/2505.13103v2)

**Authors**: Han Zheng, Ilia Shumailov, Tianqi Fan, Aiden Hall, Mathias Payer

The rapid advancement of bug-finding techniques has led to the discovery of more vulnerabilities than developers can reasonably fix, creating an urgent need for effective Automated Program Repair (APR) methods. However, the complexity of modern bugs often makes precise root cause analysis difficult and unreliable. To address this challenge, we propose crash-site repair to simplify the repair task while still mitigating the risk of exploitation. In addition, we introduce a template-guided patch generation approach that significantly reduces the token cost of Large Language Models (LLMs) while maintaining both efficiency and effectiveness.   We implement our prototype system, WILLIAMT, and evaluate it against state-of-the-art APR tools. Our results show that, when combined with the top-performing agent CodeRover-S, WILLIAMT reduces token cost by 45.9% and increases the bug-fixing rate to 73.5% (+29.6%) on ARVO, a ground-truth open source software vulnerabilities benchmark. Furthermore, we demonstrate that WILLIAMT can function effectively even without access to frontier LLMs: even a local model running on a Mac M4 Mini achieves a reasonable repair rate. These findings highlight the broad applicability and scalability of WILLIAMT.

---

## Article 114
### Title@2025-05-24: SEW: Self-Evolving Agentic Workflows for Automated Code Generation
**Title**: SEW: Self-Evolving Agentic Workflows for Automated Code Generation | SEW: Selbst-evolvierende Agentische Workflows für die automatisierte Codegenerierung | SEW:自动代码生成的自演动态制剂工作流程 [2505.18646v1](http://arxiv.org/abs/2505.18646v1)

**Authors**: Siwei Liu, Jinyuan Fang, Han Zhou, Yingxu Wang, Zaiqiao Meng

Large Language Models (LLMs) have demonstrated effectiveness in code generation tasks. To enable LLMs to address more complex coding challenges, existing research has focused on crafting multi-agent systems with agentic workflows, where complex coding tasks are decomposed into sub-tasks, assigned to specialized agents. Despite their effectiveness, current approaches heavily rely on hand-crafted agentic workflows, with both agent topologies and prompts manually designed, which limits their ability to automatically adapt to different types of coding problems. To address these limitations and enable automated workflow design, we propose \textbf{S}elf-\textbf{E}volving \textbf{W}orkflow (\textbf{SEW}), a novel self-evolving framework that automatically generates and optimises multi-agent workflows. Extensive experiments on three coding benchmark datasets, including the challenging LiveCodeBench, demonstrate that our SEW can automatically design agentic workflows and optimise them through self-evolution, bringing up to 33\% improvement on LiveCodeBench compared to using the backbone LLM only. Furthermore, by investigating different representation schemes of workflow, we provide insights into the optimal way to encode workflow information with text.

---

## Article 115
### Title@2025-05-24: ACECODER: Acing Coder RL via Automated Test-Case Synthesis
**Title**: ACECODER: Acing Coder RL via Automated Test-Case Synthesis | ACECODER: Acing Coder RL über automatisierte Test-Case-Synthese | 通过自动测试-案件综合合成检索编码器 RL [2502.01718v4](http://arxiv.org/abs/2502.01718v4)

**Authors**: Huaye Zeng, Dongfu Jiang, Haozhe Wang, Ping Nie, Xiaotong Chen, Wenhu Chen

Most progress in recent coder models has been driven by supervised fine-tuning (SFT), while the potential of reinforcement learning (RL) remains largely unexplored, primarily due to the lack of reliable reward data/model in the code domain. In this paper, we address this challenge by leveraging automated large-scale test-case synthesis to enhance code model training. Specifically, we design a pipeline that generates extensive (question, test-cases) pairs from existing code data. Using these test cases, we construct preference pairs based on pass rates over sampled programs to train reward models with Bradley-Terry loss. It shows an average of 10-point improvement for Llama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins through best-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5. Furthermore, we conduct reinforcement learning with both reward models and test-case pass rewards, leading to consistent improvements across HumanEval, MBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-style training to start from Qwen2.5-Coder-base directly and show that our RL training can improve model on HumanEval-plus by over 25\% and MBPP-plus by 6\% for merely 80 optimization steps. We believe our results highlight the huge potential of reinforcement learning in coder models.

---

## Article 116
### Title@2025-05-24: On the Structure and Semantics of Identifier Names Containing Closed   Syntactic Category Words
**Title**: On the Structure and Semantics of Identifier Names Containing Closed   Syntactic Category Words | Über die Struktur und Semantik von Identifier-Namen, die geschlossene syntaktische Kategorie Wörter enthalten | 关于含有闭合同步词类的标识名称的结构和语义 [2505.18444v1](http://arxiv.org/abs/2505.18444v1)

**Authors**: Christian D. Newman, Anthony Peruma, Eman Abdullah AlOmar, Mahie Crabbe, Syreen Banabilah, Reem S. AlSuhaibani, Michael J. Decker, Farhad Akhbardeh, Marcos Zampieri, Mohamed Wiem Mkaouer, Jonathan I. Maletic

Identifier names are crucial components of code, serving as primary clues for developers to understand program behavior. This paper investigates the linguistic structure of identifier names by extending the concept of grammar patterns; representations of the part-of-speech (PoS) sequences that underlie identifier phrases. The specific focus is on closed syntactic categories (e.g., prepositions, conjunctions, determiners), which are rarely studied in software engineering despite their central role in general natural language. The Closed Category Identifier Dataset (CCID) is presented, a new manually annotated dataset of 1,275 identifiers drawn from 30 open-source systems. The relationship between closed-category grammar patterns and program behavior is analyzed using grounded theory coding, statistical, and pattern analysis. The results reveal recurring structures that developers use to express control flow, data transformation, temporal reasoning, and behavioral roles through naming. This study contributes an empirical foundation for understanding how developers adapt linguistic resources to encode behavior in source code. By analyzing closed-category terms and their associated grammar patterns, the work highlights a previously underexplored dimension of identifier semantics and identifies promising directions for future research in naming support, comprehension, and education.

---

