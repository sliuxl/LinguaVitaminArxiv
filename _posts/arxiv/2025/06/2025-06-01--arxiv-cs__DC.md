---
title: "cs.DC @ 2025-06-01: 103"
date: 2025-06-01
layout: post
---

- [00](#article-0) | **05-29 (4)** | From Connectivity to Autonomy: The Dawn of Self-Evolving Communication   Systems | Von der Konnektivität zur Autonomie: Die Morgenröte der sich selbst entwickelnden Kommunikationssysteme | 从连接到自主:自我发展的通信系统的黎明 | [2505.23710v1](http://arxiv.org/abs/2505.23710v1)
- [01](#article-1) | 05-29 | Distributed Federated Learning for Vehicular Network Security: Anomaly   Detection Benefits and Multi-Domain Attack Threats | Verteiltes Federated Learning für die Sicherheit des Vehicular Network: Anomalieerkennungsvorteile und Multi-Domain-Angriffsbedrohungen | 分布式联邦学习促进车辆网络安全:反常探测效益和多领域攻击威胁 | [2505.23706v1](http://arxiv.org/abs/2505.23706v1)
- [02](#article-2) | 05-29 | Parallel GPU-Accelerated Randomized Construction of Approximate Cholesky   Preconditioners | Parallele GPU-beschleunigte Randomisierte Konstruktion von ungefähren Cholesky-Vorkonditionen | 平行的GPU-加速加速旋转式建造近焦天空预设装置 | [2505.02977v2](http://arxiv.org/abs/2505.02977v2)
- [03](#article-3) | 05-29 | Complementary Time-Space Tradeoff for Self-Stabilizing Leader Election:   Polynomial States Meet Sublinear Time | Komplementärer Zeit-Raum-Tradeoff für selbststabilisierende Leader-Wahl: Polynome Staaten treffen auf sublineare Zeit | 自我稳定领导人选举的补充时间-空间权衡:多民族国家满足亚线性时间 | [2505.23649v1](http://arxiv.org/abs/2505.23649v1)
- [04](#article-4) | 05-29 | Accelerated Training of Federated Learning via Second-Order Methods | Beschleunigte Ausbildung des Föderierten Lernens über Methoden der zweiten Ordnung | 通过二级方法加快联邦学习培训 | [2505.23588v1](http://arxiv.org/abs/2505.23588v1)
- [05](#article-5) | 05-29 | Sustainable Carbon-Aware and Water-Efficient LLM Scheduling in   Geo-Distributed Cloud Datacenters | Nachhaltiges CO2-basiertes und wassereffizientes LLM-Scheeduling in Geo-verteilten Cloud-Rechenzentren | 地球分布云数据中心的可持续碳软件和水效率高的LLM | [2505.23554v1](http://arxiv.org/abs/2505.23554v1)
- [06](#article-6) | 05-29 | Accelerating AllReduce with a Persistent Straggler | AllReduce mit einem persistenten Straggler beschleunigen | 使用持久性斯特拉格驱动器加速全部拖动 | [2505.23523v1](http://arxiv.org/abs/2505.23523v1)
- [07](#article-7) | 05-29 | SealOS+: A Sealos-based Approach for Adaptive Resource Optimization   Under Dynamic Workloads for Securities Trading System | SealOS+: Ein Sealos-basierter Ansatz für adaptive Ressourcenoptimierung unter dynamischen Workloads für Securities Trading System | SealOS+:证券交易系统动态工作量下的适应性资源优化的以海路为基础的办法 | [2505.23258v1](http://arxiv.org/abs/2505.23258v1)
- [08](#article-8) | 05-29 | Smaller, Smarter, Closer: The Edge of Collaborative Generative AI | Kleiner, intelligenter, enger: Der Rand der kollaborativen Generativen KI | 较小、更聪明、更近:合作创造的边缘 AI | [2505.16499v2](http://arxiv.org/abs/2505.16499v2)
- [09](#article-9) | 05-29 | MemAscend: System Memory Optimization for SSD-Offloaded LLM Fine-Tuning | MemAscend: Systemspeicheroptimierung für SSD-Offloaded LLM Fine-Tuning | MemAscend: SSD- 卸载 LLM 精密调试的系统内存优化 | [2505.23254v1](http://arxiv.org/abs/2505.23254v1)
- [10](#article-10) | 05-29 | Edge-First Language Model Inference: Models, Metrics, and Tradeoffs | Edge-First Language Model Inferenz: Modelle, Metrics und Tradeoffs | 边缘第一语言模式示范推论:模型、计量和权衡取舍 | [2505.16508v2](http://arxiv.org/abs/2505.16508v2)
- [11](#article-11) | 05-29 | Ghidorah: Fast LLM Inference on Edge with Speculative Decoding and   Hetero-Core Parallelism | Ghidorah: Schnelle LLM-Inferenz am Rand mit spekulativer Dekodierung und Hetero-Core-Parallelität | Ghidorah:快速LLM | [2505.23219v1](http://arxiv.org/abs/2505.23219v1)
- [12](#article-12) | 05-29 | Improving Parallel Program Performance with LLM Optimizers via   Agent-System Interfaces | Verbesserung der parallelen Programmleistung mit LLM-Optimierern über Agent-System-Schnittstellen | 通过代理-系统接口改进与LLM优化器的平行方案绩效 | [2410.15625v3](http://arxiv.org/abs/2410.15625v3)
- [13](#article-13) | 05-29 | The Panaceas for Improving Low-Rank Decomposition in   Communication-Efficient Federated Learning | Die Panaceas zur Verbesserung der Zersetzung mit geringem Rank im kommunikativ-effizienten Federated Learning | 改善通信-高效联邦学习中低-兰克分解的全景 | [2505.23176v1](http://arxiv.org/abs/2505.23176v1)
- [14](#article-14) | 05-29 | DOPPLER: Dual-Policy Learning for Device Assignment in Asynchronous   Dataflow Graphs | DOPPLER: Dual-Policy-Lernen für die Gerätezuordnung in asynchronen Datenflussgraphen | DOPPLER: 同步数据流图表中设备分配的双政策学习 | [2505.23131v1](http://arxiv.org/abs/2505.23131v1)
- [15](#article-15) | 05-29 | Toward Cost-Efficient Serving of Mixture-of-Experts with Asynchrony | Auf dem Weg zu einem kosteneffizienten Servieren von Mixture-of-Experts mit Asynchrony | 争取以成本低效益高的方式服务专家与非同步混合服务 | [2505.08944v2](http://arxiv.org/abs/2505.08944v2)
- [16](#article-16) | 05-29 | Shortcut-connected Expert Parallelism for Accelerating   Mixture-of-Experts | Shortcut-verbundene Experten-Parallelität für die Beschleunigung von Mixture-of-Experts | 加速混合专家专家专家平行专家 | [2404.05019v3](http://arxiv.org/abs/2404.05019v3)
- [17](#article-17) | 05-29 | Speeding up Model Loading with fastsafetensors | Beschleunigen des Modells Beladung mit Schnellsicherern | 加速装有快速保障装置的模型加载速度 | [2505.23072v1](http://arxiv.org/abs/2505.23072v1)
- [18](#article-18) | **05-28 (3)** | Profiling and optimization of multi-card GPU machine learning jobs | Profilierung und Optimierung von Multi-Card-GPU-Maschinenlernjobs | 多卡 GPPU 机器学习工作的分析和优化 | [2505.22905v1](http://arxiv.org/abs/2505.22905v1)
- [19](#article-19) | 05-28 | Visualizing Cloud-native Applications with KubeDiagrams | Cloud-native Anwendungen mit KubeDiagrammen visualisieren | 带有KubeDiagrams 的可视化云源应用 | [2505.22879v1](http://arxiv.org/abs/2505.22879v1)
- [20](#article-20) | 05-28 | The National Research Platform: Stretched, Multi-Tenant, Scientific   Kubernetes Cluster | Die Nationale Forschungsplattform: Streckiger, Multi-Tenant-Cluster, wissenschaftlicher Kubernetes-Cluster | 国家研究平台:延伸、多层、多层、科学库伯涅茨集群 | [2505.22864v1](http://arxiv.org/abs/2505.22864v1)
- [21](#article-21) | 05-28 | $Δ$-Nets: Interaction-Based System for Optimal Parallel   $λ$-Reduction | $Δ$-Nets: Interaktionsbasiertes System für eine optimale parallele $λ$-Reduktion | $$-净额:最佳平行互动系统$$$美元-削减 | [2505.20314v2](http://arxiv.org/abs/2505.20314v2)
- [22](#article-22) | 05-28 | Smart Contracts for SMEs and Large Companies | Intelligente Verträge für KMU und Großunternehmen | 中小企业和大公司的智能合同 | [2505.22619v1](http://arxiv.org/abs/2505.22619v1)
- [23](#article-23) | 05-28 | Pilot-Quantum: A Quantum-HPC Middleware for Resource, Workload and Task   Management | Pilot-Quantum: Eine Quantum-HPC Middleware für Ressourcen-, Workload- und Task-Management | 试点量子:资源、工作量和任务管理的量子-氢氯氟烃中软件 | [2412.18519v3](http://arxiv.org/abs/2412.18519v3)
- [24](#article-24) | 05-28 | Morpheus Consensus: Excelling on trails and autobahns | Morpheus Consensus: Excelling auf Trails und Autobahnen | Morpheus共识:关于足迹和自动铢的Excelling | [2502.08465v2](http://arxiv.org/abs/2502.08465v2)
- [25](#article-25) | 05-28 | Grassroots Federation: Fair Governance of Large-Scale, Decentralized,   Sovereign Digital Communities | Grassroots Federation: Faire Governance der großen, dezentralisierten, Souveränen Digitalen Gemeinschaften | 基层联合会:大、分散、主权数字共同体的公平治理 | [2505.02208v4](http://arxiv.org/abs/2505.02208v4)
- [26](#article-26) | 05-28 | Broadcast in Almost Mixing Time | In fast mischender Zeit übertragen | 几乎混合时间的广播 | [2502.02165v2](http://arxiv.org/abs/2502.02165v2)
- [27](#article-27) | 05-28 | Inclusive, Differentially Private Federated Learning for Clinical Data | Inklusives, differenziert privates Federated Learning für klinische Daten | 包容性、差异化私联校临床数据学习 | [2505.22108v1](http://arxiv.org/abs/2505.22108v1)
- [28](#article-28) | 05-28 | A Stochastic Approximation Approach for Efficient Decentralized   Optimization on Random Networks | Ein stochastischer Annäherungsansatz für eine effiziente dezentralisierte Optimierung von Random Networks | 随机网络高效分散优化优化的斯托卡接近方法 | [2410.18774v2](http://arxiv.org/abs/2410.18774v2)
- [29](#article-29) | 05-28 | Towards Efficient Key-Value Cache Management for Prefix Prefilling in   LLM Inference | Effizientes Key-Value-Cache-Management für die Präfixvorfüllung in LLM-Inferenz | 建立高效的键值缓存管理,用于在LLM 推理中预填前补全 | [2505.21919v1](http://arxiv.org/abs/2505.21919v1)
- [30](#article-30) | 05-28 | Joint$λ$: Orchestrating Serverless Workflows on Jointcloud FaaS   Systems | Joint$λ$: Orchestrierung serverloser Workflows auf Jointcloud FaaS-Systemen | 联合 $ $: 联合COLOUD FaaS系统无服务器工作流管 | [2505.21899v1](http://arxiv.org/abs/2505.21899v1)
- [31](#article-31) | 05-28 | Hybrid Batch Normalisation: Resolving the Dilemma of Batch Normalisation   in Federated Learning | Hybride Batch-Normalisierung: Lösung des Dilemmas der Batch-Normalisierung im Federated Learning | 混合批次正常化:解决联邦学习中批次正常化的难题 | [2505.21877v1](http://arxiv.org/abs/2505.21877v1)
- [32](#article-32) | 05-28 | gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM   Serving with Token Throttling | gLLM: Global Balanced Pipeline Parallelism System für verteiltes LLM Serving mit Token Throttling | gLLM:全球平衡管道平行系统 | [2504.14775v2](http://arxiv.org/abs/2504.14775v2)
- [33](#article-33) | **05-27 (2)** | Empowering Scientific Workflows with Federated Agents | Stärkung wissenschaftlicher Workflows mit Federated Agents | 赋予联邦药剂部门科学工作流程权能 | [2505.05428v2](http://arxiv.org/abs/2505.05428v2)
- [34](#article-34) | 05-27 | LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in   Multimodal Large Language Models | LV-XAttn: Verteilte Cross-Attention für lange visuelle Eingänge in multimodalen großen Sprachmodellen | LV-XAttn:多式大语言模型中长视输入分布式交叉注意 | [2502.02406v3](http://arxiv.org/abs/2502.02406v3)
- [35](#article-35) | 05-27 | Power-Capping Metric Evaluation for Improving Energy Efficiency | Leistungskapitulation Metric-Evaluierung zur Verbesserung der Energieeffizienz | 提高能源效率提高能源使用效率的节能计量评价 | [2505.21758v1](http://arxiv.org/abs/2505.21758v1)
- [36](#article-36) | 05-27 | FedCostAware: Enabling Cost-Aware Federated Learning on the Cloud | FedCostAware: Kostenbewusstes Lernen in der Cloud ermöglichen | FestAware:在云上进行成本-软件联合学习 | [2505.21727v1](http://arxiv.org/abs/2505.21727v1)
- [37](#article-37) | 05-27 | AMSFL: Adaptive Multi-Step Federated Learning via Gradient   Difference-Based Error Modeling | AMSFL: Adaptives Multi-Step-Federated Learning über gradient Difference-based Error Modeling | ASFL:通过基于差异的渐进错误建模进行适应性多阶段联邦学习 | [2505.21695v1](http://arxiv.org/abs/2505.21695v1)
- [38](#article-38) | 05-27 | Incentivizing Permissionless Distributed Learning of LLMs | Anreize für das unbefugte Lernen von LLMs | 激励对LLMM的无自由分配的学习 | [2505.21684v1](http://arxiv.org/abs/2505.21684v1)
- [39](#article-39) | 05-27 | KPerfIR: Towards an Open and Compiler-centric Ecosystem for GPU Kernel   Performance Tooling on Modern AI Workloads | KPerfIR: Auf dem Weg zu einem offenen und kompilerzentrierten Ökosystem für GPU-Kernel Performance Tooling auf modernen KI-Workloads | KPerfIR:努力建立一个开放的、以编纂者为中心的生态系统,用于在现代AI 工作负荷上使用 GPU 内核性能工具 | [2505.21661v1](http://arxiv.org/abs/2505.21661v1)
- [40](#article-40) | 05-27 | Fast and Cost-effective Speculative Edge-Cloud Decoding with Early Exits | Schnelle und kostengünstige spekulative Edge-Cloud-Dekodierung mit Early Exits | 快速和成本效益高的投机性边缘-封闭式排污与早期出口 | [2505.21594v1](http://arxiv.org/abs/2505.21594v1)
- [41](#article-41) | 05-27 | Distributed Discrete Morse Sandwich: Efficient Computation of   Persistence Diagrams for Massive Scalar Data | Distributed Diskrete Morse Sandwich: Effiziente Berechnung von Persistenzdiagrammen für massive Scalardaten | 分布式分散的莫尔斯桑威奇:有效计算大规模卡路里数据持久性图图 | [2505.21266v1](http://arxiv.org/abs/2505.21266v1)
- [42](#article-42) | 05-27 | DeepCEE: Efficient Cross-Region Model Distributed Training System under   Heterogeneous GPUs and Networks | DeepCEE: Effizientes regionsübergreifendes Schulungssystem unter heterogenen GPUs und Netzwerken | DeepCEE:在异种性全球保护单位和网络下建立高效跨区域分布示范培训系统 | [2505.15536v2](http://arxiv.org/abs/2505.15536v2)
- [43](#article-43) | 05-27 | Grassroots Consensus | Graswurzeln-Konsens | 基层共识 | [2505.19216v2](http://arxiv.org/abs/2505.19216v2)
- [44](#article-44) | 05-27 | Multi-Event Triggers for Serverless Computing | Multi-Event-Trigger für serverloses Rechnen | 无服务器电子计算多天触发器 | [2505.21199v1](http://arxiv.org/abs/2505.21199v1)
- [45](#article-45) | 05-27 | Vectorized Sequence-Based Chunking for Data Deduplication | Vektorisierte Sequenz-basiertes Chunking für Datendeduplikation | 数据解析矢量序列相键 | [2505.21194v1](http://arxiv.org/abs/2505.21194v1)
- [46](#article-46) | 05-27 | Constructive community race: full-density spiking neural network model   drives neuromorphic computing | Konstruktives Community-Rennen: Volldichte-Spitzen neuronales Netzwerkmodell treibt neuromorphes Computing an | 充满建设性的社区种族:完全密度刺激神经网络模型驱动神经形态计算 | [2505.21185v1](http://arxiv.org/abs/2505.21185v1)
- [47](#article-47) | 05-27 | SHE-LoRA: Selective Homomorphic Encryption for Federated Tuning with   Heterogeneous LoRA | SHE-LoRA: Selektive homomorphe Verschlüsselung für Federated Tuning mit Heterogene LoRA | SHE-LORA: 与异源罗拉结合的联邦调试的选择性单体单体加密 | [2505.21051v1](http://arxiv.org/abs/2505.21051v1)
- [48](#article-48) | 05-27 | A Hitchhiker's Guide to Privacy-Preserving Cryptocurrencies: A Survey on   Anonymity, Confidentiality, and Auditability | Ein Hitchhiker-Leitfaden zur Wahrung der Privatsphäre von Kryptowährungen: Eine Umfrage über Anonymität, Vertraulichkeit und Auditierbarkeit | 《希希克人保护隐私加密指南:关于匿名、保密和可审计性的调查》 | [2505.21008v1](http://arxiv.org/abs/2505.21008v1)
- [49](#article-49) | 05-27 | RACS-SADL: Robust and Understandable Randomized Consensus in the Cloud | RACS-SADL: Robuster und verständlicher Randomisierter Konsens in der Cloud | RACS-SADL:云层中的有力和可理解的随机共识 | [2404.04183v3](http://arxiv.org/abs/2404.04183v3)
- [50](#article-50) | 05-27 | EPIC: Efficient Position-Independent Caching for Serving Large Language   Models | EPIC: Effizientes positionsunabhängiges Caching für das Servieren großer Sprachmodelle | EPIC: 高效的、独立定位的为大语言模式服务的工作 | [2410.15332v3](http://arxiv.org/abs/2410.15332v3)
- [51](#article-51) | 05-27 | Complexity landscape for local certification | Komplexitätslandschaft für die lokale Zertifizierung | 当地认证的复杂环境 | [2505.20915v1](http://arxiv.org/abs/2505.20915v1)
- [52](#article-52) | 05-27 | Reduced and mixed precision turbulent flow simulations using explicit   finite difference schemes | Reduzierte und gemischte Präzision turbulente Strömungssimulationen mit expliziten Finite-Differenz-Systemen | 使用明确的有限差别办法进行减少和混合精密混杂的波动流动模拟 | [2505.20911v1](http://arxiv.org/abs/2505.20911v1)
- [53](#article-53) | 05-27 | Load Balancing in Strongly Inhomogeneous Simulations -- a Vlasiator Case   Study | Lastausgleich in stark inhomogenen Simulationen -- eine Vlasiator-Fallstudie | 在极不相异模拟器中平衡载荷 -- -- 挥发器案例研究 | [2505.20908v1](http://arxiv.org/abs/2505.20908v1)
- [54](#article-54) | 05-27 | An Efficient Implementation of Guard-Based Synchronization for an   Object-Oriented Programming Language | Effiziente Implementierung von Guard-Based Synchronization für eine objektorientierte Programmiersprache | 高效率地实施以警卫为基础的同步,以用于以目标为导向的方案编制语言 | [2505.20850v1](http://arxiv.org/abs/2505.20850v1)
- [55](#article-55) | 05-27 | Choreographies as Macros | Choreographien als Makros | 作为宏的舞蹈 | [2505.20845v1](http://arxiv.org/abs/2505.20845v1)
- [56](#article-56) | 05-27 | ECC-SNN: Cost-Effective Edge-Cloud Collaboration for Spiking Neural   Networks | ECC-SNN: Kosteneffiziente Edge-Cloud-Kollaboration für Spiking Neuronal Networks | ECC-SNN: 传播神经网络的成本-效益高的边缘-封闭式协作 | [2505.20835v1](http://arxiv.org/abs/2505.20835v1)
- [57](#article-57) | 05-27 | Work-Efficient Parallel Counting via Sampling | Arbeitseffiziente parallele Zählung über Probenahme | 通过抽样计算实现工作效率的平行计数 | [2408.09719v2](http://arxiv.org/abs/2408.09719v2)
- [58](#article-58) | 05-27 | Time-Series Learning for Proactive Fault Prediction in Distributed   Systems with Deep Neural Structures | Time-Series Learning für proaktive Fehlervorhersage in verteilten Systemen mit tiefen neuralen Strukturen | 深心神经结构分布系统预发性故障预测时间序列学习 | [2505.20705v1](http://arxiv.org/abs/2505.20705v1)
- [59](#article-59) | 05-27 | InstGenIE: Generative Image Editing Made Efficient with Mask-aware   Caching and Scheduling | InstGenIE: Generative Bildbearbeitung mit Mask-aware Caching und Scheduling effizient gemacht | InstGenie: 生成图像编辑, 高效使用防面具图像缓冲和排程 | [2505.20600v1](http://arxiv.org/abs/2505.20600v1)
- [60](#article-60) | **05-26 (1)** | Asynchronous Fault-Tolerant Language Decidability for Runtime   Verification of Distributed Systems | Asynchrone Fehler-Tolerante Sprachentscheidung für die Laufzeitverifizierung von verteilten Systemen | 分布式系统运行时核查的 Al- 同步错失容忍语言 | [2502.00191v2](http://arxiv.org/abs/2502.00191v2)
- [61](#article-61) | 05-26 | Avoid Forgetting by Preserving Global Knowledge Gradients in Federated   Learning with Non-IID Data | Vermeiden Sie das Vergessen, indem Sie globale Wissensgradienten im Föderierten Lernen mit nicht-ID-Daten bewahren | 避免在使用非二二二维数据进行联邦学习时因保留全球知识进步而被遗忘 | [2505.20485v1](http://arxiv.org/abs/2505.20485v1)
- [62](#article-62) | 05-26 | Fixing non-blocking data structures for better compatibility with memory   reclamation schemes | Fixierung von nicht blockierenden Datenstrukturen für eine bessere Kompatibilität mit Speicher-Reklamationssystemen | 固定非阻塞性数据结构,以更好地与内存回收计划兼容 | [2504.06254v2](http://arxiv.org/abs/2504.06254v2)
- [63](#article-63) | 05-26 | Efficient Optimization Accelerator Framework for Multistate Ising   Problems | Effizientes Optimierungs-Beschleuniger-Framework für Multistate Ising-Probleme | 高效高效优化多州化问题加速加速框架 | [2505.20250v1](http://arxiv.org/abs/2505.20250v1)
- [64](#article-64) | 05-26 | FedECA: A Federated External Control Arm Method for Causal Inference   with Time-To-Event Data in Distributed Settings | FedECA: Eine Federated External Control Arm Methode für ursächliche Schlussfolgerungen mit Zeit-bis-Event-Daten in verteilten Einstellungen | FedECA:在分布环境中利用时间到时间的数据进行因果关系推断的联邦外部控制武器法 | [2311.16984v9](http://arxiv.org/abs/2311.16984v9)
- [65](#article-65) | 05-26 | BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems | BurstGPT: Ein echter Workload-Datensatz zur Optimierung von LLM-Serviersystemen | BurtGPT:优化LLM服务系统的现实世界工作量数据集 | [2401.17644v5](http://arxiv.org/abs/2401.17644v5)
- [66](#article-66) | 05-26 | Parallelizing a modern GPU simulator | Parallelisierung eines modernen GPU-Simulators | 平行使用现代 GPU 模拟器 | [2502.14691v2](http://arxiv.org/abs/2502.14691v2)
- [67](#article-67) | 05-26 | Snowman for partial synchrony | Schneemann für partielle Synchronisation | 部分同步的雪人 | [2501.15904v3](http://arxiv.org/abs/2501.15904v3)
- [68](#article-68) | 05-26 | Beyond Optimal Fault Tolerance | Jenseits der optimalen Fehlertoleranz | 超越最佳错失容忍 | [2501.06044v6](http://arxiv.org/abs/2501.06044v6)
- [69](#article-69) | 05-26 | Distortion Resilience for Goal-Oriented Semantic Communication | Distortion Resilienz für zielorientierte semantische Kommunikation | 目标导向语义交流的扭曲复原力 | [2309.14587v2](http://arxiv.org/abs/2309.14587v2)
- [70](#article-70) | 05-26 | Optimizing edge AI models on HPC systems with the edge in the loop | Optimierung der Kanten-KI-Modelle auf HPC-Systemen mit der Kante in der Schleife | 优化循环边缘的HPC系统优化边缘 AI 模型 | [2505.19995v1](http://arxiv.org/abs/2505.19995v1)
- [71](#article-71) | 05-26 | Federated Domain Generalization with Data-free On-server Matching   Gradient | Föderierte Domain-Verallgemeinerung mit datenfreiem On-Server-Zustimmungs-Gradient | 具有无数据观测站上与渐变匹配的无数据观测器的联邦通用域 | [2501.14653v2](http://arxiv.org/abs/2501.14653v2)
- [72](#article-72) | 05-26 | From Few to Many Faults: Adaptive Byzantine Agreement with Optimal   Communication | Von wenigen bis zu vielen Fehlern: Adaptive byzantinische Vereinbarung mit optimaler Kommunikation | 从少到多的错失:适应性拜占庭协议与最佳沟通 | [2505.19989v1](http://arxiv.org/abs/2505.19989v1)
- [73](#article-73) | 05-26 | Differential Privacy Analysis of Decentralized Gossip Averaging under   Varying Threat Models | Differential Privacy Analyse dezentralisierter Gossip Average unter unterschiedlichen Bedrohungsmodellen | 对不同威胁模式下分散的流民的隐私差异分析 | [2505.19969v1](http://arxiv.org/abs/2505.19969v1)
- [74](#article-74) | 05-26 | Universal Workers: A Vision for Eliminating Cold Starts in Serverless   Computing | Universal Workers: Eine Vision zur Beseitigung von Kaltstarts im serverlosen Computing | 普遍工人:在无服务器计算机中消除冷源的愿景 | [2505.19880v1](http://arxiv.org/abs/2505.19880v1)
- [75](#article-75) | 05-26 | DGRAG: Distributed Graph-based Retrieval-Augmented Generation in   Edge-Cloud Systems | DGRAG: Distributed Graph-based Retrieval-Augmented Generation in Edge-Cloud-Systemen | DGGGAG: 在边缘封闭系统中分布的基于图图的回收回源-养代 | [2505.19847v1](http://arxiv.org/abs/2505.19847v1)
- [76](#article-76) | 05-26 | Will LLMs Scaling Hit the Wall? Breaking Barriers via Distributed   Resources on Massive Edge Devices | Wird LLMs Skalierung die Wand treffen? Über verteilte Ressourcen auf massiven Edge-Geräten Barrieren überwinden | LLLMs SUlia扩大会撞上隔离墙吗?通过大规模边缘装置分配资源打破障碍 | [2503.08223v2](http://arxiv.org/abs/2503.08223v2)
- [77](#article-77) | 05-26 | A Unified, Practical, and Understandable Model of Non-transactional   Consistency Levels in Distributed Replication | Ein einheitliches, praktisches und verständliches Modell nichttransaktionsfähiger Konsistenzstufen in verteilter Replikation | 分布式重复中非交易一致性水平的统一、实用和可理解的模式 | [2409.01576v4](http://arxiv.org/abs/2409.01576v4)
- [78](#article-78) | 05-26 | Justin: Hybrid CPU/Memory Elastic Scaling for Distributed Stream   Processing | Justin: Hybride CPU/Memory Elastic Scaling für verteilte Stream-Verarbeitung | Justin: 用于分布流处理的混合 CPU/Memory Elastic 缩放比例 | [2505.19739v1](http://arxiv.org/abs/2505.19739v1)
- [79](#article-79) | 05-26 | Towards Optimal Distributed Edge Coloring with Fewer Colors | Auf dem Weg zu einer optimalen verteilten Randfärbung mit weniger Farben | 向最优化分布式边缘配色,颜色更少 | [2504.13003v2](http://arxiv.org/abs/2504.13003v2)
- [80](#article-80) | 05-26 | Byzantine Consensus in the Random Asynchronous Model | Byzantinischer Konsens im zufälligen asynchronen Modell | 随机非同步模型中的拜占庭共识 | [2502.09116v2](http://arxiv.org/abs/2502.09116v2)
- [81](#article-81) | 05-26 | Mosaic: Data-Free Knowledge Distillation via Mixture-of-Experts for   Heterogeneous Distributed Environments | Mosaic: Datenfreies Wissen Destillieren über Mixture-of-Experts für Heterogene verteilte Umgebungen | Mosaic:通过混合专家进行无数据知识蒸馏,促进异基因分布式环境 | [2505.19699v1](http://arxiv.org/abs/2505.19699v1)
- [82](#article-82) | 05-26 | PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM   Serving | PRESSERVE: Prefetching Modellgewichte und KV-Cache in verteilter LLM-Servierung | PRESSERVE: 分布式LLM服务中的预伸缩模型重量和 KV-缓冲 | [2501.08192v2](http://arxiv.org/abs/2501.08192v2)
- [83](#article-83) | 05-26 | Scaling Large-scale GNN Training to Thousands of Processors on CPU-based   Supercomputers | Skalierung von großformatigen GNN-Schulungen zu Tausenden von Prozessoren auf CPU-basierten Supercomputern | 向数千台基于CPU的超级计算机处理器提供大规模GNN培训 | [2411.16025v2](http://arxiv.org/abs/2411.16025v2)
- [84](#article-84) | 05-26 | Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive   Decisions of LLMs | Gewinnen Sie schnell oder verlieren Sie langsam: Ausgleichende Geschwindigkeit und Genauigkeit in Latenz-Sensitive Entscheidungen von LLMs | 慢赢或慢输:LLMs的延缓敏感决定中平衡速度和准确性 | [2505.19481v1](http://arxiv.org/abs/2505.19481v1)
- [85](#article-85) | 05-26 | GPU acceleration of non-equilibrium Green's function calculation using   OpenACC and CUDA FORTRAN | GPU-Beschleunigung der Nicht-Equilibrium Green-Funktionsberechnung mit OpenACC und CUDA FORTRAN | 使用 OpenACC 和 CUDA FORTRAN 加速 GPU 绿色非平衡的功能计算 | [2505.19467v1](http://arxiv.org/abs/2505.19467v1)
- [86](#article-86) | 05-26 | FedHERO: A Federated Learning Approach for Node Classification Task on   Heterophilic Graphs | FedHERO: Ein Federated Learning Approach für Knotenklassifikation Aufgaben auf heterophilen Graphen | FEFHERO: 异生物图节点分类任务联邦学习方法 | [2504.21206v2](http://arxiv.org/abs/2504.21206v2)
- [87](#article-87) | **05-25 (7)** | QMIO: A tightly integrated hybrid HPCQC system | QMIO: Ein eng integriertes Hybrid-HPCQC-System | QMIO:一个严格一体化的混合高和分PCQC系统 | [2505.19267v1](http://arxiv.org/abs/2505.19267v1)
- [88](#article-88) | 05-25 | NanoFlow: Towards Optimal Large Language Model Serving Throughput | NanoFlow: Auf dem Weg zu einem optimalen Large Language Model | NanoFlow:走向最佳大语言模式 | [2408.12757v2](http://arxiv.org/abs/2408.12757v2)
- [89](#article-89) | 05-25 | Matrix Multiplication in the MPC Model | Matrix-Multiplikation im MPC-Modell | MPC 模型中的矩阵乘法 | [2505.19137v1](http://arxiv.org/abs/2505.19137v1)
- [90](#article-90) | 05-25 | Birch SGD: A Tree Graph Framework for Local and Asynchronous SGD Methods | Birke SGD: Ein Baumdiagramm-Framework für lokale und asynchrone SGD-Methoden | Birch SGD: 当地和非同步 SGD 方法树图框架 | [2505.09218v2](http://arxiv.org/abs/2505.09218v2)
- [91](#article-91) | **05-24 (6)** | Toward Malicious Clients Detection in Federated Learning | Auf dem Weg zu bösartigen Kunden Erkennung im Föderierten Lernen | 争取在联邦学习中发现恶意客户 | [2505.09110v2](http://arxiv.org/abs/2505.09110v2)
- [92](#article-92) | 05-24 | Distributed Incremental SAT Solving with Mallob: Report and Case Study   with Hierarchical Planning | Distributed Incremental SAT Solving with Mallob: Report and Case Study with Hierarchical Planning | 与马洛布公司共同解决:与等级规划有关的报告和案例研究 | [2505.18836v1](http://arxiv.org/abs/2505.18836v1)
- [93](#article-93) | 05-24 | DiSCo: Device-Server Collaborative LLM-Based Text Streaming Services | DiSCo: Geräte-Server Kollaborative LLM-basierte Text-Streaming-Dienste | DisCo: 设备-服务器协作协作LLM基于LLM的文本流服务 | [2502.11417v2](http://arxiv.org/abs/2502.11417v2)
- [94](#article-94) | 05-24 | Distributed Set-membership Filtering Frameworks For Multi-agent Systems   With Absolute and Relative Measurements | Distributed Set-Membership Filtering Frameworks für Multi-Agent-Systeme mit absoluten und relativen Messungen | 具有绝对和相对计量的多试剂系统分布式成员筛选框架 | [2305.15797v2](http://arxiv.org/abs/2305.15797v2)
- [95](#article-95) | 05-24 | EvoSort: A Genetic-Algorithm-Based Adaptive Parallel Sorting Framework   for Large-Scale High Performance Computing | EvoSort: Ein genetisch-algorithmisch-adaptives Parallelsortierungs-Framework für großformatige Hochleistungsrechnen | EvoSort: 大型高性能计算方法的基于遗传 -- -- 物理学的适应性平行排序框架 | [2505.18681v1](http://arxiv.org/abs/2505.18681v1)
- [96](#article-96) | 05-24 | Towards Round-Optimal Approximate Agreement on Trees | Auf dem Weg zu einem runden, optimalen Abkommen über Bäume | 争取达成关于树木的圆顶和最接近于 | [2502.05591v2](http://arxiv.org/abs/2502.05591v2)
- [97](#article-97) | 05-24 | Asynchronous Approximate Agreement with Quadratic Communication | Asynchrone annähernde Vereinbarung mit quadratischer Kommunikation | 与赤道通信的近似非同步协定 | [2408.05495v3](http://arxiv.org/abs/2408.05495v3)
- [98](#article-98) | 05-24 | TEE is not a Healer: Rollback-Resistant Reliable Storage | TEE ist kein Heiler: Rollback-Resistent Zuverlässige Lagerung | TEE不是救治者:回击-恢复-可靠储存 | [2505.18648v1](http://arxiv.org/abs/2505.18648v1)
- [99](#article-99) | 05-24 | CacheFL: Privacy-Preserving and Efficient Federated Cache Model   Fine-Tuning for Vision-Language Models | CacheFL: Datenschutzschonendes und effizientes Federated Cache Model Fine-Tuning für Vision-Language-Modelle | CACHFL: 视力和语言模型微调模型 | [2505.05130v2](http://arxiv.org/abs/2505.05130v2)
- [100](#article-100) | 05-24 | PacTrain: Pruning and Adaptive Sparse Gradient Compression for Efficient   Collective Communication in Distributed Deep Learning | PacTrain: Pruning and Adaptive Sparse Gradient Compression für effiziente kollektive Kommunikation im verteilten Deep Learning | PacTrain:在分布式深层学习中促进高效集体交流的审慎和适应性零散梯级压缩 | [2505.18563v1](http://arxiv.org/abs/2505.18563v1)
- [101](#article-101) | 05-24 | Consensus Under Adversary Majority Done Right | Konsens unter gegnerischer Mehrheit Rechtsbeistand | 在相反多数下达成的共识 | [2411.01689v3](http://arxiv.org/abs/2411.01689v3)
- [102](#article-102) | 05-24 | Recursive Offloading for LLM Serving in Multi-tier Networks | Rekursives Offloading für LLM-Serving in Multi-Tier-Netzwerken | 多层网络LLM服务的递归性卸载 | [2505.16502v2](http://arxiv.org/abs/2505.16502v2)

## Article 0
### Title@2025-05-29 (4): From Connectivity to Autonomy: The Dawn of Self-Evolving Communication   Systems

**Title**: From Connectivity to Autonomy: The Dawn of Self-Evolving Communication   Systems | Von der Konnektivität zur Autonomie: Die Morgenröte der sich selbst entwickelnden Kommunikationssysteme | 从连接到自主:自我发展的通信系统的黎明 [2505.23710v1](http://arxiv.org/abs/2505.23710v1)

**Authors**: Zeinab Nezami, Syed Danial Ali Shah, Maryam Hafeez, Karim Djemame, Syed Ali Raza Zaidi

This paper envisions 6G as a self-evolving telecom ecosystem, where AI-driven intelligence enables dynamic adaptation beyond static connectivity. We explore the key enablers of autonomous communication systems, spanning reconfigurable infrastructure, adaptive middleware, and intelligent network functions, alongside multi-agent collaboration for distributed decision-making. We explore how these methodologies align with emerging industrial IoT frameworks, ensuring seamless integration within digital manufacturing processes. Our findings emphasize the potential for improved real-time decision-making, optimizing efficiency, and reducing latency in networked control systems. The discussion addresses ethical challenges, research directions, and standardization efforts, concluding with a technology stack roadmap to guide future developments. By leveraging state-of-the-art 6G network management techniques, this research contributes to the next generation of intelligent automation solutions, bridging the gap between theoretical advancements and real-world industrial applications.

---

## Article 1
### Title@2025-05-29 (4): Distributed Federated Learning for Vehicular Network Security: Anomaly   Detection Benefits and Multi-Domain Attack Threats

**Title**: Distributed Federated Learning for Vehicular Network Security: Anomaly   Detection Benefits and Multi-Domain Attack Threats | Verteiltes Federated Learning für die Sicherheit des Vehicular Network: Anomalieerkennungsvorteile und Multi-Domain-Angriffsbedrohungen | 分布式联邦学习促进车辆网络安全:反常探测效益和多领域攻击威胁 [2505.23706v1](http://arxiv.org/abs/2505.23706v1)

**Authors**: Utku Demir, Yalin E. Sagduyu, Tugba Erpek, Hossein Jafari, Sastry Kompella, Mengran Xue

In connected and autonomous vehicles, machine learning for safety message classification has become critical for detecting malicious or anomalous behavior. However, conventional approaches that rely on centralized data collection or purely local training face limitations due to the large scale, high mobility, and heterogeneous data distributions inherent in inter-vehicle networks. To overcome these challenges, this paper explores Distributed Federated Learning (DFL), whereby vehicles collaboratively train deep learning models by exchanging model updates among one-hop neighbors and propagating models over multiple hops. Using the Vehicular Reference Misbehavior (VeReMi) Extension Dataset, we show that DFL can significantly improve classification accuracy across all vehicles compared to learning strictly with local data. Notably, vehicles with low individual accuracy see substantial accuracy gains through DFL, illustrating the benefit of knowledge sharing across the network. We further show that local training data size and time-varying network connectivity correlate strongly with the model's overall accuracy. We investigate DFL's resilience and vulnerabilities under attacks in multiple domains, namely wireless jamming and training data poisoning attacks. Our results reveal important insights into the vulnerabilities of DFL when confronted with multi-domain attacks, underlining the need for more robust strategies to secure DFL in vehicular networks.

---

## Article 2
### Title@2025-05-29 (4): Parallel GPU-Accelerated Randomized Construction of Approximate Cholesky   Preconditioners

**Title**: Parallel GPU-Accelerated Randomized Construction of Approximate Cholesky   Preconditioners | Parallele GPU-beschleunigte Randomisierte Konstruktion von ungefähren Cholesky-Vorkonditionen | 平行的GPU-加速加速旋转式建造近焦天空预设装置 [2505.02977v2](http://arxiv.org/abs/2505.02977v2)

**Authors**: Tianyu Liang, Chao Chen, Yotam Yaniv, Hengrui Luo, David Tench, Xiaoye S. Li, Aydin Buluc, James Demmel

We introduce a parallel algorithm to construct a preconditioner for solving a large, sparse linear system where the coefficient matrix is a Laplacian matrix (a.k.a., graph Laplacian). Such a linear system arises from applications such as discretization of a partial differential equation, spectral graph partitioning, and learning problems on graphs. The preconditioner belongs to the family of incomplete factorizations and is purely algebraic. Unlike traditional incomplete factorizations, the new method employs randomization to determine whether or not to keep fill-ins, i.e., newly generated nonzero elements during Gaussian elimination. Since the sparsity pattern of the randomized factorization is unknown, computing such a factorization in parallel is extremely challenging, especially on many-core architectures such as GPUs. Our parallel algorithm dynamically computes the dependency among row/column indices of the Laplacian matrix to be factorized and processes the independent indices in parallel. Furthermore, unlike previous approaches, our method requires little pre-processing time. We implemented the parallel algorithm for multi-core CPUs and GPUs, and we compare their performance to other state-of-the-art methods.

---

## Article 3
### Title@2025-05-29 (4): Complementary Time-Space Tradeoff for Self-Stabilizing Leader Election:   Polynomial States Meet Sublinear Time

**Title**: Complementary Time-Space Tradeoff for Self-Stabilizing Leader Election:   Polynomial States Meet Sublinear Time | Komplementärer Zeit-Raum-Tradeoff für selbststabilisierende Leader-Wahl: Polynome Staaten treffen auf sublineare Zeit | 自我稳定领导人选举的补充时间-空间权衡:多民族国家满足亚线性时间 [2505.23649v1](http://arxiv.org/abs/2505.23649v1)

**Authors**: Yuichi Sudo

We study the self-stabilizing leader election (SS-LE) problem in the population protocol model, assuming exact knowledge of the population size $n$. Burman, Chen, Chen, Doty, Nowak, Severson, and Xu (PODC 2021) showed that this problem can be solved in $O(n)$ expected time with $O(n)$ states. Recently, G\k{a}sieniec, Grodzicki, and Stachowiak (PODC 2025) proved that $n+O(\log n)$ states suffice to achieve $O(n \log n)$ time both in expectation and with high probability (w.h.p.). If substantially more states are available, sublinear time can be achieved. Burman~et~al.~(PODC 2021) presented a $2^{O(n^\rho\log n)}$-state SS-LE protocol with a parameter $\rho$: setting $\rho = \Theta(\log n)$ yields an optimal $O(\log n)$ time both in expectation and w.h.p., while $\rho = \Theta(1)$ results in $O(\rho\,n^{1/(\rho+1)})$ expected time. Very recently, Austin, Berenbrink, Friedetzky, G\"otte, and Hintze (PODC 2025) presented a novel SS-LE protocol parameterized by a positive integer $\rho$ with $1 \le \rho < n/2$ that solves SS-LE in $O(\frac{n}{\rho}\cdot\log n)$ time w.h.p.\ using $2^{O(\rho^2\log n)}$ states. This paper independently presents yet another time--space tradeoff of SS-LE: for any positive integer $\rho$ with $1 \le \rho \le \sqrt{n}$, SS-LE can be achieved within $O\left(\frac{n}{\rho}\cdot \log\rho\right)$ expected time using $2^{2\rho\lg\rho + O(\log n)}$ states. The proposed protocol uses significantly fewer states than the protocol of Austin~et~al.\ requires to achieve any expected stabilization time above $\Theta(\sqrt{n}\log n)$. When $\rho = \Theta\left(\frac{\log n}{\log \log n}\right)$,the proposed protocol is the first to achieve sublinear time while using only polynomially many states. A limitation of our protocol is that the constraint $\rho\le\sqrt{n}$ prevents achieving $o(\sqrt{n}\log n)$ time, whereas the protocol of Austin et~al.\ can surpass this bound.

---

## Article 4
### Title@2025-05-29 (4): Accelerated Training of Federated Learning via Second-Order Methods

**Title**: Accelerated Training of Federated Learning via Second-Order Methods | Beschleunigte Ausbildung des Föderierten Lernens über Methoden der zweiten Ordnung | 通过二级方法加快联邦学习培训 [2505.23588v1](http://arxiv.org/abs/2505.23588v1)

**Authors**: Mrinmay Sen, Sidhant R Nair, C Krishna Mohan

This paper explores second-order optimization methods in Federated Learning (FL), addressing the critical challenges of slow convergence and the excessive communication rounds required to achieve optimal performance from the global model. While existing surveys in FL primarily focus on challenges related to statistical and device label heterogeneity, as well as privacy and security concerns in first-order FL methods, less attention has been given to the issue of slow model training. This slow training often leads to the need for excessive communication rounds or increased communication costs, particularly when data across clients are highly heterogeneous. In this paper, we examine various FL methods that leverage second-order optimization to accelerate the training process. We provide a comprehensive categorization of state-of-the-art second-order FL methods and compare their performance based on convergence speed, computational cost, memory usage, transmission overhead, and generalization of the global model. Our findings show the potential of incorporating Hessian curvature through second-order optimization into FL and highlight key challenges, such as the efficient utilization of Hessian and its inverse in FL. This work lays the groundwork for future research aimed at developing scalable and efficient federated optimization methods for improving the training of the global model in FL.

---

## Article 5
### Title@2025-05-29 (4): Sustainable Carbon-Aware and Water-Efficient LLM Scheduling in   Geo-Distributed Cloud Datacenters

**Title**: Sustainable Carbon-Aware and Water-Efficient LLM Scheduling in   Geo-Distributed Cloud Datacenters | Nachhaltiges CO2-basiertes und wassereffizientes LLM-Scheeduling in Geo-verteilten Cloud-Rechenzentren | 地球分布云数据中心的可持续碳软件和水效率高的LLM [2505.23554v1](http://arxiv.org/abs/2505.23554v1)

**Authors**: Hayden Moore, Sirui Qi, Ninad Hogade, Dejan Milojicic, Cullen Bash, Sudeep Pasricha

In recent years, Large Language Models (LLM) such as ChatGPT, CoPilot, and Gemini have been widely adopted in different areas. As the use of LLMs continues to grow, many efforts have focused on reducing the massive training overheads of these models. But it is the environmental impact of handling user requests to LLMs that is increasingly becoming a concern. Recent studies estimate that the costs of operating LLMs in their inference phase can exceed training costs by 25x per year. As LLMs are queried incessantly, the cumulative carbon footprint for the operational phase has been shown to far exceed the footprint during the training phase. Further, estimates indicate that 500 ml of fresh water is expended for every 20-50 requests to LLMs during inference. To address these important sustainability issues with LLMs, we propose a novel framework called SLIT to co-optimize LLM quality of service (time-to-first token), carbon emissions, water usage, and energy costs. The framework utilizes a machine learning (ML) based metaheuristic to enhance the sustainability of LLM hosting across geo-distributed cloud datacenters. Such a framework will become increasingly vital as LLMs proliferate.

---

## Article 6
### Title@2025-05-29 (4): Accelerating AllReduce with a Persistent Straggler

**Title**: Accelerating AllReduce with a Persistent Straggler | AllReduce mit einem persistenten Straggler beschleunigen | 使用持久性斯特拉格驱动器加速全部拖动 [2505.23523v1](http://arxiv.org/abs/2505.23523v1)

**Authors**: Arjun Devraj, Eric Ding, Abhishek Vijaya Kumar, Robert Kleinberg, Rachee Singh

Distributed machine learning workloads use data and tensor parallelism for training and inference, both of which rely on the AllReduce collective to synchronize gradients or activations. However, bulk-synchronous AllReduce algorithms can be delayed by a persistent straggler that is slower to reach the synchronization barrier required to begin the collective. To address this challenge, we propose StragglAR: an AllReduce algorithm that accelerates distributed training and inference in the presence of persistent stragglers. StragglAR implements a ReduceScatter among the remaining GPUs during the straggler-induced delay, and then executes a novel collective algorithm to complete the AllReduce once the straggler reaches the synchronization barrier. StragglAR achieves a 2x theoretical speedup over popular bandwidth-efficient AllReduce algorithms (e.g., Ring) for large GPU clusters with persistent stragglers. On an 8-GPU server, our implementation of StragglAR yields a 22% speedup over state-of-the-art AllReduce algorithms.

---

## Article 7
### Title@2025-05-29 (4): SealOS+: A Sealos-based Approach for Adaptive Resource Optimization   Under Dynamic Workloads for Securities Trading System

**Title**: SealOS+: A Sealos-based Approach for Adaptive Resource Optimization   Under Dynamic Workloads for Securities Trading System | SealOS+: Ein Sealos-basierter Ansatz für adaptive Ressourcenoptimierung unter dynamischen Workloads für Securities Trading System | SealOS+:证券交易系统动态工作量下的适应性资源优化的以海路为基础的办法 [2505.23258v1](http://arxiv.org/abs/2505.23258v1)

**Authors**: Haojie Jia, Zhenhao Li, Gen Li, Minxian Xu, Kejiang Ye

As securities trading systems transition to a microservices architecture, optimizing system performance presents challenges such as inefficient resource scheduling and high service response delays. Existing container orchestration platforms lack tailored performance optimization mechanisms for trading scenarios, making it difficult to meet the stringent 50ms response time requirement imposed by exchanges. This paper introduces SealOS+, a Sealos-based performance optimization approach for securities trading, incorporating an adaptive resource scheduling algorithm leveraging deep reinforcement learning, a three-level caching mechanism for trading operations, and a Long Short-Term Memory (LSTM) based load prediction model. Real-world deployment at a securities exchange demonstrates that the optimized system achieves an average CPU utilization of 78\%, reduces transaction response time to 105ms, and reaches a peak processing capacity of 15,000 transactions per second, effectively meeting the rigorous performance and reliability demands of securities trading.

---

## Article 8
### Title@2025-05-29 (4): Smaller, Smarter, Closer: The Edge of Collaborative Generative AI

**Title**: Smaller, Smarter, Closer: The Edge of Collaborative Generative AI | Kleiner, intelligenter, enger: Der Rand der kollaborativen Generativen KI | 较小、更聪明、更近:合作创造的边缘 AI [2505.16499v2](http://arxiv.org/abs/2505.16499v2)

**Authors**: Roberto Morabito, SiYoung Jang

The rapid adoption of generative AI (GenAI), particularly Large Language Models (LLMs), has exposed critical limitations of cloud-centric deployments, including latency, cost, and privacy concerns. Meanwhile, Small Language Models (SLMs) are emerging as viable alternatives for resource-constrained edge environments, though they often lack the capabilities of their larger counterparts. This article explores the potential of collaborative inference systems that leverage both edge and cloud resources to address these challenges. By presenting distinct cooperation strategies alongside practical design principles and experimental insights, we offer actionable guidance for deploying GenAI across the computing continuum.

---

## Article 9
### Title@2025-05-29 (4): MemAscend: System Memory Optimization for SSD-Offloaded LLM Fine-Tuning

**Title**: MemAscend: System Memory Optimization for SSD-Offloaded LLM Fine-Tuning | MemAscend: Systemspeicheroptimierung für SSD-Offloaded LLM Fine-Tuning | MemAscend: SSD- 卸载 LLM 精密调试的系统内存优化 [2505.23254v1](http://arxiv.org/abs/2505.23254v1)

**Authors**: Yong-Cheng Liaw, Shuo-Han Chen

Owing to the huge success of generative artificial intelligence (AI), large language models (LLMs) have emerged as a core subclass, underpinning applications such as question answering, text generation, and code completion. While fine-tuning these models on domain-specific data can yield significant performance gains, it also poses daunting computational challenges, especially for researchers and small organizations with limited hardware resources. Although SSD offloading (i.e., ZeRO-Infinity) has emerged as a viable strategy to overcome the GPU memory barrier via leveraging both system memory (i.e., CPU DRAM) and storage space (i.e., solid-state devices, SSDs), its design primarily targets model-centric performance issues. As a result, key system-level issues, including system memory fragmentation, inefficient pinned buffer allocation, peak CPU usage spikes, and file system overhead, remain unaddressed, stifling scalability and inflating costs. Such an observation motivates this paper to introduce MemAscend, a framework that systematically tackles the underexplored system memory bottlenecks in SSD-offloaded LLM training, with a focus on resource-constrained environments. By streamlining pinned-memory allocation, eradicating fragmentation, and mitigating peak overhead, MemAscend reclaims a substantial system memory budget, enabling larger models, longer context windows, and higher batch sizes without exceeding modest hardware limits. Across diverse LLM benchmarks, MemAscend reduces peak system-memory consumption by an average of 55.7% compared with standard SSD offloading techniques, lowering the hardware barrier for fine-tuning and unlocking new possibilities for cost-effective large-scale training on limited-resource machines.

---

## Article 10
### Title@2025-05-29 (4): Edge-First Language Model Inference: Models, Metrics, and Tradeoffs

**Title**: Edge-First Language Model Inference: Models, Metrics, and Tradeoffs | Edge-First Language Model Inferenz: Modelle, Metrics und Tradeoffs | 边缘第一语言模式示范推论:模型、计量和权衡取舍 [2505.16508v2](http://arxiv.org/abs/2505.16508v2)

**Authors**: SiYoung Jang, Roberto Morabito

The widespread adoption of Language Models (LMs) across industries is driving interest in deploying these services across the computing continuum, from the cloud to the network edge. This shift aims to reduce costs, lower latency, and improve reliability and privacy. Small Language Models (SLMs), enabled by advances in model compression, are central to this shift, offering a path to on-device inference on resource-constrained edge platforms. This work examines the interplay between edge and cloud deployments, starting from detailed benchmarking of SLM capabilities on single edge devices, and extending to distributed edge clusters. We identify scenarios where edge inference offers comparable performance with lower costs, and others where cloud fallback becomes essential due to limits in scalability or model capacity. Rather than proposing a one-size-fits-all solution, we present platform-level comparisons and design insights for building efficient, adaptive LM inference systems across heterogeneous environments.

---

## Article 11
### Title@2025-05-29 (4): Ghidorah: Fast LLM Inference on Edge with Speculative Decoding and   Hetero-Core Parallelism

**Title**: Ghidorah: Fast LLM Inference on Edge with Speculative Decoding and   Hetero-Core Parallelism | Ghidorah: Schnelle LLM-Inferenz am Rand mit spekulativer Dekodierung und Hetero-Core-Parallelität | Ghidorah:快速LLM [2505.23219v1](http://arxiv.org/abs/2505.23219v1)

**Authors**: Jinhui Wei, Ye Huang, Yuhui Zhou, Jiazhi Jiang, Jiangsu Du

In-situ LLM inference on end-user devices has gained significant interest due to its privacy benefits and reduced dependency on external infrastructure. However, as the decoding process is memory-bandwidth-bound, the diverse processing units in modern end-user devices cannot be fully exploited, resulting in slow LLM inference. This paper presents Ghidorah, a LLM inference system for end-user devices with the unified memory architecture. The key idea of Ghidorah can be summarized in two steps: 1) leveraging speculative decoding approaches to enhance parallelism, and 2) ingeniously distributing workloads across multiple heterogeneous processing units to maximize computing power utilization. Ghidorah includes the hetero-core model parallelism (HCMP) architecture and the architecture-aware profiling (ARCA) approach. The HCMP architecture guides partitioning by leveraging the unified memory design of end-user devices and adapting to the hybrid computational demands of speculative decoding. The ARCA approach is used to determine the optimal speculative strategy and partitioning strategy, balancing acceptance rate with parallel capability to maximize the speedup. Additionally, we optimize sparse computation on ARM CPUs. Experimental results show that Ghidorah can achieve up to 7.6x speedup in the dominant LLM decoding phase compared to the sequential decoding approach in NVIDIA Jetson NX.

---

## Article 12
### Title@2025-05-29 (4): Improving Parallel Program Performance with LLM Optimizers via   Agent-System Interfaces

**Title**: Improving Parallel Program Performance with LLM Optimizers via   Agent-System Interfaces | Verbesserung der parallelen Programmleistung mit LLM-Optimierern über Agent-System-Schnittstellen | 通过代理-系统接口改进与LLM优化器的平行方案绩效 [2410.15625v3](http://arxiv.org/abs/2410.15625v3)

**Authors**: Anjiang Wei, Allen Nie, Thiago S. F. X. Teixeira, Rohan Yadav, Wonchan Lee, Ke Wang, Alex Aiken

Modern scientific discovery increasingly relies on high-performance computing for complex modeling and simulation. A key challenge in improving parallel program performance is efficiently mapping tasks to processors and data to memory, a process dictated by intricate, low-level system code known as mappers. Developing high-performance mappers demands days of manual tuning, posing a significant barrier for domain scientists without systems expertise. We introduce a framework that automates mapper development with generative optimization, leveraging richer feedback beyond scalar performance metrics. Our approach features the Agent-System Interface, which includes a Domain-Specific Language (DSL) to abstract away the low-level complexity of system code and define a structured search space, as well as AutoGuide, a mechanism that interprets raw execution output into actionable feedback. Unlike traditional reinforcement learning methods such as OpenTuner, which rely solely on scalar feedback, our method finds superior mappers in far fewer iterations. With just 10 iterations, it outperforms OpenTuner even after 1000 iterations, achieving 3.8X faster performance. Our approach finds mappers that surpass expert-written mappers by up to 1.34X speedup across nine benchmarks while reducing tuning time from days to minutes.

---

## Article 13
### Title@2025-05-29 (4): The Panaceas for Improving Low-Rank Decomposition in   Communication-Efficient Federated Learning

**Title**: The Panaceas for Improving Low-Rank Decomposition in   Communication-Efficient Federated Learning | Die Panaceas zur Verbesserung der Zersetzung mit geringem Rank im kommunikativ-effizienten Federated Learning | 改善通信-高效联邦学习中低-兰克分解的全景 [2505.23176v1](http://arxiv.org/abs/2505.23176v1)

**Authors**: Shiwei Li, Xiandi Luo, Haozhao Wang, Xing Tang, Shijie Xu, Weihong Luo, Yuhua Li, Xiuqiang He, Ruixuan Li

To improve the training efficiency of federated learning (FL), previous research has employed low-rank decomposition techniques to reduce communication overhead. In this paper, we seek to enhance the performance of these low-rank decomposition methods. Specifically, we focus on three key issues related to decomposition in FL: what to decompose, how to decompose, and how to aggregate. Subsequently, we introduce three novel techniques: Model Update Decomposition (MUD), Block-wise Kronecker Decomposition (BKD), and Aggregation-Aware Decomposition (AAD), each targeting a specific issue. These techniques are complementary and can be applied simultaneously to achieve optimal performance. Additionally, we provide a rigorous theoretical analysis to ensure the convergence of the proposed MUD. Extensive experimental results show that our approach achieves faster convergence and superior accuracy compared to relevant baseline methods. The code is available at https://github.com/Leopold1423/fedmud-icml25.

---

## Article 14
### Title@2025-05-29 (4): DOPPLER: Dual-Policy Learning for Device Assignment in Asynchronous   Dataflow Graphs

**Title**: DOPPLER: Dual-Policy Learning for Device Assignment in Asynchronous   Dataflow Graphs | DOPPLER: Dual-Policy-Lernen für die Gerätezuordnung in asynchronen Datenflussgraphen | DOPPLER: 同步数据流图表中设备分配的双政策学习 [2505.23131v1](http://arxiv.org/abs/2505.23131v1)

**Authors**: Xinyu Yao, Daniel Bourgeois, Abhinav Jain, Yuxin Tang, Jiawen Yao, Zhimin Ding, Arlei Silva, Chris Jermaine

We study the problem of assigning operations in a dataflow graph to devices to minimize execution time in a work-conserving system, with emphasis on complex machine learning workloads. Prior learning-based methods often struggle due to three key limitations: (1) reliance on bulk-synchronous systems like TensorFlow, which under-utilize devices due to barrier synchronization; (2) lack of awareness of the scheduling mechanism of underlying systems when designing learning-based methods; and (3) exclusive dependence on reinforcement learning, ignoring the structure of effective heuristics designed by experts. In this paper, we propose \textsc{Doppler}, a three-stage framework for training dual-policy networks consisting of 1) a $\mathsf{SEL}$ policy for selecting operations and 2) a $\mathsf{PLC}$ policy for placing chosen operations on devices. Our experiments show that \textsc{Doppler} outperforms all baseline methods across tasks by reducing system execution time and additionally demonstrates sampling efficiency by reducing per-episode training time.

---

## Article 15
### Title@2025-05-29 (4): Toward Cost-Efficient Serving of Mixture-of-Experts with Asynchrony

**Title**: Toward Cost-Efficient Serving of Mixture-of-Experts with Asynchrony | Auf dem Weg zu einem kosteneffizienten Servieren von Mixture-of-Experts mit Asynchrony | 争取以成本低效益高的方式服务专家与非同步混合服务 [2505.08944v2](http://arxiv.org/abs/2505.08944v2)

**Authors**: Shaoyu Wang, Guangrong He, Geon-Woo Kim, Yanqi Zhou, Seo Jin Park

Mixture-of-Experts (MoE) architectures offer the promise of larger model capacity without the prohibitive costs of fully dense designs. However, in real-world inference serving, load skew across experts often leads to suboptimal device utilization and excessive synchronization overheads. This paper introduces Asynchronous Expert Parallelism (AEP), a new paradigm that decouples layer execution from barrier-style synchronization. By dynamically queuing tokens at each layer (referred to as $\mu$-queuing) and adaptively re-batching them on demand, GPUs avoid waiting for straggling experts and instead continuously process whichever layer is ready. This asynchronous approach mitigates two major inefficiencies in traditional expert-parallel systems: (1) idle GPU time while waiting for the hottest expert, and (2) small-batch executions on colder experts that waste memory bandwidth.   We implement these ideas in a serving system called AMoE, which disaggregates attention from expert layers and uses a defragging scheduler to reduce batch fragmentation. Evaluations on prototype MoE models show that AMoE improves throughput by up to 2.7x compared to state-of-the-art baselines, incurring a manageable latency penalty and providing a cost-effective operating point. Furthermore, experiments demonstrate nearly linear scalability to multi-node settings, whereas the baseline system shows no throughput increase even when the number of GPUs is doubled.

---

## Article 16
### Title@2025-05-29 (4): Shortcut-connected Expert Parallelism for Accelerating   Mixture-of-Experts

**Title**: Shortcut-connected Expert Parallelism for Accelerating   Mixture-of-Experts | Shortcut-verbundene Experten-Parallelität für die Beschleunigung von Mixture-of-Experts | 加速混合专家专家专家平行专家 [2404.05019v3](http://arxiv.org/abs/2404.05019v3)

**Authors**: Weilin Cai, Juyong Jiang, Le Qin, Junwei Cui, Sunghun Kim, Jiayi Huang

Expert parallelism has emerged as a key strategy for distributing the computational workload of sparsely-gated mixture-of-experts (MoE) models across multiple devices, enabling the processing of increasingly large-scale models. However, the All-to-All communication inherent to expert parallelism poses a significant bottleneck, limiting the efficiency of MoE models. Although existing optimization methods partially mitigate this issue, they remain constrained by the sequential dependency between communication and computation operations. To address this challenge, we propose ScMoE, a novel shortcut-connected MoE architecture integrated with an overlapping parallelization strategy. ScMoE decouples communication from its conventional sequential ordering, enabling up to 100% overlap with computation. Compared to the prevalent top-2 MoE baseline, ScMoE achieves speedups of 1.49 times in training and 1.82 times in inference. Moreover, our experiments and analyses indicate that ScMoE not only achieves comparable but in some instances surpasses the model quality of existing approaches.

---

## Article 17
### Title@2025-05-29 (4): Speeding up Model Loading with fastsafetensors

**Title**: Speeding up Model Loading with fastsafetensors | Beschleunigen des Modells Beladung mit Schnellsicherern | 加速装有快速保障装置的模型加载速度 [2505.23072v1](http://arxiv.org/abs/2505.23072v1)

**Authors**: Takeshi Yoshimura, Tatsuhiro Chiba, Manish Sethi, Daniel Waddington, Swaminathan Sundararaman

The rapid increases in model parameter sizes introduces new challenges in pre-trained model loading. Currently, machine learning code often deserializes each parameter as a tensor object in host memory before copying it to device memory. We found that this approach underutilized storage throughput and significantly slowed down loading large models with a widely-used model file formats, safetensors. In this work, we present fastsafetensors, a Python library designed to optimize the deserialization of tensors in safetensors files. Our approach first copies groups of on-disk parameters to device memory, where they are directly instantiated as tensor objects. This design enables further optimization in low-level I/O and high-level tensor preprocessing, including parallelized copying, peer-to-peer DMA, and GPU offloading. Experimental results show performance improvements of 4.8x to 7.5x in loading models such as Llama (7, 13, and 70 billion parameters), Falcon (40 billion parameters), and the Bloom (176 billion parameters).

---

## Article 18
### Title@2025-05-28 (3): Profiling and optimization of multi-card GPU machine learning jobs

**Title**: Profiling and optimization of multi-card GPU machine learning jobs | Profilierung und Optimierung von Multi-Card-GPU-Maschinenlernjobs | 多卡 GPPU 机器学习工作的分析和优化 [2505.22905v1](http://arxiv.org/abs/2505.22905v1)

**Authors**: Marcin Lawenda, Kyrylo Khloponin, Krzesimir Samborski, Łukasz Szustak

The effectiveness and efficiency of machine learning methodologies are crucial, especially with respect to the quality of results and computational cost. This paper discusses different model optimization techniques, providing a comprehensive analysis of key performance indicators. Several parallelization strategies for image recognition, adapted to different hardware and software configurations, including distributed data parallelism and distributed hardware processing, are analyzed. Selected optimization strategies are studied in detail, highlighting the related challenges and advantages of their implementation. Furthermore, the impact of different performance improvement techniques (DPO, LoRA, QLoRA, and QAT) on the tuning process of large language models is investigated. Experimental results illustrate how the nature of the task affects the iteration time in a multiprocessor environment, VRAM utilization, and overall memory transfers. Test scenarios are evaluated on the modern NVIDIA H100 GPU architecture.

---

## Article 19
### Title@2025-05-28 (3): Visualizing Cloud-native Applications with KubeDiagrams

**Title**: Visualizing Cloud-native Applications with KubeDiagrams | Cloud-native Anwendungen mit KubeDiagrammen visualisieren | 带有KubeDiagrams 的可视化云源应用 [2505.22879v1](http://arxiv.org/abs/2505.22879v1)

**Authors**: Philippe Merle, Fabio Petrillo

Modern distributed applications increasingly rely on cloud-native platforms to abstract the complexity of deployment and scalability. As the de facto orchestration standard, Kubernetes enables this abstraction, but its declarative configuration model makes the architectural understanding difficult. Developers, operators, and architects struggle to form accurate mental models from raw manifests, Helm charts, or cluster state descriptions. We introduce KubeDiagrams, an open-source tool that transforms Kubernetes manifests into architecture diagrams. By grounding our design in a user-centered study of real-world visualization practices, we identify the specific challenges Kubernetes users face and map these to concrete design requirements. KubeDiagrams integrates seamlessly with standard Kubernetes artifacts, preserves semantic fidelity to core concepts, and supports extensibility and automation. We detail the tool's architecture, visual encoding strategies, and extensibility mechanisms. Three case studies illustrate how KubeDiagrams enhances system comprehension and supports architectural reasoning in distributed cloud-native systems. KubeDiagrams addresses concrete pain points in Kubernetes-based DevOps practices and is valued for its automation, clarity, and low-friction integration into real-world tooling environments.

---

## Article 20
### Title@2025-05-28 (3): The National Research Platform: Stretched, Multi-Tenant, Scientific   Kubernetes Cluster

**Title**: The National Research Platform: Stretched, Multi-Tenant, Scientific   Kubernetes Cluster | Die Nationale Forschungsplattform: Streckiger, Multi-Tenant-Cluster, wissenschaftlicher Kubernetes-Cluster | 国家研究平台:延伸、多层、多层、科学库伯涅茨集群 [2505.22864v1](http://arxiv.org/abs/2505.22864v1)

**Authors**: Derek Weitzel, Ashton Graves, Sam Albin, Huijun Zhu, Frank Würthwein, Mahidhar Tatineni, Dmitry Mishin, John Graham, Elham E Khoda, Mohammad Firas Sada, Larry Smarr, Thomas DeFanti

The National Research Platform (NRP) represents a distributed, multi-tenant Kubernetes-based cyberinfrastructure designed to facilitate collaborative scientific computing. Spanning over 75 locations in the U.S. and internationally, the NRP uniquely integrates varied computational resources, ranging from single nodes to extensive GPU and CPU clusters, to support diverse research workloads including advanced AI and machine learning tasks. It emphasizes flexibility through user-friendly interfaces such as JupyterHub and low level control of resources through direct Kubernetes interaction. Critical operational insights are discussed, including security enhancements using Kubernetes-integrated threat detection, extensive monitoring, and comprehensive accounting systems. This paper highlights the NRP's growing importance and scalability in addressing the increasing demands for distributed scientific computational resources.

---

## Article 21
### Title@2025-05-28 (3): $Δ$-Nets: Interaction-Based System for Optimal Parallel   $λ$-Reduction

**Title**: $Δ$-Nets: Interaction-Based System for Optimal Parallel   $λ$-Reduction | $Δ$-Nets: Interaktionsbasiertes System für eine optimale parallele $λ$-Reduktion | $$-净额:最佳平行互动系统$$$美元-削减 [2505.20314v2](http://arxiv.org/abs/2505.20314v2)

**Authors**: Daniel Augusto Rizzi Salvadori

I present a model of universal parallel computation called $\Delta$-Nets, and a method to translate $\lambda$-terms into $\Delta$-nets and back. Together, the model and the method constitute an algorithm for optimal parallel $\lambda$-reduction, solving the longstanding enigma with groundbreaking clarity. I show that the $\lambda$-calculus can be understood as a projection of $\Delta$-Nets -- one that severely restricts the structure of sharing, among other drawbacks. Unhindered by these restrictions, the $\Delta$-Nets model opens the door to new highly parallel programming language implementations and computer architectures that are more efficient and performant than previously possible.

---

## Article 22
### Title@2025-05-28 (3): Smart Contracts for SMEs and Large Companies

**Title**: Smart Contracts for SMEs and Large Companies | Intelligente Verträge für KMU und Großunternehmen | 中小企业和大公司的智能合同 [2505.22619v1](http://arxiv.org/abs/2505.22619v1)

**Authors**: C. G. Liu, P. Bodorik, D. Jutla

Research on blockchains addresses multiple issues, with one being writing smart contracts. In our previous research we described methodology and a tool to generate, in automated fashion, smart contracts from BPMN models. The generated smart contracts provide support for multi-step transactions that facilitate repair/upgrade of smart contracts. In this paper we show how the approach is used to support collaborations via smart contracts for companies ranging from SMEs with little IT capabilities to companies with IT using blockchain smart contracts. Furthermore, we also show how the approach is used for certain applications to generate smart contracts by a BPMN modeler who does not need any knowledge of blockchain technology or smart contract development - thus we are hoping to facilitate democratization of smart contracts and blockchain technology.

---

## Article 23
### Title@2025-05-28 (3): Pilot-Quantum: A Quantum-HPC Middleware for Resource, Workload and Task   Management

**Title**: Pilot-Quantum: A Quantum-HPC Middleware for Resource, Workload and Task   Management | Pilot-Quantum: Eine Quantum-HPC Middleware für Ressourcen-, Workload- und Task-Management | 试点量子:资源、工作量和任务管理的量子-氢氯氟烃中软件 [2412.18519v3](http://arxiv.org/abs/2412.18519v3)

**Authors**: Pradeep Mantha, Florian J. Kiwit, Nishant Saurabh, Shantenu Jha, Andre Luckow

As quantum hardware advances, integrating quantum processing units (QPUs) into HPC environments and managing diverse infrastructure and software stacks becomes increasingly essential. Pilot-Quantum addresses these challenges as a middleware designed to provide unified application-level management of resources and workloads across hybrid quantum-classical environments. It is built on a rigorous analysis of existing quantum middleware systems and application execution patterns. It implements the Pilot Abstraction conceptual model, originally developed for HPC, to manage resources, workloads, and tasks. It is designed for quantum applications that rely on task parallelism, including (i) hybrid algorithms, such as variational approaches, and (ii) circuit cutting systems, used to partition and execute large quantum circuits. Pilot-Quantum facilitates seamless integration of QPUs, classical CPUs, and GPUs, while supporting high-level programming frameworks like Qiskit and Pennylane. This enables users to efficiently design and execute hybrid workflows across diverse computing resources. The capabilities of Pilot-Quantum are demonstrated through mini-apps -- simplified yet representative kernels focusing on critical performance bottlenecks. We demonstrate the capabilities of Pilot-Quantum through multiple mini-apps, including different circuit executions (e.g., using IBM\'s Eagle QPU and simulators), circuit cutting, and quantum machine learning scenarios.

---

## Article 24
### Title@2025-05-28 (3): Morpheus Consensus: Excelling on trails and autobahns

**Title**: Morpheus Consensus: Excelling on trails and autobahns | Morpheus Consensus: Excelling auf Trails und Autobahnen | Morpheus共识:关于足迹和自动铢的Excelling [2502.08465v2](http://arxiv.org/abs/2502.08465v2)

**Authors**: Andrew Lewis-Pye, Ehud Shapiro

Recent research in consensus has often focussed on protocols for State-Machine-Replication (SMR) that can handle high throughputs. Such state-of-the-art protocols (generally DAG-based) induce undue overhead when the needed throughput is low, or else exhibit unnecessarily-poor latency and communication complexity during periods of low throughput.   Here we present Morpheus Consensus, which naturally morphs from a quiescent low-throughput leaderless blockchain protocol to a high-throughput leader-based DAG protocol and back, excelling in latency and complexity in both settings. During high-throughout, Morpheus pars with state-of-the-art DAG-based protocols, including Autobahn. During low-throughput, Morpheus exhibits competitive complexity and lower latency than standard protocols such as PBFT and Tendermint, which in turn do not perform well during high-throughput.   The key idea of Morpheus is that as long as blocks do not conflict (due to Byzantine behaviour, network delays, or high-throughput simultaneous production) it produces a forkless blockchain, promptly finalizing each block upon arrival. It assigns a leader only if one is needed to resolve conflicts, in a manner and with performance not unlike Autobahn.

---

## Article 25
### Title@2025-05-28 (3): Grassroots Federation: Fair Governance of Large-Scale, Decentralized,   Sovereign Digital Communities

**Title**: Grassroots Federation: Fair Governance of Large-Scale, Decentralized,   Sovereign Digital Communities | Grassroots Federation: Faire Governance der großen, dezentralisierten, Souveränen Digitalen Gemeinschaften | 基层联合会:大、分散、主权数字共同体的公平治理 [2505.02208v4](http://arxiv.org/abs/2505.02208v4)

**Authors**: Ehud Shapiro, Nimrod Talmon

Grassroots Federation aims to address the egalitarian formation and the fair democratic governance of large-scale, decentralized, sovereign digital communities, the size of the EU, the US, existing social networks, and even humanity at large. A grassroots federation evolves via the grassroots formation of digital communities and their consensual federation. Such digital communities may form according to geography, jurisdiction, affiliations, relations, interests, causes, and more. Small communities (say up to $100$ members) govern themselves; larger communities -- no matter how large -- are governed by a similarly-small assembly elected by sortition among its members.   Earlier work on Grassroots Democratic Federation explored the fair sortition of the assemblies of a federation in a static setting: Given a federation, populate its assemblies with members satisfying ex ante and ex post fairness conditions on the participation of members of a community in its assembly, and on the representation of child communities in the assembly of their parent community.   In practice, we expect a grassroots democratic federation to grow and evolve dynamically and in all directions -- bottom-up, top-down, and middle-out. To address that, we formally specify this dynamic setting and adapt the static fairness conditions to it: The ex post condition on the fair representation of a child community becomes a condition that must always hold; the ex ante conditions in expectation on the fair participation of an individual and on the fair representation of a child community become conditions satisfied in actuality in the limit, provided the federation structure eventually stabilizes. We then present a protocol that satisfies these fairness conditions.

---

## Article 26
### Title@2025-05-28 (3): Broadcast in Almost Mixing Time

**Title**: Broadcast in Almost Mixing Time | In fast mischender Zeit übertragen | 几乎混合时间的广播 [2502.02165v2](http://arxiv.org/abs/2502.02165v2)

**Authors**: Anton Paramonov, Roger Wattenhofer

We study the problem of broadcasting multiple messages in the CONGEST model. In this problem, a dedicated source node $s$ possesses a set $M$ of messages with every message of size $O(\log n)$ where $n$ is the total number of nodes. The objective is to ensure that every node in the network learns all messages in $M$. The execution of an algorithm progresses in rounds, and we focus on optimizing the round complexity of broadcasting multiple messages.   Our primary contribution is a randomized algorithm for networks with expander topology, which are widely used in practice for building scalable and robust distributed systems. The algorithm succeeds with high probability and achieves a round complexity that is optimal up to a factor of the network's mixing time and polylogarithmic terms. It leverages a multi-COBRA primitive, which uses multiple branching random walks running in parallel. To the best of our knowledge, this approach has not been applied in distributed algorithms before. A crucial aspect of our method is the use of these branching random walks to construct an optimal (up to a polylogarithmic factor) tree packing of a random graph, which is then used for efficient broadcasting. This result is of independent interest.   We also prove the problem to be NP-hard in a centralized setting and provide insights into why straightforward lower bounds for general graphs, namely graph diameter and $\frac{|M|}{\textit{minCut}}$, cannot be tight.

---

## Article 27
### Title@2025-05-28 (3): Inclusive, Differentially Private Federated Learning for Clinical Data

**Title**: Inclusive, Differentially Private Federated Learning for Clinical Data | Inklusives, differenziert privates Federated Learning für klinische Daten | 包容性、差异化私联校临床数据学习 [2505.22108v1](http://arxiv.org/abs/2505.22108v1)

**Authors**: Santhosh Parampottupadam, Melih Coşğun, Sarthak Pati, Maximilian Zenk, Saikat Roy, Dimitrios Bounias, Benjamin Hamm, Sinem Sav, Ralf Floca, Klaus Maier-Hein

Federated Learning (FL) offers a promising approach for training clinical AI models without centralizing sensitive patient data. However, its real-world adoption is hindered by challenges related to privacy, resource constraints, and compliance. Existing Differential Privacy (DP) approaches often apply uniform noise, which disproportionately degrades model performance, even among well-compliant institutions. In this work, we propose a novel compliance-aware FL framework that enhances DP by adaptively adjusting noise based on quantifiable client compliance scores. Additionally, we introduce a compliance scoring tool based on key healthcare and security standards to promote secure, inclusive, and equitable participation across diverse clinical settings. Extensive experiments on public datasets demonstrate that integrating under-resourced, less compliant clinics with highly regulated institutions yields accuracy improvements of up to 15% over traditional FL. This work advances FL by balancing privacy, compliance, and performance, making it a viable solution for real-world clinical workflows in global healthcare.

---

## Article 28
### Title@2025-05-28 (3): A Stochastic Approximation Approach for Efficient Decentralized   Optimization on Random Networks

**Title**: A Stochastic Approximation Approach for Efficient Decentralized   Optimization on Random Networks | Ein stochastischer Annäherungsansatz für eine effiziente dezentralisierte Optimierung von Random Networks | 随机网络高效分散优化优化的斯托卡接近方法 [2410.18774v2](http://arxiv.org/abs/2410.18774v2)

**Authors**: Chung-Yiu Yau, Haoming Liu, Hoi-To Wai

A challenging problem in decentralized optimization is to develop algorithms with fast convergence on random and time varying topologies under unreliable and bandwidth-constrained communication network. This paper studies a stochastic approximation approach with a Fully Stochastic Primal Dual Algorithm (FSPDA) framework. Our framework relies on a novel observation that randomness in time varying topology can be incorporated in a stochastic augmented Lagrangian formulation, whose expected value admits saddle points that coincide with stationary solutions of the decentralized optimization problem. With the FSPDA framework, we develop two new algorithms supporting efficient sparsified communication on random time varying topologies -- FSPDA-SA allows agents to execute multiple local gradient steps depending on the time varying topology to accelerate convergence, and FSPDA-STORM further incorporates a variance reduction step to improve sample complexity. For problems with smooth (possibly non-convex) objective function, within $T$ iterations, we show that FSPDA-SA (resp. FSPDA-STORM) finds an $\mathcal{O}( 1/\sqrt{T} )$-stationary (resp. $\mathcal{O}( 1/T^{2/3} )$) solution. Numerical experiments show the benefits of the FSPDA algorithms.

---

## Article 29
### Title@2025-05-28 (3): Towards Efficient Key-Value Cache Management for Prefix Prefilling in   LLM Inference

**Title**: Towards Efficient Key-Value Cache Management for Prefix Prefilling in   LLM Inference | Effizientes Key-Value-Cache-Management für die Präfixvorfüllung in LLM-Inferenz | 建立高效的键值缓存管理,用于在LLM 推理中预填前补全 [2505.21919v1](http://arxiv.org/abs/2505.21919v1)

**Authors**: Yue Zhu, Hao Yu, Chen Wang, Zhuoran Liu, Eun Kyung Lee

The increasing adoption of large language models (LLMs) with extended context windows necessitates efficient Key-Value Cache (KVC) management to optimize inference performance. Inference workloads like Retrieval-Augmented Generation (RAG) and agents exhibit high cache reusability, making efficient caching critical to reducing redundancy and improving speed. We analyze real-world KVC access patterns using publicly available traces and evaluate commercial key-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1] and Sherman [2]) for KVC metadata management. Our work demonstrates the lack of tailored storage solution for KVC prefilling, underscores the need for an efficient distributed caching system with optimized metadata management for LLM workloads, and provides insights into designing improved KVC management systems for scalable, low-latency inference.

---

## Article 30
### Title@2025-05-28 (3): Joint$λ$: Orchestrating Serverless Workflows on Jointcloud FaaS   Systems

**Title**: Joint$λ$: Orchestrating Serverless Workflows on Jointcloud FaaS   Systems | Joint$λ$: Orchestrierung serverloser Workflows auf Jointcloud FaaS-Systemen | 联合 $ $: 联合COLOUD FaaS系统无服务器工作流管 [2505.21899v1](http://arxiv.org/abs/2505.21899v1)

**Authors**: Jianfei Liu, Rui Li, Zhilin Yang, Peichang Shi, Guodong Yi, Huaimin Wang

Existing serverless workflow orchestration systems are predominantly designed for a single-cloud FaaS system, leading to vendor lock-in. This restricts performance optimization, cost reduction, and availability of applications. However, orchestrating serverless workflows on Jointcloud FaaS systems faces two main challenges: 1) Additional overhead caused by centralized cross-cloud orchestration; and 2) A lack of reliable failover and fault-tolerant mechanisms for cross-cloud serverless workflows. To address these challenges, we propose Joint$\lambda$, a distributed runtime system designed to orchestrate serverless workflows on multiple FaaS systems without relying on a centralized orchestrator. Joint$\lambda$ introduces a compatibility layer, Backend-Shim, leveraging inter-cloud heterogeneity to optimize makespan and reduce costs with on-demand billing. By using function-side orchestration instead of centralized nodes, it enables independent function invocations and data transfers, reducing cross-cloud communication overhead. For high availability, it ensures exactly-once execution via datastores and failover mechanisms for serverless workflows on Jointcloud FaaS systems. We validate Joint$\lambda$ on two heterogeneous FaaS systems, AWS and ALiYun, with four workflows. Compared to the most advanced commercial orchestration services for single-cloud serverless workflows, Joint$\lambda$ reduces up to 3.3$\times$ latency, saving up to 65\% cost. Joint$\lambda$ is also faster than the state-of-the-art orchestrators for cross-cloud serverless workflows up to 4.0$\times$, reducing up to 4.5$\times$ cost and providing strong execution guarantees.

---

## Article 31
### Title@2025-05-28 (3): Hybrid Batch Normalisation: Resolving the Dilemma of Batch Normalisation   in Federated Learning

**Title**: Hybrid Batch Normalisation: Resolving the Dilemma of Batch Normalisation   in Federated Learning | Hybride Batch-Normalisierung: Lösung des Dilemmas der Batch-Normalisierung im Federated Learning | 混合批次正常化:解决联邦学习中批次正常化的难题 [2505.21877v1](http://arxiv.org/abs/2505.21877v1)

**Authors**: Hongyao Chen, Tianyang Xu, Xiaojun Wu, Josef Kittler

Batch Normalisation (BN) is widely used in conventional deep neural network training to harmonise the input-output distributions for each batch of data. However, federated learning, a distributed learning paradigm, faces the challenge of dealing with non-independent and identically distributed data among the client nodes. Due to the lack of a coherent methodology for updating BN statistical parameters, standard BN degrades the federated learning performance. To this end, it is urgent to explore an alternative normalisation solution for federated learning. In this work, we resolve the dilemma of the BN layer in federated learning by developing a customised normalisation approach, Hybrid Batch Normalisation (HBN). HBN separates the update of statistical parameters (i.e. , means and variances used for evaluation) from that of learnable parameters (i.e. , parameters that require gradient updates), obtaining unbiased estimates of global statistical parameters in distributed scenarios. In contrast with the existing solutions, we emphasise the supportive power of global statistics for federated learning. The HBN layer introduces a learnable hybrid distribution factor, allowing each computing node to adaptively mix the statistical parameters of the current batch with the global statistics. Our HBN can serve as a powerful plugin to advance federated learning performance. It reflects promising merits across a wide range of federated learning settings, especially for small batch sizes and heterogeneous data.

---

## Article 32
### Title@2025-05-28 (3): gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM   Serving with Token Throttling

**Title**: gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM   Serving with Token Throttling | gLLM: Global Balanced Pipeline Parallelism System für verteiltes LLM Serving mit Token Throttling | gLLM:全球平衡管道平行系统 [2504.14775v2](http://arxiv.org/abs/2504.14775v2)

**Authors**: Tianyu Guo, Xianwei Zhang, Jiangsu Du, Zhiguang Chen, Nong Xiao, Yutong Lu

Pipeline parallelism has emerged as a predominant approach for deploying large language models (LLMs) across distributed nodes, owing to its lower communication overhead compared to tensor parallelism. While demonstrating high throughput in request serving, pipeline parallelism often suffers from performance limitations caused by pipeline bubbles, which are primarily resulted from imbalanced computation delays across batches. Existing methods like Sarathi-Serve attempt to address this through hybrid scheduling of chunked prefill and decode tokens using a fixed token budget. However, such methods may experience significant fluctuations due to either insufficient prefill tokens or uneven distribution of decode tokens, ultimately leading to computational imbalance. To overcome these inefficiencies, we present gLLM, a globally balanced pipeline parallelism system incorporating Token Throttling to effectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a fine-grained scheduling policy that independently regulates the quantities of prefill and decode tokens, thus enabling balanced computation by leveraging global information from the inference system. Specifically, for decode tokens, gLLM maintains near-consistent token count across processing batches. For prefill tokens, it dynamically adjusts batch sizes based on both total pending tokens and the memory utilization rates of key-value cache (KV cache). Furthermore, gLLM runtime adopts an asynchronous execution and message passing architecture specifically optimized for pipeline parallelism characteristics. Experimental evaluations with representative LLMs show that gLLM achieves significant performance improvements, delivering 11% to 398% higher maximum throughput compared to state-of-the-art pipeline or tensor parallelism systems, while simultaneously maintaining lower latency.

---

## Article 33
### Title@2025-05-27 (2): Empowering Scientific Workflows with Federated Agents

**Title**: Empowering Scientific Workflows with Federated Agents | Stärkung wissenschaftlicher Workflows mit Federated Agents | 赋予联邦药剂部门科学工作流程权能 [2505.05428v2](http://arxiv.org/abs/2505.05428v2)

**Authors**: J. Gregory Pauloski, Yadu Babuji, Ryan Chard, Mansi Sakarvadia, Kyle Chard, Ian Foster

Agentic systems, in which diverse agents cooperate to tackle challenging problems, are exploding in popularity in the AI community. However, the agentic frameworks used to build these systems have not previously enabled use with research cyberinfrastructure. Here we introduce Academy, a modular and extensible middleware designed to deploy autonomous agents across the federated research ecosystem, including HPC systems, experimental facilities, and data repositories. To meet the demands of scientific computing, Academy supports asynchronous execution, heterogeneous resources, high-throughput data flows, and dynamic resource availability. It provides abstractions for expressing stateful agents, managing inter-agent coordination, and integrating computation with experimental control. We present microbenchmark results that demonstrate high performance and scalability in HPC environments. To demonstrate the breadth of applications that can be supported by agentic workflow designs, we also present case studies in materials discovery, decentralized learning, and information extraction in which agents are deployed across diverse HPC systems.

---

## Article 34
### Title@2025-05-27 (2): LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in   Multimodal Large Language Models

**Title**: LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in   Multimodal Large Language Models | LV-XAttn: Verteilte Cross-Attention für lange visuelle Eingänge in multimodalen großen Sprachmodellen | LV-XAttn:多式大语言模型中长视输入分布式交叉注意 [2502.02406v3](http://arxiv.org/abs/2502.02406v3)

**Authors**: Tzu-Tao Chang, Shivaram Venkataraman

Cross-attention is commonly adopted in multimodal large language models (MLLMs) for integrating visual information into the language backbone. However, in applications with large visual inputs, such as video understanding, processing a large number of visual tokens in cross-attention layers leads to high memory demands and often necessitates distributed computation across multiple GPUs. Existing distributed attention mechanisms face significant communication overheads, making cross-attention layers a critical bottleneck for efficient training and inference of MLLMs. To address this, we propose LV-XAttn, a distributed, exact cross-attention mechanism with minimal communication overhead. We observe that in applications involving large visual inputs, the size of the query block is typically much smaller than that of the key-value blocks. Thus, in LV-XAttn we keep the large key-value blocks locally on each GPU and exchange smaller query blocks across GPUs. We also introduce an efficient activation recomputation technique to support longer visual context. We theoretically analyze the communication benefits of LV-XAttn and show that it can achieve speedups for a wide range of models. Our evaluations with Llama 3-V, mPLUG-Owl3 and OpenFlamingo models find that LV-XAttn achieves up to 10.62$\times$ end-to-end speedup compared to existing approaches.

---

## Article 35
### Title@2025-05-27 (2): Power-Capping Metric Evaluation for Improving Energy Efficiency

**Title**: Power-Capping Metric Evaluation for Improving Energy Efficiency | Leistungskapitulation Metric-Evaluierung zur Verbesserung der Energieeffizienz | 提高能源效率提高能源使用效率的节能计量评价 [2505.21758v1](http://arxiv.org/abs/2505.21758v1)

**Authors**: Maria Patrou, Thomas Wang, Wael Elwasif, Markus Eisenbach, Ross Miller, William Godoy, Oscar Hernandez

With high-performance computing systems now running at exascale, optimizing power-scaling management and resource utilization has become more critical than ever. This paper explores runtime power-capping optimizations that leverage integrated CPU-GPU power management on architectures like the NVIDIA GH200 superchip. We evaluate energy-performance metrics that account for simultaneous CPU and GPU power-capping effects by using two complementary approaches: speedup-energy-delay and a Euclidean distance-based multi-objective optimization method. By targeting a mostly compute-bound exascale science application, the Locally Self-Consistent Multiple Scattering (LSMS), we explore challenging scenarios to identify potential opportunities for energy savings in exascale applications, and we recognize that even modest reductions in energy consumption can have significant overall impacts. Our results highlight how GPU task-specific dynamic power-cap adjustments combined with integrated CPU-GPU power steering can improve the energy utilization of certain GPU tasks, thereby laying the groundwork for future adaptive optimization strategies.

---

## Article 36
### Title@2025-05-27 (2): FedCostAware: Enabling Cost-Aware Federated Learning on the Cloud

**Title**: FedCostAware: Enabling Cost-Aware Federated Learning on the Cloud | FedCostAware: Kostenbewusstes Lernen in der Cloud ermöglichen | FestAware:在云上进行成本-软件联合学习 [2505.21727v1](http://arxiv.org/abs/2505.21727v1)

**Authors**: Aditya Sinha, Zilinghan Li, Tingkai Liu, Volodymyr Kindratenko, Kibaek Kim, Ravi Madduri

Federated learning (FL) is a distributed machine learning (ML) approach that allows multiple clients to collaboratively train ML model without exchanging their original training data, offering a solution that is particularly valuable in sensitive domains such as biomedicine. However, training robust FL models often requires substantial computing resources from participating clients, such as GPUs, which may not be readily available at institutions such as hospitals. While cloud platforms (e.g., AWS) offer on-demand access to such resources, their usage can incur significant costs, particularly in distributed training scenarios where poor coordination strategies can lead to substantial resource wastage. To address this, we introduce FedCostAware, a cost-aware scheduling algorithm designed to optimize synchronous FL on cloud spot instances. FedCostAware addresses the challenges of training on spot instances and different client budgets by employing intelligent management of the lifecycle of spot instances. This approach minimizes resource idle time and overall expenses. Comprehensive experiments across multiple datasets demonstrate that FedCostAware significantly reduces cloud computing costs compared to conventional spot and on-demand schemes, enhancing the accessibility and affordability of FL.

---

## Article 37
### Title@2025-05-27 (2): AMSFL: Adaptive Multi-Step Federated Learning via Gradient   Difference-Based Error Modeling

**Title**: AMSFL: Adaptive Multi-Step Federated Learning via Gradient   Difference-Based Error Modeling | AMSFL: Adaptives Multi-Step-Federated Learning über gradient Difference-based Error Modeling | ASFL:通过基于差异的渐进错误建模进行适应性多阶段联邦学习 [2505.21695v1](http://arxiv.org/abs/2505.21695v1)

**Authors**: Ganglou Xu

Federated learning faces critical challenges in balancing communication efficiency and model accuracy. One key issue lies in the approximation of update errors without incurring high computational costs. In this paper, we propose a lightweight yet effective method called Gradient Difference Approximation (GDA), which leverages first-order information to estimate local error trends without computing the full Hessian matrix. The proposed method forms a key component of the Adaptive Multi-Step Federated Learning (AMSFL) framework and provides a unified error modeling strategy for large-scale multi-step adaptive training environments.

---

## Article 38
### Title@2025-05-27 (2): Incentivizing Permissionless Distributed Learning of LLMs

**Title**: Incentivizing Permissionless Distributed Learning of LLMs | Anreize für das unbefugte Lernen von LLMs | 激励对LLMM的无自由分配的学习 [2505.21684v1](http://arxiv.org/abs/2505.21684v1)

**Authors**: Joel Lidin, Amir Sarfi, Evangelos Pappas, Samuel Dare, Eugene Belilovsky, Jacob Steeves

We describe an incentive system for distributed deep learning of foundational models where peers are rewarded for contributions. The incentive system, \textit{Gauntlet}, has been deployed on the bittensor blockchain and used to train a 1.2B LLM with completely permissionless contributions of pseudo-gradients: no control over the users that can register or their hardware. \textit{Gauntlet} can be applied to any synchronous distributed training scheme that relies on aggregating updates or pseudo-gradients. We rely on a two-stage mechanism for fast filtering of peer uptime, reliability, and synchronization, combined with the core component that estimates the loss before and after individual pseudo-gradient contributions. We utilized an OpenSkill rating system to track competitiveness of pseudo-gradient scores across time. Finally, we introduce a novel mechanism to ensure peers on the network perform unique computations. Our live 1.2B run, which has paid out real-valued tokens to participants based on the value of their contributions, yielded a competitive (on a per-iteration basis) 1.2B model that demonstrates the utility of our incentive system.

---

## Article 39
### Title@2025-05-27 (2): KPerfIR: Towards an Open and Compiler-centric Ecosystem for GPU Kernel   Performance Tooling on Modern AI Workloads

**Title**: KPerfIR: Towards an Open and Compiler-centric Ecosystem for GPU Kernel   Performance Tooling on Modern AI Workloads | KPerfIR: Auf dem Weg zu einem offenen und kompilerzentrierten Ökosystem für GPU-Kernel Performance Tooling auf modernen KI-Workloads | KPerfIR:努力建立一个开放的、以编纂者为中心的生态系统,用于在现代AI 工作负荷上使用 GPU 内核性能工具 [2505.21661v1](http://arxiv.org/abs/2505.21661v1)

**Authors**: Yue Guan, Yuanwei Fang, Keren Zhou, Corbin Robeck, Manman Ren, Zhongkai Yu, Yufei Ding, Adnan Aziz

In this work, we propose KPerfIR, a novel multilevel compiler-centric infrastructure to enable the development of customizable, extendable, and portable profiling tools tailored for modern artificial intelligence (AI) workloads on modern GPUs. Our approach integrates profiling capabilities directly into the compiler workflow, allowing profiling functionalities to be implemented as compiler passes, offering a programmable and reusable framework for performance analysis. This design bridges the gap between compilers and profilers, enabling fine-grained insights into complex optimization challenges such as overlapping the execution of fine-grained function units on GPUs. KPerfIR is integrated into the Triton infrastructure to highlight the power of a compiler-centric approach to advance performance analysis and optimization in the ever-evolving landscape of AI compilers. Our evaluation shows that our tool incurs low overhead (8.2%), provides accurate measurements (2% relative error), and delivers actionable insights into complicated GPU intra-kernel optimizations.

---

## Article 40
### Title@2025-05-27 (2): Fast and Cost-effective Speculative Edge-Cloud Decoding with Early Exits

**Title**: Fast and Cost-effective Speculative Edge-Cloud Decoding with Early Exits | Schnelle und kostengünstige spekulative Edge-Cloud-Dekodierung mit Early Exits | 快速和成本效益高的投机性边缘-封闭式排污与早期出口 [2505.21594v1](http://arxiv.org/abs/2505.21594v1)

**Authors**: Yeshwanth Venkatesha, Souvik Kundu, Priyadarshini Panda

Large Language Models (LLMs) enable various applications on edge devices such as smartphones, wearables, and embodied robots. However, their deployment often depends on expensive cloud-based APIs, creating high operational costs, which limit access for smaller organizations and raise sustainability concerns. Certain LLMs can be deployed on-device, offering a cost-effective solution with reduced latency and improved privacy. Yet, limited computing resources constrain the size and accuracy of models that can be deployed, necessitating a collaborative design between edge and cloud. We propose a fast and cost-effective speculative edge-cloud decoding framework with a large target model on the server and a small draft model on the device. By introducing early exits in the target model, tokens are generated mid-verification, allowing the client to preemptively draft subsequent tokens before final verification, thus utilizing idle time and enhancing parallelism between edge and cloud. Using an NVIDIA Jetson Nano (client) and an A100 GPU (server) with Vicuna-68M (draft) and Llama2-7B (target) models, our method achieves up to a 35% reduction in latency compared to cloud-based autoregressive decoding, with an additional 11% improvement from preemptive drafting. To demonstrate real-world applicability, we deploy our method on the Unitree Go2 quadruped robot using Vision-Language Model (VLM) based control, achieving a 21% speedup over traditional cloud-based autoregressive decoding. These results demonstrate the potential of our framework for real-time LLM and VLM applications on resource-constrained edge devices.

---

## Article 41
### Title@2025-05-27 (2): Distributed Discrete Morse Sandwich: Efficient Computation of   Persistence Diagrams for Massive Scalar Data

**Title**: Distributed Discrete Morse Sandwich: Efficient Computation of   Persistence Diagrams for Massive Scalar Data | Distributed Diskrete Morse Sandwich: Effiziente Berechnung von Persistenzdiagrammen für massive Scalardaten | 分布式分散的莫尔斯桑威奇:有效计算大规模卡路里数据持久性图图 [2505.21266v1](http://arxiv.org/abs/2505.21266v1)

**Authors**: Eve Le Guillou, Pierre Fortin, Julien Tierny

The persistence diagram, which describes the topological features of a dataset, is a key descriptor in Topological Data Analysis. The "Discrete Morse Sandwich" (DMS) method has been reported to be the most efficient algorithm for computing persistence diagrams of 3D scalar fields on a single node, using shared-memory parallelism. In this work, we extend DMS to distributed-memory parallelism for the efficient and scalable computation of persistence diagrams for massive datasets across multiple compute nodes. On the one hand, we can leverage the embarrassingly parallel procedure of the first and most time-consuming step of DMS (namely the discrete gradient computation). On the other hand, the efficient distributed computations of the subsequent DMS steps are much more challenging. To address this, we have extensively revised the DMS routines by contributing a new self-correcting distributed pairing algorithm, redesigning key data structures and introducing computation tokens to coordinate distributed computations. We have also introduced a dedicated communication thread to overlap communication and computation. Detailed performance analyses show the scalability of our hybrid MPI+thread approach for strong and weak scaling using up to 16 nodes of 32 cores (512 cores total). Our algorithm outperforms DIPHA, a reference method for the distributed computation of persistence diagrams, with an average speedup of x8 on 512 cores. We show the practical capabilities of our approach by computing the persistence diagram of a public 3D scalar field of 6 billion vertices in 174 seconds on 512 cores. Finally, we provide a usage example of our open-source implementation at https://github.com/eve-le-guillou/DDMS-example.

---

## Article 42
### Title@2025-05-27 (2): DeepCEE: Efficient Cross-Region Model Distributed Training System under   Heterogeneous GPUs and Networks

**Title**: DeepCEE: Efficient Cross-Region Model Distributed Training System under   Heterogeneous GPUs and Networks | DeepCEE: Effizientes regionsübergreifendes Schulungssystem unter heterogenen GPUs und Netzwerken | DeepCEE:在异种性全球保护单位和网络下建立高效跨区域分布示范培训系统 [2505.15536v2](http://arxiv.org/abs/2505.15536v2)

**Authors**: Jinquan Wang, Xiaojian Liao, Xuzhao Liu, Jiashun Suo, Zhisheng Huo, Chenhao Zhang, Xiangrong Xu, Runnan Shen, Xilong Xie, Limin Xiao

Most existing training systems focus on a single region. In contrast, we envision that cross-region training offers more flexible GPU resource allocation and yields significant potential. However, the hierarchical cluster topology and unstable networks in the cloud-edge-end (CEE) environment, a typical cross-region scenario, pose substantial challenges to building an efficient and autonomous model training system. We propose DeepCEE, a geo-distributed model training system tailored for heterogeneous GPUs and networks in CEE environments. DeepCEE adopts a communication-centric design philosophy to tackle challenges arising from slow and unstable inter-region networks. It begins with a heterogeneous device profiler that identifies and groups devices based on both network and compute characteristics. Leveraging device groups, DeepCEE implements compact, zero-bubble pipeline parallelism, automatically deriving optimal parallel strategies. To further adapt to runtime variability, DeepCEE integrates a dynamic environment adapter that reacts to network fluctuations. Extensive evaluations demonstrate that DeepCEE achieves 1.3-2.8x higher training throughput compared to widely used and SOTA training systems.

---

## Article 43
### Title@2025-05-27 (2): Grassroots Consensus

**Title**: Grassroots Consensus | Graswurzeln-Konsens | 基层共识 [2505.19216v2](http://arxiv.org/abs/2505.19216v2)

**Authors**: Idit Keidar, Andrew Lewis-Pye, Ehud Shapiro

Grassroots platforms aim to offer an egalitarian alternative to global platforms -- centralized/autocratic and decentralized/plutocratic alike. Within the grassroots architecture, consensus is needed to realize platforms that employ digital social contracts, which are like smart contracts except that they are among people not accounts and are executed by these people's smartphones not by high-performance servers controlled by parties outside to the contract. Key envisioned grassroots platforms include sovereign democratic digital communities and federations, community banks and their grassroots cryptocurrencies, and digital cooperatives.   The grassroots architecture can benefit from a consensus protocol that is (i) quiescent, (ii) efficient during low- and high-throughput, (iii) responsive, (iv) blocklace-based, (v) UDP-ready, and (vi) grassroots. The Grassroots Consensus protocol addresses all these requirements while having competitive performance in both low- and high-throughput scenarios and being one of the most concise and elegant consensus protocols for partial synchrony. It achieves that by building on two cutting-edge consensus protocols -- the quiescent high-performance Morpheus and the blocklace-based Cordial Miners, improving the latter's dissemination protocol and making it UDP-ready, and extending the protocol with a constitution and a constitutional amendment component, making it grassroots.

---

## Article 44
### Title@2025-05-27 (2): Multi-Event Triggers for Serverless Computing

**Title**: Multi-Event Triggers for Serverless Computing | Multi-Event-Trigger für serverloses Rechnen | 无服务器电子计算多天触发器 [2505.21199v1](http://arxiv.org/abs/2505.21199v1)

**Authors**: Valentin Carl, Trever Schirmer, Joshua Adamek, Tobias Pfandzelter, Sergio Lucia, David Bermbach

Function-as-a-Service (FaaS) is an event-driven serverless cloud computing model in which small, stateless functions are invoked in response to events, such as HTTP requests, new database entries, or messages. Current FaaS platform assume that each function invocation corresponds to a single event. However, from an application perspective, it is desirable to invoke functions in response to a collection of events of different types or only with every n\textsuperscript{th} event. To implement this today, a function would need additional state management, e.g., in a database, and custom logic to determine whether its trigger condition is fulfilled and the actual application code should run. In such an implementation, most function invocations would be rendered essentially useless, leading to unnecessarily high resource usage, latency, and cost for applications. In this paper, we introduce multi-event triggers, through which complex conditions for function invocations can be specified. Specifically, we introduce abstractions for invoking functions based on a set of $n$ events and joins of multiple events of different types. This enables application developers to define intricate conditions for function invocations, workflow steps, and complex event processing. Our evaluation with a proof-of-concept prototype shows that this reduces event--invocation latency by 62.5\% in an incident detection use-case and that our system can handle more than 300,000 requests per second on limited hardware, which is sufficient load for implementation in large FaaS platforms.

---

## Article 45
### Title@2025-05-27 (2): Vectorized Sequence-Based Chunking for Data Deduplication

**Title**: Vectorized Sequence-Based Chunking for Data Deduplication | Vektorisierte Sequenz-basiertes Chunking für Datendeduplikation | 数据解析矢量序列相键 [2505.21194v1](http://arxiv.org/abs/2505.21194v1)

**Authors**: Sreeharsha Udayashankar, Samer Al-Kiswany

Data deduplication has gained wide acclaim as a mechanism to improve storage efficiency and conserve network bandwidth. Its most critical phase, data chunking, is responsible for the overall space savings achieved via the deduplication process. However, modern data chunking algorithms are slow and compute-intensive because they scan large amounts of data while simultaneously making data-driven boundary decisions.   We present SeqCDC, a novel chunking algorithm that leverages lightweight boundary detection, content-defined skipping, and SSE/AVX acceleration to improve chunking throughput for large chunk sizes. Our evaluation shows that SeqCDC achieves 15x higher throughput than unaccelerated and 1.2x-1.35x higher throughput than vector-accelerated data chunking algorithms while minimally affecting deduplication space savings.

---

## Article 46
### Title@2025-05-27 (2): Constructive community race: full-density spiking neural network model   drives neuromorphic computing

**Title**: Constructive community race: full-density spiking neural network model   drives neuromorphic computing | Konstruktives Community-Rennen: Volldichte-Spitzen neuronales Netzwerkmodell treibt neuromorphes Computing an | 充满建设性的社区种族:完全密度刺激神经网络模型驱动神经形态计算 [2505.21185v1](http://arxiv.org/abs/2505.21185v1)

**Authors**: Johanna Senk, Anno Kurth, Steve Furber, Tobias Gemmeke, Bruno Golosio, Arne Heittmann, James C. Knight, Eric Müller, Tobias Noll, Thomas Nowotny, Gorka Peraza Coppola, Luca Peres, Oliver Rhodes, Andrew Rowley, Johannes Schemmel, Tim Stadtmann, Tom Tetzlaff, Gianmarco Tiddia, Sacha J. van Albada, José Villamar, Markus Diesmann

The local circuitry of the mammalian brain is a focus of the search for generic computational principles because it is largely conserved across species and modalities. In 2014 a model was proposed representing all neurons and synapses of the stereotypical cortical microcircuit below $1\,\text{mm}^2$ of brain surface. The model reproduces fundamental features of brain activity but its impact remained limited because of its computational demands. For theory and simulation, however, the model was a breakthrough because it removes uncertainties of downscaling, and larger models are less densely connected. This sparked a race in the neuromorphic computing community and the model became a de facto standard benchmark. Within a few years real-time performance was reached and surpassed at significantly reduced energy consumption. We review how the computational challenge was tackled by different simulation technologies and derive guidelines for the next generation of benchmarks and other domains of science.

---

## Article 47
### Title@2025-05-27 (2): SHE-LoRA: Selective Homomorphic Encryption for Federated Tuning with   Heterogeneous LoRA

**Title**: SHE-LoRA: Selective Homomorphic Encryption for Federated Tuning with   Heterogeneous LoRA | SHE-LoRA: Selektive homomorphe Verschlüsselung für Federated Tuning mit Heterogene LoRA | SHE-LORA: 与异源罗拉结合的联邦调试的选择性单体单体加密 [2505.21051v1](http://arxiv.org/abs/2505.21051v1)

**Authors**: Jianmin Liu, Li Yan, Borui Li, Lei Yu, Chao Shen

Federated fine-tuning of large language models (LLMs) is critical for improving their performance in handling domain-specific tasks. However, prior work has shown that clients' private data can actually be recovered via gradient inversion attacks. Existing privacy preservation techniques against such attacks typically entail performance degradation and high costs, making them ill-suited for clients with heterogeneous data distributions and device capabilities. In this paper, we propose SHE-LoRA, which integrates selective homomorphic encryption (HE) and low-rank adaptation (LoRA) to enable efficient and privacy-preserving federated tuning of LLMs in cross-device environment. Heterogeneous clients adaptively select partial model parameters for homomorphic encryption based on parameter sensitivity assessment, with the encryption subset obtained via negotiation. To ensure accurate model aggregation, we design a column-aware secure aggregation method and customized reparameterization techniques to align the aggregation results with the heterogeneous device capabilities of clients. Extensive experiments demonstrate that SHE-LoRA maintains performance comparable to non-private baselines, achieves strong resistance to the state-of-the-art attacks, and significantly reduces communication overhead by 94.901\% and encryption computation overhead by 99.829\%, compared to baseline. Our code is accessible at https://anonymous.4open.science/r/SHE-LoRA-8D84.

---

## Article 48
### Title@2025-05-27 (2): A Hitchhiker's Guide to Privacy-Preserving Cryptocurrencies: A Survey on   Anonymity, Confidentiality, and Auditability

**Title**: A Hitchhiker's Guide to Privacy-Preserving Cryptocurrencies: A Survey on   Anonymity, Confidentiality, and Auditability | Ein Hitchhiker-Leitfaden zur Wahrung der Privatsphäre von Kryptowährungen: Eine Umfrage über Anonymität, Vertraulichkeit und Auditierbarkeit | 《希希克人保护隐私加密指南:关于匿名、保密和可审计性的调查》 [2505.21008v1](http://arxiv.org/abs/2505.21008v1)

**Authors**: Matteo Nardelli, Francesco De Sclavis, Michela Iezzi

Cryptocurrencies and central bank digital currencies (CBDCs) are reshaping the monetary landscape, offering transparency and efficiency while raising critical concerns about user privacy and regulatory compliance. This survey provides a comprehensive and technically grounded overview of privacy-preserving digital currencies, covering both cryptocurrencies and CBDCs. We propose a taxonomy of privacy goals -- including anonymity, confidentiality, unlinkability, and auditability -- and map them to underlying cryptographic primitives, protocol mechanisms, and system architectures. Unlike previous surveys, our work adopts a design-oriented perspective, linking high-level privacy objectives to concrete implementations. We also trace the evolution of privacy-preserving currencies through three generations, highlighting shifts from basic anonymity guarantees toward more nuanced privacy-accountability trade-offs. Finally, we identify open challenges at the intersection of cryptography, distributed systems, and policy definition, which motivate further investigation into the primitives and design of digital currencies that balance real-world privacy and auditability needs.

---

## Article 49
### Title@2025-05-27 (2): RACS-SADL: Robust and Understandable Randomized Consensus in the Cloud

**Title**: RACS-SADL: Robust and Understandable Randomized Consensus in the Cloud | RACS-SADL: Robuster und verständlicher Randomisierter Konsens in der Cloud | RACS-SADL:云层中的有力和可理解的随机共识 [2404.04183v3](http://arxiv.org/abs/2404.04183v3)

**Authors**: Pasindu Tennage, Antoine Desjardins, Lefteris Kokoris-Kogias

Widely deployed consensus protocols in the cloud are often leader-based and optimized for low latency under synchronous network conditions. However, cloud networks can experience disruptions such as network partitions, high-loss links, and configuration errors. These disruptions interfere with the operation of leader-based protocols, as their view change mechanisms interrupt the normal case replication and cause the system to stall.   We propose RACS, a novel randomized consensus protocol that ensures robustness against adversarial network conditions. RACS achieves optimal one-round trip latency under synchronous network conditions while remaining resilient to adversarial network conditions. RACS follows a simple design inspired by Raft, the most widely used consensus protocol in the cloud, and therefore enables seamless integration with the existing cloud software stack.   Experiments with a prototype running on Amazon EC2 show that RACS achieves 28k cmd/sec throughput, ninefold higher than Raft under adversarial cloud network conditions. Under synchronous network conditions, RACS matches the performance of Multi-Paxos and Raft, achieving a throughput of 200k cmd/sec with a median latency of 300ms, confirming that RACS introduces no unnecessary overhead. Finally, SADL-RACS, a throughput-optimized version of RACS, achieves a throughput of 500k cmd/sec, delivering 150% higher throughput than Raft.

---

## Article 50
### Title@2025-05-27 (2): EPIC: Efficient Position-Independent Caching for Serving Large Language   Models

**Title**: EPIC: Efficient Position-Independent Caching for Serving Large Language   Models | EPIC: Effizientes positionsunabhängiges Caching für das Servieren großer Sprachmodelle | EPIC: 高效的、独立定位的为大语言模式服务的工作 [2410.15332v3](http://arxiv.org/abs/2410.15332v3)

**Authors**: Junhao Hu, Wenrui Huang, Weidong Wang, Haoyi Wang, Tiancheng Hu, Qin Zhang, Hao Feng, Xusheng Chen, Yizhou Shan, Tao Xie

Large Language Models (LLMs) show great capabilities in a wide range of applications, but serving them efficiently becomes increasingly challenging as requests (prompts) become more complex. Context caching improves serving performance by reusing Key-Value (KV) vectors, the intermediate representations of tokens that are repeated across requests. However, existing context caching requires exact prefix matches across requests, limiting reuse cases in settings such as few-shot learning and retrieval-augmented generation, where immutable content (e.g., documents) remains unchanged across requests but is preceded by varying prefixes. Position-Independent Caching (PIC) addresses this issue by enabling modular reuse of the KV vectors regardless of prefixes. We formalize PIC and advance prior work by introducing EPIC, a serving system incorporating our new LegoLink algorithm, which mitigates the inappropriate "attention sink" effect at every document beginning, to maintain accuracy with minimal computation. Experiments show that EPIC achieves up to 8x improvements in Time-To-First-Token (TTFT) and 7x throughput gains over existing systems, with negligible or no accuracy loss.

---

## Article 51
### Title@2025-05-27 (2): Complexity landscape for local certification

**Title**: Complexity landscape for local certification | Komplexitätslandschaft für die lokale Zertifizierung | 当地认证的复杂环境 [2505.20915v1](http://arxiv.org/abs/2505.20915v1)

**Authors**: Nicolas Bousquet, Laurent Feuilloley, Sébastien Zeitoun

An impressive recent line of work has charted the complexity landscape of distributed graph algorithms. For many settings, it has been determined which time complexities exist, and which do not (in the sense that no local problem could have an optimal algorithm with that complexity). In this paper, we initiate the study of the landscape for space complexity of distributed graph algorithms. More precisely, we focus on the local certification setting, where a prover assigns certificates to nodes to certify a property, and where the space complexity is measured by the size of the certificates.   Already for anonymous paths and cycles, we unveil a surprising landscape:   - There is a gap between complexity $O(1)$ and $\Theta(\log \log n)$ in paths. This is the first gap established in local certification.   - There exists a property that has complexity $\Theta(\log \log n)$ in paths, a regime that was not known to exist for a natural property.   - There is a gap between complexity $O(1)$ and $\Theta(\log n)$ in cycles, hence a gap that is exponentially larger than for paths.   We then generalize our result for paths to the class of trees. Namely, we show that there is a gap between complexity $O(1)$ and $\Theta(\log \log d)$ in trees, where $d$ is the diameter. We finally describe some settings where there are no gaps at all. To prove our results we develop a new toolkit, based on various results of automata theory and arithmetic, which is of independent interest.

---

## Article 52
### Title@2025-05-27 (2): Reduced and mixed precision turbulent flow simulations using explicit   finite difference schemes

**Title**: Reduced and mixed precision turbulent flow simulations using explicit   finite difference schemes | Reduzierte und gemischte Präzision turbulente Strömungssimulationen mit expliziten Finite-Differenz-Systemen | 使用明确的有限差别办法进行减少和混合精密混杂的波动流动模拟 [2505.20911v1](http://arxiv.org/abs/2505.20911v1)

**Authors**: Bálint Siklósi, Pushpender K. Sharma, David J. Lusher, István Z. Reguly, Neil D. Sandham

The use of reduced and mixed precision computing has gained increasing attention in high-performance computing (HPC) as a means to improve computational efficiency, particularly on modern hardware architectures like GPUs. In this work, we explore the application of mixed precision arithmetic in compressible turbulent flow simulations using explicit finite difference schemes. We extend the OPS and OpenSBLI frameworks to support customizable precision levels, enabling fine-grained control over precision allocation for different computational tasks. Through a series of numerical experiments on the Taylor-Green vortex benchmark, we demonstrate that mixed precision strategies, such as half-single and single-double combinations, can offer significant performance gains without compromising numerical accuracy. However, pure half-precision computations result in unacceptable accuracy loss, underscoring the need for careful precision selection. Our results show that mixed precision configurations can reduce memory usage and communication overhead, leading to notable speedups, particularly on multi-CPU and multi-GPU systems.

---

## Article 53
### Title@2025-05-27 (2): Load Balancing in Strongly Inhomogeneous Simulations -- a Vlasiator Case   Study

**Title**: Load Balancing in Strongly Inhomogeneous Simulations -- a Vlasiator Case   Study | Lastausgleich in stark inhomogenen Simulationen -- eine Vlasiator-Fallstudie | 在极不相异模拟器中平衡载荷 -- -- 挥发器案例研究 [2505.20908v1](http://arxiv.org/abs/2505.20908v1)

**Authors**: Leo Kotipalo, Markus Battarbee, Yann Pfau-Kempf, Vertti Tarvus, Minna Palmroth

Parallelization is a necessity for large-scale simulations due to the amount of data processed. In this article we investigate different load balancing methods using Vlasiator, a global magnetospheric simulation as our case study.   The theoretical basis for load balancing is the (hyper)graph partitioning problem, modeling simulation units as vertices and their data dependencies as edges. As it is an NP-hard problem, heuristics are necessary for dynamic runtime balancing.   We consider first hypergraph partitioning via an algorithm called parallel hypergraph partitioner (PHG); this is done by partitioning a simplified grid and then attempting to optimize the solution on the finer grid. The second and third are the geometric methods of recursive coordinate bisection (RCB) and recursive inertial bisection (RIB).   Finally we consider the method of Hilbert space filling curves (HSFC). The algorithm projects simulation cells along a Hilbert curve and makes cuts along the curve. This works well due to the excellent locality of Hilbert curves, and can be optimized further by choice of curve. We introduce and investigate six three-dimensional Hilbert curves in total.   Our findings on runs of two different scales indicate the HSFC method provides optimal load balance, followed by RIB and PHG methods and finally by RCB. Of the Hilbert curves evaluated, the Beta curve outperformed the most commonly used curve by a few percent.

---

## Article 54
### Title@2025-05-27 (2): An Efficient Implementation of Guard-Based Synchronization for an   Object-Oriented Programming Language

**Title**: An Efficient Implementation of Guard-Based Synchronization for an   Object-Oriented Programming Language | Effiziente Implementierung von Guard-Based Synchronization für eine objektorientierte Programmiersprache | 高效率地实施以警卫为基础的同步,以用于以目标为导向的方案编制语言 [2505.20850v1](http://arxiv.org/abs/2505.20850v1)

**Authors**: Shucai Yao, Emil Sekerinski

In the shared variable model of concurrency, guarded atomic actions restrict the possible interference between processes by regions of atomic execution. The guard specifies the condition for entering an atomic region. That is a convenient model for the specification and verification of concurrent programs, but has eschewed efficient execution so far. This article shows how guarded atomic actions, when attached to objects, can be implemented highly efficiently using a combination of coroutines, operating-system worker threads, and dedicated management of object queues and stacks. The efficiency of an experimental language, Lime, is shown to compare favourably with that of C/Pthreads, Go, Erlang, Java, and Haskell on synthetic benchmarks.

---

## Article 55
### Title@2025-05-27 (2): Choreographies as Macros

**Title**: Choreographies as Macros | Choreographien als Makros | 作为宏的舞蹈 [2505.20845v1](http://arxiv.org/abs/2505.20845v1)

**Authors**: Alexander Bohosian, Andrew K. Hirsch

Concurrent programming often entails meticulous pairing of sends and receives between participants to avoid deadlock. Choreographic programming alleviates this burden by specifying the system as a single program. However, there are more applications than implementations of choreographies, and developing new implementations takes a lot of time and effort. Our work uses Racket to expedite building a new choreographic language called Choret. Racket has a powerful macro system which allows Choret to reuse much of its infrastructure for greater functionality and correctness.

---

## Article 56
### Title@2025-05-27 (2): ECC-SNN: Cost-Effective Edge-Cloud Collaboration for Spiking Neural   Networks

**Title**: ECC-SNN: Cost-Effective Edge-Cloud Collaboration for Spiking Neural   Networks | ECC-SNN: Kosteneffiziente Edge-Cloud-Kollaboration für Spiking Neuronal Networks | ECC-SNN: 传播神经网络的成本-效益高的边缘-封闭式协作 [2505.20835v1](http://arxiv.org/abs/2505.20835v1)

**Authors**: Di Yu, Changze Lv, Xin Du, Linshan Jiang, Wentao Tong, Zhenyu Liao, Xiaoqing Zheng, Shuiguang Deng

Most edge-cloud collaboration frameworks rely on the substantial computational and storage capabilities of cloud-based artificial neural networks (ANNs). However, this reliance results in significant communication overhead between edge devices and the cloud and high computational energy consumption, especially when applied to resource-constrained edge devices. To address these challenges, we propose ECC-SNN, a novel edge-cloud collaboration framework incorporating energy-efficient spiking neural networks (SNNs) to offload more computational workload from the cloud to the edge, thereby improving cost-effectiveness and reducing reliance on the cloud. ECC-SNN employs a joint training approach that integrates ANN and SNN models, enabling edge devices to leverage knowledge from cloud models for enhanced performance while reducing energy consumption and processing latency. Furthermore, ECC-SNN features an on-device incremental learning algorithm that enables edge models to continuously adapt to dynamic environments, reducing the communication overhead and resource consumption associated with frequent cloud update requests. Extensive experimental results on four datasets demonstrate that ECC-SNN improves accuracy by 4.15%, reduces average energy consumption by 79.4%, and lowers average processing latency by 39.1%.

---

## Article 57
### Title@2025-05-27 (2): Work-Efficient Parallel Counting via Sampling

**Title**: Work-Efficient Parallel Counting via Sampling | Arbeitseffiziente parallele Zählung über Probenahme | 通过抽样计算实现工作效率的平行计数 [2408.09719v2](http://arxiv.org/abs/2408.09719v2)

**Authors**: Hongyang Liu, Yitong Yin, Yiyao Zhang

A canonical approach to approximating the partition function of a Gibbs distribution via sampling is simulated annealing. This method has led to efficient reductions from counting to sampling, including:   $\bullet$ classic non-adaptive (parallel) algorithms with sub-optimal cost (Dyer-Frieze-Kannan '89; Bez\'akov\'a-\v{S}tefankovi\v{c}-Vazirani-Vigoda '08);   $\bullet$ adaptive (sequential) algorithms with near-optimal cost (\v{S}tefankovi\v{c}-Vempala-Vigoda '09; Huber '15; Kolmogorov '18; Harris-Kolmogorov '24).   We present an algorithm that achieves both near-optimal total work and efficient parallelism, providing a reduction from counting to sampling with logarithmic depth and near-optimal work. As consequences, we obtain work-efficient parallel counting algorithms for several important models, including the hardcore and Ising models within the uniqueness regime.

---

## Article 58
### Title@2025-05-27 (2): Time-Series Learning for Proactive Fault Prediction in Distributed   Systems with Deep Neural Structures

**Title**: Time-Series Learning for Proactive Fault Prediction in Distributed   Systems with Deep Neural Structures | Time-Series Learning für proaktive Fehlervorhersage in verteilten Systemen mit tiefen neuralen Strukturen | 深心神经结构分布系统预发性故障预测时间序列学习 [2505.20705v1](http://arxiv.org/abs/2505.20705v1)

**Authors**: Yang Wang, Wenxuan Zhu, Xuehui Quan, Heyi Wang, Chang Liu, Qiyuan Wu

This paper addresses the challenges of fault prediction and delayed response in distributed systems by proposing an intelligent prediction method based on temporal feature learning. The method takes multi-dimensional performance metric sequences as input. We use a Gated Recurrent Unit (GRU) to model the evolution of system states over time. An attention mechanism is then applied to enhance key temporal segments, improving the model's ability to identify potential faults. On this basis, a feedforward neural network is designed to perform the final classification, enabling early warning of system failures. To validate the effectiveness of the proposed approach, comparative experiments and ablation analyses were conducted using data from a large-scale real-world cloud system. The experimental results show that the model outperforms various mainstream time-series models in terms of Accuracy, F1-Score, and AUC. This demonstrates strong prediction capability and stability. Furthermore, the loss function curve confirms the convergence and reliability of the training process. It indicates that the proposed method effectively learns system behavior patterns and achieves efficient fault detection.

---

## Article 59
### Title@2025-05-27 (2): InstGenIE: Generative Image Editing Made Efficient with Mask-aware   Caching and Scheduling

**Title**: InstGenIE: Generative Image Editing Made Efficient with Mask-aware   Caching and Scheduling | InstGenIE: Generative Bildbearbeitung mit Mask-aware Caching und Scheduling effizient gemacht | InstGenie: 生成图像编辑, 高效使用防面具图像缓冲和排程 [2505.20600v1](http://arxiv.org/abs/2505.20600v1)

**Authors**: Xiaoxiao Jiang, Suyi Li, Lingyun Yang, Tianyu Feng, Zhipeng Di, Weiyi Lu, Guoxuan Zhu, Xiu Lin, Kan Liu, Yinghao Yu, Tao Lan, Guodong Yang, Lin Qu, Liping Zhang, Wei Wang

Generative image editing using diffusion models has become a prevalent application in today's AI cloud services. In production environments, image editing typically involves a mask that specifies the regions of an image template to be edited. The use of masks provides direct control over the editing process and introduces sparsity in the model inference. In this paper, we present InstGenIE, a system that efficiently serves image editing requests. The key insight behind InstGenIE is that image editing only modifies the masked regions of image templates while preserving the original content in the unmasked areas. Driven by this insight, InstGenIE judiciously skips redundant computations associated with the unmasked areas by reusing cached intermediate activations from previous inferences. To mitigate the high cache loading overhead, InstGenIE employs a bubble-free pipeline scheme that overlaps computation with cache loading. Additionally, to reduce queuing latency in online serving while improving the GPU utilization, InstGenIE proposes a novel continuous batching strategy for diffusion model serving, allowing newly arrived requests to join the running batch in just one step of denoising computation, without waiting for the entire batch to complete. As heterogeneous masks induce imbalanced loads, InstGenIE also develops a load balancing strategy that takes into account the loads of both computation and cache loading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving systems for image editing, achieving up to 3x higher throughput and reducing average request latency by up to 14.7x while ensuring image quality.

---

## Article 60
### Title@2025-05-26 (1): Asynchronous Fault-Tolerant Language Decidability for Runtime   Verification of Distributed Systems

**Title**: Asynchronous Fault-Tolerant Language Decidability for Runtime   Verification of Distributed Systems | Asynchrone Fehler-Tolerante Sprachentscheidung für die Laufzeitverifizierung von verteilten Systemen | 分布式系统运行时核查的 Al- 同步错失容忍语言 [2502.00191v2](http://arxiv.org/abs/2502.00191v2)

**Authors**: Armando Castañeda, Gilde Valeria Rodríguez

Implementing correct distributed systems is an error-prone task. Runtime Verification (RV) offers a lightweight formal method to improve reliability by monitoring system executions against correctness properties. However, applying RV in distributed settings - where no process has global knowledge - poses fundamental challenges, particularly under full asynchrony and fault tolerance. This paper addresses the Distributed Runtime Verification (DRV) problem under such conditions. In our model, each process in a distributed monitor receives a fragment of the input word describing system behavior and must decide whether this word belongs to the language representing the correctness property being verified. Hence, the goal is to decide languages in a distributed fault-tolerant manner. We propose several decidability definitions, study the relations among them, and prove possibility and impossibility results. One of our main results is a characterization of the correctness properties that can be decided asynchronously. Remarkably, it applies to any language decidability definition. Intuitively, the characterization is that only properties with no real-time order constraints can be decided in asynchronous fault-tolerant settings. These results expose the expressive limits of DRV in realistic systems, as several properties of practical interest rely on reasoning about real-time order of events in executions. To overcome these limitations, we introduce a weaker model where the system under inspection is verified indirectly. Under this weaker model we define predictive decidability, a decidability definition that turn some real-time sensitive correctness properties verifiable. Our framework unifies and extends existing DRV theory and sharpens the boundary of runtime monitorability under different assumptions.

---

## Article 61
### Title@2025-05-26 (1): Avoid Forgetting by Preserving Global Knowledge Gradients in Federated   Learning with Non-IID Data

**Title**: Avoid Forgetting by Preserving Global Knowledge Gradients in Federated   Learning with Non-IID Data | Vermeiden Sie das Vergessen, indem Sie globale Wissensgradienten im Föderierten Lernen mit nicht-ID-Daten bewahren | 避免在使用非二二二维数据进行联邦学习时因保留全球知识进步而被遗忘 [2505.20485v1](http://arxiv.org/abs/2505.20485v1)

**Authors**: Abhijit Chunduru, Majid Morafah, Mahdi Morafah, Vishnu Pandi Chellapandi, Ang Li

The inevitable presence of data heterogeneity has made federated learning very challenging. There are numerous methods to deal with this issue, such as local regularization, better model fusion techniques, and data sharing. Though effective, they lack a deep understanding of how data heterogeneity can affect the global decision boundary. In this paper, we bridge this gap by performing an experimental analysis of the learned decision boundary using a toy example. Our observations are surprising: (1) we find that the existing methods suffer from forgetting and clients forget the global decision boundary and only learn the perfect local one, and (2) this happens regardless of the initial weights, and clients forget the global decision boundary even starting from pre-trained optimal weights. In this paper, we present FedProj, a federated learning framework that robustly learns the global decision boundary and avoids its forgetting during local training. To achieve better ensemble knowledge fusion, we design a novel server-side ensemble knowledge transfer loss to further calibrate the learned global decision boundary. To alleviate the issue of learned global decision boundary forgetting, we further propose leveraging an episodic memory of average ensemble logits on a public unlabeled dataset to regulate the gradient updates at each step of local training. Experimental results demonstrate that FedProj outperforms state-of-the-art methods by a large margin.

---

## Article 62
### Title@2025-05-26 (1): Fixing non-blocking data structures for better compatibility with memory   reclamation schemes

**Title**: Fixing non-blocking data structures for better compatibility with memory   reclamation schemes | Fixierung von nicht blockierenden Datenstrukturen für eine bessere Kompatibilität mit Speicher-Reklamationssystemen | 固定非阻塞性数据结构,以更好地与内存回收计划兼容 [2504.06254v2](http://arxiv.org/abs/2504.06254v2)

**Authors**: Md Amit Hasan Arovi, Ruslan Nikolaev

We present a new technique, Safe Concurrent Optimistic Traversals (SCOT), to address a well-known problem related to optimistic traversals with both classical and more recent memory reclamation schemes, such as Hazard Pointers (HP), Hazard Eras (HE), Interval-Based Reclamation (IBR), and Hyaline. Unlike Epoch-Based Reclamation (EBR), these schemes guarantee protection against stalled threads (robustness) but lack support for well-known data structures with optimistic traversals such as Harris' original list, Natarajan-Mittal tree, among others. For these reclamation schemes, existing data structure implementations are either buggy (e.g., Natarajan-Mittal tree) or come with performance trade-offs (e.g., Harris-Michael modified list).   A recent work, HP++, supports optimistic traversals but uses a different API and is generally slower than even HP, not to mention more recent schemes such as IBR or Hyaline. Moreover, it has undesirable applicability trade-offs, and more complex implementation, among other issues. We propose a different method which keeps existing reclamation schemes intact but instead relies on data structure adaptations. Unlike existing Harris-Michael approach or HP++, our method retains performance benefits of the original data structure and also does not compromise performance of the underlying reclamation scheme. In fact, for IBR and Hyaline, our results almost match those of EBR, which often serves as a practical upper bound due to its great performance.   We implement and evaluate two fundamentally different data structures: Harris' list and Natarajan-Mittal tree. SCOT enables their first correct implementations with optimistic traversals for HP, HE, IBR, and Hyaline.

---

## Article 63
### Title@2025-05-26 (1): Efficient Optimization Accelerator Framework for Multistate Ising   Problems

**Title**: Efficient Optimization Accelerator Framework for Multistate Ising   Problems | Effizientes Optimierungs-Beschleuniger-Framework für Multistate Ising-Probleme | 高效高效优化多州化问题加速加速框架 [2505.20250v1](http://arxiv.org/abs/2505.20250v1)

**Authors**: Chirag Garg, Sayeef Salahuddin

Ising Machines are a prominent class of hardware architectures that aim to solve NP-hard combinatorial optimization problems. These machines consist of a network of interacting binary spins/neurons that evolve to represent the optimum ground state energy solution. Generally, combinatorial problems are transformed into quadratic unconstrained binary optimization (QUBO) form to harness the computational efficiency of these Ising machines. However, this transformation, especially for multi-state problems, often leads to a more complex exploration landscape than the original problem, thus severely impacting the solution quality. To address this challenge, we model the spin interactions as a generalized boolean logic function to significantly reduce the exploration space. We benchmark the graph coloring problem from the class of multi-state NP-hard optimization using probabilistic Ising solvers to illustrate the effectiveness of our framework. The proposed methodology achieves similar accuracy compared to state-of-the-art heuristics and machine learning algorithms, and demonstrates significant improvement over the existing Ising methods. Additionally, we demonstrate that combining parallel tempering with our existing framework further reduces the coloring error by up to 50% compared to the conventionally used Gibbs sampling algorithm. We also design a 1024-neuron all-to-all connected probabilistic Ising accelerator that shows up to 10000x performance acceleration compared to heuristics while reducing the number of required physical neurons by 1.5-4x compared to conventional Ising machines. Indeed, this accelerator solution demonstrates improvement across all metrics over the current methods, i.e., energy, performance, area, and solution quality. Thus, this work expands the potential of existing Ising hardware to solve a broad class of these multistate optimization problems.

---

## Article 64
### Title@2025-05-26 (1): FedECA: A Federated External Control Arm Method for Causal Inference   with Time-To-Event Data in Distributed Settings

**Title**: FedECA: A Federated External Control Arm Method for Causal Inference   with Time-To-Event Data in Distributed Settings | FedECA: Eine Federated External Control Arm Methode für ursächliche Schlussfolgerungen mit Zeit-bis-Event-Daten in verteilten Einstellungen | FedECA:在分布环境中利用时间到时间的数据进行因果关系推断的联邦外部控制武器法 [2311.16984v9](http://arxiv.org/abs/2311.16984v9)

**Authors**: Jean Ogier du Terrail, Quentin Klopfenstein, Honghao Li, Imke Mayer, Nicolas Loiseau, Mohammad Hallal, Michael Debouver, Thibault Camalon, Thibault Fouqueray, Jorge Arellano Castro, Zahia Yanes, Laëtitia Dahan, Julien Taïeb, Pierre Laurent-Puig, Jean-Baptiste Bachet, Shulin Zhao, Remy Nicolle, Jérome Cros, Daniel Gonzalez, Robert Carreras-Torres, Adelaida Garcia Velasco, Kawther Abdilleh, Sudheer Doss, Félix Balazard, Mathieu Andreux

External control arms (ECA) can inform the early clinical development of experimental drugs and provide efficacy evidence for regulatory approval. However, the main challenge in implementing ECA lies in accessing real-world or historical clinical trials data. Indeed, regulations protecting patients' rights by strictly controlling data processing make pooling data from multiple sources in a central server often difficult. To address these limitations, we develop a new method, 'FedECA' that leverages federated learning (FL) to enable inverse probability of treatment weighting (IPTW) for time-to-event outcomes on separate cohorts without needing to pool data. To showcase the potential of FedECA, we apply it in different settings of increasing complexity culminating with a real-world use-case in which FedECA is used to compare the treatment effect of two approved chemotherapy regimens using data from three separate cohorts of patients with metastatic pancreatic cancer. By sharing our code, we hope FedECA will foster the creation of federated research networks and thus accelerate drug development.

---

## Article 65
### Title@2025-05-26 (1): BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems

**Title**: BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems | BurstGPT: Ein echter Workload-Datensatz zur Optimierung von LLM-Serviersystemen | BurtGPT:优化LLM服务系统的现实世界工作量数据集 [2401.17644v5](http://arxiv.org/abs/2401.17644v5)

**Authors**: Yuxin Wang, Yuhan Chen, Zeyu Li, Xueze Kang, Yuchu Fang, Yeju Zhou, Yang Zheng, Zhenheng Tang, Xin He, Rui Guo, Xin Wang, Qiang Wang, Amelie Chi Zhou, Xiaowen Chu

Serving systems for Large Language Models (LLMs) are often optimized to improve quality of service (QoS) and throughput. However, due to the lack of open-source LLM serving workloads, these systems are frequently evaluated under unrealistic workload assumptions. Consequently, performance may degrade when systems are deployed in real-world scenarios. This work presents BurstGPT, an LLM serving workload with 10.31 million traces from regional Azure OpenAI GPT services over 213 days. BurstGPT captures LLM serving characteristics from user, model and system perspectives: (1) User request concurrency: burstiness variations of requests in Azure OpenAI GPT services, revealing diversified concurrency patterns in different services and model types. (2) User conversation patterns: counts and intervals within conversations for service optimizations. (3) Model response lengths: auto-regressive serving processes of GPT models, showing statistical relations between requests and their responses. (4) System response failures: failures of conversation and API services, showing intensive resource needs and limited availability of LLM services in Azure. The details of the characteristics can serve multiple purposes in LLM serving optimizations, such as system evaluation and trace provisioning. In our demo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines in efficiency, stability, or reliability in realistic LLM serving. We identify that the generalization of KV cache management, scheduling and disaggregation optimizations can be improved under realistic workload evaluations. BurstGPT is publicly available now at https://github.com/HPMLL/BurstGPT and is widely used to develop prototypes of LLM serving frameworks in the industry.

---

## Article 66
### Title@2025-05-26 (1): Parallelizing a modern GPU simulator

**Title**: Parallelizing a modern GPU simulator | Parallelisierung eines modernen GPU-Simulators | 平行使用现代 GPU 模拟器 [2502.14691v2](http://arxiv.org/abs/2502.14691v2)

**Authors**: Rodrigo Huerta, Antonio González

Simulators are a primary tool in computer architecture research but are extremely computationally intensive. Simulating modern architectures with increased core counts and recent workloads can be challenging, even on modern hardware. This paper demonstrates that simulating some GPGPU workloads in a single-threaded state-of-the-art simulator such as Accel-sim can take more than five days. In this paper we present a simple approach to parallelize this simulator with minimal code changes by using OpenMP. Moreover, our parallelization technique is deterministic, so the simulator provides the same results for single-threaded and multi-threaded simulations. Compared to previous works, we achieve a higher speed-up, and, more importantly, the parallel simulation does not incur any inaccuracies. When we run the simulator with 16 threads, we achieve an average speed-up of 5.8x and reach 14x in some workloads. This allows researchers to simulate applications that take five days in less than 12 hours. By speeding up simulations, researchers can model larger systems, simulate bigger workloads, add more detail to the model, increase the efficiency of the hardware platform where the simulator is run, and obtain results sooner.

---

## Article 67
### Title@2025-05-26 (1): Snowman for partial synchrony

**Title**: Snowman for partial synchrony | Schneemann für partielle Synchronisation | 部分同步的雪人 [2501.15904v3](http://arxiv.org/abs/2501.15904v3)

**Authors**: Aaron Buchwald, Stephen Buttolph, Andrew Lewis-Pye, Kevin Sekniqi

Snowman is the consensus protocol run by blockchains on Avalanche. Recent work established a rigorous proof of probabilistic consistency for Snowman in the \emph{synchronous} setting, under the simplifying assumption that correct processes execute sampling rounds in `lockstep'. In this paper, we describe a modification of the protocol that ensures consistency in the \emph{partially synchronous} setting, and when correct processes carry out successive sampling rounds at their own speed, with the time between sampling rounds determined by local message delays.

---

## Article 68
### Title@2025-05-26 (1): Beyond Optimal Fault Tolerance

**Title**: Beyond Optimal Fault Tolerance | Jenseits der optimalen Fehlertoleranz | 超越最佳错失容忍 [2501.06044v6](http://arxiv.org/abs/2501.06044v6)

**Authors**: Andrew Lewis-Pye, Tim Roughgarden

The optimal fault-tolerance achievable by any protocol has been characterized in a wide range of settings. For example, for state machine replication (SMR) protocols operating in the partially synchronous setting, it is possible to simultaneously guarantee consistency against $\alpha$-bounded adversaries (i.e., adversaries that control less than an $\alpha$ fraction of the participants) and liveness against $\beta$-bounded adversaries if and only if $\alpha + 2\beta \leq 1$.   This paper characterizes to what extent "better-than-optimal" fault-tolerance guarantees are possible for SMR protocols when the standard consistency requirement is relaxed to allow a bounded number $r$ of consistency violations. We prove that bounding rollback is impossible without additional timing assumptions and investigate protocols that tolerate and recover from consistency violations whenever message delays around the time of an attack are bounded by a parameter $\Delta^*$ (which may be arbitrarily larger than the parameter $\Delta$ that bounds post-GST message delays in the partially synchronous model). Here, a protocol's fault-tolerance can be a non-constant function of $r$, and we prove, for each $r$, matching upper and lower bounds on the optimal "recoverable fault-tolerance" achievable by any SMR protocol. For example, for protocols that guarantee liveness against 1/3-bounded adversaries in the partially synchronous setting, a 5/9-bounded adversary can always cause one consistency violation but not two, and a 2/3-bounded adversary can always cause two consistency violations but not three. Our positive results are achieved through a generic "recovery procedure" that can be grafted on to any accountable SMR protocol and restores consistency following a violation while rolling back only transactions that were finalized in the previous $2\Delta^*$ timesteps.

---

## Article 69
### Title@2025-05-26 (1): Distortion Resilience for Goal-Oriented Semantic Communication

**Title**: Distortion Resilience for Goal-Oriented Semantic Communication | Distortion Resilienz für zielorientierte semantische Kommunikation | 目标导向语义交流的扭曲复原力 [2309.14587v2](http://arxiv.org/abs/2309.14587v2)

**Authors**: Minh-Duong Nguyen, Quang-Vinh Do, Zhaohui Yang, Quoc-Viet Pham, Won-Joo Hwang

Recent research efforts on Semantic Communication (SemCom) have mostly considered accuracy as a main problem for optimizing goal-oriented communication systems. However, these approaches introduce a paradox: the accuracy of Artificial Intelligence (AI) tasks should naturally emerge through training rather than being dictated by network constraints. Acknowledging this dilemma, this work introduces an innovative approach that leverages the rate distortion theory to analyze distortions induced by communication and compression, thereby analyzing the learning process. Specifically, we examine the distribution shift between the original data and the distorted data, thus assessing its impact on the AI model's performance. Founding upon this analysis, we can preemptively estimate the empirical accuracy of AI tasks, making the goal-oriented SemCom problem feasible. To achieve this objective, we present the theoretical foundation of our approach, accompanied by simulations and experiments that demonstrate its effectiveness. The experimental results indicate that our proposed method enables accurate AI task performance while adhering to network constraints, establishing it as a valuable contribution to the field of signal processing. Furthermore, this work advances research in goal-oriented SemCom and highlights the significance of data-driven approaches in optimizing the performance of intelligent systems.

---

## Article 70
### Title@2025-05-26 (1): Optimizing edge AI models on HPC systems with the edge in the loop

**Title**: Optimizing edge AI models on HPC systems with the edge in the loop | Optimierung der Kanten-KI-Modelle auf HPC-Systemen mit der Kante in der Schleife | 优化循环边缘的HPC系统优化边缘 AI 模型 [2505.19995v1](http://arxiv.org/abs/2505.19995v1)

**Authors**: Marcel Aach, Cyril Blanc, Andreas Lintermann, Kurt De Grave

Artificial intelligence and machine learning models deployed on edge devices, e.g., for quality control in Additive Manufacturing (AM), are frequently small in size. Such models usually have to deliver highly accurate results within a short time frame. Methods that are commonly employed in literature start out with larger trained models and try to reduce their memory and latency footprint by structural pruning, knowledge distillation, or quantization. It is, however, also possible to leverage hardware-aware Neural Architecture Search (NAS), an approach that seeks to systematically explore the architecture space to find optimized configurations. In this study, a hardware-aware NAS workflow is introduced that couples an edge device located in Belgium with a powerful High-Performance Computing system in Germany, to train possible architecture candidates as fast as possible while performing real-time latency measurements on the target hardware. The approach is verified on a use case in the AM domain, based on the open RAISE-LPBF dataset, achieving ~8.8 times faster inference speed while simultaneously enhancing model quality by a factor of ~1.35, compared to a human-designed baseline.

---

## Article 71
### Title@2025-05-26 (1): Federated Domain Generalization with Data-free On-server Matching   Gradient

**Title**: Federated Domain Generalization with Data-free On-server Matching   Gradient | Föderierte Domain-Verallgemeinerung mit datenfreiem On-Server-Zustimmungs-Gradient | 具有无数据观测站上与渐变匹配的无数据观测器的联邦通用域 [2501.14653v2](http://arxiv.org/abs/2501.14653v2)

**Authors**: Trong-Binh Nguyen, Minh-Duong Nguyen, Jinsun Park, Quoc-Viet Pham, Won Joo Hwang

Domain Generalization (DG) aims to learn from multiple known source domains a model that can generalize well to unknown target domains. One of the key approaches in DG is training an encoder which generates domain-invariant representations. However, this approach is not applicable in Federated Domain Generalization (FDG), where data from various domains are distributed across different clients. In this paper, we introduce a novel approach, dubbed Federated Learning via On-server Matching Gradient (FedOMG), which can \emph{efficiently leverage domain information from distributed domains}. Specifically, we utilize the local gradients as information about the distributed models to find an invariant gradient direction across all domains through gradient inner product maximization. The advantages are two-fold: 1) FedOMG can aggregate the characteristics of distributed models on the centralized server without incurring any additional communication cost, and 2) FedOMG is orthogonal to many existing FL/FDG methods, allowing for additional performance improvements by being seamlessly integrated with them. Extensive experimental evaluations on various settings to demonstrate the robustness of FedOMG compared to other FL/FDG baselines. Our method outperforms recent SOTA baselines on four FL benchmark datasets (MNIST, EMNIST, CIFAR-10, and CIFAR-100), and three FDG benchmark datasets (PACS, VLCS, and OfficeHome).

---

## Article 72
### Title@2025-05-26 (1): From Few to Many Faults: Adaptive Byzantine Agreement with Optimal   Communication

**Title**: From Few to Many Faults: Adaptive Byzantine Agreement with Optimal   Communication | Von wenigen bis zu vielen Fehlern: Adaptive byzantinische Vereinbarung mit optimaler Kommunikation | 从少到多的错失:适应性拜占庭协议与最佳沟通 [2505.19989v1](http://arxiv.org/abs/2505.19989v1)

**Authors**: Andrei Constantinescu, Marc Dufay, Anton Paramonov, Roger Wattenhofer

Achieving agreement among distributed parties is a fundamental task in modern systems, underpinning applications such as consensus in blockchains, coordination in cloud infrastructure, and fault tolerance in critical services. However, this task can be communication-intensive, often requiring a large number of messages to be exchanged, especially in the presence of Byzantine faults, making efficiency a central challenge in the design of practical agreement protocols.   In this paper, we study the problem of Strong Byzantine Agreement and establish tight upper and lower bounds on communication complexity, parameterized by the actual number of Byzantine faults. Specifically, for a system of $n$ parties tolerating up to $t$ Byzantine faults, out of which only $f \leq t$ are actually faulty, we obtain the following results:   In the partially synchronous setting, we present the first Byzantine Agreement protocol that achieves adaptive communication complexity of $\mathcal{O}(n + t \cdot f)$ words, which is asymptotically optimal. Our protocol has an optimal resilience of $t < n/3$.   In the asynchronous setting, we prove a lower bound of $\Omega(n + t^2)$ on the expected number of messages, and design an almost matching protocol with an optimal resilience that solves agreement with $\mathcal{O}((n + t^2)\cdot \log n)$ words. Our main technical contribution in the asynchronous setting is the utilization of a bipartite expander graph that allows for low-cost information dissemination.

---

## Article 73
### Title@2025-05-26 (1): Differential Privacy Analysis of Decentralized Gossip Averaging under   Varying Threat Models

**Title**: Differential Privacy Analysis of Decentralized Gossip Averaging under   Varying Threat Models | Differential Privacy Analyse dezentralisierter Gossip Average unter unterschiedlichen Bedrohungsmodellen | 对不同威胁模式下分散的流民的隐私差异分析 [2505.19969v1](http://arxiv.org/abs/2505.19969v1)

**Authors**: Antti Koskela, Tejas Kulkarni

Fully decentralized training of machine learning models offers significant advantages in scalability, robustness, and fault tolerance. However, achieving differential privacy (DP) in such settings is challenging due to the absence of a central aggregator and varying trust assumptions among nodes. In this work, we present a novel privacy analysis of decentralized gossip-based averaging algorithms with additive node-level noise, both with and without secure summation over each node's direct neighbors. Our main contribution is a new analytical framework based on a linear systems formulation that accurately characterizes privacy leakage across these scenarios. This framework significantly improves upon prior analyses, for example, reducing the R\'enyi DP parameter growth from $O(T^2)$ to $O(T)$, where $T$ is the number of training rounds. We validate our analysis with numerical results demonstrating superior DP bounds compared to existing approaches. We further illustrate our analysis with a logistic regression experiment on MNIST image classification in a fully decentralized setting, demonstrating utility comparable to central aggregation methods.

---

## Article 74
### Title@2025-05-26 (1): Universal Workers: A Vision for Eliminating Cold Starts in Serverless   Computing

**Title**: Universal Workers: A Vision for Eliminating Cold Starts in Serverless   Computing | Universal Workers: Eine Vision zur Beseitigung von Kaltstarts im serverlosen Computing | 普遍工人:在无服务器计算机中消除冷源的愿景 [2505.19880v1](http://arxiv.org/abs/2505.19880v1)

**Authors**: Saman Akbari, Manfred Hauswirth

Serverless computing enables developers to deploy code without managing infrastructure, but suffers from cold start overhead when initializing new function instances. Existing solutions such as "keep-alive" or "pre-warming" are costly and unreliable under bursty workloads. We propose universal workers, which are computational units capable of executing any function with minimal initialization overhead. Based on an analysis of production workload traces, our key insight is that requests in Function-as-a-Service (FaaS) platforms show a highly skewed distribution, with most requests invoking a small subset of functions. We exploit this observation to approximate universal workers through locality groups and three-tier caching (handler, install, import). With this work, we aim to enable more efficient and scalable FaaS platforms capable of handling diverse workloads with minimal initialization overhead.

---

## Article 75
### Title@2025-05-26 (1): DGRAG: Distributed Graph-based Retrieval-Augmented Generation in   Edge-Cloud Systems

**Title**: DGRAG: Distributed Graph-based Retrieval-Augmented Generation in   Edge-Cloud Systems | DGRAG: Distributed Graph-based Retrieval-Augmented Generation in Edge-Cloud-Systemen | DGGGAG: 在边缘封闭系统中分布的基于图图的回收回源-养代 [2505.19847v1](http://arxiv.org/abs/2505.19847v1)

**Authors**: Wenqing Zhou, Yuxuan Yan, Qianqian Yang

Retrieval-Augmented Generation (RAG) has emerged as a promising approach to enhance the capabilities of language models by integrating external knowledge. Due to the diversity of data sources and the constraints of memory and computing resources, real-world data is often scattered in multiple devices. Conventional RAGs that store massive amounts of scattered data centrally face increasing privacy concerns and high computational costs. Additionally, RAG in a central node raises latency issues when searching over a large-scale knowledge base. To address these challenges, we propose a distributed Knowledge Graph-based RAG approach, referred to as DGRAG, in an edge-cloud system, where each edge device maintains a local knowledge base without the need to share it with the cloud, instead sharing only summaries of its knowledge. Specifically, DGRAG has two main phases. In the Distributed Knowledge Construction phase, DGRAG organizes local knowledge using knowledge graphs, generating subgraph summaries and storing them in a summary database in the cloud as information sharing. In the Collaborative Retrieval and Generation phase, DGRAG first performs knowledge retrieval and answer generation locally, and a gate mechanism determines whether the query is beyond the scope of local knowledge or processing capabilities. For queries that exceed the local knowledge scope, the cloud retrieves knowledge from the most relevant edges based on the summaries and generates a more precise answer. Experimental results demonstrate the effectiveness of the proposed DGRAG approach in significantly improving the quality of question-answering tasks over baseline approaches.

---

## Article 76
### Title@2025-05-26 (1): Will LLMs Scaling Hit the Wall? Breaking Barriers via Distributed   Resources on Massive Edge Devices

**Title**: Will LLMs Scaling Hit the Wall? Breaking Barriers via Distributed   Resources on Massive Edge Devices | Wird LLMs Skalierung die Wand treffen? Über verteilte Ressourcen auf massiven Edge-Geräten Barrieren überwinden | LLLMs SUlia扩大会撞上隔离墙吗?通过大规模边缘装置分配资源打破障碍 [2503.08223v2](http://arxiv.org/abs/2503.08223v2)

**Authors**: Tao Shen, Didi Zhu, Ziyu Zhao, Zexi Li, Chao Wu, Fei Wu

The remarkable success of foundation models has been driven by scaling laws, demonstrating that model performance improves predictably with increased training data and model size. However, this scaling trajectory faces two critical challenges: the depletion of high-quality public data, and the prohibitive computational power required for larger models, which have been monopolized by tech giants. These two bottlenecks pose significant obstacles to the further development of AI. In this position paper, we argue that leveraging massive distributed edge devices can break through these barriers. We reveal the vast untapped potential of data and computational resources on massive edge devices, and review recent technical advancements in distributed/federated learning that make this new paradigm viable. Our analysis suggests that by collaborating on edge devices, everyone can participate in training large language models with small edge devices. This paradigm shift towards distributed training on edge has the potential to democratize AI development and foster a more inclusive AI community.

---

## Article 77
### Title@2025-05-26 (1): A Unified, Practical, and Understandable Model of Non-transactional   Consistency Levels in Distributed Replication

**Title**: A Unified, Practical, and Understandable Model of Non-transactional   Consistency Levels in Distributed Replication | Ein einheitliches, praktisches und verständliches Modell nichttransaktionsfähiger Konsistenzstufen in verteilter Replikation | 分布式重复中非交易一致性水平的统一、实用和可理解的模式 [2409.01576v4](http://arxiv.org/abs/2409.01576v4)

**Authors**: Guanzhou Hu, Andrea Arpaci-Dusseau, Remzi Arpaci-Dusseau

We present a practical model of non-transactional consistency levels in the context of distributed data replication. Unlike prior work, our simple Shared Object Pool (SOP) model defines common consistency levels in a unified framework centered around the single concept of ordering. This naturally reflects modern cloud object storage services and is thus easy to understand. We show that a consistency level can be intuitively defined by specifying two types of constraints on the validity of orderings allowed by the level: convergence, which bounds the lineage shape of the ordering, and relationship, which bounds the relative positions between operations. We give examples of representative protocols and systems, and discuss their availability upper bound. To further demonstrate the expressiveness and practical relevance of our model, we use it to implement a Jepsen-integrated consistency checker for the four most common levels (linearizable, sequential, causal+, and eventual); the checker analyzes consistency conformity for small-scale histories of real system runs (etcd, ZooKeeper, and RabbitMQ).

---

## Article 78
### Title@2025-05-26 (1): Justin: Hybrid CPU/Memory Elastic Scaling for Distributed Stream   Processing

**Title**: Justin: Hybrid CPU/Memory Elastic Scaling for Distributed Stream   Processing | Justin: Hybride CPU/Memory Elastic Scaling für verteilte Stream-Verarbeitung | Justin: 用于分布流处理的混合 CPU/Memory Elastic 缩放比例 [2505.19739v1](http://arxiv.org/abs/2505.19739v1)

**Authors**: Donatien Schmitz, Guillaume Rosinosky, Etienne Rivière

Distributed Stream Processing (DSP) engines analyze continuous data via queries expressed as a graph of operators. Auto-scalers adjust the number of parallel instances of these operators to support a target rate. Current auto-scalers couple CPU and memory scaling, allocating resources as one-size-fits-all packages. This contrasts with operators' high diversity of requirements. We present Justin, an auto-scaler that enables hybrid CPU and memory scaling of DSP operators. Justin monitors both CPU usage and the performance of operators' storage operations. Its mechanisms enable finegrain memory allocation for tasks upon a query reconfiguration. The Justin policy identifies individual operators' memory pressure and decides between adjusting parallelism and/or memory assignment. We implement Justin in Apache Flink, extending the Flink Kubernetes Operator and the DS2 CPU-only auto-scaler. Using the Nexmark benchmark, our evaluation shows that Justin identifies suitable resource allocation in as many or fewer reconfiguration steps as DS2 and supports a target rate with significantly fewer CPU and memory resources.

---

## Article 79
### Title@2025-05-26 (1): Towards Optimal Distributed Edge Coloring with Fewer Colors

**Title**: Towards Optimal Distributed Edge Coloring with Fewer Colors | Auf dem Weg zu einer optimalen verteilten Randfärbung mit weniger Farben | 向最优化分布式边缘配色,颜色更少 [2504.13003v2](http://arxiv.org/abs/2504.13003v2)

**Authors**: Manuel Jakob, Yannic Maus, Florian Schager

There is a huge difference in techniques and runtimes of distributed algorithms for problems that can be solved by a sequential greedy algorithm and those that cannot. A prime example of this contrast appears in the edge coloring problem: while $(2\Delta-1)$-edge coloring can be solved in $\mathcal{O}(\log^{\ast}(n))$ rounds on constant-degree graphs, the seemingly minor reduction to $(2\Delta-2)$ colors leads to an $\Omega(\log n)$ lower bound [Chang, He, Li, Pettie & Uitto, SODA'18]. Understanding this sharp divide between very local problems and inherently more global ones remains a central open question in distributed computing and it is a core focus of this paper.   As our main contribution we design a deterministic distributed $\mathcal{O}(\log n)$-round reduction from the $(2\Delta-2)$-edge coloring problem to the much easier $(2\Delta-1)$-edge coloring problem. This reduction is optimal, as the $(2\Delta-2)$-edge coloring problem admits an $\Omega(\log n)$ lower bound, whereas the $2\Delta-1$-edge coloring problem can be solved in $\mathcal{O}(\log^{\ast}n)$ rounds. By plugging in the $(2\Delta-1)$-edge coloring algorithms from [Balliu, Brandt, Kuhn & Olivetti, PODC'22] running in $\mathcal{O}(\log^{12}\Delta + \log^{\ast} n)$ rounds, we obtain an optimal runtime of $\mathcal{O}(\log n)$ rounds as long as $\Delta = 2^{\mathcal{O}(\log^{1/12} n)}$. Furthermore, on general graphs our reduction improves the runtime from $\widetilde{\mathcal{O}}(\log^3 n)$ to $\widetilde{\mathcal{O}}(\log^{5/3} n)$.   In addition, we also obtain an optimal $\mathcal{O}(\log \log n)$-round randomized reduction of $(2\Delta - 2)$-edge coloring to $(2\Delta - 1)$-edge coloring. Lastly, we obtain an $\mathcal{O}(\log_\Delta n)$-round reduction from the $(2\Delta-1)$-edge coloring, albeit to the somewhat harder maximal independent set (MIS) problem.

---

## Article 80
### Title@2025-05-26 (1): Byzantine Consensus in the Random Asynchronous Model

**Title**: Byzantine Consensus in the Random Asynchronous Model | Byzantinischer Konsens im zufälligen asynchronen Modell | 随机非同步模型中的拜占庭共识 [2502.09116v2](http://arxiv.org/abs/2502.09116v2)

**Authors**: George Danezis, Jovan Komatovic, Lefteris Kokoris-Kogias, Alberto Sonnino, Igor Zablotchi

We propose a novel relaxation of the classic asynchronous network model, called the random asynchronous model, which removes adversarial message scheduling while preserving unbounded message delays and Byzantine faults. Instead of an adversary dictating message order, delivery follows a random schedule. We analyze Byzantine consensus at different resilience thresholds ($n=3f+1$, $n=2f+1$, and $n=f+2$) and show that our relaxation allows consensus with probabilistic guarantees which are impossible in the standard asynchronous model or even the partially synchronous model. We complement these protocols with corresponding impossibility results, establishing the limits of consensus in the random asynchronous model.

---

## Article 81
### Title@2025-05-26 (1): Mosaic: Data-Free Knowledge Distillation via Mixture-of-Experts for   Heterogeneous Distributed Environments

**Title**: Mosaic: Data-Free Knowledge Distillation via Mixture-of-Experts for   Heterogeneous Distributed Environments | Mosaic: Datenfreies Wissen Destillieren über Mixture-of-Experts für Heterogene verteilte Umgebungen | Mosaic:通过混合专家进行无数据知识蒸馏,促进异基因分布式环境 [2505.19699v1](http://arxiv.org/abs/2505.19699v1)

**Authors**: Junming Liu, Yanting Gao, Siyuan Meng, Yifei Sun, Aoqi Wu, Yufei Jin, Yirong Chen, Ding Wang, Guosun Zeng

Federated Learning (FL) is a decentralized machine learning paradigm that enables clients to collaboratively train models while preserving data privacy. However, the coexistence of model and data heterogeneity gives rise to inconsistent representations and divergent optimization dynamics across clients, ultimately hindering robust global performance. To transcend these challenges, we propose Mosaic, a novel data-free knowledge distillation framework tailored for heterogeneous distributed environments. Mosaic first trains local generative models to approximate each client's personalized distribution, enabling synthetic data generation that safeguards privacy through strict separation from real data. Subsequently, Mosaic forms a Mixture-of-Experts (MoE) from client models based on their specialized knowledge, and distills it into a global model using the generated data. To further enhance the MoE architecture, Mosaic integrates expert predictions via a lightweight meta model trained on a few representative prototypes. Extensive experiments on standard image classification benchmarks demonstrate that Mosaic consistently outperforms state-of-the-art approaches under both model and data heterogeneity. The source code has been published at https://github.com/Wings-Of-Disaster/Mosaic.

---

## Article 82
### Title@2025-05-26 (1): PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM   Serving

**Title**: PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM   Serving | PRESSERVE: Prefetching Modellgewichte und KV-Cache in verteilter LLM-Servierung | PRESSERVE: 分布式LLM服务中的预伸缩模型重量和 KV-缓冲 [2501.08192v2](http://arxiv.org/abs/2501.08192v2)

**Authors**: Ahmet Caner Yüzügüler, Jiawei Zhuang, Lukas Cavigelli

Large language models (LLMs) are typically served from clusters of GPUs/NPUs that consist of large number of devices. Unfortunately, communication between these devices incurs significant overhead, increasing the inference latency and cost while limiting the scalability. Prior work addressed this issue by overlapping communication with compute, but has severe limitations due to the data dependencies between these operations. In this paper, we propose PRESERVE, a novel framework that prefetches model weights and KV-cache from off-chip HBM memory to the on-chip cache of AI accelerators during the communication operations, which offers various advantages and performance improvements compared to prior methods.   Through extensive experiments conducted on commercial AI accelerators, we demonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs. Additionally, we perform a design space exploration that identifies the optimal hardware configuration for the proposed method, showing a further 1.25x improvement in performance per cost by selecting the optimal L2 cache size. Our results show that PRESERVE has the potential to mitigate the memory bottlenecks and communication overheads, offering a solution to improve the performance and scalability of the LLM inference systems.

---

## Article 83
### Title@2025-05-26 (1): Scaling Large-scale GNN Training to Thousands of Processors on CPU-based   Supercomputers

**Title**: Scaling Large-scale GNN Training to Thousands of Processors on CPU-based   Supercomputers | Skalierung von großformatigen GNN-Schulungen zu Tausenden von Prozessoren auf CPU-basierten Supercomputern | 向数千台基于CPU的超级计算机处理器提供大规模GNN培训 [2411.16025v2](http://arxiv.org/abs/2411.16025v2)

**Authors**: Chen Zhuang, Lingqi Zhang, Du Wu, Peng Chen, Jiajun Huang, Xin Liu, Rio Yokota, Nikoli Dryden, Toshio Endo, Satoshi Matsuoka, Mohamed Wahib

Graph Convolutional Networks (GCNs), particularly for large-scale graphs, are crucial across numerous domains. However, training distributed full-batch GCNs on large-scale graphs suffers from inefficient memory access patterns and high communication overhead. To address these challenges, we introduce \method{}, an efficient and scalable distributed GCN training framework tailored for CPU-powered supercomputers. Our contributions are threefold: (1) we develop general and efficient aggregation operators designed for irregular memory access, (2) we propose a hierarchical aggregation scheme that reduces communication costs without altering the graph structure, and (3) we present a communication-aware quantization scheme to enhance performance. Experimental results demonstrate that \method{} achieves a speedup of up to 6$\times$ compared with the SoTA implementations, and scales to 1000s of HPC-grade CPUs on the largest publicly available datasets, without sacrificing model convergence and accuracy. Moreover, due to the effective strong scaling of \method{}, we outperform SoTA GPU-based and CPU-based distributed full-batch GCN training frameworks, in absolute performance, for large-scale graphs.

---

## Article 84
### Title@2025-05-26 (1): Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive   Decisions of LLMs

**Title**: Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive   Decisions of LLMs | Gewinnen Sie schnell oder verlieren Sie langsam: Ausgleichende Geschwindigkeit und Genauigkeit in Latenz-Sensitive Entscheidungen von LLMs | 慢赢或慢输:LLMs的延缓敏感决定中平衡速度和准确性 [2505.19481v1](http://arxiv.org/abs/2505.19481v1)

**Authors**: Hao Kang, Qingru Zhang, Han Cai, Weiyuan Xu, Tushar Krishna, Yilun Du, Tsachy Weissman

Large language models (LLMs) have shown remarkable performance across diverse reasoning and generation tasks, and are increasingly deployed as agents in dynamic environments such as code generation and recommendation systems. However, many real-world applications, such as high-frequency trading and real-time competitive gaming, require decisions under strict latency constraints, where faster responses directly translate into higher rewards. Despite the importance of this latency quality trade off, it remains underexplored in the context of LLM based agents. In this work, we present the first systematic study of this trade off in real time decision making tasks. To support our investigation, we introduce two new benchmarks: HFTBench, a high frequency trading simulation, and StreetFighter, a competitive gaming platform. Our analysis reveals that optimal latency quality balance varies by task, and that sacrificing quality for lower latency can significantly enhance downstream performance. To address this, we propose FPX, an adaptive framework that dynamically selects model size and quantization level based on real time demands. Our method achieves the best performance on both benchmarks, improving win rate by up to 80% in Street Fighter and boosting daily yield by up to 26.52% in trading, underscoring the need for latency aware evaluation and deployment strategies for LLM based agents. These results demonstrate the critical importance of latency aware evaluation and deployment strategies for real world LLM based agents. Our benchmarks are available at Latency Sensitive Benchmarks.

---

## Article 85
### Title@2025-05-26 (1): GPU acceleration of non-equilibrium Green's function calculation using   OpenACC and CUDA FORTRAN

**Title**: GPU acceleration of non-equilibrium Green's function calculation using   OpenACC and CUDA FORTRAN | GPU-Beschleunigung der Nicht-Equilibrium Green-Funktionsberechnung mit OpenACC und CUDA FORTRAN | 使用 OpenACC 和 CUDA FORTRAN 加速 GPU 绿色非平衡的功能计算 [2505.19467v1](http://arxiv.org/abs/2505.19467v1)

**Authors**: Jia Yin, Khaled Z. Ibrahim, Mauro Del Ben, Jack Deslippe, Yang-hao Chan, Chao Yang

The numerical solution of the Kadanoff-Baym nonlinear integro-differential equations, which yields the non-equilibrium Green's functions (NEGFs) of quantum many-body systems, poses significant computational challenges due to its high computational complexity. In this work, we present efficient implementations of a numerical method for solving these equations on distributed-memory architectures, including many-core CPUs and multi-GPU systems. For CPU-based platforms, we adopt a hybrid MPI/OpenMP programming model to exploit both inter-node and intra-node parallelism. On GPU-accelerated systems, we implement the method using two distinct approaches: MPI/OpenACC and MPI/CUDA FORTRAN. Several optimization strategies are employed to enhance GPU performance, including techniques to maximize computational resource utilization and minimize the overhead associated with kernel launches and memory management. Although OpenACC is easy to use, CUDA FORTRAN provides more advanced features for configuring and managing multiple levels of concurrency, while also simplifying memory allocation and data movement between host and device. This flexibility translates into significant performance improvements. We compare the performance of the three implementations and demonstrate that the GPU-based approaches achieve substantial speedups over CPU-based implementations. Furthermore, both CPU and GPU versions exhibit excellent strong and weak scaling, confirming the scalability and efficiency of our approach for large-scale NEGF computations.

---

## Article 86
### Title@2025-05-26 (1): FedHERO: A Federated Learning Approach for Node Classification Task on   Heterophilic Graphs

**Title**: FedHERO: A Federated Learning Approach for Node Classification Task on   Heterophilic Graphs | FedHERO: Ein Federated Learning Approach für Knotenklassifikation Aufgaben auf heterophilen Graphen | FEFHERO: 异生物图节点分类任务联邦学习方法 [2504.21206v2](http://arxiv.org/abs/2504.21206v2)

**Authors**: Zihan Chen, Xingbo Fu, Yushun Dong, Jundong Li, Cong Shen

Federated Graph Learning (FGL) empowers clients to collaboratively train Graph neural networks (GNNs) in a distributed manner while preserving data privacy. However, FGL methods usually require that the graph data owned by all clients is homophilic to ensure similar neighbor distribution patterns of nodes. Such an assumption ensures that the learned knowledge is consistent across the local models from all clients. Therefore, these local models can be properly aggregated as a global model without undermining the overall performance. Nevertheless, when the neighbor distribution patterns of nodes vary across different clients (e.g., when clients hold graphs with different levels of heterophily), their local models may gain different and even conflict knowledge from their node-level predictive tasks. Consequently, aggregating these local models usually leads to catastrophic performance deterioration on the global model. To address this challenge, we propose FedHERO, an FGL framework designed to harness and share insights from heterophilic graphs effectively. At the heart of FedHERO is a dual-channel GNN equipped with a structure learner, engineered to discern the structural knowledge encoded in the local graphs. With this specialized component, FedHERO enables the local model for each client to identify and learn patterns that are universally applicable across graphs with different patterns of node neighbor distributions. FedHERO not only enhances the performance of individual client models by leveraging both local and shared structural insights but also sets a new precedent in this field to effectively handle graph data with various node neighbor distribution patterns. We conduct extensive experiments to validate the superior performance of FedHERO against existing alternatives.

---

## Article 87
### Title@2025-05-25 (7): QMIO: A tightly integrated hybrid HPCQC system

**Title**: QMIO: A tightly integrated hybrid HPCQC system | QMIO: Ein eng integriertes Hybrid-HPCQC-System | QMIO:一个严格一体化的混合高和分PCQC系统 [2505.19267v1](http://arxiv.org/abs/2505.19267v1)

**Authors**: Javier Cacheiro, Álvaro C Sánchez, Russell Rundle, George B Long, Gavin Dold, Jamie Friel, Andrés Gómez

High-Performance Computing (HPC) systems are the most powerful tools that we currently have to solve complex scientific simulations. Quantum computing (QC) has the potential to enhance HPC systems by accelerating the execution of specific kernels that can be offloaded to a Quantum Processing Unit (QPU), granting them new capabilities, improving the speed of computation, or reducing energy consumption. In this paper, we present QMIO: a state-of-the-art hybrid HPCQC system, which tightly integrates HPC and QC. We describe its hardware and software components, the integration middleware, and the lessons learned during the design, implementation, and operation of the system.

---

## Article 88
### Title@2025-05-25 (7): NanoFlow: Towards Optimal Large Language Model Serving Throughput

**Title**: NanoFlow: Towards Optimal Large Language Model Serving Throughput | NanoFlow: Auf dem Weg zu einem optimalen Large Language Model | NanoFlow:走向最佳大语言模式 [2408.12757v2](http://arxiv.org/abs/2408.12757v2)

**Authors**: Kan Zhu, Yufei Gao, Yilong Zhao, Liangyu Zhao, Gefei Zuo, Yile Gu, Dedong Xie, Tian Tang, Qinyu Xu, Zihao Ye, Keisuke Kamahori, Chien-Yu Lin, Ziren Wang, Stephanie Wang, Arvind Krishnamurthy, Baris Kasikci

Large Language Models (LLMs) have resulted in a surging demand for planet-scale serving systems, where tens of thousands of GPUs continuously serve hundreds of millions of users. Consequently, throughput has emerged as a key metric that determines serving systems' performance. Due to large model sizes and memory-intensive self-attention, LLM serving has been commonly assumed to be memory-bound. Through a detailed analysis, we show that despite having memory-intensive components, end-to-end LLM serving is compute bound for most common workloads and LLMs. Alas, most existing serving engines fall short from optimal compute utilization, because the heterogeneous operations that comprise LLM serving--compute, memory, networking--are executed sequentially within a device.   We propose NanoFlow, a novel serving framework that exploits intra-device parallelism, which overlaps the usage of heterogeneous resources within a single device. NanoFlow splits inputs into smaller nano-batches and duplicates operations to operate on each portion independently, enabling overlapping. NanoFlow automatically identifies the number, size, ordering, and GPU resource allocation of nano-batches to minimize the execution time, while considering the interference of concurrent operations. We evaluate NanoFlow's end-to-end serving throughput on several popular models such as LLaMA-2-70B, Mixtral 8x7B, LLaMA-3-8B, etc. With practical workloads, NanoFlow provides 1.91x throughput boost compared to state-of-the-art serving systems achieving 50% to 72% of optimal throughput across popular models.

---

## Article 89
### Title@2025-05-25 (7): Matrix Multiplication in the MPC Model

**Title**: Matrix Multiplication in the MPC Model | Matrix-Multiplikation im MPC-Modell | MPC 模型中的矩阵乘法 [2505.19137v1](http://arxiv.org/abs/2505.19137v1)

**Authors**: Atharv Chhabra, Arya Deshmukh, Chetan Gupta, Lakshya Joshi

In this paper, we study the matrix multiplication problem in the MPC model. We have two matrices, and the task is to compute their product. These matrices are evenly distributed over $P$ processors. Each processor has $M$ memory such that $P \cdot M \geq $ (size of the matrices). The computation proceeds in synchronous rounds. In a communication round, a processor can send and receive messages to(from) any other processor, with the total size of messages sent or received being $O(M)$. We give an almost complete characterisation of the problem in various settings. We prove tight upper bounds and lower bounds for the problems in three different settings--when the given input matrices are (i) general square matrices, (ii) rectangular matrices, and (iii) sparse square matrices (that is, each row and column contains a bounded number of nonzero elements). In particular, we prove the following results:   1. Multiplication of two $n \times n$ matrices in the MPC model with $n^\alpha$ processors each with $O(n^{2-\alpha})$ memory, requires $\Theta(n^{\frac{\alpha}{2}})$ rounds in semirings.   2. Multiplication of two rectangular matrices of size $n \times d$ and $d \times n$ (where $d \leq n$) respectively, with $n$ processors of $O(n)$ memory requires $\Theta(\frac{d}{\sqrt{n}})$ rounds in semirings.   3. Multiplication of two rectangular matrices of size $d \times n$ and $n \times d$ ( where $d \leq n$) respectively requires   i. $\Theta(\sqrt{d} + \log_d n)$ rounds with $n$ processors and $O(d)$ memory per processor in semirings   ii. $\Theta (\frac{d}{\sqrt{n}})$ rounds with $d$ processors and $O(n)$ memory per processor in semirings.   4. Multiplication of two $d$-sparse matrices (each row and column of the matrices contains at most $d$-nonzero elements) with $n$ processors and $O(d)$ memory per processor can be done in $O(d^{0.9})$ rounds in semirings.

---

## Article 90
### Title@2025-05-25 (7): Birch SGD: A Tree Graph Framework for Local and Asynchronous SGD Methods

**Title**: Birch SGD: A Tree Graph Framework for Local and Asynchronous SGD Methods | Birke SGD: Ein Baumdiagramm-Framework für lokale und asynchrone SGD-Methoden | Birch SGD: 当地和非同步 SGD 方法树图框架 [2505.09218v2](http://arxiv.org/abs/2505.09218v2)

**Authors**: Alexander Tyurin, Danil Sivtsov

We propose a new unifying framework, Birch SGD, for analyzing and designing distributed SGD methods. The central idea is to represent each method as a weighted directed tree, referred to as a computation tree. Leveraging this representation, we introduce a general theoretical result that reduces convergence analysis to studying the geometry of these trees. This perspective yields a purely graph-based interpretation of optimization dynamics, offering a new and intuitive foundation for method development. Using Birch SGD, we design eight new methods and analyze them alongside previously known ones, with at least six of the new methods shown to have optimal computational time complexity. Our research leads to two key insights: (i) all methods share the same "iteration rate" of $O\left(\frac{(R + 1) L \Delta}{\varepsilon} + \frac{\sigma^2 L \Delta}{\varepsilon^2}\right)$, where $R$ the maximum "tree distance" along the main branch of a tree; and (ii) different methods exhibit different trade-offs-for example, some update iterates more frequently, improving practical performance, while others are more communication-efficient or focus on other aspects. Birch SGD serves as a unifying framework for navigating these trade-offs. We believe these results provide a unified foundation for understanding, analyzing, and designing efficient asynchronous and parallel optimization methods.

---

## Article 91
### Title@2025-05-24 (6): Toward Malicious Clients Detection in Federated Learning

**Title**: Toward Malicious Clients Detection in Federated Learning | Auf dem Weg zu bösartigen Kunden Erkennung im Föderierten Lernen | 争取在联邦学习中发现恶意客户 [2505.09110v2](http://arxiv.org/abs/2505.09110v2)

**Authors**: Zhihao Dou, Jiaqi Wang, Wei Sun, Zhuqing Liu, Minghong Fang

Federated learning (FL) enables multiple clients to collaboratively train a global machine learning model without sharing their raw data. However, the decentralized nature of FL introduces vulnerabilities, particularly to poisoning attacks, where malicious clients manipulate their local models to disrupt the training process. While Byzantine-robust aggregation rules have been developed to mitigate such attacks, they remain inadequate against more advanced threats. In response, recent advancements have focused on FL detection techniques to identify potentially malicious participants. Unfortunately, these methods often misclassify numerous benign clients as threats or rely on unrealistic assumptions about the server's capabilities. In this paper, we propose a novel algorithm, SafeFL, specifically designed to accurately identify malicious clients in FL. The SafeFL approach involves the server collecting a series of global models to generate a synthetic dataset, which is then used to distinguish between malicious and benign models based on their behavior. Extensive testing demonstrates that SafeFL outperforms existing methods, offering superior efficiency and accuracy in detecting malicious clients.

---

## Article 92
### Title@2025-05-24 (6): Distributed Incremental SAT Solving with Mallob: Report and Case Study   with Hierarchical Planning

**Title**: Distributed Incremental SAT Solving with Mallob: Report and Case Study   with Hierarchical Planning | Distributed Incremental SAT Solving with Mallob: Report and Case Study with Hierarchical Planning | 与马洛布公司共同解决:与等级规划有关的报告和案例研究 [2505.18836v1](http://arxiv.org/abs/2505.18836v1)

**Authors**: Dominik Schreiber

This report describes an extension of the distributed job scheduling and SAT solving platform Mallob by incremental SAT solving, embedded in a case study on SAT-based hierarchical planning. We introduce a low-latency interface for incremental jobs and specifically for IPASIR-style incremental SAT solving to Mallob. This also allows to process many independent planning instances in parallel via Mallob's scheduling capabilities. In an experiment where 587 planning inputs are resolved in parallel on 2348 cores, we observe significant speedups for several planning domains where SAT solving constitutes a major part of the planner's running time. These findings indicate that our approach to distributed incremental SAT solving may be useful for a wide range of SAT applications.

---

## Article 93
### Title@2025-05-24 (6): DiSCo: Device-Server Collaborative LLM-Based Text Streaming Services

**Title**: DiSCo: Device-Server Collaborative LLM-Based Text Streaming Services | DiSCo: Geräte-Server Kollaborative LLM-basierte Text-Streaming-Dienste | DisCo: 设备-服务器协作协作LLM基于LLM的文本流服务 [2502.11417v2](http://arxiv.org/abs/2502.11417v2)

**Authors**: Ting Sun, Penghan Wang, Fan Lai

The rapid rise of large language models (LLMs) in text streaming services has introduced significant cost and Quality of Experience (QoE) challenges in serving millions of daily requests, especially in meeting Time-To-First-Token (TTFT) and Time-Between-Token (TBT) requirements for real-time interactions. Our real-world measurements show that both server-based and on-device deployments struggle to meet diverse QoE demands: server deployments face high costs and last-hop issues (e.g., Internet latency and dynamics), while on-device LLM inference is constrained by resources.   We introduce DiSCo, a device-server cooperative scheduler designed to optimize users' QoE by adaptively routing requests and migrating response generation between endpoints while maintaining cost constraints. DiSCo employs cost-aware scheduling, leveraging the predictable speed of on-device LLM inference with the flexible capacity of server-based inference to dispatch requests on the fly, while introducing a token-level migration mechanism to ensure consistent token delivery during migration. Evaluations on real-world workloads -- including commercial services like OpenAI GPT and DeepSeek, and open-source deployments such as LLaMA3 -- show that DiSCo can improve users' QoE by reducing tail TTFT (11-52\%) and mean TTFT (6-78\%) across different model-device configurations, while dramatically reducing serving costs by up to 84\% through its migration mechanism while maintaining comparable QoE levels.

---

## Article 94
### Title@2025-05-24 (6): Distributed Set-membership Filtering Frameworks For Multi-agent Systems   With Absolute and Relative Measurements

**Title**: Distributed Set-membership Filtering Frameworks For Multi-agent Systems   With Absolute and Relative Measurements | Distributed Set-Membership Filtering Frameworks für Multi-Agent-Systeme mit absoluten und relativen Messungen | 具有绝对和相对计量的多试剂系统分布式成员筛选框架 [2305.15797v2](http://arxiv.org/abs/2305.15797v2)

**Authors**: Yu Ding, Yirui Cong, Xiangke Wang

In this paper, we focus on the distributed set-membership filtering (SMFing) problem for a multi-agent system with absolute (taken from agents themselves) and relative (taken from neighbors) measurements. In the literature, the relative measurements are difficult to deal with, and the SMFs highly rely on specific set descriptions. As a result, establishing the general distributed SMFing framework having relative measurements is still an open problem. To solve this problem, first, we provide the set description based on uncertain variables determined by the relative measurements between two agents as the foundation. Surprisingly, the accurate description requires only a single calculation step rather than multiple iterations, which can effectively reduce computational complexity. Based on the derived set description, called the uncertain range, we propose two distributed SMFing frameworks: one calculates the joint uncertain range of the agent itself and its neighbors, while the other only computes the marginal uncertain range of each local system. Furthermore, we compare the performance of our proposed two distributed SMFing frameworks and the benchmark -- centralized SMFing framework. A rigorous set analysis reveals that the distributed SMF can be essentially considered as the process of computing the marginal uncertain range to outer bound the projection of the uncertain range obtained by the centralized SMF in the corresponding subspace. Simulation results corroborate the effectiveness of our proposed distributed frameworks and verify our theoretical analysis.

---

## Article 95
### Title@2025-05-24 (6): EvoSort: A Genetic-Algorithm-Based Adaptive Parallel Sorting Framework   for Large-Scale High Performance Computing

**Title**: EvoSort: A Genetic-Algorithm-Based Adaptive Parallel Sorting Framework   for Large-Scale High Performance Computing | EvoSort: Ein genetisch-algorithmisch-adaptives Parallelsortierungs-Framework für großformatige Hochleistungsrechnen | EvoSort: 大型高性能计算方法的基于遗传 -- -- 物理学的适应性平行排序框架 [2505.18681v1](http://arxiv.org/abs/2505.18681v1)

**Authors**: Shashank Raj, Kalyanmoy Deb

In today's era of big data, sorting enormous datasets is a major challenge. We present EvoSort, an adaptive parallel sorting framework that employs a Genetic Algorithm (GA) to automatically discover and refine critical parameters, including insertion sort and fallback thresholds, tile size, and mergesort vs Least Significant Digit (LSD) radix sort. EvoSort integrates parallel sorting primitives and adapts continuously to input data and system architecture, ensuring optimal performance. Experiments on up to 10 billion elements show that EvoSort consistently outperforms NumPy sorting by factors from three to over 90 times. EvoSort exemplifies a powerful auto-tuning solution for large-scale data processing.

---

## Article 96
### Title@2025-05-24 (6): Towards Round-Optimal Approximate Agreement on Trees

**Title**: Towards Round-Optimal Approximate Agreement on Trees | Auf dem Weg zu einem runden, optimalen Abkommen über Bäume | 争取达成关于树木的圆顶和最接近于 [2502.05591v2](http://arxiv.org/abs/2502.05591v2)

**Authors**: Marc Fuchs, Diana Ghinea, Zahra Parsaeian

Approximate Agreement (AA) is a key consensus primitive that, even in the presence of Byzantine faults, allows honest parties to obtain close (but not necessarily identical) outputs that lie within the range of their inputs. While the optimal round complexity of synchronous AA on real values is well understood, its extension to other input spaces remains an open problem.   Our work is concerned with AA on trees, where the parties hold as inputs vertices from a publicly known labeled tree $T$ and must output $1$-close vertices in the honest inputs' convex hull. We present an optimal-resilience protocol in the synchronous model, with round complexity $O\left(\frac{\log |V(T)|}{\log \log |V(T)|} \right)$, where $V(T)$ is the set of vertices in the input space tree $T$. Our protocol non-trivially reduces the problem of AA on trees to AA on real values.   Additionally, we extend the impossibility results regarding the round complexity of synchronous AA protocols on real values to trees: we prove a lower bound of $\Omega\left(\frac{\log D(T)}{\log \log D(T) + \log \frac{n + t}{t}} \right)$ rounds, where $D(T)$ denotes the diameter of the tree, $n$ denotes the number of parties, and $t$ denotes the number of Byzantine parties. This establishes the asymptotic optimality of our protocol for trees $T$ of diameter $D(T) \in |V(T)|^{\Theta(1)}$ given that $t \in \Theta(n)$.

---

## Article 97
### Title@2025-05-24 (6): Asynchronous Approximate Agreement with Quadratic Communication

**Title**: Asynchronous Approximate Agreement with Quadratic Communication | Asynchrone annähernde Vereinbarung mit quadratischer Kommunikation | 与赤道通信的近似非同步协定 [2408.05495v3](http://arxiv.org/abs/2408.05495v3)

**Authors**: Mose Mizrahi Erbes, Roger Wattenhofer

We consider an asynchronous network of $n$ message-sending parties, up to $t$ of which are byzantine. We study approximate agreement, where the parties obtain approximately equal outputs in the convex hull of their inputs. In their seminal work, Abraham, Amit and Dolev [OPODIS '04] solve this problem in $\mathbb{R}$ with the optimal resilience $t < \frac{n}{3}$ with a protocol where each party reliably broadcasts a value in every iteration. This takes $\Theta(n^2)$ messages per reliable broadcast, or $\Theta(n^3)$ messages per iteration.   In this work, we forgo reliable broadcast to achieve asynchronous approximate agreement against $t < \frac{n}{3}$ faults with quadratic communication. In trees of diameter $D$ and maximum degree $\Delta$, we achieve edge agreement in $\lceil{6\log_2 D}\rceil$ rounds with $\mathcal{O}(n^2)$ messages of size $\mathcal{O}(\log \Delta + \log\log D)$ per round. We do this by designing a 6-round multivalued 2-graded consensus protocol, and by repeatedly using it to reduce edge agreement in a tree of diameter $D$ to edge agreement in a tree of diameter $\frac{D}{2}$. Then, we achieve edge agreement in the infinite path $\mathbb{Z}$, again with the help of 2-graded consensus. Finally, by reducing $\varepsilon$-agreement in $\mathbb{R}$ to edge agreement in $\mathbb{Z}$, we show that our edge agreement protocol enables $\varepsilon$-agreement in $\mathbb{R}$ in $6\log_2(\frac{M}{\varepsilon} + 1) + \mathcal{O}(\log \log \frac{M}{\varepsilon})$ rounds with $\mathcal{O}(n^2 \log \frac{M}{\varepsilon})$ messages and $\mathcal{O}(n^2\log \frac{M}{\varepsilon}\log \log \frac{M}{\varepsilon})$ bits of communication, where $M$ is the maximum input magnitude.

---

## Article 98
### Title@2025-05-24 (6): TEE is not a Healer: Rollback-Resistant Reliable Storage

**Title**: TEE is not a Healer: Rollback-Resistant Reliable Storage | TEE ist kein Heiler: Rollback-Resistent Zuverlässige Lagerung | TEE不是救治者:回击-恢复-可靠储存 [2505.18648v1](http://arxiv.org/abs/2505.18648v1)

**Authors**: Sadegh Keshavarzi, Gregory Chockler, Alexey Gotsman

Recent advances in secure hardware technologies, such as Intel SGX or ARM TrustZone, offer an opportunity to substantially reduce the costs of Byzantine fault-tolerance by placing the program code and state within a secure enclave known as a Trusted Execution Environment (TEE). However, the protection offered by a TEE only applies during program execution. Once power is switched off, the non-volatile portion of the program state becomes vulnerable to rollback attacks wherein it is undetectably reverted to an older version. In this paper, we consider a problem of implementing reliable read/write registers out of failure-prone replicas subject to state rollbacks. To this end, we introduce a new unified model that captures the multiple failure types that can affect a TEE-based system. We then establish tight bounds on the fault-tolerance of register constructions in this model for both the static case, where failure thresholds hold throughout the entire execution, and the dynamic case, where they only hold eventually. Our dynamic register emulation algorithm resolves a long-standing question of how to correctly rebuild replica state upon restart without relying on additional hardware assumptions such as trusted monotonic counters.

---

## Article 99
### Title@2025-05-24 (6): CacheFL: Privacy-Preserving and Efficient Federated Cache Model   Fine-Tuning for Vision-Language Models

**Title**: CacheFL: Privacy-Preserving and Efficient Federated Cache Model   Fine-Tuning for Vision-Language Models | CacheFL: Datenschutzschonendes und effizientes Federated Cache Model Fine-Tuning für Vision-Language-Modelle | CACHFL: 视力和语言模型微调模型 [2505.05130v2](http://arxiv.org/abs/2505.05130v2)

**Authors**: Mengjun Yi, Hanwen Zhang, Hui Dou, Jian Zhao, Furao Shen

Large pre-trained Vision-Language Models (VLMs), such as Contrastive Language-Image Pre-training (CLIP), have exhibited remarkable zero-shot performance across various image classification tasks. Fine-tuning these models on domain-specific datasets further enhances their effectiveness for downstream applications. However, fine-tuning in cloud environments raises significant concerns regarding data security and privacy. Federated Learning (FL) offers a decentralized solution by enabling model training across local clients without centralizing sensitive data, but the high communication and computation costs of transmitting full pre-trained models during training limit its scalability. Additionally, non-Independent and Identically Distributed (non-IID) data across local clients can negatively impact model convergence and performance. To address these challenges, we propose CacheFL, a novel federated learning method that replaces traditional full model fine-tuning with lightweight cache model fine-tuning. The cache model is initialized using a class-balanced dataset generated by a generative pre-trained model, effectively mitigating the impact of non-IID data. This cache model is then distributed to local clients for fine-tuning, and the updated parameters from each client are aggregated on the server and redistributed. With the updated cache model, the classification performance of CLIP is improved after just a few epochs. By limiting the training and communication to the cache model, CacheFL significantly reduces resource demands while ensuring data privacy and security. Extensive experiments conducted on ImageNet and 10 additional datasets demonstrate that CacheFL outperforms traditional approaches in terms of classification accuracy, resource efficiency, and privacy preservation.

---

## Article 100
### Title@2025-05-24 (6): PacTrain: Pruning and Adaptive Sparse Gradient Compression for Efficient   Collective Communication in Distributed Deep Learning

**Title**: PacTrain: Pruning and Adaptive Sparse Gradient Compression for Efficient   Collective Communication in Distributed Deep Learning | PacTrain: Pruning and Adaptive Sparse Gradient Compression für effiziente kollektive Kommunikation im verteilten Deep Learning | PacTrain:在分布式深层学习中促进高效集体交流的审慎和适应性零散梯级压缩 [2505.18563v1](http://arxiv.org/abs/2505.18563v1)

**Authors**: Yisu Wang, Ruilong Wu, Xinjiao Li, Dirk Kutscher

Large-scale deep neural networks (DNN) exhibit excellent performance for various tasks. As DNNs and datasets grow, distributed training becomes extremely time-consuming and demands larger clusters. A main bottleneck is the resulting gradient aggregation overhead. While gradient compression and sparse collective communication techniques are commonly employed to alleviate network load, many gradient compression schemes do not achieve acceleration of the training process while also preserving accuracy. This paper introduces PacTrain, a novel framework that accelerates distributed training by combining pruning with sparse gradient compression. Active pruning of the neural network makes the model weights and gradients sparse. By ensuring the global knowledge of the gradient sparsity among all distributed training workers, we can perform lightweight compression communication without harming accuracy. We show that the PacTrain compression scheme achieves a near-optimal compression strategy while remaining compatible with the all-reduce primitive. Experimental evaluations show that PacTrain improves training throughput by 1.25 to 8.72 times compared to state-of-the-art compression-enabled systems for representative vision and language models training tasks under bandwidth-constrained conditions.

---

## Article 101
### Title@2025-05-24 (6): Consensus Under Adversary Majority Done Right

**Title**: Consensus Under Adversary Majority Done Right | Konsens unter gegnerischer Mehrheit Rechtsbeistand | 在相反多数下达成的共识 [2411.01689v3](http://arxiv.org/abs/2411.01689v3)

**Authors**: Srivatsan Sridhar, Ertem Nusret Tas, Joachim Neu, Dionysis Zindros, David Tse

A specter is haunting consensus protocols--the specter of adversary majority. Dolev and Strong in 1983 showed an early possibility for up to 99% adversaries. Yet, other works show impossibility results for adversaries above 50% under synchrony, seemingly the same setting as Dolev and Strong's. What gives? It is high time that we pinpoint a key culprit for this ostensible contradiction: the modeling details of clients. Are the clients sleepy or always-on? Are they silent or communicating? Can validators be sleepy too? We systematize models for consensus across four dimensions (sleepy/always-on clients, silent/communicating clients, sleepy/always-on validators, and synchrony/partial-synchrony), some of which are new, and tightly characterize the achievable safety and liveness resiliences with matching possibilities and impossibilities for each of the sixteen models. To this end, we unify folklore and earlier results, and fill gaps left in the literature with new protocols and impossibility theorems.

---

## Article 102
### Title@2025-05-24 (6): Recursive Offloading for LLM Serving in Multi-tier Networks

**Title**: Recursive Offloading for LLM Serving in Multi-tier Networks | Rekursives Offloading für LLM-Serving in Multi-Tier-Netzwerken | 多层网络LLM服务的递归性卸载 [2505.16502v2](http://arxiv.org/abs/2505.16502v2)

**Authors**: Zhiyuan Wu, Sheng Sun, Yuwei Wang, Min Liu, Bo Gao, Jinda Lu, Zheming Yang, Tian Wen

Heterogeneous device-edge-cloud computing infrastructures have become widely adopted in telecommunication operators and Wide Area Networks (WANs), offering multi-tier computational support for emerging intelligent services. With the rapid proliferation of Large Language Model (LLM) services, efficiently coordinating inference tasks and reducing communication overhead within these multi-tier network architectures becomes a critical deployment challenge. Existing LLM serving paradigms exhibit significant limitations: on-device deployment supports only lightweight LLMs due to hardware constraints, while cloud-centric deployment suffers from resource congestion and considerable prompt communication overhead caused by frequent service requests during peak periods. Although the model-cascading-based inference strategy adapts better to multi-tier networks, its reliance on fine-grained, manually adjusted thresholds makes it less responsive to dynamic network conditions and varying task complexities. To address these challenges, we propose RecServe, a recursive offloading framework tailored for LLM serving in multi-tier networks. RecServe integrates a task-specific hierarchical confidence evaluation mechanism that guides offloading decisions based on inferred task complexity in progressively scaled LLMs across device, edge, and cloud tiers. To further enable intelligent task routing across tiers, RecServe employs a sliding-window-based dynamic offloading strategy with quantile interpolation, enabling real-time tracking of historical confidence distributions and adaptive offloading threshold adjustments. Experiments on eight datasets demonstrate that RecServe outperforms CasServe in both service quality and communication efficiency, and reduces the communication burden by over 50\% compared to centralized cloud-based serving.

---

