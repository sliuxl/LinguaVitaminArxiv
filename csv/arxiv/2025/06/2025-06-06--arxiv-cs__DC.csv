,abstract,abstract-zh,authors,date,title,title-de,title-zh,url
0,"Communication between agents often constitutes a major computational bottleneck in distributed learning. One of the most common mitigation strategies is to compress the information exchanged, thereby reducing communication overhead. To counteract the degradation in convergence associated with compressed communication, error feedback schemes -- most notably $\mathrm{EF}$ and $\mathrm{EF}^{21}$ -- were introduced. In this work, we provide a tight analysis of both of these methods. Specifically, we find the Lyapunov function that yields the best possible convergence rate for each method -- with matching lower bounds. This principled approach yields sharp performance guarantees and enables a rigorous, apples-to-apples comparison between $\mathrm{EF}$, $\mathrm{EF}^{21}$, and compressed gradient descent. Our analysis is carried out in a simplified yet representative setting, which allows for clean theoretical insights and fair comparison of the underlying mechanisms.","代理商之间的沟通往往构成分布式学习中的主要计算瓶颈。 最常见的缓解战略之一是压缩所交流的信息,从而减少通信管理费用。 为了应对与压缩通信相关的趋同性差,引入了错误反馈计划(主要是$\mathrm{EF}$和$\mathrm{EF}_21}$) 。在这项工作中,我们对这两种方法进行了严格的分析。具体地说,我们发现Lyapunov功能为每种方法提供了尽可能最佳的趋同率 -- -- 与较低的界限相匹配。这一原则性方法提供了敏锐的绩效保障,使得能够对$\mathrm{EF}美元、$\mathrm{EF}21}美元和压缩梯度下降进行严格的苹果到应用的比较。我们的分析是在一个简化但具有代表性的环境下进行的,从而可以对基本机制进行干净的理论洞察和公平比较。","Daniel Berg Thomsen, Adrien Taylor, Aymeric Dieuleveut",2025-06-05T17:30:18Z,Tight analyses of first-order methods with error feedback,Enge Analysen von First-Order-Methoden mit Fehlerrückmeldung,利用错误反馈对一级处理方法进行严格分析,http://arxiv.org/abs/2506.05271v1
1,"Distributed certification is a framework in distributed computing where nodes in a network jointly verify whether the whole graph satisfies a given property. A locally checkable proof (LCP) is a non-deterministic distributed algorithm used to verify global properties of a graph $G$, involving a prover and a verifier. The prover is a powerful entity that assigns certificates to nodes, which are then locally checked by the verifier.   An LCP is correct if it satisfies completeness and soundness. Completeness means that, for any graph $G$ satisfying a property $\Pi$, there exists a certificate assignment accepted by all nodes. Soundness ensures that for every graph not satisfying $\Pi$, at least one node rejects any certificate assignment.   We study how to certify that a graph is bipartite (i.e., $2$-colorable) with an LCP that hides the $2$-coloring from the verifier. An LCP is hiding if no local algorithm can reconstruct the coloring from a valid certificate. Motivated by promise-free separations in the LOCAL model and its variants, we also require strong soundness: in a no-instance, the subgraph induced by accepting nodes must be $2$-colorable. An LCP with completeness, soundness, hiding, and strong soundness is called strong and hiding.   We show that such LCPs for $2$-coloring exist in specific graph classes, using only $O(\log n)$-size certificates. If the input is a cycle or has a node of degree 1, these LCPs also work in anonymous networks with constant-size certificates.   We also prove that no strong and hiding LCP exists for general graphs unless node identifiers are available and certificates are of size $\omega(1)$. In anonymous networks, this lower bound holds regardless of the certificate size. We also present a characterization of the hiding property for $k$-coloring, which plays a key role in future investigations.","分布式认证是一个分布式计算的框架, 其中网络中的节点可以共同验证整个图形是否满足给定的属性。 本地可检查的证明 (LCP) 是用于核查图形$G$的全球属性的非确定性分布式算法, 涉及一个验证人和一个验证人。 验证人是一个强大的实体, 将证书指定给节点, 然后由校验人进行本地检查。 一个 LCP 是准确的。 完整性意味着, 任何图形 $G$ 符合一个属性 $\Pi$ , 都存在一个被所有节点所接受的证书任务。 一个本地可检查的证明( LCP) 是一个非确定性分布式的算法, 用于核查$GP$, 涉及验证人, 涉及验证人, 证明每个图表是双部分的( $- 彩色) , 由校验人 校验人进行。 一个我们无法重的本地算人 , 也意味着, 以纯度 美元 身份来存储 。","Augusto Modanese, Pedro Montealegre, Martín Ríos-Wilson",2025-06-05T16:00:44Z,Strong and Hiding Distributed Certification of Bipartiteness,Starke und versteckte Zertifizierung der Zweiparteilichkeit,强有力的和隐藏的两党分布证明书,http://arxiv.org/abs/2502.13854v2
2,"Recently, federated learning frameworks such as Python TestBed for Federated Learning Algorithms and MicroPython TestBed for Federated Learning Algorithms have emerged to tackle user privacy concerns and efficiency in embedded systems. Even more recently, an efficient federated anomaly detection algorithm, FLiForest, based on Isolation Forests has been developed, offering a low-resource, unsupervised method well-suited for edge deployment and continuous learning. In this paper, we present an application of Isolation Forest-based temperature anomaly detection, developed using the previously mentioned federated learning frameworks, aimed at small edge devices and IoT systems running MicroPython. The system has been experimentally evaluated, achieving over 96% accuracy in distinguishing normal from abnormal readings and above 78% precision in detecting anomalies across all tested configurations, while maintaining a memory usage below 160 KB during model training. These results highlight its suitability for resource-constrained environments and edge systems, while upholding federated learning principles of data privacy and collaborative learning.","最近,联邦学习联盟测试仪(Python TestBed)和联邦学习联盟测试仪(MicroPython TestBed)等联合学习框架(MicroPython TestBed)已经出现,以解决嵌入系统中的用户隐私问题和效率问题,甚至最近,基于隔离森林开发了高效的联邦异常检测算法(FLiForest),为边缘部署和持续学习提供了低资源、不受监督的方法。在本文中,我们介绍了采用隔离森林温度异常检测法(Isocation Forest)的做法,该方法是利用上述联合学习框架开发的,针对小型边缘装置和运行MicroPython的IoT系统。这个系统已经进行了实验性评估,实现了超过96%的准确度,与所有测试的配置的异常读数的正常值和超过78%的精确度,同时在模型培训期间将记忆使用率维持在160 KB以下。这些结果突出表明它适合资源紧张的环境和边缘系统,同时坚持数据隐私和协作学习的联邦学习原则。","Pavle Vasiljevic, Milica Matic, Miroslav Popovic",2025-06-05T15:22:04Z,Federated Isolation Forest for Efficient Anomaly Detection on Edge IoT   Systems,Föderierter Isolationswald für effiziente Anomalienerkennung an Edge IoT-Systemen,在边缘IOT系统中高效异常探测的联邦隔离林,http://arxiv.org/abs/2506.05138v1
3,"Federated learning (FL) has come forward as a critical approach for privacy-preserving machine learning in healthcare, allowing collaborative model training across decentralized medical datasets without exchanging clients' data. However, current security implementations for these systems face a fundamental trade-off: rigorous cryptographic protections like fully homomorphic encryption (FHE) impose prohibitive computational overhead, while lightweight alternatives risk vulnerable data leakage through model updates. To address this issue, we present FAS (Fast and Secure Federated Learning), a novel approach that strategically combines selective homomorphic encryption, differential privacy, and bitwise scrambling to achieve robust security without compromising practical usability. Our approach eliminates the need for model pretraining phases while dynamically protecting high-risk model parameters through layered encryption and obfuscation. We implemented FAS using the Flower framework and evaluated it on a cluster of eleven physical machines. Our approach was up to 90\% faster than applying FHE on the model weights. In addition, we eliminated the computational overhead that is required by competitors such as FedML-HE and MaskCrypt. Our approach was up to 1.5$\times$ faster than the competitors while achieving comparable security results.   Experimental evaluations on medical imaging datasets confirm that FAS maintains similar security results to conventional FHE against gradient inversion attacks while preserving diagnostic model accuracy. These results position FAS as a practical solution for latency-sensitive healthcare applications where both privacy preservation and computational efficiency are requirements.","联邦学习联合会(FL)作为在保健方面保护隐私的机器学习的关键方法,在不交换客户数据的情况下,允许在分散医疗数据集中进行合作示范培训,在分散医疗数据集中进行合作示范培训。然而,目前这些系统的安全实施面临一个根本性的权衡:严格的加密保护,如完全同质加密(FHE),造成令人望而却步的计算间接费用,而轻量替代方案则有可能通过更新模型而使脆弱的数据泄漏。为了解决这一问题,我们介绍了FAS(FAS)(Fast and Security Freedom Learning),这是一种新颖的办法,它从战略上将选择性的对同质加密、差异隐私和微小的拼凑结合起来,以便在不损害实际可用性的情况下实现稳健的安全性安全性安全性。我们的方法通过分层加密和易变异性安全性安全性安全性分析,在稳定性安全性安全性稳定性稳定性稳定性稳定性安全性稳定性评估中,在稳定性安全性稳定性稳定性稳定性稳定性稳定性稳定性稳定性稳定性稳定性评估中,在稳定性安全性稳定性稳定性稳定性稳定性稳定性稳定性稳定性分析性能方面,同时,我们的方法将安全性安全性能标准性分析性分析性能结果提高到性能,在一种性能上,在一种性能上,在稳定性能上,在稳定性能性能稳定性能上保持一种性能上的结果。","Abdulkadir Korkmaz, Praveen Rao",2025-06-05T13:47:27Z,A Selective Homomorphic Encryption Approach for Faster   Privacy-Preserving Federated Learning,Ein selektiver homomorpher Verschlüsselungsansatz für schnelleres Datenschutz-Erhalten von Federated Learning,为更快的隐私-保护联邦学习采取选择性单态加密方法,http://arxiv.org/abs/2501.12911v4
4,"Function-as-a-Service (FaaS) is an event-driven serverless cloud computing model in which small, stateless functions are invoked in response to events, such as HTTP requests, new database entries, or messages. Current FaaS platform assume that each function invocation corresponds to a single event. However, from an application perspective, it is desirable to invoke functions in response to a collection of events of different types or only with every n\textsuperscript{th} event. To implement this today, a function would need additional state management, e.g., in a database, and custom logic to determine whether its trigger condition is fulfilled and the actual application code should run. In such an implementation, most function invocations would be rendered essentially useless, leading to unnecessarily high resource usage, latency, and cost for applications. In this paper, we introduce multi-event triggers, through which complex conditions for function invocations can be specified. Specifically, we introduce abstractions for invoking functions based on a set of $n$ events and joins of multiple events of different types. This enables application developers to define intricate conditions for function invocations, workflow steps, and complex event processing. Our evaluation with a proof-of-concept prototype shows that this reduces event--invocation latency by 62.5\% in an incident detection use-case and that our system can handle more than 300,000 requests per second on limited hardware, which is sufficient load for implementation in large FaaS platforms.","函数- a- service (FaaS) 是一种由事件驱动的无服务器的云计算模型, 在发生HTTP请求、 新的数据库条目或信息等事件时, 援引小型、 无国籍的功能, 以响应小型、 无国籍的功能; 当前的 FaaS 平台假设, 每个函数的引用都对应一个单一事件。 然而, 从应用角度来说, 有必要援引功能来应对不同类型事件的集合, 或仅针对每个 n\ textsuperscript{th} 事件。 今天, 要实施此功能, 一个函数将需要额外的州管理, 例如, 在数据库和定制逻辑中, 以确定其触发条件是否已经满足, 实际应用代码是否运行。 在这样的执行中, 大多数功能的引用将基本上变得毫无用处, 导致不必要高的资源使用、 延时、 应用成本。 然而, 我们引入了多重事件触发触发点, 具体地, 我们引入了基于一组美元事件启动的功能, 和多种事件合并的事件。","Valentin Carl, Trever Schirmer, Niklas Kowallik, Joshua Adamek, Tobias Pfandzelter, Sergio Lucia, David Bermbach",2025-06-05T12:43:30Z,Multi-Event Triggers for Serverless Computing,Multi-Event-Trigger für serverloses Rechnen,无服务器电子计算多天触发器,http://arxiv.org/abs/2505.21199v2
5,"We develop and analyze new scheduling algorithms for solving sparse triangular linear systems (SpTRSV) in parallel. Our approach produces highly efficient synchronous schedules for the forward- and backward-substitution algorithm. Compared to state-of-the-art baselines HDagg and SpMP, we achieve a $3.32 \times$ and $1.42 \times$ geometric-mean speed-up, respectively. We achieve this by obtaining an up to $12.07 \times$ geometric-mean reduction in the number of synchronization barriers over HDagg, whilst maintaining a balanced workload, and by applying a matrix reordering step for locality. We show that our improvements are consistent across a variety of input matrices and hardware architectures.","我们同时开发并分析解决分散三角线性系统(SpTRSV)的新时间安排算法(SpTRSV) 。 我们的方法为前向和后向替代算法提供了高效同步的进度表。 与最先进的基线HDagg和SpMP相比,我们分别实现了3.32美元和1.42美元的几何平均速度。 我们通过在HDagg的同步屏障数量上获得高达12.07美元的几何平均减幅,同时保持平衡的工作量,并通过对地点采用矩阵重新排序步骤来实现这一目标。 我们显示,我们的各种投入矩阵和硬件结构都取得了一致的改进。","Toni Böhnlein, Pál András Papp, Raphael S. Steiner, Christos K. Matzoros, A. N. Yzelman",2025-06-05T12:31:43Z,Efficient Parallel Scheduling for Sparse Triangular Solvers,Effizientes paralleles Scheduling für Sparse Dreieckslöser,Sparse 三角式溶剂的高效平行排列,http://arxiv.org/abs/2503.05408v2
6,"We analyze blocks proposed for inclusion in the Ethereum blockchain during 8 minutes on December 3rd, 2024. Our dataset comprises 38 winning blocks, 15,097 proposed blocks, 10,793 unique transactions, and 2,380,014 transaction-block pairings. We find that exclusive transactions--transactions present only in blocks proposed by a single builder--account for 85% of the fees paid by all transactions included in winning blocks. We also find that a surprisingly large number of user transactions are delayed: although proposed during a bidding cycle, they are not included in the corresponding winning block. Many such delayed transactions are exclusive to a losing builder. We also identify two arbitrage bots trading between decentralized (DEX) and centralized exchanges (CEX). By examining their bidding dynamics, we estimate that the implied price at which these bots trade USDC/WETH and USDT/WETH on CEXes is between 3.4 and 4.2 basis points better than the contemporaneous price reported on Binance.","2024年12月3日,我们分析了2024年12月3日8分钟内拟纳入Eceenum区块的区块。我们的数据集由38个中区块、15,097个拟议区块、10,793个独特交易和2,380,014个交易区块配对组成。我们发现,独家交易-交易只存在于单一建筑商账户提议的区块中,占所有交易所付费用的85%,包括胜出区块。我们还发现,数量惊人的大量用户交易被推迟:虽然在投标周期内提出,但并未包括在相应的赢家区块中。许多此类延迟交易是输家独家独家经营的。我们还确定了两个分散交易(DEX)和集中交易(CEX)之间的套利交易。我们通过审查它们的投标动态,估计这些机器人买卖USC/WETH和UST/WETH的隐含价格比Binness报告的价格高出3.4至4.2个基点。","Andrea Canidio, Vabuk Pahari",2025-06-05T12:16:04Z,Becoming Immutable: How Ethereum is Made,Unwandelbar werden: Wie Ethereum gemacht wird,变得易变:Eeterum是如何制造的,http://arxiv.org/abs/2506.04940v1
7,"Byzantine agreement is a fundamental problem in fault-tolerant distributed computing that has been studied intensively for the last four decades. Much of the research has focused on a static Byzantine adversary, where the adversary is constrained to choose the Byzantine nodes in advance of the protocol's execution. This work focuses on the harder case of an adaptive Byzantine adversary that can choose the Byzantine nodes \emph{adaptively} based on the protocol's execution. While efficient $O(\log n)$-round protocols ($n$ is the total number of nodes) are known for the static adversary (Goldwasser, Pavlov, and Vaikuntanathan, FOCS 2006) tolerating up to $t < n/(3+\epsilon)$ Byzantine nodes, $\Omega(t/\sqrt{n \log n})$ rounds is a well-known lower bound for adaptive adversary [Bar-Joseph and Ben-Or, PODC 1998]. The best-known protocol for adaptive adversary runs in $O(t/\log n)$ rounds [Chor and Coan, IEEE Trans. Soft. Engg., 1985].   This work presents a synchronous randomized Byzantine agreement protocol under an adaptive adversary that improves over previous results. Our protocol works under the powerful \emph{adaptive rushing adversary in the full information model}. That is, we assume that the Byzantine nodes can behave arbitrarily and maliciously, have knowledge about the entire state of the network at every round, including random choices made by all the nodes up to and including the current round, have unlimited computational power, and may collude among themselves. Furthermore, the adversary can \emph{adaptively} corrupt up to $t < n/3$ nodes based on the protocol's execution. We present a simple randomized Byzantine agreement protocol that runs in $O(\min\{t^2\log n/n, t/\log n\})$ rounds that improves over the long-standing bound of $O(t/\log n)$ rounds due to Chor and Coan [IEEE Trans. Soft. Engg., 1985].","拜占庭协议是过去四十年来一直深入研究的耐错分配计算中的一个基本问题。 许多研究都集中在静态的拜占庭对手( Goldwaser、 Pavlov 和 Vaikuntanathan, FOCS 2006) 上, 在协议执行前, 对手只能选择有适应性的拜占庭节点( t/\qrt{ attal) 。 虽然高效的 O( log n) 平坦协议( 美元 ) 上方协议( 美元 ) 上方协议( 美元) 上方协议( 美元) 上方协议( 美元) 上方协议( 美元) 上方协议( 美元) 上方协议( 美元) 上方协议( 美元) 上方协议( 美元) 上方协议( 美元) 上下方协议( 美元) 上下方协议( 美元) 上下方协议( 而不是上方协议( 美元 ) 上下方协议 , 上方协议( 上方( 美元) 上方协议) 上方( 上方) 上方( 上方协议) 上方( 上方) 上方( 上方) 上方( 上方) 上方( 上方) 上方协议) 上方( ) 上方( ) 上方) 上方( ) 上方) 上方) 上方( 上方协议) , 上方( 上方) 上方( 上方) 上方) , 上方) , , , 上方( 上方) ( 上方) ( , 上方) ( 上方) 方) 方) 方( 上方) ( 上方) , , 上方) , 上方) , 上方协议( 上方协议( 上方) 上方) 上方) 上方) 上方( 上方) 上方) 上方) 上方) 上方) 上方( 上方) 上方) 上方) 上方) , ,","Fabien Dufoulon, Gopal Pandurangan",2025-06-05T11:53:00Z,Improved Byzantine Agreement under an Adaptive Adversary,Verbessertes byzantinisches Abkommen unter einem adaptiven Widersacher,在适应性反逆之下改进拜占庭协定,http://arxiv.org/abs/2506.04919v1
8,"AIoT workloads demand energy-efficient orchestration across cloud-edge infrastructures, but Kubernetes' default scheduler lacks multi-criteria optimization for heterogeneous environments. This paper presents GreenPod, a TOPSIS-based scheduler optimizing pod placement based on execution time, energy consumption, processing core, memory availability, and resource balance. Tested on a heterogeneous Google Kubernetes cluster, GreenPod improves energy efficiency by up to 39.1% over the default Kubernetes (K8s) scheduler, particularly with energy-centric weighting schemes. Medium complexity workloads showed the highest energy savings, despite slight scheduling latency. GreenPod effectively balances sustainability and performance for AIoT applications.","AIOT工作量要求跨越云端基础设施进行节能调控,但Kubernetes的默认排程器缺乏多种环境的多标准优化。 本文介绍了基于TOPSIS的托普西排程器GreenPod根据执行时间、能源消耗、处理核心、内存可用量和资源平衡优化舱位配置。 在一个混杂的Google Kubernetes集群测试后,GreenPod提高了能源效率,比默认的Kubernetes排程器(K8s)提高了39.1%,特别是以能源为中心的权重计划。 中等复杂的工作量显示,尽管排长时间,但节能率最高。 绿色Pod有效地平衡了AIOT应用的可持续性和绩效。","Preethika Pradeep, Eyhab Al-Masri",2025-06-05T11:36:02Z,Energy-Optimized Scheduling for AIoT Workloads Using TOPSIS,Energieoptimierte Planung für AIoT-Workloads mit TOPSIS,利用TOPSIS对AIOT工作量进行能源优化安排,http://arxiv.org/abs/2506.04902v1
9,"This review investigates the pivotal role of distributed architectures and intelligent resource allocation in enabling robust and scalable wireless systems, with a particular emphasis on backscatter communication, indoor localization, battery-free networks, and Simultaneous Wireless Information and Power Transfer (SWIPT).","这项审查调查了分布式建筑和智能资源分配在使无线系统具有稳健和可扩缩能力方面的关键作用,特别强调后散式通信、室内本地化、无电池网络和同声无线信息和电源传输。","Tonghuan Xiao, Jiecheng Zhou",2025-06-05T10:48:53Z,A distributed system perspective on Backscatter systems: A review,Eine verteilte Systemperspektive auf Backscatter-Systeme: Ein Rückblick,关于后散系统分布式系统观点:审查,http://arxiv.org/abs/2506.04873v1
10,"Backscatter system is a system based on backscatter communication technology, which is a low cost, low power consumption and easy to deploy communication technology. At present, the backscatter technology is mainly applied to RFID tags and the Internet of Things and other fields. With the rapid development of the Internet of Things, the application of backscatter systems is increasing. Moreover, the backscatter system is essentially a distributed system, but existing research rarely conducts studies and analyses from a distributed perspective. This paper conducts a study on the backscattering system from the perspective of distributed systems, comprehensively reviewing the basic principles of the backscattering system, and analyzing the distributed system architectures of different backscattering systems. Then, it introduces the application scenarios, research status and challenges of the backscattering system, and finally discusses the future research directions of the backscattering system, hoping to provide references for future research.","后散射系统是一种基于后散射通讯技术的系统,这是一个低成本、低耗能和易于部署通信技术的系统。目前,后散射技术主要应用于RFID标签和物的互联网及其他领域。随着物的互联网的迅速发展,后散射系统的应用正在增加。此外,后散射系统基本上是一个分布式系统,但现有的研究很少从分布式的角度进行研究和分析。本文从分布式系统的角度对后散射系统进行研究,全面审查后散射系统的基本原则,分析不同后散射系统分布式系统结构。随后,它介绍了后散射系统的应用情景、研究状况和挑战,最后讨论了后散射系统的未来研究方向,希望为今后的研究提供参考。","Jincheng Guan, Jun Zhang",2025-06-05T09:54:05Z,Distributed system perspective on Backscatter systems,Verteilte Systemperspektive auf Backscatter-Systeme,关于后散散系统分布式系统视角,http://arxiv.org/abs/2506.04833v1
11,"Federated Learning (FL) offers a promising approach for training clinical AI models without centralizing sensitive patient data. However, its real-world adoption is hindered by challenges related to privacy, resource constraints, and compliance. Existing Differential Privacy (DP) approaches often apply uniform noise, which disproportionately degrades model performance, even among well-compliant institutions. In this work, we propose a novel compliance-aware FL framework that enhances DP by adaptively adjusting noise based on quantifiable client compliance scores. Additionally, we introduce a compliance scoring tool based on key healthcare and security standards to promote secure, inclusive, and equitable participation across diverse clinical settings. Extensive experiments on public datasets demonstrate that integrating under-resourced, less compliant clinics with highly regulated institutions yields accuracy improvements of up to 15% over traditional FL. This work advances FL by balancing privacy, compliance, and performance, making it a viable solution for real-world clinical workflows in global healthcare.","联邦学习联合会(FL)为培训临床AI模式提供了一种很有希望的方法,而没有集中敏感的病人数据,然而,其实际采用受到与隐私、资源限制和合规有关的挑战的阻碍;现有的差异隐私(DP)方法往往采用统一噪音,这种噪音不成比例地降低了示范性业绩,即使在遵守标准的机构中也是如此;在这项工作中,我们提议了一个新的了解合规性的FL框架,根据可量化客户合规分数调整噪音,从而增强DP;此外,我们引入了一个基于关键保健和安全标准的合规评分工具,以促进不同临床环境的安全、包容和公平参与;关于公共数据集的广泛实验表明,将资源不足、不合规性强的诊所与监管程度高的机构相结合,可以比传统FL提高高达15%的准确性。 这项工作通过平衡隐私、合规性和绩效,使FL成为全球保健中真实世界临床工作流程的一个可行解决方案,从而推进FL。","Santhosh Parampottupadam, Melih Coşğun, Sarthak Pati, Maximilian Zenk, Saikat Roy, Dimitrios Bounias, Benjamin Hamm, Sinem Sav, Ralf Floca, Klaus Maier-Hein",2025-06-05T09:01:10Z,"Inclusive, Differentially Private Federated Learning for Clinical Data","Inklusives, differenziert privates Federated Learning für klinische Daten",包容性、差异化私联校临床数据学习,http://arxiv.org/abs/2505.22108v2
12,"The modeling and simulation of multiphase fluid flow receive significant attention in reservoir engineering. Many time discretization schemes for multiphase flow equations are either explicit or semi-implicit, relying on the decoupling between the saturation equation and the pressure equation. In this study, we delve into a fully coupled and fully implicit framework for simulating multiphase flow in heterogeneous porous media, considering gravity and capillary effects. We utilize the Vertex-Centered Finite Volume Method for spatial discretization and propose an efficient implementation of interface conditions for heterogeneous porous media within the current scheme. Notably, we introduce the Linearly Implicit Extrapolation Method (LIMEX) with an error estimator, adapted for the first time to multiphase flow problems. To solve the resulting linear system, we employ the BiCGSTAB method with the Geometric Multigrid (GMG) preconditioner. The implementations of models and methods are based on the open-source software: UG4. The results from parallel computations on the supercomputer demonstrate that the scalability of our proposed framework is sufficient, supporting a scale of thousands of processors with Degrees of Freedom (DoF) extending up to billions.","多相流体模型和模拟多相流体模型和模拟在储油层工程中受到极大注意。许多多相流方程式的时间分解计划是明确或半隐含的,依靠的是饱和方程式和压力方程式之间的脱钩。在本研究中,我们钻入了一个完全结合和完全隐含的框架,用于模拟多相流的多相流,其中考虑到重力和毛毛效应。我们使用Vertex-Central Finite 量法进行空间分解,并提议在当前方案内有效地实施多相差多管介质介质的界面条件。特别是,我们引入了带有误差估计器的线性隐含外推法(LIMEX),首次适应了多相位流的问题。为了解决由此产生的线性系统,我们采用BICGSTAB法,同时使用几何多格多格(GMG)的先决条件。模型和方法的实施以开放源软件为基础:UG4。超级计算机的平行计算结果表明,我们提议的框架的可扩展性已经足够,支持有数十亿自由度的处理器。",Shuai Lu,2025-06-05T08:46:48Z,A highly scalable numerical framework for reservoir simulation on UG4   platform,Ein hoch skalierbares numerisches Framework für die Reservoir-Simulation auf UG4-Plattform,UG4平台储油层模拟的高度可伸缩数字框架,http://arxiv.org/abs/2506.04763v1
13,"Recent advancements in Large Language Models (LLMs) have led to increasingly diverse requests, accompanied with varying resource (compute and memory) demands to serve them. However, this in turn degrades the cost-efficiency of LLM serving as common practices primarily rely on homogeneous GPU resources. In response to this problem, this work conducts a thorough study about serving LLMs over heterogeneous GPU resources on cloud platforms. The rationale is that different GPU types exhibit distinct compute and memory characteristics, aligning well with the divergent resource demands of diverse requests. Particularly, through comprehensive benchmarking, we discover that the cost-efficiency of LLM serving can be substantially optimized by meticulously determining GPU composition, deployment configurations, and workload assignments. Subsequently, we design a scheduling algorithm via mixed-integer linear programming, aiming at deducing the most cost-efficient serving plan under the constraints of price budget and real-time GPU availability. Remarkably, our approach effectively outperforms homogeneous and heterogeneous baselines under a wide array of scenarios, covering diverse workload traces, varying GPU availablilities, and multi-model serving. This casts new light on more accessible and efficient LLM serving over heterogeneous cloud resources.","近来在大语言模型(LLMs)方面的进步导致要求日益多样化,同时需要不同的资源(计算和记忆)来为这些要求服务,然而,这反过来降低了作为通用做法的LLM的成本效益,主要依靠的是单一的GPU资源。针对这一问题,这项工作对在云平台上各种GPU资源中为LLM提供服务进行了透彻的研究。理由是,不同的GPU类型表现出不同的计算和记忆特点,与不同要求的资源需求相匹配。特别是,通过综合基准,我们发现,通过仔细确定GPU的组成、部署配置和工作量任务,可以大大优化LLM服务的成本效益。随后,我们设计了一种通过混合的混合整流线性编程的排程算法,目的是在价格预算的制约和实时GPU的可用性下,对最具成本效益的服务计划进行教育。值得注意的是,我们的方法有效地超越了多种情景下的单一和不同基线,涵盖了不同的工作量差距、不同的GPUPU的利用率和多模型。这为更方便和高效的LM云层资源带来了新的光线。","Youhe Jiang, Fangcheng Fu, Xiaozhe Yao, Guoliang He, Xupeng Miao, Ana Klimovic, Bin Cui, Binhang Yuan, Eiko Yoneki",2025-06-05T07:54:05Z,Demystifying Cost-Efficiency in LLM Serving over Heterogeneous GPUs,Entmystifizierende Kosteneffizienz bei LLM-Diensten über heterogene GPUs,消除在异种性GSP单位上服务LLM成本效率的神秘性,http://arxiv.org/abs/2502.00722v2
14,"The computational sparsity of Mixture-of-Experts (MoE) models enables sub-linear growth in compute cost as model size increases, offering a scalable path to training massive neural networks. However, existing implementations suffer from \emph{low GPU utilization}, \emph{significant latency overhead}, and a fundamental \emph{inability to leverage task locality}, primarily due to CPU-managed scheduling, host-initiated communication, and frequent kernel launches. To overcome these limitations, we develop FlashDMoE, a fully GPU-resident MoE operator that fuses expert computation and inter-GPU communication into a \emph{single persistent GPU kernel}. FlashDMoE enables fine-grained pipelining of dispatch, compute, and combine phases, eliminating launch overheads and reducing idle gaps. Its device-initiated communication protocol introduces \emph{payload-efficient} data transfers, significantly shrinking buffer sizes in sparsely activated MoE layers. When evaluated on a single 8-H100 GPU node with MoE models having up to 128 experts and 16K token sequences, FlashDMoE achieves up to \textbf{6}x lower latency, \textbf{5,7}x higher throughput, \textbf{4}x better weak scaling efficiency, and \textbf{9}x higher GPU utilization compared to state-of-the-art baselines, despite using FP32 while baselines use FP16. FlashDMoE demonstrates that principled GPU kernel-hardware co-design is key to unlocking the performance ceiling of large-scale distributed ML workloads.","计算 { Experts (MoE) 模型的计算宽度主要由于 CPU 管理的时间安排、 主机启动的通信和频繁的内核发布。 为了克服这些限制, 我们开发了FlashDMoE, 一个完全的GPU- 驻地的MOE操作器, 将专家的计算和 GPU之间的通信连接到一个 emph{low GPU 使用} 、\ emph{ 显著的悬浮管理 、 以及一个基本的 emph{ act- Explace (MoE) , 以及一个基本的\ emph@ acredition to point lockets} , 它的启动通信协议将 emph { payf lost- pload- putevel} 数据传输引入了 emphlational moE , 一个完全的 G9- HPUX 高级计算器计算器 和 mE - 高级的运行程序, 显示在 16 MAE 模型中, DPUDx 流流 流 上, 显示, 最低的运行 和最低的运行 直径 直径","Osayamen Jonathan Aimuyo, Byungsoo Oh, Rachee Singh",2025-06-05T06:29:14Z,FlashDMoE: Fast Distributed MoE in a Single Kernel,FlashDMoE: Schnell verteiltes MoE in einem einzigen Kernel,FlashDMoE: 在一个单一核心中快速分布的教育部,http://arxiv.org/abs/2506.04667v1
15,"In this report, we propose Triton-distributed, an extension of existing Triton compiler, to overcome the programming challenges in distributed AI systems. Triton-distributed is the first compiler that supports native overlapping optimizations for distributed AI workloads, providing a good coverage of existing optimizations from different frameworks. First, we integrate communication primitives compliant with the OpenSHMEM standard into the compiler. This enables programmers to utilize these primitives with a higher-level Python programming model. Second, we illustrate how to achieve complex joint optimization of computation, memory access, and communication with the assistance of the compiler. In particular, we show how to use overlapping techniques to hide latency and present our compiler-based programming methods in both single-node and multi-node scenarios. Finally, we showcase the performance of the code generated by our compiler. In a test environment with up to 64 devices, our compiler can fully utilize heterogeneous communication and computation resources to provide effective overlapping and high performance. In many cases, the performance of the generated code can even outperform hand-optimized code. Moreover, the development difficulty and the time cost for development using our compiler are far less than those of low-level programming such as CUDA/C++, which clearly demonstrates significant productivity advantages.","在本报告中,我们建议使用Triton分布式程序,作为现有Triton编译器的延伸,以克服分布式AI系统中的编程挑战。Triton分布式是第一个支持对分布式AI工作量进行本地重叠优化的编译器,对不同框架的现有优化提供了很好的覆盖面。首先,我们将符合 OpenSHEM 标准的通信原始版本纳入编译器。这样,编译器能够利用更高层次的Python 编程模型来充分利用这些原始版本。第二,我们说明如何在编译器的协助下,实现计算、记忆存取和通信的复杂联合优化。特别是,我们展示了如何使用重叠技术来隐藏延迟度,并在单节和多节情景中展示我们基于编译器的编程方法。最后,我们展示了编译器生成的代码的性能。在一个高达64个装置的测试环境中,我们的编译器可以充分利用混集式通信和计算资源,以提供有效的重叠和高性能。在许多情况下,生成的代码的性能甚至超出手动式手动式代码的精确性。此外,我们展示了在单节制式和多功能上的发展难度和低成本。此外,这种编译法的难度也展示了低水平。","Size Zheng, Wenlei Bao, Qi Hou, Xuegui Zheng, Jin Fang, Chenhui Huang, Tianqi Li, Haojie Duanmu, Renze Chen, Ruifan Xu, Yifan Guo, Ningxin Zheng, Ziheng Jiang, Xinyi Di, Dongyang Wang, Jianxi Ye, Haibin Lin, Li-Wen Chang, Liqiang Lu, Yun Liang, Jidong Zhai, Xin Liu",2025-06-05T05:48:26Z,Triton-distributed: Programming Overlapping Kernels on Distributed AI   Systems with the Triton Compiler,Triton-distributed: Programmierung überlappender Kernel auf verteilten KI-Systemen mit dem Triton Compiler,Triton 分布式: 与 Tritton 汇编者一起制作关于分配的 AI 系统的程序设计,http://arxiv.org/abs/2504.19442v3
16,"With the widespread adoption of Large Language Models (LLMs), serving LLM inference requests has become an increasingly important task, attracting active research advancements. Practical workloads play an essential role in this process: they are critical for motivating and benchmarking serving techniques and systems. However, the existing understanding of real-world LLM serving workloads is limited due to the lack of a comprehensive workload characterization. Prior analyses remain insufficient in scale and scope, thus failing to fully capture intricate workload characteristics.   In this paper, we fill the gap with an in-depth characterization of LLM serving workloads collected from our worldwide cloud inference serving service, covering not only language models but also emerging multimodal and reasoning models, and unveiling important new findings in each case. Moreover, based on our findings, we propose ServeGen, a principled framework for generating realistic LLM serving workloads by composing them on a per-client basis. A practical use case in production validates that ServeGen avoids 50% under-provisioning compared to naive workload generation, demonstrating ServeGen's advantage in performance benchmarking. ServeGen is available at https://github.com/alibaba/ServeGen.","随着广泛采用大语言模型(LLM),为LLM推断请求提供服务,这已成为一项越来越重要的任务,吸引了积极的研究进展。实际工作量在这一过程中发挥着关键作用:它们对激励和基准制定服务技术和系统至关重要;然而,由于缺乏全面的工作量定性,目前对现实世界LLM服务工作量的了解有限。先前的分析在规模和范围上仍然不够充分,无法充分捕捉复杂的工作量特点。在本文件中,我们填补了这一空白,对LLM服务于从我们全球云推断服务处收集的工作量作了深入的定性,不仅包括语言模型,也包括新兴的多式联运和推理模型,并公布了每个案例的重要新发现。此外,根据我们的调查结果,我们提议SeeGen为创造现实的LLM服务工作量提供一个原则框架,将LM工作量按客户的人均分类。在生产中有一个实用的论证案例,即SerpGen避免与天真的工作量生成者相比,50%的供给不足,表明SeeGen在业绩基准方面的优势。在https://github.com/alibaba/Servement Gen。","Yuxing Xiang, Xue Li, Kun Qian, Wenyuan Yu, Ennan Zhai, Xin Jin",2025-06-05T05:46:07Z,ServeGen: Workload Characterization and Generation of Large Language   Model Serving in Production,ServeGen: Workload Charakterisierung und Generierung von großen Sprachmodellen in der Produktion,ServerGen: 生产中大型语文服务模式的工作负荷特征化和生成,http://arxiv.org/abs/2505.09999v2
17,"We develop a theoretical model that addresses the economic trade-off between cost per token versus serial token generation speed when deploying LLMs for inference at scale. Our model takes into account arithmetic, memory bandwidth, network bandwidth and latency constraints; and optimizes over different parallelism setups and batch sizes to find the ones that optimize serial inference speed at a given cost per token. We use the model to compute Pareto frontiers of serial speed versus cost per token for popular language models.","我们开发了一种理论模型,用以在部署用于规模推论的LLMs时,解决按象征性成本和按序列代号生成速度进行经济权衡的问题。 我们的模型考虑到算术、记忆带宽、网络带宽和延时限制; 优化了不同的平行设置和批量尺寸,以找到以每个象征性成本优化序列推断速度的模式。 我们用这个模型来计算Pareto系列速度的边界和流行语言模型的按象征性成本计算的序列速度。",Ege Erdil,2025-06-05T05:28:09Z,Inference economics of language models,Schlußfolgerung Wirtschaftlichkeit von Sprachmodellen,语言模式的推论经济学,http://arxiv.org/abs/2506.04645v1
18,"Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by handling diverse inputs such as images, audio, and video, but at the cost of adding a multimodal encoding stage that increases both computational and memory overhead. This step negatively affects key Service Level Objectives (SLOs), such as time to first token (TTFT) and time per output token (TPOT). We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates the encoding, prefill, and decode stages onto dedicated resources. Unlike current systems, which bundle encoding and prefill together, our approach decouples these steps, unlocking new opportunities and optimizations. These include a mechanism to cache multimedia tokens for efficient transfer, a novel way to parallelize the encoding load within a request, a module for optimal resource allocation for disaggregated serving, and a novel role-switching method to handle changing workload characteristics. Experimental evaluations with popular LMMs show substantial gains in memory efficiency (up to 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more images per request, and 2.2x larger KV caches. Furthermore, it leads to significant improvements in SLO attainment (up to 90-100% improvement) and TTFT (up to 71% reduction), compared to systems that do not disaggregate. The code is available at https://github.com/vbdi/epdserve.","大型多式模型(LMMs)通过处理图像、音频和视频等多种投入,扩展了大语言模型(LLMMs),处理图像、音频和视频等多种投入,但成本是增加多式编码阶段,增加计算和记忆管理。这一步骤对关键服务级目标(SLOs)产生了负面影响,如第一到第一令(TTFT)的时间和每个输出符号(TPOT)的时间等。我们引入了Ecco-Prefrip-Decode(EPD)分解,这是一个将编码、预填和解码阶段分离到专用资源的新框架。与将编码、预填、预填和解码到专用资源的当前系统不同的是,我们的方法将这些步骤拆分解,打开新的机会和优化。这些步骤包括一个机制,为高效传输存储多媒体标牌(SLOLOs),在请求中将编码与编码相平行,将编码与编码相平行的MMUP/CFTA系统相平行,将S-CFS-CS-CS-delevxxxxxxx 递缩缩缩到递缩系统。","Gursimran Singh, Xinglu Wang, Yifan Hu, Timothy Yu, Linzi Xing, Wei Jiang, Zhefeng Wang, Xiaolong Bai, Yi Li, Ying Xiong, Yong Zhang, Zhenan Fan",2025-06-05T04:21:30Z,Efficiently Serving Large Multimodal Models Using EPD Disaggregation,Effizientes Servieren großer multimodaler Modelle mit EPD-Disaggregation,利用EPD拆分有效服务大型多模式模式,http://arxiv.org/abs/2501.05460v3
19,"Real-time arbitrary waveform generation (AWG) is essential in various engineering and research applications. This paper introduces a novel AWG architecture using an NVIDIA graphics processing unit (GPU) and a commercially available high-speed digital-to-analog converter (DAC) card, both running on a desktop personal computer (PC). The GPU accelerates the ""embarrassingly"" data-parallel additive synthesis framework for AWG, and the DAC reconstructs the generated waveform in the analog domain at high speed. The AWG software is developed using the developer-friendly compute unified device architecture (CUDA) runtime application programming interface (API) from NVIDIA. With this architecture, we achieve a 586-fold increase in the speed of computing periodic radio-frequency (rf) arbitrary waveforms compared to a central processing unit (CPU). We also demonstrate two different pathways for dynamically controlling multi-tone rf waveforms, which we characterize by chirping individual single-frequency tones in the multi-tone waveforms. One pathway offers arbitrary simultaneous chirping of 1000 individual Nyquist-limited single-frequency tones at a sampling rate of 280 megasamples per second (MS/s) for a limited time duration of 35 ms. The other pathway offers simultaneous chirping of 340 individual Nyquist-limited single-frequency tones at 50 MS/s, or 55 individual tones at 280 MS/s for an arbitrary duration. Using the latter pathway, we demonstrate control over 5000-tone and 10,000-tone waveforms by chirping all of their constituent tones in groups of up to 100 tones. This AWG architecture is designed for creating large defect-free optical tweezer arrays of single neutral atoms or molecules for quantum simulation and quantum computation.","在各种工程和研究应用中,实时任意波形生成(AWG)是各种工程和研究应用中必不可少的。本文介绍了使用 NVIDIA 图形处理器(GPU) 和在商业上可用的高速数字至分析器卡(DAC) 的新AWG 结构。 GPU加速了AWG的“干扰”数据平行添加合成框架,而发援会以高速的方式重建了模拟域生成的波形。AWG软件是使用NVIDIA的开发者友好型计算器统一设备结构(CUDA)运行时间应用程序界面(API)开发出来的。有了这个结构,我们实现了586倍的计算周期性无线电频率(rf)任意波形与中央处理器(CPU)运行。我们还展示了两种不同的途径,即动态控制多调调频波形,我们通过多调波形中单个频率(CUDA) 自动同步调控器(CUDAP) 运行时间界面(AVDI) 运行程序界面(NYQS-rental) 直径直径直径直径直径直径直径35(ral) 直径直径直径(rass) 直径) 直径直径直径直径直径直径直径直径直至后的单个直径直至直径直径直径直径直径直径直至直至直至直径直径直径直径直径直至直至直至直至直至直至直径直径直至直至直至直至直至直至各直至直至直方形直方形直方形直方形直方形直至直至直至直至直至直至直至直至直至直至直至直至直至直至直至直至直至直方形直至直至直至直至各各直至直至直至直至直至直至直至直方形直方形直方形直方形直方形直至直至直方形直方形直方形直方形直方形直至直方形直方形直方形直方形直方形直方形直方形直方形直方形直方形直方形直方形直方形直方形","Juntian Tu, Sarthak Subhankar",2025-06-05T00:58:40Z,Fast real-time arbitrary waveform generation using graphic processing   units,Schnelle Echtzeit-Erzeugung beliebiger Wellenformen mit grafischen Verarbeitungseinheiten,使用图形处理器快速实时任意生成波形,http://arxiv.org/abs/2403.15582v2
20,"Data analysis in high-energy physics (HEP) begins with data reduction, where vast datasets are filtered to extract relevant events. At the Large Hadron Collider (LHC), this process is bottlenecked by slow data transfers between storage and compute nodes. To address this, we introduce SkimROOT, a near-data filtering system leveraging Data Processing Units (DPUs) to accelerate LHC data analysis. By performing filtering directly on storage servers and returning only the relevant data, SkimROOT minimizes data movement and reduces processing delays. Our prototype demonstrates significant efficiency gains, achieving a 44.3$\times$ performance improvement, paving the way for faster physics discoveries.","高能物理数据分析(HEP)首先从数据减少开始,大量数据集被过滤,以获取相关事件。在大型高频对撞机(LHC),这一过程因储存和计算节点之间数据传输缓慢而受阻。为此,我们引入了SkimROOT,这是一个利用数据处理单位(DPUs)加速LHC数据分析的近数据过滤系统。通过直接在存储服务器上进行过滤,只返回相关数据,SkimROOT将数据流动减少到最低程度,并减少处理延误。我们的原型显示效率的显著提高,实现了44.3亿美元时间的性能改进,为更快的物理发现铺平了道路。","Narangerelt Batsoyol, Jonathan Guiang, Diego Davila, Aashay Arora, Philip Chang, Frank Würthwein, Steven Swanson",2025-06-04T23:11:05Z,SkimROOT: Accelerating LHC Data Filtering with Near-Storage Processing,SkimROOT: Beschleunigung des LHC-Datenfilterns mit Nahspeicherverarbeitung,SKIMROOT: 加速使用近标准处理法过滤LHC数据,http://arxiv.org/abs/2506.04507v1
21,"Vehicle edge computing (VEC) brings abundant computing resources close to vehicles by deploying them at roadside units (RSUs) or base stations, thereby enabling diverse computation-intensive and delay sensitive applications. Existing task offloading strategies are often computationally expensive to execute or generate suboptimal solutions. In this paper, we propose a novel learning-based approach, Knowledge-guided Attention-inspired Task Offloading (KATO), designed to efficiently offload tasks from moving vehicles to nearby RSUs. KATO integrates an attention-inspired encoder-decoder model for selecting a subset of RSUs that can reduce overall task processing time, along with an efficient iterative algorithm for computing optimal task allocation among the selected RSUs. Simulation results demonstrate that KATO achieves optimal or near-optimal performance with significantly lower computational overhead and generalizes well across networks of varying sizes and configurations.","车辆边缘计算(VEC)使大量计算资源接近车辆,在路边单位(RSUs)或基地站部署这些资源,从而能够实现多种计算密集和延迟的敏感应用。现有的任务卸载战略往往计算出执行或产生亚最佳解决方案的成本很高。在本文中,我们建议采用一种新的学习方法,即知识引导引人注意的卸载(KATO),目的是有效地卸载从将车辆运到附近的区域支助股的任务。KATO结合了一个受关注的编码器-解码器模型,用于选择可减少总体任务处理时间的一组区域支助股,同时采用高效的迭代算法计算选定区域支助股的最佳任务分配。模拟结果表明,KATO实现了最佳或接近最佳的绩效,其计算间接费用大大降低,并贯穿不同规模和配置的网络。","Ke Ma, Junfei Xie",2025-06-04T21:15:57Z,Knowledge-Guided Attention-Inspired Learning for Task Offloading in   Vehicle Edge Computing,Knowledge-Guided Aufmerksamkeit-inspiriertes Lernen für die Aufgabe Offloading in Fahrzeug Edge Computing,以知识引导的引人注意的学习促进车辆边缘电子计算中卸载任务,http://arxiv.org/abs/2506.04456v1
22,"We address a fundamental problem in Peer-to-Peer (P2P) networks, namely, constructing and maintaining dynamic P2P overlay network topologies with essential properties such as connectivity, low diameter, and high expansion, that are resilient to continuous high churn and the presence of a large number of malicious (Byzantine) nodes. Our main goal is to construct and maintain a sparse (bounded degree) expander topology despite high churn and a large number of Byzantine nodes. Such an expander topology has logarithmic diameter, high expansion, and is robust to churn and the presence of a large number of bad nodes, and facilitates efficient and robust algorithms for fundamental problems in distributed computing, such as agreement, broadcasting, routing, etc.   Our main contribution is a randomized, fully-distributed dynamic P2P protocol that works with only local initial knowledge and guarantees, with a high probability, the maintenance of a constant degree graph with high expansion even under continuous churn and in the presence of a large number of Byzantine nodes. Our protocol can tolerate up to $o(n/poly\log(n))$ Byzantine nodes (where $n$ is the stable network size). Our protocol is efficient, lightweight, and scalable, and it incurs only $O(poly\log(n))$ overhead for topology maintenance: only polylogarithmic (in $n$) bits need to be processed and sent by each honest node per round, and any honest node's computation cost per round is also polylogarithmic.   Our protocol can be used as a building block for solving fundamental distributed computing problems in highly dynamic networks, such as Byzantine agreement and Byzantine leader election, and enables fast and scalable algorithms for these problems.","P2P(P2P) 网络解决了Peper-Peer(P2P) 网络中的一个根本性问题, 即建设和维护具有连通性、 低直径和高扩张等基本特性的动态 P2P 溢出网络表层, 适应持续高热量和大量恶意( Byzantine) 节点的出现。 我们的主要目标是构建和保持一个稀薄( 限制程度) 的扩展型 P2P 的表层。 这种扩大型的顶层结构具有对数直径、 高扩张度、 坚固且具有刺痛性, 并有大量的坏结点存在, 便于对分布式计算中的基本问题, 如协议、 广播、 路由等。 我们的主要贡献是随机、 完全分散的动态 P2P 协议, 它只能使用本地初始知识和保证, 很有可能, 维持一个高密度的可控度图层图, 即使在连续的顶层和大量Bytantine 节点的运行中, 也需要大量存储 。 我们的顶层的计算和顶层网络可以容忍到 美元 。","Aayush Gupta, Gopal Pandurangan",2025-06-04T18:30:47Z,Fully-Distributed Construction of Byzantine-Resilient Dynamic   Peer-to-Peer Networks,Vollverteilter Bau von Byzantinisch-Resilienten Dynamischen Peer-to-Peer-Netzwerken,完全分布式建造拜占庭 -- -- 抗御性动态同侪和同侪网络,http://arxiv.org/abs/2506.04368v1
23,"Recent advances in large language models (LLMs) have intensified the need to deliver both rapid responses and high-quality answers. More powerful models yield better results but incur higher inference latency, whereas smaller models are faster yet less capable. Recent work proposes balancing this latency-quality trade-off using model cascades, which route simpler queries to smaller models and more complex ones to larger models. However, enabling efficient cascade serving remains challenging. Current frameworks lack effective mechanisms for handling (i) the huge and varying resource demands of different LLMs, (ii) the inherent heterogeneity of LLM workloads, and (iii) the co-optimization of system deployment and routing strategy. Motivated by these observations, we introduce Cascadia, a novel cascade serving framework designed explicitly to schedule request routing and deploy model cascades for fast, quality-preserving LLM serving. Cascadia employs a bi-level optimization method: at the inner level, it uses a mixed-integer linear program to select resource allocations and parallelism strategies based on LLM information and workload characteristics; at the outer level, it applies a weighted Tchebycheff algorithm to iteratively co-optimize the routing strategy and the system deployment produced by the inner level. Our extensive evaluation on diverse workload traces and different model cascades (DeepSeek and the Llama series) demonstrates that Cascadia significantly outperforms both single-model deployments and the state-of-the-art cascade serving baseline, achieving up to 4x (2.3x on average) tighter latency SLOs and up to 5x (2.4x on average) higher throughput while maintaining target answer quality.","大型语言模型(LLMS)的近期进展强化了提供快速反应和高质量答案的需要。更强大的模型产生更好的结果,但具有更高的推推力,而较小的模型则更快,但能力更弱。最近的工作提议使用模型级联来平衡这种低等质量的权衡,模型级联将更简单的查询引向较小的模型,而较复杂的模型则推向更大的模型。然而,使高效级联服务仍然具有挑战性。目前的框架缺乏有效的机制来处理以下事项:(一)不同LLMS的巨大和不同的资源需求;(二)LLM工作量的内在异质性;(三)系统部署和航程战略的同步优化。受这些观察的驱动,我们引入了Cascadia,这是一个新型级联服务框架,其明确旨在为快速、优质的LMM服务安排路线和部署模型。 Cascadia采用了双级优化方法:在内部一级,它使用混合内线程序根据LLM信息和工作量特性选择资源分配和平行战略;在外部一级,我们采用平均部署水平的Scadia-trax系统,同时通过不同级平级的Straction-traxAx 进行双级评估。","Youhe Jiang, Fangcheng Fu, Wanru Zhao, Stephan Rabanser, Nicholas D. Lane, Binhang Yuan",2025-06-04T17:48:38Z,Cascadia: A Cascade Serving System for Large Language Models,Cascadia: Ein Cascade Serving System für große Sprachmodelle,Cascadia:大型语言模型连级服务系统,http://arxiv.org/abs/2506.04203v1
24,"Inter-datacenter communication is a significant part of cloud operations and produces a substantial amount of carbon emissions for cloud data centers, where the environmental impact has already been a pressing issue. In this paper, we present a novel carbon-aware temporal data transfer scheduling framework, called LinTS, which promises to significantly reduce the carbon emission of data transfers between cloud data centers. LinTS produces a competitive transfer schedule and makes scaling decisions, outperforming common heuristic algorithms. LinTS can lower carbon emissions during inter-datacenter transfers by up to 66% compared to the worst case and up to 15% compared to other solutions while preserving all deadline constraints.","数据中心间通信是云运行的一个重要部分,为云数据中心产生了大量碳排放,而云数据中心的环境影响已经是一个紧迫问题。 在本文中,我们提出了一个新的碳意识时间数据传输时间表框架,称为LINTS,它保证显著减少云数据中心之间数据传输的碳排放。 LINTS制作了一个竞争性传输时间表并做出规模化决策,优于普通的超常算法。 LINTS可以在数据中心间传输过程中将碳排放量降低至66%,而最坏的情况是后者,与其他解决方案相比则高达15%,同时保留所有期限限制。","Elvis Rodrigues, Jacob Goldverg, Tevfik Kosar",2025-06-04T16:11:02Z,Carbon-Aware Temporal Data Transfer Scheduling Across Cloud Datacenters,Zeitliche Datenübertragung im Carbon-Aware-Bereich Planung von Cloud-Rechenzentren,跨越云云数据中心的碳软件时空数据传输,http://arxiv.org/abs/2506.04117v1
25,"Inference for Large Language Models (LLMs) is computationally demanding. To reduce the cost of auto-regressive decoding, Key-Value (KV) cache is used to store intermediate activations, which significantly lowers the computational overhead for token generation. However, the memory required for the KV cache grows rapidly, often exceeding the capacity of GPU memory. A cost-effective alternative is to offload KV cache to CPU memory, which alleviates GPU memory pressure, but shifts the bottleneck to the limited bandwidth of the PCIe connection between the CPU and GPU. Existing methods attempt to address these issues by overlapping GPU computation with I/O or employing CPU-GPU heterogeneous execution, but they are hindered by excessive data movement and dependence on CPU capabilities. Fully overlapping PCIe communication latency gets challenging as the size of the KV cache grows and/or the GPU compute capabilities increase. In this paper, we introduce KVPR, an efficient I/O-aware LLM inference method where the CPU first transfers a partial set of activations, from which the GPU can start recomputing the KV cache values. While the GPU recomputes the partial KV cache, the remaining portion of the KV cache is transferred concurrently from the CPU. This approach overlaps GPU recomputation with KV cache transfer to minimize idle GPU time and maximize inference performance. KVPR is fully automated by integrating a profiler module that utilizes input characteristics and system hardware information, a scheduler module to optimize the distribution of computation and communication workloads, and a runtime module to efficiently execute the derived execution plan. Experimental results show that KVPR achieves up to 35.8% lower latency and 46.2% higher throughput during decoding compared to state-of-the-art approaches. The code is available at https://github.com/chaoyij/KVPR.","大语言模型(LLMS) 的推断在计算上要求很高。 为了降低自动递减解解码的成本, Key-Value (KV) 缓存用于存储中间激活, 从而大大降低代币生成的计算管理。 但是, KV 缓存所需的记忆迅速增长, 往往超过 GPU 内存的能力。 一个具有成本效益的替代方案是将 KV 缓存装入 CPU 内存, 减轻 GPU 的内存压力, 但将瓶颈转到 CPU 和 GPU 之间的 PCI 连接的有限带宽。 现有的方法试图解决这些问题, 与 I/ O 重复的 GPU 计算, 或使用 CPU- GPU 混合执行, 但由于过度的数据移动和依赖 CPU 能力。 完全重叠 PCI 通信延迟, 将 KVVPU 的存储量增加和/ COME 能力提高。 在本文中, 我们引入 KVPRPU 高效的 I/O 和 LM 计算方法, 将启动部分启动启动启动启动启动, IM 运行, 运行的运行的运行中, 运行中, 运行的运行的运行中, 运行中, 运行中, 运行的运行中的运行中的运行中的运行中, 将运行中运行中运行中运行中运行中运行中运行中运行中运行中, 将运行中运行中运行中的运行中, 运行中, 运行中运行中运行中运行中运行中, 运行中运行中运行中的运行中运行中运行中运行中运行中运行中运行中运行中, K。","Chaoyi Jiang, Lei Gao, Hossein Entezari Zarch, Murali Annavaram",2025-06-04T16:08:50Z,KVPR: Efficient LLM Inference with I/O-Aware KV Cache Partial   Recomputation,KVPR: Effiziente LLM-Inferenz mit I/O-Aware KV Cache Partielle Recomputation,KVPR: 高效LLM 与 I/O-Aware KV 缓存部分撤回,http://arxiv.org/abs/2411.17089v2
26,"Large Language Models (LLMs) increasingly rely on Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to align model responses with human preferences. While RLHF employs a reinforcement learning approach with a separate reward model, SFT uses human-curated datasets for supervised learning. Both approaches traditionally depend on small, vetted groups of annotators, making them costly, prone to bias, and limited in scalability. We propose an open, crowd-sourced fine-tuning framework that addresses these limitations by enabling broader feedback collection for SFT without extensive annotator training. Our framework promotes incentive fairness via a point-based reward system correlated with Shapley values and guides model convergence through iterative model updates. Our multi-model selection framework demonstrates up to a 55% reduction in target distance over single-model selection, enabling subsequent experiments that validate our point-based reward mechanism's close alignment with Shapley values (a well-established method for attributing individual contributions) thereby supporting fair and scalable participation.","大型语言模型(LLMS)日益依赖监督的精细调试和人类反馈强化学习(RLHF)来调整模型反应,使其符合人类的喜好。虽然RLHF采用一种强化学习方法,采用单独的奖赏模式,但SFT使用人造数据集来监督学习。这两种方法传统上都依赖少数经过审查的批注者群体,使其费用昂贵,容易产生偏向,而且可缩放性有限。我们提议了一个开放的、众源的微调框架,通过在不进行广泛的说明培训的情况下为SFT提供更广泛的反馈收集来克服这些限制。我们的框架通过基于点的奖励系统促进奖励公平,通过迭代模式更新与“光彩”价值和指南模式趋同。我们的多模式选择框架显示,目标距离比单一模型选择减少55%,使随后的实验能够验证我们基于点的奖励机制与Shapley值的密切吻合(一种确定个人贡献的既定方法),从而支持公平和可扩展的参与。","Alex Sotiropoulos, Sulyab Thottungal Valapu, Linus Lei, Jared Coleman, Bhaskar Krishnamachari",2025-06-04T15:26:38Z,Crowd-SFT: Crowdsourcing for LLM Alignment,Crowd-SFT: Crowdsourcing für LLM Alignment,人-人-人-SFT:LLM对齐的众包,http://arxiv.org/abs/2506.04063v1
27,"Scientific research in many fields routinely requires the analysis of large datasets, and scientists often employ workflow systems to leverage clusters of computers for their data analysis. However, due to their size and scale, these workflow applications can have a considerable environmental footprint in terms of compute resource use, energy consumption, and carbon emissions. Mitigating this is critical in light of climate change and the urgent need to reduce carbon emissions.   In this chapter, we exemplify the problem by estimating the carbon footprint of three real-world scientific workflows from different scientific domains. We then describe techniques for reducing the energy consumption and, thereby, carbon footprint of individual workflow tasks and entire workflow applications, such as using energy-efficient heterogeneous architectures, generating optimised code, scaling processor voltages and frequencies, consolidating workloads on shared cluster nodes, and scheduling workloads for optimised energy efficiency.","许多领域的科学研究经常需要分析大型数据集,科学家往往利用工作流程系统来利用计算机群群进行数据分析,然而,由于其规模和规模,这些工作流程应用程序在计算资源使用、能源消耗和碳排放方面可以具有相当大的环境足迹。鉴于气候变化和减少碳排放的迫切需要,这一点至关重要。在本章中,我们通过估计不同科学领域的三个真实世界科学工作流程的碳足迹来说明这一问题。我们然后描述了减少能源消耗的技术,从而说明了个人工作流程任务和整个工作流程应用的碳足迹,例如使用节能的多功能结构、生成优化的编码、按比例增加处理器电压和频率、合并共享集群节点的工作量以及优化能源效率的时间安排。","Lauritz Thamsen, Yehia Elkhatib, Paul Harvey, Syed Waqar Nabi, Jeremy Singer, Wim Vanderbauwhede",2025-06-04T15:26:24Z,Energy-Aware Workflow Execution: An Overview of Techniques for Saving   Energy and Emissions in Scientific Compute Clusters,Energy-Aware Workflow Execution: Ein Überblick über Techniken zur Einsparung von Energie und Emissionen in wissenschaftlichen Compute Clustern,能源软件工作流程执行:在科学计算组中节省能源和排放的技术概览,http://arxiv.org/abs/2506.04062v1
28,"Resilience against malicious participants and data privacy are essential for trustworthy federated learning, yet achieving both with good utility typically requires the strong assumption of a trusted central server. This paper shows that a significantly weaker assumption suffices: each pair of participants shares a randomness seed unknown to others. In a setting where malicious participants may collude with an untrusted server, we propose CafCor, an algorithm that integrates robust gradient aggregation with correlated noise injection, using shared randomness between participants. We prove that CafCor achieves strong privacy-utility trade-offs, significantly outperforming local differential privacy (DP) methods, which do not make any trust assumption, while approaching central DP utility, where the server is fully trusted. Empirical results on standard benchmarks validate CafCor's practicality, showing that privacy and robustness can coexist in distributed systems without sacrificing utility or trusting the server.","针对恶意参与者的复原力和数据隐私对于可靠的联合会学习至关重要,但是,要同时实现良好的实用性,通常需要有一个可靠的中央服务器的有力假设。本文表明,一个明显弱小的假设就足够了:每对参与者共享一个别人不知道的随机种子。在一个恶意参与者可能与一个不信任的服务器串通的环境中,我们提议CafCor,这是一种利用参与者之间共享的随机性,将稳健的梯度聚合与相关噪音注入结合起来的算法。我们证明,CafCor实现了强大的隐私和利用率的权衡,大大超过了当地差异性隐私(DP)方法,这些方法在接近服务器完全信任的中央DP效用时并没有作出任何信任的假设。标准基准的经验结果验证了CafCor的实用性,表明隐私和稳健性可以在分配的系统中共存,同时不牺牲功能或信任服务器。","Youssef Allouah, Rachid Guerraoui, John Stephan",2025-06-04T15:22:18Z,Towards Trustworthy Federated Learning with Untrusted Participants,Auf dem Weg zu vertrauensvollem Federated Learning mit nicht vertrauenswürdigen Teilnehmern,争取与未受信任的参与者进行可信赖的联邦学习,http://arxiv.org/abs/2505.01874v2
29,"The burgeoning field of foundation models necessitates advanced data processing mechanisms capable of harnessing vast and valuable data with various types used by these models. Nevertheless, the current landscape presents unique challenges that traditional data processing frameworks struggle to handle effectively, particularly in handling the complexity of multimodal data. In response, we present Data-Juicer 2.0, a data processing system backed by 100+ data processing operators spanning text, image, video, and audio modalities, supporting more critical tasks including data analysis, synthesis, annotation, and foundation model post-training. With seamless compatibility and dedicated optimization for popular dataset hubs like Hugging Face and computing engines like Ray, it improves upon its predecessor in terms of usability, efficiency, and programmability. It features an easily accessible user interface layer that supports decoupled Python interactions, RESTful APIs, and conversational commands. It contains a new runtime layer optimized for adaptive execution and management across varying dataset scales, processing demands, and computational environments, while hiding unnecessary system details. Extensive empirical evaluations demonstrate Data-Juicer 2.0's remarkable performance and scalability, highlighting its capability to efficiently process TB-level data with 10k+ CPU cores. The system is publicly available and has been widely adopted in diverse research fields and real-world products such as Alibaba Cloud PAI. We actively maintain it and share insights from practical feedback, with the goal of facilitating research and application of next-generation foundation models.","基础模型的涌现领域要求具备先进的数据处理机制,能够利用这些模型所使用的各种类型的广泛而宝贵的数据来利用大量宝贵的数据。然而,目前的情况呈现出传统数据处理框架难以有效处理的独特挑战,特别是在处理多式数据的复杂性方面。作为回应,我们展示了由100+数据处理操作者支持的包含文字、图像、视频和音频模式的数据处理系统Data-Juice 2.0,这是一个由100+数据处理操作者支持的数据处理系统,涵盖文字、图像、视频和音频模式,支持更为关键的任务,包括数据分析、合成、批注和基础模式培训后模式。随着对像Ray这样的热门化和计算引擎的无缝兼容和专门优化,它改善了其前身的可用性、效率和可编程性。它拥有一个容易获取的用户界面界面,支持分解的Python互动、RESTform API和谈话命令。它包含一个新的运行最优化的运行时间层层层层,在隐藏不必要的系统细节的同时,广泛的实验性评价显示了数据-Jucer 2.0的显著性业绩和可及可扩展性,突出性,突出其用于公共研究领域的研究领域和可及可及可获取性研究领域。","Daoyuan Chen, Yilun Huang, Xuchen Pan, Nana Jiang, Haibin Wang, Yilei Zhang, Ce Ge, Yushuo Chen, Wenhao Zhang, Zhijian Ma, Jun Huang, Wei Lin, Yaliang Li, Bolin Ding, Jingren Zhou",2025-06-04T13:46:21Z,Data-Juicer 2.0: Cloud-Scale Adaptive Data Processing for and with   Foundation Models,Data-Juicer 2.0: Cloud-Scale Adaptive Datenverarbeitung für und mit Basismodellen,Data-Juicer 2.0:云层-空间适应性数据处理与基础模型,http://arxiv.org/abs/2501.14755v2
30,"As AI evolves, collaboration among heterogeneous models helps overcome data scarcity by enabling knowledge transfer across institutions and devices. Traditional Federated Learning (FL) only supports homogeneous models, limiting collaboration among clients with heterogeneous model architectures. To address this, Heterogeneous Federated Learning (HtFL) methods are developed to enable collaboration across diverse heterogeneous models while tackling the data heterogeneity issue at the same time. However, a comprehensive benchmark for standardized evaluation and analysis of the rapidly growing HtFL methods is lacking. Firstly, the highly varied datasets, model heterogeneity scenarios, and different method implementations become hurdles to making easy and fair comparisons among HtFL methods. Secondly, the effectiveness and robustness of HtFL methods are under-explored in various scenarios, such as the medical domain and sensor signal modality. To fill this gap, we introduce the first Heterogeneous Federated Learning Library (HtFLlib), an easy-to-use and extensible framework that integrates multiple datasets and model heterogeneity scenarios, offering a robust benchmark for research and practical applications. Specifically, HtFLlib integrates (1) 12 datasets spanning various domains, modalities, and data heterogeneity scenarios; (2) 40 model architectures, ranging from small to large, across three modalities; (3) a modularized and easy-to-extend HtFL codebase with implementations of 10 representative HtFL methods; and (4) systematic evaluations in terms of accuracy, convergence, computation costs, and communication costs. We emphasize the advantages and potential of state-of-the-art HtFL methods and hope that HtFLlib will catalyze advancing HtFL research and enable its broader applications. The code is released at https://github.com/TsingZ0/HtFLlib.","随着大赦国际的发展,不同模式之间的协作有助于通过促进跨机构和装置的知识转让来克服数据稀缺性。传统联邦学习(FL)仅支持同质模式,限制不同模式结构的客户之间的协作。为了解决这个问题,发展了异质联邦学习(HtFL)方法,以便能够在不同差异模式之间开展合作,同时解决数据异质性问题。然而,缺乏一个对快速增长的HtFL方法进行标准化评价和分析的全面基准。第一,高度多样化的数据集、模型异质假设以及不同方法的实施成为对HtFL方法进行简单和公平比较的障碍。第二,HtFL方法的效力和稳健性在各种情景中,例如医疗领域和传感器信号模式。为了填补这一差距,我们引入了第一个超异质性联邦学习图书馆(HtFlliclibliblib),一个易于使用和可扩展的框架,将多重数据集集集和模型的异质性州级假设,为研究和实用应用的可靠基准。具体而言,HtFLFLT方法在HlFLF的模型应用中具有代表性(1) 12个模型成本。","Jianqing Zhang, Xinghao Wu, Yanbing Zhou, Xiaoting Sun, Qiqi Cai, Yang Liu, Yang Hua, Zhenzhe Zheng, Jian Cao, Qiang Yang",2025-06-04T13:44:00Z,HtFLlib: A Comprehensive Heterogeneous Federated Learning Library and   Benchmark,HtFLlib: Eine umfassende heterogene Föderierte Lernbibliothek und Benchmark,HtFLLlilib:综合异种联邦学习图书馆和基准,http://arxiv.org/abs/2506.03954v1
31,"In blockchain networks, so-called ""full nodes"" serve data to and relay transactions from clients through an RPC interface. This serving layer enables integration of ""Web3"" data, stored on blockchains, with ""Web2"" mobile or web applications that cannot directly participate as peers in a blockchain network. In practice, the serving layer is dominated by a small number of centralized services (""node providers"") that offer permissioned access to RPC endpoints. Clients register with these providers because they offer reliable and convenient access to blockchain data: operating a full node themselves requires significant computational and storage resources, and public (permissionless) RPC nodes lack financial incentives to serve large numbers of clients with consistent performance.   Permissioned access to an otherwise permissionless blockchain network raises concerns regarding the privacy, integrity, and availability of data access. To address this, we propose a Permissionless Accountable RPC Protocol (PARP). It enables clients and full nodes to interact pseudonymously while keeping both parties accountable. PARP leverages ""light client"" schemes for essential data integrity checks, combined with fraud proofs, to keep full nodes honest and accountable. It integrates payment channels to facilitate micro-payments, holding clients accountable for the resources they consume and providing an economic incentive for full nodes to serve. Our prototype implementation for Ethereum demonstrates the feasibility of PARP, and we quantify its overhead compared to the base RPC protocol.","在连锁网络中,所谓的“完全节点”为客户提供数据和通过 RPC 接口转发交易。 这个服务层能够将“Web3”数据与“Web2”移动或网络应用程序结合起来,而“Web3”数据存储在链链中无法作为同侪直接参与。实际上,服务层由少数中央服务机构(“节点提供者”)主导,它们允许访问RPC终点。客户向这些提供者登记,因为它们提供了可靠和方便的访问链数据:运行一个全节点本身需要大量的计算和存储资源,而公共(无授权的)RPC节点缺乏为大量业绩一致的客户服务的财政激励。允许进入一个“Web2”移动或网络引起了对数据访问的隐私、完整性和可用性的关切。为了解决这个问题,我们建议一个允许不受允许的 RPC 终端协议(PAP 协议 ) 。 客户和全节点可以进行假称的互动,同时保持双方的问责制。 PARP 利用“ 光客户” 计划进行基本数据完整性检查, 与欺诈证据相结合, 公共(无限制) 节点节点节点节点缺乏金融节点缺乏奖励节点节点节点节点的节点节点的节点的节点 , 使得我们的客户能够完全负责地支付。","Weihong Wang, Tom Van Cutsem",2025-06-04T13:31:32Z,Depermissioning Web3: a Permissionless Accountable RPC Protocol for   Blockchain Networks,Depermissioning Web3: ein zulässiges RPC-Protokoll für Blockchain-Netzwerke,禁用 Web3: 设置链链网络的 RPC RPC 协议,http://arxiv.org/abs/2506.03940v1
32,"Given two different collections of sets, the exact set similarity R-S Join finds all set pairs with similarity no less than a given threshold, which has widespread applications. While existing algorithms accelerate large-scale R-S Joins using a two-stage filter-and-verification framework along with the parallel and distributed MapReduce framework, they suffer from excessive candidate set pairs, leading to significant I/O, data transfer, and verification overhead, and ultimately degrading the performance. This paper proposes novel candidate-free R-S Join (CF-RS-Join) algorithms that integrate filtering and verification into a single stage through filter-and-verification trees (FVTs) and their linear variants (LFVTs). First, CF-RS-Join with FVT (CF-RS-Join/FVT) is proposed to leverage an innovative FVT structure that compresses elements and associated sets in memory, enabling single-stage processing that eliminates the candidate set generation, fast lookups, and reduced database scans. Correctness proofs are provided. Second, CF-RS-Join with LFVT (CF-RS-Join/LFVT) is proposed to exploit a more compact Linear FVT, which compresses non-branching paths into single nodes and stores them in linear arrays for optimized traversal. Third, MR-CF-RS-Join/FVT and MR-CF-RS-Join/LFVT have been proposed to extend our approaches using MapReduce for parallel processing. Empirical studies on 7 real-world datasets have been conducted to evaluate the performance of the proposed algorithms against selected existing algorithms in terms of execution time, scalability, memory usage, and disk usage. Experimental results demonstrate that our algorithm using MapReduce, i.e., MR-CF-RS-Join/LFVT, achieves the best performance.","以两种不同的数据集收藏, 精确设定的相近 R- S Join 发现所有相近的配对均不少于一个类似阈值,这具有广泛的应用性。虽然现有的算法使用两个阶段的过滤和核查框架以及平行和分布式的 MapReduce 框架加速了大型 R- S join 组合,但是它们遭受了过多的候选配对,导致大量I/ O、数据传输和核查管理,最终降低了性能。本文件提议采用新的无候选人的 R- S 联合(CF-RS- Join ) 算法,通过过滤和核查树(FVT) 及其线性变体(LFVT ) 加速大型 R- S- S 联合。 首先, CF- RS- Jin 与 FVT (C- RF- Rival- Iral- Serveral- Serveral- Serveral- Serveral- Serval- lavements ) 使用一个创新 工具, i- the real- real- lif- ex- list- sal- lif- sal- sleval-leval- serval- us- liversal-lational-lations) liver- serval- sal- slations, liver- liver- liver- 和i- slations- slations- 和i- slations- suptal- suptal- sal- slations- sal-s-s-s-s- sal- , lads- sal-s-s-s-s-s-s-s-s-s-s- sal-s- sal- sal- li- sal-s-s-s- sal-s- sal-s- sal- sal-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s- sl-s- serv) lad- li-s-s-s- li- li-","Yuhong Feng, Fangcao Jian, Yixuan Cao, Xiaobin Jian, Jia Wang, Haiyue Feng, Chunyan Miao",2025-06-04T12:42:36Z,An Efficient Candidate-Free R-S Set Similarity Join Algorithm with the   Filter-and-Verification Tree and MapReduce,"Eine effiziente, kandidatfreie R-S-Set-Ähnlichkeit Begleiten Sie den Algorithmus mit dem Filter-und-Verifikationsbaum und MapReduce",与过滤和核查树和地图显示的高效无候选人候选人 R-S 设置相似性,http://arxiv.org/abs/2506.03893v1
33,"Safe memory reclamation techniques that utilize per read reservations, such as hazard pointers, often cause significant overhead in traversals of linked concurrent data structures. This is primarily due to the need to announce a reservation, and fence to enforce appropriate ordering, before each read. In read-intensive workloads, this overhead is amplified because, even if relatively little memory reclamation actually occurs, the full overhead of reserving records is still incurred while traversing data structures.   In this paper, we propose a novel memory reclamation technique by combining POSIX signals and delayed reclamation, introducing a publish-on-ping approach. This method eliminates the need to make reservations globally visible before use. Instead, threads privately track which records they are accessing, and share this information on demand with threads that intend to reclaim memory. The approach can serve as a drop-in replacement for hazard pointers and hazard eras. Furthermore, the capability to retain reservations during traversals in data structure operations and publish them on demand facilitates the construction of a variant of hazard pointers (EpochPOP). This variant uses epochs to approach the performance of epoch-based reclamation in the common case where threads are not frequently delayed (while retaining the robustness of hazard pointers).   Our publish-on-ping implementations based on hazard pointers (HP) and hazard eras, when applied to various data structures, exhibit significant performance improvements. The improvements across various workloads and data structures range from 1.2X to 4X over the original HP, up to 20% compared to a heavily optimized HP implementation similar to the one in the Folly open-source library, and up to 3X faster than hazard eras. EpochPOP delivers performance similar to epoch-based reclamation while providing stronger guarantees.","安全存储回收技术使用每读保留物,例如危险指示器,常常在连接的并行数据结构的穿梭过程中造成巨大的间接成本。这主要是由于需要在每次阅读之前宣布保留物和栅栏,以便执行适当的订购。在阅读密集的工作量中,这种间接成本被放大,因为即使实际发生相对较少的记忆回收,保留记录的全部间接成本仍然在数据结构穿行过程中发生。在本文件中,我们建议一种新型的记忆回收技术,将POSIX信号和延迟回收结合起来,采用一种出版即时的方法。这种方法消除了使全球在使用之前能够看到相关数据结构的隐性能。相反,需要将它们访问的私人记录线条连接起来,用希望恢复记忆的线条分享这种需求信息。这个方法可以用来取代危险点和危险时间。此外,在数据结构穿行期间保留保留保留记录物的能力有助于构建一个以POSIX为主的开放点(EpochPOPP),采用这种方法在使用之前就消除了全球范围的保留物。相反的私隐隐隐性记录器,在运行过程中,在运行过程中经常使用一种动态的精确的危害等级数据,在运行期间提供一种危险等级的数据,在使用。","Ajay Singh, Trevor Brown",2025-06-04T11:59:33Z,Publish on Ping: A Better Way to Publish Reservations in Memory   Reclamation for Concurrent Data Structures,Publishing on Ping: Ein besserer Weg zur Veröffentlichung von Reservierungen in Speicherreklamation für parallele Datenstrukturen,出版《Ping:同时数据结构内存检索保留书出版的更好方法》,http://arxiv.org/abs/2501.04250v2
34,"Federated learning is a distributed machine learning paradigm through centralized model aggregation. However, standard federated learning relies on a centralized server, making it vulnerable to server failures. While existing solutions utilize blockchain technology to implement Decentralized Federated Learning (DFL), the statistical heterogeneity of data distributions among clients severely degrades the performance of DFL. Driven by this issue, this paper proposes a decentralized federated prototype learning framework, named DFPL, which significantly improves the performance of DFL across heterogeneous data distributions. Specifically, DFPL introduces prototype learning into DFL to mitigate the impact of statistical heterogeneity and reduces the amount of parameters exchanged between clients. Additionally, blockchain is embedded into our framework, enabling the training and mining processes to be implemented locally on each client. From a theoretical perspective, we provide convergence guarantee of DFPL by modeling the resource allocation between training and mining. The experiments highlight the superiority of our DFPL framework in model performance and communication efficiency across four benchmark datasets with heterogeneous data distributions.","联邦学习是一种分散的机械学习模式,通过集中的模型集成,但标准联合学习依赖中央服务器,使其容易受服务器故障的影响。虽然现有解决方案利用链链技术实施分权联邦学习(DFL),但客户之间数据分布的统计差异性严重削弱了DFL的绩效。受这一问题驱动,本文件提出一个名为DFPL的分散化联邦原型学习框架,它大大改善了DFL在不同数据分布中的性能。具体地说,DFL将原型学习引入DFL,以减轻统计多样性的影响,并减少客户之间交换的参数数量。此外,将块链嵌入我们的框架中,使培训和采矿过程能够在当地对每个客户实施。从理论角度看,我们通过模拟培训和采矿之间的资源分配,为DFPL提供融合保证。实验突出了我们的DPL框架在模式性能和通信效率方面的优势,覆盖了四个具有多种数据分布的基准数据集。","Hongliang Zhang, Fenghua Xu, Zhongyuan Yu, Chunqiang Hu, Shanchen Pang, Xiaofen Wang, Jiguo Yu",2025-06-04T11:57:22Z,DFPL: Decentralized Federated Prototype Learning Across Heterogeneous   Data Distributions,DFPL: Dezentrales Federated Prototype Learning über unterschiedliche Datenverteilungen hinweg,"DFPL: 分散的联邦原型学习,跨异种数据分布",http://arxiv.org/abs/2505.04947v2
35,"Managed big data frameworks, such as Apache Spark and Giraph demand a large amount of memory per core to process massive volume datasets effectively. The memory pressure that arises from the big data processing leads to high garbage collection (GC) overhead. Big data analytics frameworks attempt to remove this overhead by offloading objects to storage devices. At the same time, infrastructure providers, trying to address the same problem, attribute more memory to increase memory per instance leaving cores underutilized. For frameworks, trying to avoid GC through offloading to storage devices leads to high Serialization/Deserialization (S/D) overhead. For infrastructure, the result is that resource usage is decreased. These limitations prevent managed big data frameworks from effectively utilizing the CPU thus leading to low server throughput.   We conduct a methodological analysis of server throughput for managed big data analytics frameworks. More specifically, we examine, whether reducing GC and S/D can help increase the effective CPU utilization of the server. We use a system called TeraHeap that moves objects from the Java managed heap (H1) to a secondary heap over a fast storage device (H2) to reduce the GC overhead and eliminate S/D over data. We focus on analyzing the system's performance under the co-location of multiple memory-bound instances to utilize all available DRAM and study server throughput. Our detailed methodology includes choosing the DRAM budget for each instance and how to distribute this budget among H1 and Page Cache (PC). We try two different distributions for the DRAM budget, one with more H1 and one with more PC to study the needs of both approaches. We evaluate both techniques under 3 different memory-per-core scenarios using Spark and Giraph with native JVM or JVM with TeraHeap. We do this to check throughput changes when memory capacity increases.","管理大数据框架, 如 Apache Spark 和 Giraph 管理大数据框架, 如 Apache Spark 和 Giraph 等, 要求每个核心都有大量的内存, 以有效处理大量数量数据集。 大型数据处理产生的内存压力导致大量垃圾收集( GC) 间接费用。 大数据分析框架试图通过将物件卸载到存储设备来清除这一间接费用。 与此同时, 基础设施提供者, 试图解决同样的问题, 赋予更多的内存, 以增加每个事件留下的核心的内存。 对于框架, 试图通过卸载到存储设备, 避免每个核心都有大量的内存。 对于基础设施而言, 由大数据处理( S/ D) 管理高序列/ Desireal化( S/ D) 管理大数据框架, 结果是资源使用更少的资源使用。 这些限制使得大数据框架无法有效地使用 CPUA ( H1) 和 DRA 快速存储设备( H2 ) 的内存数据系统, 将所有内存的内存系统 的内存数据系统 都通过 S- 将内存的内存系统 的内存系统 的内存系统 和 D 压缩的内存的内存系统 减少。","Emmanouil Anagnostakis, Polyvios Pratikakis",2025-06-04T11:37:51Z,Analysis of Server Throughput For Managed Big Data Analytics Frameworks,Analyse des Serverdurchsatzes für verwaltete Big Data Analytics Frameworks,用于管理大数据分析框架的服务器传输流量分析,http://arxiv.org/abs/2506.03854v1
36,"A method for efficient scheduling of hybrid classical-quantum workflows is presented, based on standard tools available on common supercomputer systems. Moderate interventions by the user are required, such as splitting a monolithic workflow in to basic building blocks and ensuring the data flow. This bares the potential to significantly reduce idle time of the quantum resource as well as overall wall time of co-scheduled workflows. Relevant pseudo-code samples and scripts are provided to demonstrate the simplicity and working principles of the method.","根据通用超级计算机系统的现有标准工具,介绍了一种高效安排混合古典-分子工作流程的方法,需要用户采取适度干预措施,例如将单一工作流程分为基本构件和确保数据流动,这暴露了大大减少量子资源闲置时间以及共同排定工作流程整体墙上时间的潜力,提供了相关的伪代码样本和脚本,以展示方法的简单性和工作原则。","Aniello Esposito, Utz-Uwe Haus",2025-06-04T11:24:10Z,SLURM Heterogeneous Jobs for Hybrid Classical-Quantum Workflows,SLURM Heterogene Jobs für Hybrid-Klassisch-Quantum-Workflows,SLURM SLURM 混合古产量流量的多样化工作,http://arxiv.org/abs/2506.03846v1
37,"Achieving differentially private computations in decentralized settings poses significant challenges, particularly regarding accuracy, communication cost, and robustness against information leakage. While cryptographic solutions offer promise, they often suffer from high communication overhead or require centralization in the presence of network failures. Conversely, existing fully decentralized approaches typically rely on relaxed adversarial models or pairwise noise cancellation, the latter suffering from substantial accuracy degradation if parties unexpectedly disconnect. In this work, we propose IncA, a new protocol for fully decentralized mean estimation, a widely used primitive in data-intensive processing. Our protocol, which enforces differential privacy, requires no central orchestration and employs low-variance correlated noise, achieved by incrementally injecting sensitive information into the computation. First, we theoretically demonstrate that, when no parties permanently disconnect, our protocol achieves accuracy comparable to that of a centralized setting-already an improvement over most existing decentralized differentially private techniques. Second, we empirically show that our use of low-variance correlated noise significantly mitigates the accuracy loss experienced by existing techniques in the presence of dropouts.","在分散化环境中实现不同的私人计算带来了巨大的挑战,特别是在准确性、通信成本和对信息泄漏的稳健性方面。虽然加密解决方案带来了希望,但它们往往会受到高通信间接费用的影响,或者在出现网络故障时需要集中处理。相反,现有的完全分散化的方法通常依赖于放松对抗模式或双向取消噪音,如果当事方意外地脱节,后者则会受到相当程度的准确性下降的影响。在这项工作中,我们建议IncA(IncA),这是一个完全分散化平均估算的新协议,在数据密集处理中广泛使用原始程序。我们的协议是强制实行差异性隐私,不需要中央管弦化和采用低差异性相关噪音,通过在计算中逐步注入敏感信息来实现。首先,我们理论上表明,当没有缔约方长期脱节时,我们的协议的准确性可以与集中式设置的准确性相比,从而可以改进大多数现有的分散化的私人技术。第二,我们的经验表明,我们使用低差异相关噪音可以大大减轻现有技术在辍学时的准确性损失。","César Sabater, Sonia Ben Mokhtar, Jan Ramon",2025-06-04T09:16:34Z,Dropout-Robust Mechanisms for Differentially Private and Fully   Decentralized Mean Estimation,Dropout-Robust-Mechanismen für unterschiedlich private und voll dezentralisierte mittlere Abschätzungen,区别对待的私人和完全分散平均估算的辍学和辍学 -- -- 抽样机制,http://arxiv.org/abs/2506.03746v1
38,"Decentralized federated learning (DFL) enables devices to collaboratively train models over complex network topologies without relying on a central controller. In this setting, local data remains private, but its quality and quantity can vary significantly across nodes. The extent to which a fully decentralized system is vulnerable to poor-quality or corrupted data remains unclear, but several factors could contribute to potential risks. Without a central authority, there can be no unified mechanism to detect or correct errors, and each node operates with a localized view of the data distribution, making it difficult for the node to assess whether its perspective aligns with the true distribution. Moreover, models trained on low-quality data can propagate through the network, amplifying errors. To explore the impact of low-quality data on DFL, we simulate two scenarios with degraded data quality -- one where the corrupted data is evenly distributed in a subset of nodes and one where it is concentrated on a single node -- using a decentralized implementation of FedAvg. Our results reveal that averaging-based decentralized learning is remarkably robust to localized bad data, even when the corrupted data resides in the most influential nodes of the network. Counterintuitively, this robustness is further enhanced when the corrupted data is concentrated on a single node, regardless of its centrality in the communication network topology. This phenomenon is explained by the averaging process, which ensures that no single node -- however central -- can disproportionately influence the overall learning process.","分散化的联邦学习(DFL) 使设备能够在不依赖中央控制器的情况下,对复杂的网络地形模型进行协作培训。 在这种环境下,本地数据仍然是私有的,但其质量和数量在节点上可能有很大差异。 完全分散化的系统在多大程度上容易受到低质量或腐败数据的影响,仍然不明确,但有几个因素可能助长潜在风险。没有中央当局,就无法统一机制来检测或纠正错误,每个节点的运作都以局部的数据分布方式进行操作,使得节点难以评估其观点是否与真实分布相一致。此外,在网络中,经过低质量数据培训的模型可以传播,但是在质量和质量方面差异很大。为了探索低质量数据对DFLL的影响,我们模拟了两种情况,数据质量下降。 腐败数据在一组节点中分布不均匀,在单一节点上集中使用FedAvg的分散化执行方式。 我们的结果表明,平均分散化的学习对于局部坏数据的影响非常强,即使腐败数据存在于最有影响力的节点中,这种核心的模型也可以通过网络的单一的集中化数据,反直观地解释。","Samuele Sabella, Chiara Boldrini, Lorenzo Valerio, Andrea Passarella, Marco Conti",2025-06-04T08:47:33Z,The Built-In Robustness of Decentralized Federated Averaging to Bad Data,"Die eingebaute Robustheit dezentralisierter, verdichteter Mittelung zu schlechten Daten",分权联邦对坏数据采用错误数据的预测的内在强力,http://arxiv.org/abs/2502.18097v2
39,"Lightweight containers provide an efficient approach for deploying computation-intensive applications in network edge. The layered storage structure of container images can further reduce the deployment cost and container startup time. Existing researches discuss layer sharing scheduling theoretically but with little attention paid to the practical implementation. To fill in this gap, we propose and implement a Layer-aware and Resource-adaptive container Scheduler (LRScheduler) in edge computing. Specifically, we first utilize container image layer information to design and implement a node scoring and container scheduling mechanism. This mechanism can effectively reduce the download cost when deploying containers, which is very important in edge computing with limited bandwidth. Then, we design a dynamically weighted and resource-adaptive mechanism to enhance load balancing in edge clusters, increasing layer sharing scores when resource load is low to use idle resources effectively. Our scheduler is built on the scheduling framework of Kubernetes, enabling full process automation from task information acquisition to container dep=loyment. Testing on a real system has shown that our design can effectively reduce the container deployment cost as compared with the default scheduler.","为填补这一空白,我们建议并采用边缘计算中的多层和可调整资源的集装箱调度系统(LRScheduler),具体地说,我们首先利用集装箱图像层信息来设计和实施节点评分和集装箱排期机制。这一机制可以有效降低集装箱的下载费用,因为边端计算使用带宽非常重要。然后,我们设计一个动态加权和资源适应机制,以加强边缘组的负载平衡,在资源负荷低时增加分数,以便有效使用闲置资源。我们的调度器建在库伯涅斯的排期框架上,使得从任务信息获取到集装箱调值的全过程自动化。在实际系统上进行的测试表明,我们的设计可以有效地降低集装箱部署费用,而默认排期则与此相比。","Zhiqing Tang, Wentao Peng, Jianxiong Guo, Jiong Lou, Hanshuai Cui, Tian Wang, Yuan Wu, Weijia Jia",2025-06-04T08:26:43Z,LRScheduler: A Layer-aware and Resource-adaptive Container Scheduler in   Edge Computing,LRScheduler: Ein schicht- und ressourcenadaptiver Container-Scheduler im Edge Computing,LRScheduler:边际计算中具有图层意识和资源适应性的集装箱调度表,http://arxiv.org/abs/2506.03694v1
40,"Tensor permutation is a fundamental operation widely applied in AI, tensor networks, and related fields. However, it is extremely complex, and different shapes and permutation maps can make a huge difference. SIMD permutation began to be studied in 2006, but the best method at that time was to split complex permutations into multiple simple permutations to do SIMD, which might increase the complexity for very complex permutations. Subsequently, as tensor contraction gained significant attention, researchers explored structured permutations associated with tensor contraction. Progress on general permutations has been limited, and with increasing SIMD bit widths, achieving efficient performance for these permutations has become increasingly challenging. We propose a SIMD permutation toolkit, \system, that generates optimized permutation code for arbitrary instruction sets, bit widths, tensor shapes, and permutation patterns, while maintaining low complexity. In our experiments, \system is able to achieve up to $38\times$ speedup for special cases and $5\times$ for general gases compared to Numpy.","色素变异是AI、强子网络和相关领域广泛应用的一项基本操作。 然而,它极其复杂,不同的形状和变异图可以产生巨大的变化。 SIMD变异于2006年开始研究,但当时最好的方法是将复杂的变异分成多个简单的变异以进行SIMD,这可能会增加非常复杂的变异的复杂性。随后,随着变速收缩受到极大关注,研究人员探索了与变速收缩相关的结构变异。一般变异的进展有限,随着SIMD位宽度的提高,这些变异的高效性能越来越具有挑战性。我们建议SIMD变异工具包(\ System)为任意教学组、位宽度、变速形状和变异模式生成优化的变异代码,同时保持低复杂性。在我们的实验中,\系统能够达到特殊情况下38美元的速度加速,而普通气体为5美元的时间值。","Yaojian Chen, Tianyu Ma, An Yang, Lin Gan, Wenlai Zhao, Guangwen Yang",2025-06-04T08:17:27Z,GenTT: Generate Vectorized Codes for General Tensor Permutation,GenTT: Generieren von vektorisierten Codes für allgemeine Tensor-Permutation,GENTT: 生成一般Tensor 变异的矢量编码,http://arxiv.org/abs/2506.03686v1
41,"The growing demand for real-time processing tasks is driving the need for multi-model inference pipelines on edge devices. However, cost-effectively deploying these pipelines while optimizing Quality of Service (QoS) and costs poses significant challenges. Existing solutions often neglect device resource constraints, focusing mainly on inference accuracy and cost efficiency. To address this, we develop a framework for configuring multi-model inference pipelines. Specifically: 1) We model the decision-making problem by considering the pipeline's QoS, costs, and device resource limitations. 2) We create a feature extraction module using residual networks and a load prediction model based on Long Short-Term Memory (LSTM) to gather comprehensive node and pipeline status information. Then, we implement a Reinforcement Learning (RL) algorithm based on policy gradients for online configuration decisions. 3) Experiments conducted in a real Kubernetes cluster show that our approach significantly improve QoS while reducing costs and shorten decision-making time for complex pipelines compared to baseline algorithms.","对实时处理任务日益增长的需求正在推动对边缘装置多模型推导管道的需求。然而,以成本效益高的方式部署这些管道,同时优化服务质量和成本带来了重大挑战。现有的解决方案往往忽视了资源限制,主要侧重于推论准确性和成本效率。为此,我们制定了一个配置多模型推论管道的框架。具体地说:1)我们通过考虑管道的QOS、成本和装置资源限制来模拟决策问题。 2)我们利用残余网络和基于长期短期内存的负载预测模型创建了特征提取模块,以收集全面的节点和管道状况信息。然后,我们根据在线配置决策的政策梯度实施强化学习(RL)算法。 3)在真正的Kubernetes集群中进行的实验表明,我们的方法大大改进了QOS,同时降低了与基线算法相比复杂的管道的成本并缩短了决策时间。","Jinhao Sheng, Zhiqing Tang, Jianxiong Guo, Tian Wang",2025-06-04T07:58:37Z,Adaptive Configuration Selection for Multi-Model Inference Pipelines in   Edge Computing,Adaptive Konfigurationsauswahl für Multi-Model-Inferenzpipelines im Edge Computing,边缘计算中多模式推推导管道的适应配置选择,http://arxiv.org/abs/2506.02814v2
42,"The increasing complexity of deep neural networks poses significant barriers to democratizing them to resource-limited edge devices. To address this challenge, split federated learning (SFL) has emerged as a promising solution by of floading the primary training workload to a server via model partitioning while enabling parallel training among edge devices. However, although system optimization substantially influences the performance of SFL under resource-constrained systems, the problem remains largely uncharted. In this paper, we provide a convergence analysis of SFL which quantifies the impact of model splitting (MS) and client-side model aggregation (MA) on the learning performance, serving as a theoretical foundation. Then, we propose AdaptSFL, a novel resource-adaptive SFL framework, to expedite SFL under resource-constrained edge computing systems. Specifically, AdaptSFL adaptively controls client-side MA and MS to balance communication-computing latency and training convergence. Extensive simulations across various datasets validate that our proposed AdaptSFL framework takes considerably less time to achieve a target accuracy than benchmarks, demonstrating the effectiveness of the proposed strategies.","深层神经网络日益复杂,严重阻碍了它们进入资源有限的边缘装置。为了应对这一挑战,通过模型分割将初级培训工作量加到服务器上,同时使边缘装置能够进行平行培训,从而将初级培训工作量加到服务器上,从而形成一个大有希望的解决办法。然而,尽管系统优化在资源紧张的系统下对SFL的性能产生了重大影响,但问题在很大程度上仍没有被探索出来。在本文件中,我们对SFL的趋同分析量化了模式分割和客户方模式组合对学习业绩的影响,作为理论基础。然后,我们建议SDSFL(一个新的资源适应性能强的SFL框架)在资源紧张的边缘计算系统下加速SFL(SFL),具体地说,SDSFL对客户方MA和MS(MS)的适应性能控制,以平衡通信-消耗层和培训的趋同。各种数据集的广泛模拟证实,我们提议的SDSFL框架比基准要花很多时间达到目标准确性,表明拟议战略的有效性。","Zheng Lin, Guanqiao Qu, Wei Wei, Xianhao Chen, Kin K. Leung",2025-06-04T07:17:02Z,AdaptSFL: Adaptive Split Federated Learning in Resource-constrained Edge   Networks,AdaptSFL: Adaptives Split-Federiertes Lernen in ressourcengebundenen Edge-Netzwerken,SSCSFL: 资源限制的边缘网络中的适应性分裂联邦学习,http://arxiv.org/abs/2403.13101v4
43,"Efficient inference of Multi-Head Latent Attention (MLA) is challenged by deploying the DeepSeek-R1 671B model on a single Multi-GPU server. This paper introduces FlashMLA-ETAP, a novel framework that enhances MLA inference for the single-instance deployment scenario on NVIDIA H20 GPUs. We propose the Efficient Transpose Attention Pipeline (ETAP), which reconfigures attention computation through transposition to align the KV context length with the \(M\)-dimension in WGMMA operations, significantly reducing redundant computations. FlashMLA-ETAP achieves a 2.78x speedup over FlashMLA at 64K sequence length (batch size 16), with 5.24x and 4.94x improvements over FlashAttention-3 and FlashInfer, respectively, while maintaining numerical stability with a 15.2x lower RMSE (\(1.25 \times 10^{-5}\)) than FlashAttention-3. Furthermore, ETAP's design enables seamless integration into frameworks like FlashAttention-3 and FlashInfer, supported by a detailed theoretical analysis. Our work addresses a critical gap in resource-constrained inference, offering a scalable solution for mid-tier GPUs and paving the way for broader adoption in hardware-aware optimization. Code is available at https://github.com/pengcuo/FlashMLA-ETAP.","将DeepSeek-R1-671B模型用于一个单一的多保服务器,这给多保服务器的DeepSeek-R1-R1671B模型带来了挑战。本文介绍了FlashMLA-ETAP,这是一个新的框架,可以加强对NVIDIA H20 GPUs单向部署情景的司法协助推断。我们建议高效传输注意管道(ETAP),通过转换将注意力计算方法重新配置为将KV的上下文长度与(M\\\\\)调整到WGMMA操作中,大大减少了多余的计算。FlashMLA-FATAP在64K序列长度(批量为16)上比FlashMLA加速了2.78x速度,分别加强了5.24x和4.94x对NVIDIA H20 GPUPS的单一部署情景的改进,同时将数字稳定与15.2x的RMSE(1.25\time 10-5)比闪快AVA-A3。此外,ETAP的设计的设计使得能够顺利地纳入FLass-Ang-Ang-A-Inferflass-ass-ass-ass-ass-ass-assimagrelassimagestal Solational commlational commal commal commal romal commal 提供一个详细的理论分析。我们可用于可提供一个详细的理论,在可提供详细的理论分析。","Pengcuo Dege, Qiuming Luo, Rui Mao, Chang Kong",2025-06-04T03:20:26Z,FlashMLA-ETAP: Efficient Transpose Attention Pipeline for Accelerating   MLA Inference on NVIDIA H20 GPUs,FlashMLA-ETAP: Effiziente Übertragung der Aufmerksamkeitspipeline zur Beschleunigung der MLA-Inferenz auf NVIDIA H20 GPUs,"FlammMLA-ETAP: 高效转引注意管道,加速NVIDIA H20 GPUs的司法协助推断",http://arxiv.org/abs/2506.01969v2
44,"Leaking information about the execution behavior of critical real-time tasks may lead to serious consequences, including violations of temporal constraints and even severe failures. We study information leakage for a special class of real-time tasks that have two execution modes, namely, typical execution (which invokes the majority of times) and critical execution (to tackle exceptional conditions). The data flow-driven applications inherit such a multimode execution model. In this paper, we investigate whether a low-priority ""observer"" task can infer the execution patterns of a high-priority ""victim"" task (especially the critical executions). We develop a new statistical analysis technique and show that by analyzing the response times of the low-priority task, it becomes possible to extract the execution behavior of the high-priority task. We test our approach against a random selection technique that arbitrarily classifies a job as critical. We find that correlating the observer's response times with the victim's jobs can result in higher precision in identifying critical invocations compared to a random guess. We conduct extensive evaluations with systemically generated workloads, including a case study using a UAV autopilot (ArduPilot) taskset parameters. We found that our inference algorithm can achieve relatively low false positive rates (less than 25%) with relatively low footprint (1 MB memory and 50 ms timing overhead on a Raspberry Pi 4 platform). We further demonstrate the feasibility of inference on two cyber-physical platforms: an off-the-shelf manufacturing robot and a custom-built surveillance system.","有关关键实时任务执行过程的信息泄漏可能会导致严重后果,包括违反时间限制甚至严重失灵。我们研究具有两种执行模式的一类特殊实时任务的信息泄漏,即典型执行(援引大多数时间)和关键执行(处理特殊条件)。数据流驱动应用程序继承了这样一个多模式执行模式。在本文件中,我们调查低优先“观察者”任务能否推断出高优先“受害者”任务(特别是关键处决)的执行模式。我们开发了新的统计分析技术,并表明通过分析低优先任务的反应时间,有可能提取高优先任务的执行行为。我们测试我们的方法,以随机选择技术将一项工作任意归类为关键任务。我们发现,观察员的反应时间与受害者的工作挂钩,可以导致与随机猜测相比更精确地确定关键职业。我们用系统生成的网络平台(特别是关键处决)进行广泛的评估,包括利用UAV自动驾驶(ArduPilot)对低优先任务的反应时间段,从而可以提取高优先任务的执行行为。我们测试了方法,我们发现,随机选择了一种方法,任意将某类工作与低比例的磁测算(我们测算中的低度机机机)比高的磁测算。我们发现,我们发现,可以在相对地测算系统上两个不精确测测算。我们可以得出一个不精确测测算。在25的平平平平平平的平的平的平的轨道上。我们。我们发现,可以取得一个相对的平平的平的平的平的平的平的平的平的平的平的平的平的平的平的平的平的平的轨道。","Mohammad Fakhruddin Babar, Zain A. H. Hammadeh, Mohammad Hamad, Monowar Hasan",2025-06-04T02:39:58Z,Investigating Timing-Based Information Leakage in Data Flow-Driven   Real-Time Systems,Untersuchung von Timing-basierten Informationen Leckage in datenflussgetriebenen Echtzeit-Systemen,数据流动驱动实时系统中基于时间的调查信息泄漏,http://arxiv.org/abs/2506.01991v2
45,"Scheduling virtual machines (VMs) on hosts in cloud data centers dictates efficiency and is an NP-hard problem with incomplete information. Prior work improved VM scheduling with predicted VM lifetimes. Our work further improves lifetime-aware scheduling using repredictions with lifetime distributions versus one-shot prediction. Our approach repredicts and adjusts VM and host lifetimes when incorrect predictions emerge. We also present novel approaches for defragmentation and regular system maintenance, which are essential to our data center reliability and optimizations, and are not explored in prior work. We show repredictions deliver a fundamental advance in effectiveness over one-shot prediction.   We call our novel combination of distribution-based lifetime predictions and scheduling algorithms Lifetime Aware VM Allocation (LAVA). LAVA reduces resource stranding and increases the number of empty hosts, which are critical for large VM scheduling, cloud system updates, and reducing dynamic energy consumption. Our approach runs in production within Google's hyperscale cloud data centers, where it improves efficiency by decreasing stranded compute and memory resources by ~3% and ~2% respectively. It increases empty hosts by 2.3-9.2 pp in production, reducing dynamic energy consumption, and increasing availability for large VMs and cloud system updates. We also show a reduction in VM migrations for host defragmentation and maintenance. In addition to our fleet-wide production deployment, we perform simulation studies to characterize the design space and show that our algorithm significantly outperforms the prior state of the art lifetime-based scheduling approach.","云中数据中心主机虚拟机(VMs)的配置是效率决定的,并且是一个NP-硬性问题,信息不完整。先前的工作改进了VM的进度,预测VM的寿命寿命期。我们的工作进一步改进了一生的进度安排,使用一生分布的回覆和一次性预测来改进一生的进度安排。我们的方法在出现不正确的预测时会减少资源紧张,调整VM和东道主的寿命期。我们还提出了分解和定期系统维护的新办法,这对数据中心的可靠性和优化至关重要,而且以前的工作也不会对此进行探讨。我们显示的回调在一次预测中提高了效力。我们称之为基于分配的一生预测和安排的算法新组合,我们称之为基于终身预测和安排的算法的全寿命期安排。我们称之为基于分配的一生预测和安排的算法的全寿命期安排。LAVVVAVA减少资源紧张,增加空宿主机组数量,这对于大型 VM 更新和快速能源消耗量。我们在谷内生产过程中会提高效率,通过缩缩缩缩缩缩缩缩缩缩缩缩图,在VMS 更新我们的版生产进度上显示我们VMRDRDM 。","Jianheng Ling, Pratik Worah, Yawen Wang, Yunchuan Kong, Anshul Kapoor, Chunlei Wang, Clifford Stein, Diwakar Gupta, Jason Behmer, Logan A. Bush, Prakash Ramanan, Rajesh Kumar, Thomas Chestna, Yajing Liu, Ying Liu, Ye Zhao, Kathryn S. McKinley, Meeyoung Park, Martin Maas",2025-06-04T01:30:15Z,LAVA: Lifetime-Aware VM Allocation with Learned Distributions and   Adaptation to Mispredictions,LAVA: Lifetime-Aware VM-Zuweisung mit erfahrenen Distributionen und Anpassung an falsche Vorlieben,"LAVA: 终生软件VM分配,配有学术分发和适应错误处理",http://arxiv.org/abs/2412.09840v2
46,"Datacenters have become the backbone of modern digital infrastructure, powering the rapid rise of artificial intelligence and promising economic growth and technological progress. However, this expansion has brought growing tensions in the local communities where datacenters are already situated or being proposed. While the mainstream discourse often focuses on energy usage and carbon footprint of the computing sector at a global scale, the local socio-environmental consequences -- such as health impacts, water usage, noise pollution, infrastructural strain, and economic burden -- remain largely underexplored and poorly addressed. In this work, we surface these community-level consequences through a mixed-methods study that combines quantitative data with qualitative insights. Focusing on Northern Virginia's ``Data Center Valley,'' we highlight how datacenter growth reshapes local environments and everyday life, and examine the power dynamics that determine who benefits and who bears the costs. Our goal is to bring visibility to these impacts and prompt more equitable and informed decisions about the future of digital infrastructure.","数据中心已成为现代数字基础设施的支柱,推动了人工智能的迅速上升和有希望的经济增长和技术进步。然而,这种扩张在已经或正在提出数据中心所在的地方社区造成了日益紧张的局面。主流讨论往往侧重于全球范围计算部门的能源使用和碳足迹,而当地社会-环境后果 -- -- 例如健康影响、用水、噪音污染、基础设施紧张和经济负担 -- -- 在很大程度上仍然得不到充分探讨和处理。在这项工作中,我们通过混合方法研究,将定量数据与定性洞察相结合,呈现出这些社区一级的后果。我们着重关注北弗吉尼亚州“数据中心谷”的“数据中心谷”,我们强调数据中心增长如何改变当地环境和日常生活,并审查确定谁受益和谁承担成本的动力动态。我们的目标是使人们了解这些影响,并促使就数字基础设施的未来作出更加公平和知情的决定。","Wacuka Ngata, Noman Bashir, Michelle Westerlaken, Laurent Liote, Yasra Chandio, Elsa Olivetti",2025-06-03T20:21:53Z,The Cloud Next Door: Investigating the Environmental and Socioeconomic   Strain of Datacenters on Local Communities,Die Cloud Next Door: Untersuchung des ökologischen und sozioökonomischen Einflusses von Rechenzentren auf lokale Gemeinschaften,云下一个门:调查地方社区数据中心的环境和社会经济趋势,http://arxiv.org/abs/2506.03367v1
47,"5G/6G sidelink communications addresses the challenge of connecting outer UEs, which are unable to directly access a base station (gNodeB), through inner UEs that act as relays to connect to the gNodeB. The key performance indicators include the achievable rates, the number of outer UEs that can connect to a gNodeB, and the latency experienced by outer UEs in establishing connections. We consider problem of determining the assignment of outer UEs to inner UEs based on the channel, interference, and traffic characteristics. We formulate an optimization problem to maximize a weighted sum rate of UEs, where weights can represent priority, waiting time, and queue length. This optimization accommodates constraints related to channel and interference characteristics that influence the rates at which links can successfully carry assigned traffic. While an exhaustive search can establish an upper bound on achievable rates by this non-convex optimization problem, it becomes impractical for larger number of outer UEs due to scalability issues related to high computational complexity. To address this, we present a greedy algorithm that incrementally selects links to maximize the sum rate, considering already activated links. This algorithm, although effective in achieving high sum rates, may inadvertently overlook some UEs, raising concerns about fairness. To mitigate this, we introduce a fairness-oriented algorithm that adjusts weights based on waiting time or queue length, ensuring that UEs with initially favorable conditions do not unduly disadvantage others over time. We show that this strategy not only improves the average admission ratio of UEs but also ensures a more equitable distribution of service among them, thereby providing a balanced and fair solution to sidelink communications.","5G/6G 侧链接通信处理将无法直接访问基站(gNodeB)的外部UE连接起来的挑战。关键业绩指标包括可实现的速率、可连接到 gNodeB 的外部UE数量,以及外部UE在建立连接方面经历的延迟度。我们考虑根据频道、干扰和交通特点确定将外部UE分配到内部UE(gNodeB)的问题。我们形成了一个优化问题,以最大限度地增加UE加权总和率,其中重量可以代表优先度、等待时间和排队长度。这种优化包含与频道和干扰特征有关的限制,这些特征影响连接能够成功连接到 gNodeB 的外部UEUE数量,以及外部UE在建立连接方面经历的延迟度。我们认为,确定外部UE的较大比例分配问题不切实际可行,因为与高计算复杂性有关。为了解决这个问题,我们提出了一种贪婪的算法,即从渐进的角度选择了时间联系,而不是使平均比例最大化,服务期和排队速度。考虑到已经快速进行的调整,我们提出了这样的算法,这样可以有效地理解。","Yalin E. Sagduyu, Tugba Erpek, Sastry Kompella, Kemal Davaslioglu",2025-06-03T19:19:04Z,Relay Selection and User Equipment Admission in Resource-Efficient NextG   Sidelink Communications,Relaisauswahl und Benutzerausstattung Zulassung in ressourceneffizienter NextG Sidelink-Kommunikation,在资源效率高的NextG 侧链接通信中中继选择和用户设备接收,http://arxiv.org/abs/2506.03328v1
48,"Deploying large language models (LLMs) for online inference is often constrained by limited GPU memory, particularly due to the growing KV cache during auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a promising solution by offloading KV cache management and parts of attention computation to the CPU. However, a key bottleneck remains: existing schedulers fail to effectively overlap CPU-offloaded tasks with GPU execution during the latency-critical, bandwidth-bound decode phase. This particularly penalizes real-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning) which are currently underserved by existing systems, especially under memory pressure typical of edge or low-cost deployments.   We present APEX, a novel, profiling-informed scheduling strategy that maximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems relying on static rules or purely heuristic approaches, APEX dynamically dispatches compute across heterogeneous resources by predicting execution times of CPU and GPU subtasks to maximize overlap while avoiding scheduling overheads.We evaluate APEX on diverse workloads and GPU architectures (NVIDIA T4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only schedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89% on A10 GPUs, while preserving latency. Against the best existing hybrid schedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in long-output settings.APEX significantly advances hybrid LLM inference efficiency on such memory-constrained hardware and provides a blueprint for scheduling in heterogeneous AI systems, filling a critical gap for efficient real-time LLM applications.","用于在线推断的大型语言模型(LLMS)的部署往往受到有限 GPU 记忆的限制,特别是由于在自动递增解码过程中KV缓存日益增长。混合 GPU-CPU 执行通过卸载 KV缓存管理和部分关注计算到 CPU 的典型存储压力而成为一个大有希望的解决办法。然而,一个关键的瓶颈仍然存在:现有的调度器未能有效地将 CPU 上载任务与GPU 执行工作重叠,而GPU-GPU 紧要带带带带宽宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带。 APEX IM IM IM IM IM IM IM IM 将现有资源, IM IM IM IM IMDFl IMDVPLVPl IM 以预测 CLTFT 递接宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽","Jiakun Fan, Yanglin Zhang, Xiangchen Li, Dimitrios S. Nikolopoulos",2025-06-03T18:35:56Z,Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs,Parallele CPU-GPU-Execution für LLM-Inferenz auf eingeschränkten GPUs,LLM LLM 受控 GPU 推论的平行 CPU-GPU 执行,http://arxiv.org/abs/2506.03296v1
49,"This work tackles the fundamental challenges in Federated Learning (FL) posed by arbitrary client participation and data heterogeneity, prevalent characteristics in practical FL settings. It is well-established that popular FedAvg-style algorithms struggle with exact convergence and can suffer from slow convergence rates since a decaying learning rate is required to mitigate these scenarios. To address these issues, we introduce the concept of stochastic matrix and the corresponding time-varying graphs as a novel modeling tool to accurately capture the dynamics of arbitrary client participation and the local update procedure. Leveraging this approach, we offer a fresh perspective on designing FL algorithms, provide a rigorous quantitative analysis of the limitations inherent in the FedAvg algorithm, and present FOCUS, Federated Optimization with Exact Convergence via Push-pull Strategy, a provably convergent algorithm designed to effectively overcome the previously mentioned two challenges. More specifically, we provide a rigorous proof demonstrating that FOCUS achieves exact convergence with a linear rate regardless of the arbitrary client participation, establishing it as the first work to demonstrate this significant result.","这项工作解决了联邦学习联盟(FL)中由任意的客户参与和数据差异性构成的基本挑战,这是实用FL环境中普遍存在的特点。众所周知,流行的FedAvg式算法与精确趋同并存,并可能受到缓慢的趋同率的影响,因为需要通过逐渐衰减的学习率来缓解这些情况。为了解决这些问题,我们引入了随机矩阵概念和相应的时间分布图,作为一个新的模型工具,以精确地捕捉任意客户参与和本地更新程序的动态。利用这一方法,我们提出了设计FL算法的新视角,对FedAvg算法所固有的局限性提供了严格的定量分析,并提出了FOCUS、FedAvg算法与Exact Convergence的优化,这是旨在有效克服前面提到的两个挑战的一个可察觉的趋同式算法。更具体地说,我们提供了有力的证据,证明FOCUS公司无论任意客户参与与否,都与线性比率完全一致,将它确定为证明这一重要结果的首项工作。","Bicheng Ying, Zhe Li, Haibo Yang",2025-06-03T18:32:53Z,Exact and Linear Convergence for Federated Learning under Arbitrary   Client Participation is Attainable,Exakte und lineare Konvergenz für das Föderierte Lernen unter willkürlicher Kundenbeteiligung ist nachhaltig,在任意客户参与下实现联邦学习联盟的精确和线性融合,http://arxiv.org/abs/2503.20117v2
50,"A litany of theoretical and numerical results have established the sketch-and-precondition paradigm as a powerful approach to solving large linear regression problems in standard computing environments. Perhaps surprisingly, much less work has been done on understanding how sketch-and-precondition performs on graphics processing unit (GPU) systems. We address this gap by benchmarking an implementation of sketch-and-precondition based on sparse sign-sketches on single and multi-GPU systems. In doing so, we describe a novel, easily parallelized, rejection-sampling based method for generating sparse sign sketches. Our approach, which is particularly well-suited for GPUs, is easily adapted to a variety of computing environments. Taken as a whole, our numerical experiments indicate that sketch-and-precondition with sparse sign sketches is particularly well-suited for GPUs, and may be suitable for use in black-box least-squares solvers.","一系列的理论和数字结果已经确立了草图和先决条件范式,作为解决标准计算环境中大规模线性回归问题的有力方法。也许令人惊讶的是,在理解草图和先决条件如何在图形处理器(GPU)系统上运行方面所做的工作要少得多。我们通过在单一和多GPU系统上以稀少的手动手动手动器为基础,为草图和先决条件的实施制定基准来弥补这一差距。在这样做的时候,我们描述了一种新颖的、容易平行的、基于拒绝抽样的方法,用于生成稀有的标志草图。我们的方法,特别适合GPUPS,很容易适应各种计算环境。总的来说,我们的数字实验表明,草图和带有稀有标志的草图的前提条件特别适合GPUPUs,并且可能适合用于黑箱中最小方格的解算器。","Tyler Chen, Pradeep Niroula, Archan Ray, Pragna Subrahmanya, Marco Pistoia, Niraj Kumar",2025-06-03T16:48:06Z,GPU-Parallelizable Randomized Sketch-and-Precondition for Linear   Regression using Sparse Sign Sketches,GPU-Parallelisierbare Randomized Sketch-and-Precondition für lineare Regression mit Sparse Sign Sketches,GPU-Paral- Paralable 使用微缩信号密片进行线性递减的可随机随机 Strach 预设条件,http://arxiv.org/abs/2506.03070v1
51,"Decentralized exchanges (DEXs) are crucial to decentralized finance (DeFi) as they enable trading without intermediaries. However, they face challenges like impermanent loss (IL), where liquidity providers (LPs) see their assets' value change unfavorably within a liquidity pool compared to outside it. To tackle these issues, we propose dynamic fee mechanisms over traditional fixed-fee structures used in automated market makers (AMM). Our solution includes asymmetric fees via block-adaptive, deal-adaptive, and the ""ideal but unattainable"" oracle-based fee algorithm, utilizing all data available to arbitrageurs to mitigate IL. We developed a simulation-based framework to compare these fee algorithms systematically. This framework replicates trading on a DEX, considering both informed and uninformed users and a psychological relative loss factor. Results show that adaptive algorithms outperform fixed-fee baselines in reducing IL while maintaining trading activity among uninformed users. Additionally, insights from oracle-based performance underscore the potential of dynamic fee strategies to lower IL, boost LP profitability, and enhance overall market efficiency.","分散化的交易所(DEXs)对于分散化金融(DeFi)至关重要,因为它们能够无中介地进行交易,然而,它们面临着诸如长期损失(IL)等挑战,流动性提供者(LPs)在流动性池内看到其资产价值的变化与外部相比不利。为了解决这些问题,我们提议对自动化市场制造者(AMM)使用的传统固定收费结构采用动态收费机制。我们的解决方案包括通过块适应、交易适应和“理想但无法实现”或以奇迹为基础的收费算法,利用仲裁家掌握的所有数据来减少IL。我们开发了一个模拟框架,系统地比较这些收费算法。这个框架复制了DEX交易,既考虑到知情和不知情的用户,又考虑到心理相对损失因素。结果显示,适应性算法在减少IL的同时,在维持不知情的用户之间的交易活动方面超过了固定收费基线。此外,基于“基于奇迹的业绩”的洞察了动态收费战略的潜力,以降低IL,提高LP的盈利能力,并提高总体市场效率。","Irina Lebedeva, Dmitrii Umnov, Yury Yanovich, Ignat Melnikov, George Ovchinnikov",2025-06-03T15:41:30Z,Dynamic Fee for Reducing Impermanent Loss in Decentralized Exchanges,Dynamische Gebühr für die Reduzierung von impermanenten Verlusten in dezentralisierten Börsen,减少分散化交易所永久损失的动态费用,http://arxiv.org/abs/2506.03001v1
52,"This paper studies density-based clustering of point sets. These methods use dense regions of points to detect clusters of arbitrary shapes. In particular, we study variants of density peaks clustering, a popular type of algorithm that has been shown to work well in practice. Our goal is to cluster large high-dimensional datasets, which are prevalent in practice. Prior solutions are either sequential, and cannot scale to large data, or are specialized for low-dimensional data.   This paper unifies the different variants of density peaks clustering into a single framework, PECANN, by abstracting out several key steps common to this class of algorithms. One such key step is to find nearest neighbors that satisfy a predicate function, and one of the main contributions of this paper is an efficient way to do this predicate search using graph-based approximate nearest neighbor search (ANNS). To provide ample parallelism, we propose a doubling search technique that enables points to find an approximate nearest neighbor satisfying the predicate in a small number of rounds. Our technique can be applied to many existing graph-based ANNS algorithms, which can all be plugged into PECANN.   We implement five clustering algorithms with PECANN and evaluate them on synthetic and real-world datasets with up to 1.28 million points and up to 1024 dimensions on a 30-core machine with two-way hyper-threading. Compared to the state-of-the-art FASTDP algorithm for high-dimensional density peaks clustering, which is sequential, our best algorithm is 45x-734x faster while achieving competitive ARI scores. Compared to the state-of-the-art parallel DPC-based algorithm, which is optimized for low dimensions, we show that PECANN is two orders of magnitude faster. As far as we know, our work is the first to evaluate DPC variants on large high-dimensional real-world image and text embedding datasets.","本文研究基于密度的点群集。 这些方法将密度峰值的不同变量集中成一个单一的框架, PECANN , 抽取这类算法共有的若干关键步骤。 特别是, 我们研究密度峰值群群群群的变异, 这是一种在实践中显示效果良好的流行算法。 我们的目标是将大型高维数据集集成, 这是实践中普遍存在的。 之前的解决方案要么是相继的, 无法缩放到大数据, 要么是低维数据。 本文将密度峰值峰值峰值的各种不同变异组合成一个单一的框架, PECANNNN, 抽取这类算法中常见的一些关键步骤。 其中一个关键步骤是找到最接近的近端端峰值群群群群, 满足上游功能的功能, 以及本文的主要贡献之一是找到最近端端点, 使用基于图表的直径直流值的直流值直径直径直径直径直径直径直的直径直径直的直径直径直行的直径直径直径直径直径直径直行的直径直径直径直径直径直径直行算算算算算。","Shangdi Yu, Joshua Engels, Yihao Huang, Julian Shun",2025-06-03T15:39:34Z,PECANN: Parallel Efficient Clustering with Graph-Based Approximate   Nearest Neighbor Search,PECANN: Paralleles effizientes Clustering mit grafisch naher Nachbarschaftssuche,PECANN: 与以图表为基础的近邻近近近近搜索平行高效集群,http://arxiv.org/abs/2312.03940v3
53,"In this paper, we propose an edge-assisted split federated learning framework to facilitate large language model (LLM) fine-tuning on heterogeneous mobile devices while alleviating memory pressures on both mobile devices and the edge server. Specifically, mobile devices perform low-rank adaptation (LoRA) fine-tuning on only a subset of lower layers of the pre-trained LLM, tailored to their individual capacities. On the server, a full LLM is maintained, and the corresponding LoRA modules are selectively fine-tuned in a sequential manner for each device. To further enhance training efficiency, we propose a server-side training scheduling method that optimizes the processing order of devices for accelerating fine-tuning. Extensive experiments demonstrate that compared to the baselines, our scheme can reduce 79\% memory footprint and 6\% training time while achieving comparable performance.","在本文中,我们提议了一个边际辅助的分联式学习框架,以便利对多种移动设备进行大型语言模型的微调,同时减轻移动设备和边缘服务器的记忆压力。具体地说,移动设备只对经过预先培训的低层进行低层次的微调(LORA),这只适合他们的个人能力。在服务器上,保留了一个完整的LLM,对相应的LORA模块按顺序对每个设备进行有选择的微调。为了进一步提高培训效率,我们提议了一个服务器侧培训时间安排方法,优化设备处理顺序,以加快微调。广泛的实验表明,与基线相比,我们的计划可以减少79的记忆足迹,6的培训时间,同时实现类似的业绩。","Xiaopei Chen, Liang Li, Fei Ji, Wen Wu",2025-06-03T14:39:56Z,Memory-Efficient Split Federated Learning for LLM Fine-Tuning on   Heterogeneous Mobile Devices,Speichereffizientes Split-Federated-Learning für LLM-Fine-Tuning auf heterogenen mobilen Geräten,用于不同差异移动设备LLM精美应用的记忆-有效分裂联邦学习,http://arxiv.org/abs/2506.02940v1
54,"Federated Learning (FL) is a learning mechanism that falls under the distributed training umbrella, which collaboratively trains a shared global model without disclosing the raw data from different clients. This paper presents an extensive survey on the impact of partial client participation in federated learning. While much of the existing research focuses on addressing issues such as generalization, robustness, and fairness caused by data heterogeneity under the assumption of full client participation, limited attention has been given to the practical and theoretical challenges arising from partial client participation, which is common in real-world scenarios. This survey provides an in-depth review of existing FL methods designed to cope with partial client participation. We offer a comprehensive analysis supported by theoretical insights and empirical findings, along with a structured categorization of these methods, highlighting their respective advantages and disadvantages.","联邦学习组织(FL)是一个学习机制,属于分布式培训伞,它合作培训一个共同的全球模式,但不披露不同客户的原始数据,本文对部分客户参与联合会学习的影响进行了广泛调查,虽然现有研究大多侧重于解决在假设客户充分参与的情况下数据不均造成的普遍性、稳健性和公正性等问题,但对部分客户参与所带来的实际和理论挑战重视有限,这在现实世界的情景中是常见的。这一调查深入审查了旨在应对部分客户参与的现有FL方法。我们根据理论见解和经验调查结果提供了全面分析,同时对这些方法进行了结构化分类,突出了各自的利弊。","Mrinmay Sen, Shruti Aparna, Rohit Agarwal, Chalavadi Krishna Mohan",2025-06-03T13:52:27Z,Overcoming Challenges of Partial Client Participation in Federated   Learning : A Comprehensive Review,Herausforderungen der Teilkundenbeteiligung am Föderierten Lernen überwinden : Ein umfassender Überblick,克服部分客户参与联邦学习的挑战:全面审查,http://arxiv.org/abs/2506.02887v1
55,"Asynchronous Stochastic Gradient Descent (Asynchronous SGD) is a cornerstone method for parallelizing learning in distributed machine learning. However, its performance suffers under arbitrarily heterogeneous computation times across workers, leading to suboptimal time complexity and inefficiency as the number of workers scales. While several Asynchronous SGD variants have been proposed, recent findings by Tyurin & Richt\'arik (NeurIPS 2023) reveal that none achieve optimal time complexity, leaving a significant gap in the literature. In this paper, we propose Ringmaster ASGD, a novel Asynchronous SGD method designed to address these limitations and tame the inherent challenges of Asynchronous SGD. We establish, through rigorous theoretical analysis, that Ringmaster ASGD achieves optimal time complexity under arbitrarily heterogeneous and dynamically fluctuating worker computation times. This makes it the first Asynchronous SGD method to meet the theoretical lower bounds for time complexity in such scenarios.","分散式机器学习中平行学习的一个基石方法,是分散式机器学习中平行学习的一种基本方法,但是,其绩效在工人的任意不同计算时间段中受到损害,导致工人人数规模中的时间复杂性和效率低下。虽然提出了若干非同步式 SGD变体,但Tyurin & Richt\'arik(NeurIPS 2023)最近的调查结果显示,没有取得最佳的时间复杂性,在文献中留下很大的空白。在本文件中,我们建议Ringmaster ASGD,这是旨在解决这些限制和抑制Asynchronous SGD固有挑战的新型的ASGD方法。我们通过严格的理论分析确定,Ringmaster ASGD在任意的多元性和动态波动式工人计算时间段内实现了最佳的时间复杂性。这使得Asynchronous SGD是第一个在这种情景中满足理论上较低时间复杂性的理论界限的Asynchronous SGD方法。","Artavazd Maranjyan, Alexander Tyurin, Peter Richtárik",2025-06-03T13:26:09Z,Ringmaster ASGD: The First Asynchronous SGD with Optimal Time Complexity,Ringmaster ASGD: Das erste asynchrone SGD mit optimaler Zeitkomplexität,ASGD:第一个具有最佳时间复杂性的同步 SGD,http://arxiv.org/abs/2501.16168v3
56,"Major domains such as logistics, healthcare, and smart cities increasingly rely on sensor technologies and distributed infrastructures to monitor complex processes in real time. These developments are transforming the data landscape from discrete, structured records stored in centralized systems to continuous, fine-grained, and heterogeneous event streams collected across distributed environments. As a result, traditional process mining techniques, which assume centralized event logs from enterprise systems, are no longer sufficient. In this paper, we discuss the conceptual and methodological foundations for this emerging field. We identify three key shifts: from offline to online analysis, from centralized to distributed computing, and from event logs to sensor data. These shifts challenge traditional assumptions about process data and call for new approaches that integrate infrastructure, data, and user perspectives. To this end, we define a research agenda that addresses six interconnected fields, each spanning multiple system dimensions. We advocate a principled methodology grounded in algorithm engineering, combining formal modeling with empirical evaluation. This approach enables the development of scalable, privacy-aware, and user-centric process mining techniques suitable for distributed environments. Our synthesis provides a roadmap for advancing process mining beyond its classical setting, toward a more responsive and decentralized paradigm of process intelligence.","物流、医疗保健和智能城市等主要领域日益依赖传感器技术和分布式基础设施来实时监测复杂进程。这些发展正在将数据景观从集中系统中储存的离散、结构化记录转变为分布式环境所收集的连续、细微和多式事件流。因此,传统的过程采矿技术,即企业系统中央事件日志,已经不够充分。我们在本文件中讨论了这个新兴领域的概念和方法基础。我们确定了三个关键转变:从离线分析到在线分析,从集中计算到分布式计算,以及从事件日志到传感器数据。这些变化挑战了传统的过程数据假设,要求采用新的方法,将基础设施、数据和用户观点结合起来。为此,我们确定了一个研究议程,涉及六个相互关联的领域,每个领域都跨越多个系统层面。我们主张以算法工程为基础的原则方法,将正式模型与经验评估结合起来。这一方法有助于开发适合分布式环境的可扩展、隐私意识和以用户为中心的过程采矿技术。我们的综合为超越其典型环境的推进进程提供了一个路线图,转向更反应性和分散式的情报工作。","Maximilian Weisenseel, Julia Andersen, Samira Akili, Christian Imenkamp, Hendrik Reiter, Christoffer Rubensson, Wilhelm Hasselbring, Olaf Landsiedel, Xixi Lu, Jan Mendling, Florian Tschorsch, Matthias Weidlich, Agnes Koschmider",2025-06-03T12:59:36Z,Process Mining on Distributed Data Sources,Prozessbergbau auf verteilten Datenquellen,关于分配数据来源的开采,http://arxiv.org/abs/2506.02830v1
57,"We propose a novel, lightweight, and physically inspired approach to modeling the dynamics of parallel distributed-memory programs. Inspired by the Kuramoto model, we represent MPI processes as coupled oscillators with topology-aware interactions, custom coupling potentials, and stochastic noise. The resulting system of nonlinear ordinary differential equations opens a path to modeling key performance phenomena of parallel programs, including synchronization, delay propagation and decay, bottlenecks, and self-desynchronization.   This paper introduces interaction potentials to describe memory- and compute-bound workloads and employs multiple quantitative metrics -- such as an order parameter, synchronization entropy, phase gradients, and phase differences -- to evaluate phase coherence and disruption. We also investigate the role of local noise and show that moderate noise can accelerate resynchronization in scalable applications. Our simulations align qualitatively with MPI trace data, showing the potential of physics-informed abstractions to predict performance patterns, which offers a new perspective for performance modeling and software-hardware co-design in parallel computing.","我们提出了一个新型、轻量级和体质启发性的方法来建模平行分布式模拟程序动态。在仓本模型的启发下,我们把MPI进程作为与表层-意识互动、定制组合潜力和随机噪音相结合的振荡器来代表。由此产生的非线性普通差异方程式系统为模拟平行程序的关键性能现象开辟了道路,包括同步、延迟传播和衰变、瓶颈和自发同步。本文件介绍了描述内存和计算受计算工作量的交互潜力,并采用多个量化指标 -- -- 如订单参数、同步酶、阶段梯度和阶段差异 -- -- 来评估阶段一致性和破坏性。我们还调查了当地噪音的作用,并表明中度噪音可以加速可缩放应用中的静脉交。我们的模拟与MPI跟踪数据质量一致,展示了物理学知情的抽象数据预测性能模式的潜力,这为平行计算中的性能建模和软件硬件联合设计提供了新的视角。","Ayesha Afzal, Georg Hager, Gerhard Wellen",2025-06-03T12:15:49Z,Exploring metrics for analyzing dynamic behavior in MPI programs via a   coupled-oscillator model,Untersuchung von Metriken zur Analyse des dynamischen Verhaltens in MPI-Programmen über ein gekoppeltes Oszillator-Modell,探索通过混合振动模型模型分析MPI程序动态行为的衡量标准,http://arxiv.org/abs/2506.02792v1
58,"Hybrid parallelism techniques are essential for efficiently training large language models (LLMs). Nevertheless, current automatic parallel planning frameworks often overlook the simultaneous consideration of node heterogeneity and dynamic network topology changes, limiting their effectiveness in practical applications. In this paper, we address these limitations by modeling heterogeneous nodes within dynamically changing network environments and leveraging simulation-based strategies to determine optimal parallel configurations. Our approach enables fine-grained workload allocation tailored for heterogeneous nodes and complex network scenarios, achieving performance competitive with state-of-the-art methods under regular and stable network conditions. Additionally, we introduce a strategy pruning technique to rapidly discard infeasible parallel configurations, substantially reducing the search space and accelerating the search process through parallel execution within the simulator. Preliminary evaluations confirm that our method notably enhances training performance on heterogeneous nodes and demonstrates improved adaptability in complex, dynamic scenarios such as cloud computing environments.","然而,目前的自动平行规划框架往往忽视同时考虑节点差异和动态网络地形变化,限制了其实际应用的有效性。在本文件中,我们通过在动态变化的网络环境中建模多种节点,并利用模拟战略来确定最佳平行配置,来解决这些局限性。我们的方法使得能够根据不同节点和复杂网络情景进行细微的分工,在正常和稳定的网络条件下实现与最先进方法的性能竞争。此外,我们引入了快速丢弃不可行的平行配置的战略,大大减少搜索空间,并通过模拟器的平行执行加快搜索进程。初步评估证实,我们的方法特别加强了对不同节点的培训,并展示了在云计算环境等复杂、动态情况下的适应性。","Ruilong Wu, Xinjiao Li, Yisu Wang, Xinyu Chen, Dirk Kutscher",2025-06-03T12:14:17Z,Rethinking Dynamic Networks and Heterogeneous Computing with Automatic   Parallelization,Dynamische Netzwerke und heterogenes Rechnen mit automatischer Parallelisierung neu denken,重新思考动态网络和具有自动平行化的多样化计算,http://arxiv.org/abs/2506.02787v1
59,"The rise of AI and the economic dominance of cloud computing have created a new nexus of innovation for high performance computing (HPC), which has a long history of driving scientific discovery. In addition to performance needs, scientific workflows increasingly demand capabilities of cloud environments: portability, reproducibility, dynamism, and automation. As converged cloud environments emerge, there is growing need to study their fit for HPC use cases. Here we present a cross-platform usability study that assesses 11 different HPC proxy applications and benchmarks across three clouds (Microsoft Azure, Amazon Web Services, and Google Cloud), six environments, and two compute configurations (CPU and GPU) against on-premises HPC clusters at a major center. We perform scaling tests of applications in all environments up to 28,672 CPUs and 256 GPUs. We present methodology and results to guide future study and provide a foundation to define best practices for running HPC workloads in cloud.","AI的崛起和云计算在经济上的主导地位为高性能计算(HPC)创造了新的创新关系,高性能计算(HPC)有很长的历史驱动科学发现。除了绩效需求外,科学工作流程对云层环境的需求也越来越大:可移动性、可复制性、活力和自动化。随着云层环境的趋同,越来越需要研究它们是否适合HPC使用案例。在这里,我们提出了一项跨平台可用性研究,评估了高PC在三个云层(微软Azure、亚马逊网络服务和谷歌云)、六个环境的11种不同的代用应用和基准,以及两个主要中心对地基HPC集群的计算配置(CPU和GPU)。我们对所有环境中的应用进行了测试,测试达到28,672个CPU和256个GPUs。我们提出了指导未来研究的方法和结果,并为确定云层管理HPC工作量的最佳做法提供了一个基础。","Vanessa Sochat, Daniel Milroy, Abhik Sarkar, Aniruddha Marathe",2025-06-03T10:05:30Z,Usability Evaluation of Cloud for HPC Applications,Usability Evaluation von Cloud für HPC-Anwendungen,高高常委会应用云云的可用性评价,http://arxiv.org/abs/2506.02709v1
60,"Serving large language models (LLMs) is important for cloud providers, and caching intermediate results (KV\$) after processing each request substantially improves serving throughput and latency. However, there is limited understanding of how LLM serving benefits from KV\$ caching, where system design decisions like cache eviction policies are highly workload-dependent. In this paper, we present the first systematic characterization of the KV\$ workload patterns from one of the leading LLM service providers. We draw observations that were not covered by previous studies focusing on synthetic workloads, including: KV\$ reuses are skewed across requests, where reuses between single-turn requests are equally important as multi-turn requests; the reuse time and probability are diverse considering all requests, but for a specific request category, the pattern tends to be predictable; and the overall cache size required for an ideal cache hit ratio is moderate. Based on the characterization, we further propose a workload-aware cache eviction policy that improves the serving performance under real-world traces, especially with limited cache capacity.","使用大型语言模型(LLMs)对于云端提供者十分重要,在处理每项请求后,缓存中间结果(KV\$)对于云端提供者十分重要,但对于LLM服务如何从KV\$缓存中受益,了解有限,因为缓存驱逐政策等系统设计决定高度依赖工作量。在本文中,我们从一个主要的LLM服务提供者对KV\$工作量模式的首次系统描述中,得出了以往侧重于合成工作量的研究所没有涉及的意见,包括:KV\$再利用在各种请求中被扭曲,其中单点再利用与多点请求同样重要;再利用时间和概率各不相同,考虑到所有请求,但具体的请求类别,模式往往可以预测;理想缓存打击比率所需的总体缓存规模是适度的。根据特征,我们进一步提议一项工作量-觉缓存驱逐政策,在现实世界痕迹下改进服务绩效,特别是缓存能力有限。","Jiahao Wang, Jinbo Han, Xingda Wei, Sijie Shen, Dingyan Zhang, Chenguang Fang, Rong Chen, Wenyuan Yu, Haibo Chen",2025-06-03T08:51:38Z,KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache   at a Large Cloud Provider,KVCache Cache in der Wildnis: KVCache Cache bei einem großen Cloud-Anbieter charakterisieren und optimieren,KVcache 野生缓存: 大云提供方的 KVcache 缓存的特性和优化 KVcache 缓存,http://arxiv.org/abs/2506.02634v1
61,"Scientific workflows facilitate the automation of data analysis, and are used to process increasing amounts of data. Therefore, they tend to be resource-intensive and long-running, leading to significant energy consumption and carbon emissions. With ever-increasing emissions from the ICT sector, it is crucial to quantify and understand the carbon footprint of scientific workflows. However, existing tooling requires significant effort from users - such as setting up power monitoring before executing workloads, or translating monitored metrics into the carbon footprints post-execution. In this paper, we introduce a system to estimate the carbon footprint of Nextflow scientific workflows that enables post-hoc estimation based on existing workflow traces, power models for computational resources utilised, and carbon intensity data aligned with the execution time. We discuss our automated power modelling approach, and compare it with commonly used estimation methodologies. Furthermore, we exemplify several potential use cases and evaluate our energy consumption estimation approach, finding its estimation error to be between 3.9-10.3%, outperforming both baseline methodologies.","科学工作流程有助于数据分析自动化,并被用于处理越来越多的数据。因此,这些工作流程往往是资源密集型和长期的,导致大量能源消耗和碳排放。随着信通技术部门的排放量不断增加,对科学工作流程的碳足迹进行量化和理解至关重要。然而,现有工具需要用户做出巨大努力,如在执行工作量之前进行电力监测,或将监测到的测量数据转化为碳足迹后的执行。在本文件中,我们引入了一个估算下流科学工作流程的碳足迹的系统,以便根据现有工作流程痕迹、使用计算资源的电动模型和与执行时间相一致的碳密度数据进行热后估算。我们讨论我们的自动化电动建模方法,并将其与常用的估计方法进行比较。此外,我们举例说明了几个潜在的使用案例,并评估了我们的能源消费估算方法,发现其估算误差介于3.9-10.3%之间,优于两种基线方法。","Kathleen West, Magnus Reid, Yehia Elkhatib, Lauritz Thamsen",2025-06-03T08:21:06Z,Ichnos: A Carbon Footprint Estimator for Scientific Workflows,Ichnos: Ein Kohlenstoff-Fußabdruck-Schätzer für wissenschaftliche Workflows,Ichnos: 用于科学工作流程的碳足印模拟器,http://arxiv.org/abs/2411.12456v2
62,"Efficient utilization of computing resources in a Kubernetes cluster is often constrained by the uneven distribution of pods with similar usage patterns. This paper presents a novel scheduling strategy designed to optimize the distributedness of Kubernetes resources based on their usage magnitude and patterns across CPU, memory, network, and storage. By categorizing resource usage into labels such as ""cpu high spike"" or ""memory medium always,"" and applying these to deployed pods, the system calculates the variance or distributedness factor of similar resource types across cluster nodes. A lower variance indicates a more balanced distribution. The Kubernetes scheduler is enhanced to consider this factor during scheduling decisions, placing new pods on nodes that minimize resource clustering. Furthermore, the approach supports redistribution of existing pods through simulated scheduling to improve balance. This method is adaptable at the cluster, namespace, or application level and is integrated within the standard Kubernetes scheduler, providing a scalable, label-driven mechanism to improve overall resource efficiency in cloud-native environments.","Kubernetes 群集中的计算资源的有效利用往往受到使用模式相似的播客舱分布不均的限制。 本文介绍了一种新的调度战略, 目的是根据库伯涅斯资源在CPU、 内存、 网络和存储中的使用规模和模式, 优化库伯涅斯资源的分布。 将资源使用分类到标签上, 如“ 高刺” 或“ 模擬介质 ” , 并将其应用于部署的播客舱, 系统计算出各组节点中类似资源类型的差异或分布系数。 更低的差异显示分布更加均衡。 库伯涅斯 调度器在列表决定时会考虑这一因素, 将新播客舱放在节点上, 以最大限度地减少资源组合。 此外, 该方法通过模拟调度支持现有播客舱的再分配, 以改善平衡性。 这种方法在集束、 命名空间或应用级别上适应, 并在标准库伯涅茨排程中整合, 提供一个可缩放的标签驱动机制, 以提高云层环境中的总体资源效率 。","Paritosh Ranjan, Surajit Majumder, Prodip Roy, Bhuban Padhan",2025-06-03T08:02:21Z,Distributedness based scheduling,Verteilbarkeitsbasierte Terminplanung,以分配分配为基础的日程安排,http://arxiv.org/abs/2506.02581v1
63,"Kubernetes, a notably complex and distributed system, utilizes an array of controllers to uphold cluster management logic through state reconciliation. Nevertheless, maintaining state consistency presents significant challenges due to unexpected failures, network disruptions, and asynchronous issues, especially within dynamic cloud environments. These challenges result in operational disruptions and economic losses, underscoring the necessity for robust root cause analysis (RCA) to enhance Kubernetes reliability. The development of large language models (LLMs) presents a promising direction for RCA. However, existing methodologies encounter several obstacles, including the diverse and evolving nature of Kubernetes incidents, the intricate context of incidents, and the polymorphic nature of these incidents. In this paper, we introduce SynergyRCA, an innovative tool that leverages LLMs with retrieval augmentation from graph databases and enhancement with expert prompts. SynergyRCA constructs a StateGraph to capture spatial and temporal relationships and utilizes a MetaGraph to outline entity connections. Upon the occurrence of an incident, an LLM predicts the most pertinent resource, and SynergyRCA queries the MetaGraph and StateGraph to deliver context-specific insights for RCA. We evaluate SynergyRCA using datasets from two production Kubernetes clusters, highlighting its capacity to identify numerous root causes, including novel ones, with high efficiency and precision. SynergyRCA demonstrates the ability to identify root causes in an average time of about two minutes and achieves an impressive precision of approximately 0.90.","Kubernetes是一个显著的复杂和分布式系统,它利用一系列控制器来通过国家和解来维护集束管理逻辑。然而,由于出乎意料的失败、网络中断和不同步问题,特别是在动态云层环境中,保持国家一致性提出了重大挑战。这些挑战导致业务中断和经济损失,突出表明必须进行强有力的根本原因分析(RCA)以提高Kubernetes的可靠性。开发大型语言模型(LLLMs)为RCA提供了一个充满希望的方向。然而,现有方法遇到了若干障碍,包括Kubernetes事件的多样性和演变性质、事件的复杂背景以及这些事件的多变性质。在本文件中,我们引入了SymergyRCA这一创新工具,它利用图表数据库的检索和专家提示来利用LLMSMS进行回升,以捕捉到空间和时间关系,并利用MetaGraph来概述实体关系。一旦发生事件,LLMM预测最相关的资源,SymergyRCA询问MGraph和州级事件多形态特性,从而提供具体背景的直观数据。","Yong Xiang, Charley Peter Chen, Liyi Zeng, Wei Yin, Xin Liu, Hu Li, Wei Xu",2025-06-03T06:09:13Z,Simplifying Root Cause Analysis in Kubernetes with StateGraph and LLM,Vereinfachende Ursachenanalyse in Kubernetes mit StateGraph und LLM,利用国家格普和法学硕士简化Kubernetes公司的根本原因分析,http://arxiv.org/abs/2506.02490v1
64,"As core counts and heterogeneity rise in HPC, traditional hybrid programming models face challenges in managing distributed GPU memory and ensuring portability. This paper presents DiOMP, a distributed OpenMP framework that unifies OpenMP target offloading with the Partitioned Global Address Space (PGAS) model. Built atop LLVM/OpenMP and using GASNet-EX or GPI-2 for communication, DiOMP transparently handles global memory, supporting both symmetric and asymmetric GPU allocations. It leverages OMPCCL, a portable collective communication layer compatible with vendor libraries. DiOMP simplifies programming by abstracting device memory and communication, achieving superior scalability and programmability over traditional approaches. Evaluations on NVIDIA A100, Grace Hopper, and AMD MI250X show improved performance in micro-benchmarks and applications like matrix multiplication and Minimod, highlighting DiOMP's potential for scalable, portable, and efficient heterogeneous computing.","作为HPC的核心计数和异质性上升,传统的混合编程模式在管理分布式GPU内存和确保可移动性方面面临挑战。本文介绍了DiOMP,这是一个分布式的开放MP框架,它使OpenMP目标与分隔式全球地址空间(PGAS)模式统一卸载。在Atop LLLVM/OpenMP 上建起,并使用 GASNet-EX 或 GPI-2 进行通信,DiOMP透明地处理全球记忆,支持对称和不对称的GPU的分配。它利用了OMPCCL,这是一个与供应商图书馆兼容的便携式集体通信层。DimOMP通过抽象设备内存和通信简化编程,实现了优于传统方法的可缩放性和可编程性。关于NVDIDIA A100、Grace Hopper和AMMMI250X的评估显示,在微信箱倍增缩和Minimod等应用方面业绩有所改善,突出DOMP在可缩缩放、可移动和高效混合计算方面的潜力。","Baodi Shan, Mauricio Arayr-Polo, Barbara Chapman",2025-06-03T05:57:06Z,DiOMP-Offloading: Toward Portable Distributed Heterogeneous OpenMP,DiOMP-Offloading: Auf dem Weg zu portablen verteilten Heterogenen OpenMP,DiOMP-卸载: 迈向可移植分布式异异质 OpenMP,http://arxiv.org/abs/2506.02486v1
65,"This paper presents a comprehensive analysis of historical data across two popular blockchain networks: Ethereum and Solana. Our study focuses on two key aspects: transaction conflicts and the maximum theoretical parallelism within historical blocks. We aim to quantify the degree of transaction parallelism and assess how effectively it can be exploited by systematically examining block-level characteristics, both within individual blocks and across different historical periods. In particular, this study is the first of its kind to leverage historical transactional workloads to evaluate transactional conflict patterns. By offering a structured approach to analyzing these conflicts, our research provides valuable insights and an empirical basis for developing more efficient parallel execution techniques for smart contracts in the Ethereum and Solana virtual machines. Our empirical analysis reveals that historical Ethereum blocks frequently achieve high independence, over 50\% in more than 50\% of blocks, while Solana historical blocks contain longer conflict chains, comprising $\sim$59\% of the block size compared to $\sim$18\% in Ethereum, reflecting fundamentally different parallel execution dynamics.","本文件全面分析了两个广受欢迎的链条网络:Etheum和Sarana的历史数据。我们的研究侧重于两个关键方面:交易冲突和历史区块内最大的理论平行主义。我们的目标是量化交易平行主义的程度,并评估通过系统地审查各个区块内和不同历史时期的区块特性,可以如何有效地利用这种平行主义。特别是,这一研究是利用历史交易工作量来评价交易冲突模式的首个此类研究。我们的研究为分析这些冲突提供了结构化的方法,为开发Etheum和Solana虚拟机器中智能合同的更有效平行执行技术提供了宝贵的见解和经验基础。我们的经验分析表明,历史Eteum区块经常实现高度独立,50个以上的区块内有50个以上,而索拉纳历史区块内则包含更长的冲突链,由区块大小的$sim$59,与Etheum的$sim$18,反映基本不同的平行执行动态。","Parwat Singh Anjana, Srivatsan Ravi",2025-06-03T05:33:38Z,Empirical Analysis of Transaction Conflicts in Ethereum and Solana for   Parallel Execution,Empirische Analyse von Transaktionskonflikten in Ethereum und Solana zur parallelen Ausführung,Etheum和Solana的平行执行交易冲突经验分析,http://arxiv.org/abs/2505.05358v2
66,"Personalized federated learning (PFL) offers a solution to balancing personalization and generalization by conducting federated learning (FL) to guide personalized learning (PL). Little attention has been given to wireless PFL (WPFL), where privacy concerns arise. Performance fairness of PL models is another challenge resulting from communication bottlenecks in WPFL. This paper exploits quantization errors to enhance the privacy of WPFL and proposes a novel quantization-assisted Gaussian differential privacy (DP) mechanism. We analyze the convergence upper bounds of individual PL models by considering the impact of the mechanism (i.e., quantization errors and Gaussian DP noises) and imperfect communication channels on the FL of WPFL. By minimizing the maximum of the bounds, we design an optimal transmission scheduling strategy that yields min-max fairness for WPFL with OFDMA interfaces. This is achieved by revealing the nested structure of this problem to decouple it into subproblems solved sequentially for the client selection, channel allocation, and power control, and for the learning rates and PL-FL weighting coefficients. Experiments validate our analysis and demonstrate that our approach substantially outperforms alternative scheduling strategies by 87.08%, 16.21%, and 38.37% in accuracy, the maximum test loss of participating clients, and fairness (Jain's index), respectively.","38. 通过开展联合学习,引导个性化学习(PL),个人化、个人化学习(PFL)提供了平衡个性化和普遍性的解决方案。在出现隐私问题的地方,对无线PFL(WPFL)几乎没有重视。PL模型的性能公平性是WPFL中通信瓶颈的另一个挑战。本文利用量化错误来提高WPFL的隐私,并提议了一个创新的量化辅助高斯差异性隐私(DP)机制。我们通过考虑机制的影响(即量化错误和高斯DP的噪音)以及WPFL的不完善的通信渠道,分析个人PL模型的趋同性上限。通过最大限度地缩小界限,我们设计了最佳传输时间安排战略,使WPFL与DMA接口的最小值公平性。通过揭示这一问题的嵌套结构,将其分解为分解分解的子问题,通过客户选择、频道分配和权力控制,以及学习率和PLFLF的公平性能度,37的噪音。我们分别通过87 %参与率的计算方法,验证了我们16 %的计算损失率的计算。","Xiyu Zhao, Qimei Cui, Ziqiang Du, Weicai Li, Xi Yu, Wei Ni, Ji Zhang, Xiaofeng Tao, Ping Zhang",2025-06-03T04:13:07Z,"Enhancing Convergence, Privacy and Fairness for Wireless Personalized   Federated Learning: Quantization-Assisted Min-Max Fair Scheduling","Verbesserung von Konvergenz, Privatsphäre und Fairness für kabelloses personalisiertes Federated Learning: Quantization Assisted Min-Max Fair Scheduling",加强无线个性化联邦学习的融合、隐私和公平:量化-辅助的中马克斯公平日程安排,http://arxiv.org/abs/2506.02422v1
67,"Recent dimension-free communication frameworks in Federated Learning (FL), such as DeComFL, significantly reduce per-round communication by transmitting only scalars via zeroth-order stochastic gradient descent (ZO-SGD). This method is particularly advantageous for federated fine-tuning of Large Language Models (LLMs). Yet, the high variance in ZO gradient estimation typically leads to slow convergence. Although leveraging Hessian information is known to enhance optimization speed, integrating this into FL presents significant challenges. These include clients' restrictions on local data and the critical need to maintain the dimension-free communication property. To overcome this limitation, we first introduce a generalized scalar-only communication FL framework that decouples dimension-free communication from standard ZO-SGD, enabling the integration of more advanced optimization strategies. Building on this framework, we propose HiSo, a fast federated fine-tuning method via Hessian-informed zeroth-order optimization and Scalar-only communication. Specifically, it leverages global curvature information to accelerate convergence while preserving the same minimal communication cost per round. Theoretically, we establish convergence guarantees that are independent of the global Lipschitz constant, and further show that HiSo achieves faster rates when the global Hessian exhibits a low effective rank -- a common phenomenon in LLMs. Extensive experiments on benchmark datasets and LLM fine-tuning tasks confirm that HiSo significantly outperforms existing ZO-based FL methods in both convergence speed and communication efficiency.","联邦学习联合会(FL)最近的无维度通信框架,如DeComFLFL,通过零阶梯度梯度梯度下降(ZO-SGD),通过只通过零阶梯度梯度梯度下降(ZO-SGD)传递卡路里,显著减少全方位通信。这种方法对大语言模型(LLMS)进行联合微调特别有利。然而,ZO梯度估计差异很大,通常会导致缓慢趋同。尽管众所周知,利用Hesian信息提高优化速度,将Hesian信息纳入FLFL带来重大挑战,其中包括客户对当地数据的限制,以及维护无维度通信财产的迫切需要。为了克服这一限制,我们首先推出一个通用的、仅使用卡路里通信速度的卡路里通信框架,将无维度通信与标准ZO-SGD(LM)的无维度分解,从而能够整合更先进的优化战略。我们建议HISO,一个快速通化的微调的微调方法,通过Hesian知情零级优化的零级优化和只限通信。具体地利用全球曲线定位信息加速加速融合信息,同时保持最低通信成本成本成本。","Zhe Li, Bicheng Ying, Zidong Liu, Chaosheng Dong, Haibo Yang",2025-06-03T02:13:31Z,Reconciling Hessian-Informed Acceleration and Scalar-Only Communication   for Efficient Federated Zeroth-Order Fine-Tuning,Vereinbarkeit von hessisch-informierter Beschleunigung und Scalar-Only-Kommunikation für effizientes Federated Zeroth-Order Fine-Tuning,"统一黑森州一体化加速和斯卡拉-唯一通信,以达到节能的联邦零分级精调",http://arxiv.org/abs/2506.02370v1
68,"As one of the most well-studied cohesive subgraph models, the $k$-core is widely used to find graph nodes that are ``central'' or ``important'' in many applications, such as biological networks, social networks, ecological networks, and financial networks. For Decentralized Online Social Networks (DOSNs), where each vertex is a client as a single computing unit, distributed k-core decomposition algorithms have already been proposed. However, current distributed approaches fail to adequately protect privacy and security. In today's data-driven world, data privacy and security have attracted more and more attention, e.g., DOSNs are proposed to protect privacy by storing user information locally without using a single centralized server. In this work, we are the first to propose the secure version of the distributed $k$-core decomposition.","作为研究最周密的具有凝聚力的子图模型之一,美元核心被广泛用于寻找在生物网络、社交网络、生态网络和金融网络等许多应用中“中央”或“重要”的图形节点。对于分散式在线社会网络(DOSNs),每个顶点都是单一计算单位的客户,已经提出了分配式的k-核心分解算法。然而,目前分布式的方法未能充分保护隐私和安全。在今天的数据驱动世界中,数据隐私和安全吸引了越来越多的关注,例如,DOSNs建议通过不使用单一集中服务器就在当地储存用户信息来保护隐私。在这项工作中,我们首先提出分配的美元核心分解算法的安全版本。","Bin Guo, Emil Sekerinski, Lingyang Chu",2025-06-03T01:38:52Z,Federated k-Core Decomposition: A Secure Distributed Approach,Federated k-Core Zersetzung: Ein sicherer verteilter Ansatz,联邦k-核心分解:安全分配办法,http://arxiv.org/abs/2410.02544v2
69,"Federated Learning (FL) offers a promising framework for collaborative and privacy-preserving machine learning across distributed data sources. However, the substantial communication costs associated with FL significantly challenge its efficiency. Specifically, in each communication round, the communication costs scale linearly with the model's dimension, which presents a formidable obstacle, especially in large model scenarios. Despite various communication-efficient strategies, the intrinsic dimension-dependent communication cost remains a major bottleneck for current FL implementations. This paper proposes a novel dimension-free communication algorithm - DeComFL, which leverages the zeroth-order optimization techniques and reduces the communication cost from $\mathscr{O}(d)$ to $\mathscr{O}(1)$ by transmitting only a constant number of scalar values between clients and the server in each round, regardless of the dimension $d$ of the model parameters. Theoretically, in non-convex functions, we prove that our algorithm achieves state-of-the-art rates, which show a linear speedup of the number of clients and local steps under standard assumptions. With additional low effective rank assumption, we can further show the convergence rate is independent of the model dimension $d$ as well. Empirical evaluations, encompassing both classic deep learning training and large language model fine-tuning, demonstrate significant reductions in communication overhead. Notably, DeComFL achieves this by transmitting only around 1MB of data in total between the server and a client to fine-tune a model with billions of parameters. Our code is available at https://github.com/ZidongLiu/DeComFL.","联邦学习联盟(FL)为在分布式数据源中开展协作和保密的机器学习提供了一个充满希望的框架。然而,与FL相关的大量通信成本对其效率提出了巨大的挑战。具体地说,在每轮通信中,通信费用与模型的尺寸成线,这构成了巨大的障碍,特别是在大型模型情景中。尽管有各种通信效率战略,但基于层面的内在通信成本仍然是当前FL实施过程中的一个主要瓶颈。本文建议采用一个新的无层面通信算法(DeComFL),利用零级优化技术,降低通信费用,从$\mathscr{O}(d)美元到$\mathcr{O}(1)美元。具体地说,在每轮通信中,客户和服务器之间的卡路里值不变,无论模式参数的尺寸为$d美元。理论上,我们证明我们的算法只达到了标准假设下,客户数量和当地步骤的直线性超速速度。由于额外的低级别假设,我们可以在每轮的客户之间传递一个固定的卡路里标准,在1号上进一步展示了我们数据库/网站的大规模数据流流化的升级。","Zhe Li, Bicheng Ying, Zidong Liu, Chaosheng Dong, Haibo Yang",2025-06-02T23:58:41Z,Achieving Dimension-Free Communication in Federated Learning via   Zeroth-Order Optimization,Dimensionsfreie Kommunikation im Federated Learning durch Zeroth-Order-Optimierung erreichen,通过零分优化在联邦学习中实现多层次自由交流,http://arxiv.org/abs/2405.15861v5
70,"Epidemiologists and social scientists have used the Network Scale-Up Method (NSUM) for over thirty years to estimate the size of a hidden sub-population within a social network. This method involves querying a subset of network nodes about the number of their neighbours belonging to the hidden sub-population. In general, NSUM assumes that the social network topology and the hidden sub-population distribution are well-behaved; hence, the NSUM estimate is close to the actual value. However, bounds on NSUM estimation errors have not been analytically proven. This paper provides analytical bounds on the error incurred by the two most popular NSUM estimators. These bounds assume that the queried nodes accurately provide their degree and the number of neighbors belonging to the hidden population. Our key findings are twofold. First, we show that when an adversary designs the network and places the hidden sub-population, then the estimate can be a factor of $\Omega(\sqrt{n})$ off from the real value (in a network with $n$ nodes). Second, we also prove error bounds when the underlying network is randomly generated, showing that a small constant factor can be achieved with high probability using samples of logarithmic size $O(\log{n})$. We present improved analytical bounds for Erdos-Renyi and Scale-Free networks. Our theoretical analysis is supported by an extensive set of numerical experiments designed to determine the effect of the sample size on the accuracy of the estimates in both synthetic and real networks.","流行病学家和社会科学家三十多年来一直使用网络缩放方法(NSUM)来估计社交网络中隐藏子人口的规模。 这种方法包括询问一组网络节点, 了解属于隐藏子人口群的邻居人数。 一般来说, NSUM假设社交网络的表层和隐藏子人口分布状况良好; 因此, NSUM的估计接近实际值。 然而, 有关NSUM估计错误的界限没有经过分析证明。 本文提供了两个最受欢迎的NSUM估计者所出错误的分析界限。 这些界限假定, 询问的节点准确地提供了属于隐藏子人口群的邻居人数和程度。 我们的主要结论是双重的。 首先, 我们显示当一个对手设计网络时, 隐藏的子人口分布状况与实际价值接近; 然而, 该估计可能是一个从实际价值( 以美元为单位的网络 ) 的界限。 其次, 当基础网络准确度准确度准确度准确度准确度和属于我们当前数字比例分析的概率时, 我们还证明有误差值。 使用一个不变的标定的精确度 , 以我们当前数字 的精确度 标定的精确度 。","Sergio Díaz-Aranda, Juan Marcos Ramírez, Mohit Daga, Jaya Prakash Champati, José Aguilar, Rosa Elvira Lillo, Antonio Fernández Anta",2025-06-02T20:52:06Z,Error Bounds for the Network Scale-Up Method,Fehlergrenzen für die Netzwerk-Skalierungsmethode,网络缩放方法的误差环径,http://arxiv.org/abs/2407.10640v2
71,"The decentralized gradient descent (DGD) algorithm, and its sibling, diffusion, are workhorses in decentralized machine learning, distributed inference and estimation, and multi-agent coordination. We propose a novel, principled framework for the analysis of DGD and diffusion for strongly convex, smooth objectives, and arbitrary undirected topologies, using contraction mappings coupled with a result called the mean Hessian theorem (MHT). The use of these tools yields tight convergence bounds, both in the noise-free and noisy regimes. While these bounds are qualitatively similar to results found in the literature, our approach using contractions together with the MHT decouples the algorithm dynamics (how quickly the algorithm converges to its fixed point) from its asymptotic convergence properties (how far the fixed point is from the global optimum). This yields a simple, intuitive analysis that is accessible to a broader audience. Extensions are provided to multiple local gradient updates, time-varying step sizes, noisy gradients (stochastic DGD and diffusion), communication noise, and random topologies.","分散的梯度下沉算法(DGD)及其分布、扩散,是分散的机器学习、分布的推论和估计以及多剂协调中的工作马。我们提出了一个新的原则性框架,用于分析DGD及其扩散,以达到强烈的曲线、平稳的目标和任意的无方向的表层,使用缩进图,加上一个称为平均海珊定理(MHT)的结果。这些工具的使用在无噪音和吵闹的制度下都产生紧密的趋同界限。虽然这些界限在质量上与文献中发现的结果相似,但我们与MHT一道使用缩缩缩的方法,从它的亚性趋同性趋同特性(算法迅速汇合到其固定点)(固定点离全球最佳点远处)。这产生了一个简单、直观的分析,广大观众可以使用。推广到多个本地梯度更新、时间变换的步数、高度梯度梯度(蒸发式DGDD和扩散)、通讯噪音和随机的表层学。","Erik G. Larsson, Nicolo Michelusi",2025-06-02T19:50:30Z,Unified Analysis of Decentralized Gradient Descent: a Contraction   Mapping Framework,Unified Analysis of Decentralized Gradient Descent: a Contraction Mapping Framework,分散的梯层综合分析:收缩绘图框架,http://arxiv.org/abs/2503.14353v2
72,"Speculative decoding is widely adopted to reduce latency in large language model (LLM) inference by leveraging smaller draft models capable of handling diverse user tasks. However, emerging AI applications, such as LLM-based agents, present unique workload characteristics: instead of diverse independent requests, agentic frameworks typically submit repetitive inference requests, such as multi-agent pipelines performing similar subtasks or self-refinement loops iteratively enhancing outputs. These workloads result in long and highly predictable sequences, which current speculative decoding methods do not effectively exploit. To address this gap, we introduce \emph{SuffixDecoding}, a novel method that utilizes efficient suffix trees to cache long token sequences from prompts and previous outputs. By adaptively speculating more tokens when acceptance likelihood is high and fewer when it is low, SuffixDecoding effectively exploits opportunities for longer speculations while conserving computation when those opportunities are limited. Evaluations on agentic benchmarks, including SWE-Bench and Text-to-SQL, demonstrate that SuffixDecoding achieves speedups of up to 5.3$\times$, outperforming state-of-the-art methods -- 2.8$\times$ faster than model-based approaches like EAGLE-2/3 and 1.9$\times$ faster than model-free approaches such as Token Recycling. SuffixDecoding is open-sourced at https://github.com/snowflakedb/ArcticInference.","为降低大型语言模式(LLM)的延迟度,广泛采用投机性解码法,通过利用能够处理不同用户任务的小型模型草案,降低大语言模式(LLM)的延迟度。然而,新兴的AI应用程序,如LLM代理,呈现出独特的工作量特点:与不同的独立请求不同,代理框架通常提交重复的推理请求,如执行类似子任务或自我精炼环的多试管管道,迭接地提高产出。这些工作量导致长期和高度自由的序列,而当前投机性开解码方法没有有效地利用这些序列。为弥补这一差距,我们引入了\emph{SafixDecod},这是一种新颖的方法,利用高效的后缀树来从提示和先前的产出中存储长期的代号序列。在接受可能性高、低时缩略图则有效地利用延长投机的机会,同时在模型机会有限时保存计算结果。包括SWE-Bench和文本至SQL在内的代理基准评估,表明Sfreph{res-deflifleximal_deal_deal_deal-deal_deal-deal-de-lexmaxeal-le-de-de-de-de-de-levelupslations-lexxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx","Gabriele Oliaro, Zhihao Jia, Daniel Campos, Aurick Qiao",2025-06-02T19:27:12Z,SuffixDecoding: Extreme Speculative Decoding for Emerging AI   Applications,SuffixDecoding: Extreme spekulative Dekodierung für neu auftretende KI-Anwendungen,后缀值:新出现的AI型应用的极端投机代号,http://arxiv.org/abs/2411.04975v2
73,"We present ""Reciprocating Locks"", a novel mutual exclusion locking algorithm, targeting cache-coherent shared memory (CC), that enjoys a number of desirable properties. The doorway arrival phase and the release operation both run in constant-time. Waiting threads use local spinning and only a single waiting element is required per thread, regardless of the number of locks a thread might hold at a given time. While our lock does not provide strict FIFO admission, it bounds bypass and has strong anti-starvation properties. The lock is compact, space efficient, and has been intentionally designed to be readily usable in real-world general purpose computing environments such as the linux kernel, pthreads, or C++. We show the lock exhibits high throughput under contention and low latency in the uncontended case. The performance of Reciprocating Locks is competitive with and often better than the best state-of-the-art scalable spin locks.","我们展示了“反转锁”这一新颖的相互排斥锁定算法,针对缓存一致的共享记忆(CC),该算法具有一些可取的特性。 门到门阶段和释放操作都是在固定时间运行的。 等待线使用本地旋转, 只需要每条线有一个单独的等待元素, 不论线索在特定时间可能持有的锁数多少。 虽然我们的锁不提供严格的FIFFO 进入, 但它会绕开, 并具有很强的反星系特性。 锁定是紧凑的, 空间效率高, 并且被有意设计成可以随时用于现实世界通用的计算环境, 如 Linux 内核、 prthread 或 C++ 。 我们展示了在未连接的案例中的锁定显示高度过量和低耐久性。 重新定位锁的性能与最佳的状态可缩放的旋转锁有竞争力, 并且通常比最好的效果更好。","Dave Dice, Alex Kogan",2025-06-02T17:46:50Z,Reciprocating Locks,Umschaltschlösser,回收锁,http://arxiv.org/abs/2501.02380v8
74,"Smart contracts, the cornerstone of blockchain technology, enable secure, automated distributed execution. Given their role in handling large transaction volumes across clients, miners, and validators, exploring concurrency is critical. This includes concurrent transaction execution or validation within blocks, block processing across shards, and miner competition to select and persist transactions. Concurrency and parallelism are a double-edged sword: while they improve throughput, they also introduce risks like race conditions, non-determinism, and vulnerabilities such as deadlock and livelock.   This paper presents the first survey of concurrency in smart contracts, offering a systematic literature review organized into key dimensions. First, it establishes a taxonomy of concurrency levels in blockchain systems and discusses proposed solutions for future adoption. Second, it examines vulnerabilities, attacks, and countermeasures in concurrent operations, emphasizing the need for correctness and security. Crucially, we reveal a flawed concurrency assumption in a major research category, which has led to widespread misinterpretation. This work aims to correct that and guide future research toward more accurate models. Finally, we identify gaps in each category to outline future research directions and support blockchain's advancement.","智能合同是链式技术的基石,能够安全、自动地执行。鉴于它们在处理客户、矿工和验证人之间的大量交易中所起的作用,探索货币交易至关重要。这包括同时在区块内执行或验证交易,在碎片之间进行整块加工,采矿者竞争选择和持续交易。货币和平行主义是一把双刃剑:在它们改进吞吐的同时,它们还带来种族条件、非确定性以及僵局和活锁等脆弱性等风险。本文介绍对智能合同中同值货币的第一次调查,提供按关键方面分类的系统文献审查。首先,它确定链式系统同值的分类,并讨论未来采用的拟议解决办法。第二,它审查了同时作业的脆弱性、攻击和对应措施,强调正确和安全的必要性。关键是,我们发现一个主要研究类别中的同值假设存在缺陷,导致广泛的误解。这项工作旨在纠正这一假设,并指导今后的研究走向更准确的模式。最后,我们找出了每个类别中的差距,以概述未来研究方向,支持链式发展。","Atefeh Zareh Chahoki, Maurice Herlihy, Marco Roveri",2025-06-02T17:13:03Z,SoK: Concurrency in Blockchain -- A Systematic Literature Review and the   Unveiling of a Misconception,SoK: Concurrency in Blockchain -- Ein systematischer Literaturbericht und die Enthüllung eines Missverständnisses,SoK: 链链中的货币 -- -- 系统文学评论和误解的融合,http://arxiv.org/abs/2506.01885v1
75,"Multimodal large language models (MLLMs) have extended the success of large language models (LLMs) to multiple data types, such as image, text and audio, achieving significant performance in various domains, including multimodal translation, visual question answering and content generation. Nonetheless, existing systems are inefficient to train MLLMs due to substantial GPU bubbles caused by the heterogeneous modality models and complex data dependencies in 3D parallelism. This paper proposes Optimus, a distributed MLLM training system that reduces end-to-end MLLM training time. Optimus is based on our principled analysis that scheduling the encoder computation within the LLM bubbles can reduce bubbles in MLLM training. To make scheduling encoder computation possible for all GPUs, Optimus searches the separate parallel plans for encoder and LLM, and adopts a bubble scheduling algorithm to enable exploiting LLM bubbles without breaking the original data dependencies in the MLLM model architecture. We further decompose encoder layer computation into a series of kernels, and analyze the common bubble pattern of 3D parallelism to carefully optimize the sub-millisecond bubble scheduling, minimizing the overall training time. Our experiments in a production cluster show that Optimus accelerates MLLM training by 20.5%-21.3% with ViT-22B and GPT-175B model over 3072 GPUs compared to baselines.","大型多模式语言模型(MLLLMM)已经将大型语言模型(LLMM)的成功扩大到多种数据类型,如图像、文本和音频,在包括多式联运、视觉问答和内容生成在内的不同领域取得显著成绩。然而,由于3D平行模式中混合模式模型和复杂的数据依赖性造成的巨大的 GPU 泡沫,现有系统在培训MLLM 方面效率低下,因为3D平行模式模型和复杂的数据依赖性造成了巨大的 GPU 泡沫。本文提出Optimus,一个分布式MLLM培训系统,一个分布式的MLLM培训系统,减少MLM培训时间的尾端至端。Optimus基于我们的原则分析,即将编码器计算在LLM泡沫泡沫泡沫中进行安排,可以减少MLLM培训的泡沫泡沫泡沫泡泡泡。 3 将G72-B平行模式的通用泡泡泡模型与GMM-22的基底基模模30 优化整个BMLVILA的基级的基数,将GM- 30M-CLILLLLLA 的底的基模模缩缩缩缩成。","Weiqi Feng, Yangrui Chen, Shaoyu Wang, Yanghua Peng, Haibin Lin, Minlan Yu",2025-06-02T17:02:34Z,Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble   Exploitation,Optimus: Beschleunigen von großräumigen Multi-Modal LLM-Trainings durch Bubble Exploitation,顶柱:通过泡沫开采加速大型多式多模式LM培训,http://arxiv.org/abs/2408.03505v2
76,"The local circuitry of the mammalian brain is a focus of the search for generic computational principles because it is largely conserved across species and modalities. In 2014 a model was proposed representing all neurons and synapses of the stereotypical cortical microcircuit below $1\,\text{mm}^2$ of brain surface. The model reproduces fundamental features of brain activity but its impact remained limited because of its computational demands. For theory and simulation, however, the model was a breakthrough because it removes uncertainties of downscaling, and larger models are less densely connected. This sparked a race in the neuromorphic computing community and the model became a de facto standard benchmark. Within a few years real-time performance was reached and surpassed at significantly reduced energy consumption. We review how the computational challenge was tackled by different simulation technologies and derive guidelines for the next generation of benchmarks and other domains of science.","哺乳动物大脑的局部电路是寻找通用计算原则的一个焦点,因为它在物种和模式上都得到了很大程度的保护。 2014年,提出了一个模型,代表了大脑表面1美元以下的陈规定型皮质微电路的所有神经元和突触。模型复制了大脑活动的基本特征,但由于其计算需求,其影响仍然有限。然而,对于理论和模拟来说,模型是一个突破,因为它消除了缩小规模的不确定性,而更大的模型则不那么密集地连接。这在神经形态计算界引发了一场竞赛,模型成为了一个事实上的标准基准。几年内,当能源消耗大幅减少时,实现了实时性能并超过了实时性能。我们审查了不同的模拟技术如何应对计算挑战,并为下一代基准和其他科学领域制定了指导方针。","Johanna Senk, Anno C. Kurth, Steve Furber, Tobias Gemmeke, Bruno Golosio, Arne Heittmann, James C. Knight, Eric Müller, Tobias Noll, Thomas Nowotny, Gorka Peraza Coppola, Luca Peres, Oliver Rhodes, Andrew Rowley, Johannes Schemmel, Tim Stadtmann, Tom Tetzlaff, Gianmarco Tiddia, Sacha J. van Albada, José Villamar, Markus Diesmann",2025-06-02T16:44:17Z,Constructive community race: full-density spiking neural network model   drives neuromorphic computing,Konstruktives Community-Rennen: Volldichte-Spitzen neuronales Netzwerkmodell treibt neuromorphes Computing an,充满建设性的社区种族:完全密度刺激神经网络模型驱动神经形态计算,http://arxiv.org/abs/2505.21185v2
77,"The evolving landscape of scientific computing requires seamless transitions from experimental to production HPC environments for interactive workflows. This paper presents a structured transition pathway developed at OLCF that bridges the gap between development testbeds and production systems. We address both technological and policy challenges, introducing frameworks for data streaming architectures, secure service interfaces, and adaptive resource scheduling for time-sensitive workloads and improved HPC interactivity. Our approach transforms traditional batch-oriented HPC into a more dynamic ecosystem capable of supporting modern scientific workflows that require near real-time data analysis, experimental steering, and cross-facility integration.","不断变化的科学计算格局要求从试验到生产高氯联苯的互动式工作流程环境的无缝过渡。本文件介绍了OLCF开发的结构性过渡路径,它弥合了发展试验台和生产系统之间的差距。我们既应对技术和政策挑战,又引入了数据流结构框架、安全服务界面以及适应性资源安排框架,以适应时间敏感的工作量和改进高氯联苯互动性。我们的方法将传统的批量导向高氯联苯转化为更有活力的生态系统,能够支持现代科学工作流程,这需要近实时数据分析、实验引导和跨设施整合。","Brian D. Etz, David M. Rogers, Michael J. Brim, Ketan Maheshwari, Kellen Leland, Tyler J. Skluzacek, Jack Lange, Daniel Pelfrey, Jordan Webb, Patrick Widener, Ryan Adamson, Christopher Zimmer, Veronica G. Melesse Vergara, Mallikarjun Shankar, Sarp Oral, Rafael Ferreira da Silva",2025-06-02T14:50:31Z,Enabling Seamless Transitions from Experimental to Production HPC for   Interactive Workflows,Ermöglichung nahtloser Übergänge von Experimental zu Produktions-HPC für interaktive Workflows,使从实验向生产阶段的无缝过渡能够实现交互式工作流程的HPC,http://arxiv.org/abs/2506.01744v1
78,"Synchronization is a fundamental enabler for low-power backscatter communication systems, where passive or semi-passive tags modulate ambient RF signals for ultra-low-power data transfer. In this survey, we review recent advances in synchronization techniques across Bluetooth Low Energy (BLE), Long-Term Evolution (LTE), and WiFi-based backscatter platforms. We categorize existing methods by their synchronization granularity, accuracy, compatibility, and power cost. We then compare representative systems including PassiveBLE, Bitalign, LScatter, SyncLTE, LiTEfoot, SyncScatter, and BiScatter, highlighting design trade-offs and performance metrics. Furthermore, we delve into the trade-offs between high throughput and low power synchronization, examining key approaches and challenges such as the balance between throughput, synchronization accuracy, and power consumption in various backscatter systems. Finally, we discuss open challenges and outline future directions toward scalable, secure, and ultra-low-power backscatter synchronization.","同步化是低功率后向散射通信系统的基本助推器,在这些系统中,被动或半被动标签调节超低功率数据传输的环境RF信号。在本调查中,我们审查了蓝牙低能、长期进化和无线Fi的后向散射平台同步技术的最新进展。我们按照同步颗粒性、准确性、兼容性和电费对现有方法进行分类。然后,我们比较具有代表性的系统,包括被动、比特利特、激光散射、同步LTE、利TEfoot、同步蒸发和双向散射,突出设计交易和性能尺度。此外,我们深入探讨高吞吐量和低功率同步之间的权衡问题,审查关键的方法和挑战,如各种后向散射系统中的吞吐、同步精度和电力消耗之间的平衡。最后,我们讨论公开的挑战,并概述未来走向可缩放、安全和超低功率后向同步的方向。","Wenyuan Jiang, Shuo Guo",2025-06-02T14:50:19Z,A Survey of Synchronization Technologies for Low-power Backscatter   Communication,Eine Übersicht über Synchronisierungstechnologien für die Kommunikation mit schwachen Backscatter-Leistungen,低功率后推散通信同步技术调查,http://arxiv.org/abs/2506.01743v1
79,"Blockchain benefits are due to immutability, replication, and storage-and-execution of smart contracts on the blockchain. However, the benefits come at increased costs due to the blockchain size and execution. We address three fundamental issues that arise in transferring certain parts of a smart contract to be executed off-chain: (i) identifying which parts (patterns) of the smart contract should be considered for processing off-chain, (ii) under which conditions should a smart-contract pattern to be processed off-chain, and (iii) how to facilitate interaction between the computation off and on-chain. We use separation of concerns and FSM modeling to model a smart contract and generate its code. We then (i) use our algorithm to determine which parts (patterns) of the smart contract are to be processed off-chain; (ii) consider conditions under which to move the pattern off-chain; and (iii) provide model for automatically generating the interface between on and off-chain computation.","链链的好处是由于在链条上智能合同的可移动性、复制和储存及执行造成的。但是,由于链条的大小和执行,这些好处的成本会增加。我们讨论了在转让拟在链外执行的智能合同的某些部分时产生的三个基本问题:(一) 确定应考虑将智能合同的哪些部分(模式)用于离链外加工,(二) 在哪些条件下应处理离链外的智能合同模式,以及(三) 如何促进计算与链外的相互作用。我们利用将关切和密克罗尼西亚的模型化来模拟智能合同并生成其代码。我们随后(一) 使用我们的算法来确定智能合同的哪些部分(模式)将进行离链外加工;(二) 考虑将模式向链外加工的条件;(三) 为自动生成链外计算与离链外的界面提供模式。",Christian Gang Liu,2025-06-02T12:46:31Z,FSM Modeling For Off-Blockchain Computation,FSM-Modellierung für Off-Blockchain-Computation,FSM 离锁链计算模型,http://arxiv.org/abs/2506.02086v1
80,"We introduce the Series-Parallel Workflow Decomposition (SP\-WD) heuristic algorithm for the Workflow Scheduling Problem (WSP) decomposition. We demonstrate that the SPWD algorithm facilitates the scheduling of large WSP instances with the hybrid D-Wave Constrained Quadratic Model solver, enabling the scheduling of instances that would otherwise exceed its capacity limitations. We also describe the accompanying execution environment used to obtain the results of the experiments with real-life workflow instances available in the WfCommons standardization initiative repository.","我们为工作流量调度问题(WSP)分解引入了Starel 工作流量分解(SP\-WD)系列超速算法(SP\-WD),我们证明,SPWD算法有利于将大型WSP案件与混合D-Wave Concorded Quadratic模型求解器安排在一起,从而能够将本来会超过其能力限制的情况安排在日程上,我们还描述了用于获取WfCommons标准化倡议储存库中存在的实时工作流程实验结果的相应执行环境。","Marcin Kroczek, Justyna Zawalska, Katarzyna Rycerz",2025-06-02T11:47:43Z,Workflow decomposition algorithm for scheduling with quantum   annealer-based hybrid solver,Workflow-Dekompositionsalgorithmus für die Planung mit quantenannealerbasiertem Hybridlöser,与量量安nealer基混合求解器排期的工作流分解算法,http://arxiv.org/abs/2506.01567v1
81,"Low Earth Orbit (LEO) satellite constellations are quickly being recognized as an upcoming extension of the Edge-Cloud Continuum into a 3D Continuum. Low-latency connectivity around the Earth and increasing computational power with every new satellite generation lead to a vision of workflows being seamlessly executed across Edge, Cloud, and space nodes. High launch costs for new satellites and the need to experiment with large constellations mandate the use of simulators for validating new orchestration algorithms. Unfortunately, existing simulators only allow for relatively small constellations to be simulated without scaling to a large number of host machines. In this paper, we present Stardust, a scalable and extensible simulator for the 3D Continuum. Stardust supports i) simulating mega constellations with 3x the size of the currently largest LEO mega constellation on a single machine, ii) experimentation with custom network routing protocols through its dynamic routing mechanism, and iii) rapid testing of orchestration algorithms or software by integrating them into the simulation as SimPlugins. We evaluate Stardust in multiple simulations to show that it is more scalable than the state-of-the-art and that it can simulate a mega constellation with up to 20.6k satellites on a single machine.","低地球轨道(LEO)卫星星座很快很快被公认为是Edge-Cloud Continuum 即将扩展为3D Continuum的3D Continuum。地球周围的低纬度连接以及每组新卫星生成的计算能力不断提高,导致工作流程在边缘、云和空间节点之间得到无缝执行的愿景。新卫星的高发射成本和对大型星座进行实验的必要性要求使用模拟器来验证新的管弦算法。不幸的是,现有的模拟器只能允许模拟相对较小的星座而不向大量主机缩放。在本文件中,我们介绍星尘,这是3D Continum 的可缩放和可扩展的模拟器。星尘支持i) 模拟巨型星座,其规模相当于目前最大的低地轨道巨型星座星座的3x大小,二) 通过其动态路程机制对定制网络线路协议进行实验。以及快速测试管弦算法或软件,将其作为SimPlugins的模拟器,作为SimPluggins。我们评估了20号星系的模拟模型显示,而该星座的模型比它更能模拟了20-modal-dal-dal-dal-daldal-daldaldal-daldaldaldaldaldaldaldaldaldaldals可以显示。","Thomas Pusztai, Jan Hisberger, Cynthia Marcelino, Stefan Nastic",2025-06-02T10:20:21Z,Stardust: A Scalable and Extensible Simulator for the 3D Continuum,Stardust: Ein skalierbarer und erweiterbarer Simulator für das 3D Continuum,Stardust: 3D 连续波的可缩缩和可扩展模拟器,http://arxiv.org/abs/2506.01513v1
82,"This paper presents a mathematically rigorous formal analysis of Simplified Payment Verification (SPV) clients, as specified in Section 8 of the original Bitcoin white paper, versus non-mining full nodes operated by home users. It defines security as resistance to divergence from global consensus and models transaction acceptance, enforcement capability, and divergence probability under adversarial conditions. The results demonstrate that SPV clients, despite omitting script verification, are cryptographically sufficient under honest-majority assumptions and topologically less vulnerable to attack than structurally passive, non-enforcing full nodes. The paper introduces new axioms on behavioral divergence and communication topology, proving that home-based full nodes increase systemic entropy without contributing to consensus integrity. Using a series of formally defined lemmas, propositions, and Monte Carlo simulation results, it is shown that SPV clients represent the rational equilibrium strategy for non-mining participants. This challenges the prevailing narrative that home validators enhance network security, providing formal and operational justifications for the sufficiency of SPV models.","本文件根据原Bitcoin白皮书第8节的规定,从数学角度对简化支付核实客户进行了严格的正式分析,而没有挖掘家庭用户经营的全部节点,将安全定义为抵制与全球共识和模式交易接受、执行能力和对抗条件下的差异概率的差异。结果显示,SPV客户尽管省略了文字核查,但在诚实多数的假设下,在字面上是足够的,在地形学上比结构上被动、非强制的完整节点更容易受到攻击。本文介绍了关于行为差异和通信结构学的新轴心,证明基于家庭的全面节点增加了系统的环球,而没有促进共识的完整性。使用一系列正式定义的利玛斯、提议和蒙特卡洛模拟结果,表明SPV客户代表了非采矿参与者的合理平衡战略。这挑战了主控者加强网络安全、为SPV模型充分性提供正式和业务理由的普遍叙述。",Craig Steven Wright,2025-06-02T07:20:25Z,Formal Security Analysis of SPV Clients Versus Home-Based Full Nodes in   Bitcoin-Derived Systems,Formale Sicherheitsanalyse von SPV-Clients Versus Home-Based Full Nodes in Bitcoin-Derived Systems,Bitcoin-Derived 系统中SPV客户对以家为基础的Bittcoin-Derived系统全节点进行的正式安全分析,http://arxiv.org/abs/2506.01384v1
83,"In recent years, the development of specialized edge computing devices has significantly increased, driven by the growing demand for AI models. These devices, such as the NVIDIA Jetson series, must efficiently handle increased data processing and storage requirements. However, despite these advancements, there remains a lack of frameworks that automate the optimal execution of optimal execution of deep neural network (DNN). Therefore, efforts have been made to create schedulers that can manage complex data processing needs while ensuring the efficient utilization of all available accelerators within these devices, including the CPU, GPU, deep learning accelerator (DLA), programmable vision accelerator (PVA), and video image compositor (VIC). Such schedulers would maximize the performance of edge computing systems, crucial in resource-constrained environments. This paper aims to comprehensively review the various DNN schedulers implemented on NVIDIA Jetson devices. It examines their methodologies, performance, and effectiveness in addressing the demands of modern AI workloads. By analyzing these schedulers, this review highlights the current state of the research in the field. It identifies future research and development areas, further enhancing edge computing devices' capabilities.","近年来,由于对AI模型的需求不断增长,专门边缘计算装置的开发有了显著的提高,这些装置,如NVIDIA Jetson系列,必须有效地处理更多的数据处理和储存要求,然而,尽管取得了这些进展,仍然缺乏使最佳执行深神经网络的最佳执行方式自动化的框架。因此,已作出努力,建立能够管理复杂的数据处理需求的调度器,同时确保有效利用这些装置中所有可用的加速器,包括CPU、GPU、深学习加速器(DLA)、可编程视觉加速器(PVA)和视频图像编集器(VIC)。这些排程器将最大限度地提高边缘计算系统的性能,这对于资源紧张的环境至关重要。本文件旨在全面审查在NVIDIA喷气器装置上实施的各种DNN的排程,审查它们处理现代AI工作量要求的方法、性能和有效性。通过分析这些排程器,本审查突出了实地研究的现状。它确定了未来边缘计算装置的研发能力,进一步提升了边缘计算装置的能力。","Ashiyana Abdul Majeed, Mahmoud Meribout",2025-06-02T07:11:10Z,Scheduling Techniques of AI Models on Modern Heterogeneous Edge GPU -- A   Critical Review,Scheduling Techniques of KI Models on Modern Heterogeneous Edge GPU -- A Critical Review,AI现代异异异性边缘GPU模型 -- -- 关键审查,http://arxiv.org/abs/2506.01377v1
84,"We present the massively parallel performance of a $h$-adaptive solver for atmosphere dynamics that allows for non-conforming mesh refinement. The numerical method is based on a Discontinuous Galerkin (DG) spatial discretization, highly scalable thanks to its data locality properties, and on a second order Implicit-Explicit Runge-Kutta (IMEX-RK) method for time discretization, particularly well suited for low Mach number flows. Simulations with non-conforming meshes for flows over orography can increase the accuracy of the local flow description without affecting the larger scales, which can be solved on coarser meshes. We show that the local refining procedure has no significant impact on the parallel performance and, therefore, both efficiency and scalability can be achieved in this framework.","数字方法基于不连续的Galerkin(DG)空间分解,由于其数据位置特性,高度可伸缩,以及第二顺序的隐性显性龙格-库塔(IMEX-RK)时间分解方法,特别适合低马赫数流。与不兼容的模合物模拟过量流或成文流,可以提高本地流量描述的准确性,而不影响大尺度,而大尺度则可以在粗体中解决。我们表明,本地精炼程序对平行性能没有重大影响,因此,在这个框架内,既可以实现效率和可伸缩性。","Giuseppe Orlando, Tommaso Benacchio, Luca Bonaventura",2025-06-02T04:24:15Z,Efficient and scalable atmospheric dynamics simulations using   non-conforming meshes,Effiziente und skalierbare Simulationen der atmosphärischen Dynamik mit nicht konformen Netzen,使用不兼容的摩贝模拟器进行高效和可缩放的大气动态模拟,http://arxiv.org/abs/2408.08129v2
85,"Public cloud serverless platforms have attracted a large user base due to their high scalability, plug-and-play deployment model, and pay-per-use billing. However, compared to virtual machines and container hosting services, modern serverless offerings typically impose higher per-unit time and resource charges. Additionally, billing practices such as wall-clock allocation-based billing, invocation fees, and usage rounding up can further increase costs.   This work, for the first time, holistically demystifies these costs by conducting an in-depth, top-down characterization and analysis from user-facing billing models, through request serving architectures, and down to operating system scheduling on major public serverless platforms. We quantify, for the first time, how current billing practices inflate billable resources up to 5.49x beyond actual consumption. Also, our analysis reveals previously unreported cost drivers, such as operational patterns of serving architectures that create overheads, details of resource allocation during keep-alive periods, and OS scheduling granularity effects that directly impact both performance and billing. By tracing the sources of costs from billing models down to OS scheduling, we uncover the rationale behind today's expensive serverless billing model and practices and provide insights for designing performant and cost-effective serverless systems.","无云公共平台因其可扩缩性高、插插和播放部署模式高,以及每个用户付费计费等原因,吸引了庞大的用户基础。然而,与虚拟机器和集装箱托管服务相比,现代无服务器服务通常会给每个单位带来更高的时间和资源收费。此外,基于墙昼夜分配计费、调用费和使用四舍五入等计费做法可能会进一步增加成本。这项工作首次从整体上消除了这些费用的神秘性,通过要求服务结构,从以用户为主的计费模型进行深入、自上而下的特点描述和分析,并下至操作无服务器主要平台的系统时间安排。我们第一次量化了当前计费做法如何将可计费资源提高到5.49x实际消费以外的5.49x。此外,我们的分析还揭示了以前未报告的成本驱动因素,如创建管理费用的结构的运作模式、保持时段期间资源分配的细节,以及直接影响到业绩和计费功能的OS工作表效应。我们通过追踪从计费模型的成本来源以及设计高成本的服务器系统,从而在今天设计高成本的服务器上实现高清晰的系统。","Changyuan Lin, Gigi, Ma, Mohammad Shahrad",2025-06-02T03:40:24Z,Getting to the Bottom of Serverless Billing,Auf den Grund der serverlosen Abrechnung,正在进入无服务器比林的底部,http://arxiv.org/abs/2506.01283v1
86,"LoRA has emerged as one of the most promising fine-tuning techniques, especially for federated learning (FL), since it significantly reduces communication and computation costs at resource-constrained clients. However, data heterogeneity remains a significant challenge for LoRA-based FL, and the conventional aggregation strategy based on FedAvg suffers from slow convergence and suboptimal accuracy. Motivated by recent advances in model merging, particularly Task Arithmetic, we explore the idea of aggregating client LoRA parameters using scaled averaging. We first observe that a naive application of Task Arithmetic is ineffective due to the high cosine similarity between client updates, indicating significant common knowledge in the updates across clients. To address this issue, we propose decomposing client LoRA updates via Robust Principal Component Analysis (Robust-PCA) into a common low-rank component and client-specific sparse components. Our proposed algorithm FedRPCA aggregates the low-rank components through averaging, consolidating common knowledge, and applies scaled averaging to the sparse components to amplify client-specific knowledge. We evaluate our approach across a variety of vision and language tasks and demonstrate that it achieves higher final accuracy and faster convergence compared to competing baselines.","LoRA已成为最有希望的微调技术之一,特别是联谊学习(FL),因为它大大减少了资源受限制客户的通信和计算成本;然而,数据差异仍然是以LORA为基础的FL的重大挑战,基于FedAvg的常规汇总战略的趋同速度缓慢,而且不够最佳。受模型合并,特别是Arithmetit任务的最近进展的推动,我们探索了使用平均比例来汇总客户LORA参数的想法。我们首先发现,由于客户更新的高度共通性表明客户在更新方面有着相当的共同知识,任务天真地应用Arithmetiat是无效的。为了解决这一问题,我们提议通过Robust主元分析(Robust-PCA)将客户LORA更新变成共同的低级组件和客户特有的稀薄组件。我们提议的FedRPCA算法通过平均、巩固共同知识来汇总低级组件,并将平均比例用于稀薄的组件,以扩大客户特定知识。我们评价了各种愿景和语言任务之间的大量共同知识。我们评价了我们的各种办法,并表明,与最终基准的趋同较快。","Divyansh Jhunjhunwala, Arian Raje, Madan Ravi Ganesh, Chaithanya Kumar Mummadi, Chaoqun Dong, Jiawei Zhou, Wan-Yi Lin, Gauri Joshi, Zhenzhen Li",2025-06-01T22:07:00Z,FedRPCA: Enhancing Federated LoRA Aggregation Using Robust PCA,FedRPCA: Verbesserung der Federated LoRA Aggregation mit robustem PCA,FFPPCA: 使用硬性五氯苯甲醚增强联邦罗拉聚合物,http://arxiv.org/abs/2506.01194v1
87,"One-Shot Federated Learning (OSFL) restricts communication between the server and clients to a single round, significantly reducing communication costs and minimizing privacy leakage risks compared to traditional Federated Learning (FL), which requires multiple rounds of communication. However, existing OSFL frameworks remain vulnerable to distributional heterogeneity, as they primarily focus on model heterogeneity while neglecting data heterogeneity. To bridge this gap, we propose FedHydra, a unified, data-free, OSFL framework designed to effectively address both model and data heterogeneity. Unlike existing OSFL approaches, FedHydra introduces a novel two-stage learning mechanism. Specifically, it incorporates model stratification and heterogeneity-aware stratified aggregation to mitigate the challenges posed by both model and data heterogeneity. By this design, the data and model heterogeneity issues are simultaneously monitored from different aspects during learning. Consequently, FedHydra can effectively mitigate both issues by minimizing their inherent conflicts. We compared FedHydra with five SOTA baselines on four benchmark datasets. Experimental results show that our method outperforms the previous OSFL methods in both homogeneous and heterogeneous settings. The code is available at https://github.com/Jun-B0518/FedHydra.","为了缩小这一差距,我们提议FedHydra(一个统一、无数据、OSFL(OFDRA)框架,一个旨在有效解决模型和数据差异性的统一框架,即FedHydra(FedHydra),与现有的OSFL(FSL)方法不同,FedHydra(FedHydra)推出了一个新的两阶段学习机制,具体地说,它包含模型分层和异性(异性)综合,以减轻模型和数据差异性(ODF)带来的挑战。通过这一设计,数据和模型异性(FT)问题从学习的不同方面同时得到监测。因此,FedHydra(FedHydra)可以通过尽量减少其内在冲突,有效地缓解这两个问题。我们将FedHydra(FedHydra)和SOTA(SOTA)在四个基准数据集(OFDR)中的五个基准(SDR)基准(O18/FDR)/FSF(OA)的系统化结果显示我们以往的方法。","Jun Bai, Yiliao Song, Di Wu, Atul Sajjanhar, Yong Xiang, Wei Zhou, Xiaohui Tao, Yan Li, Yue Li",2025-06-01T14:54:51Z,A Unified Solution to Diverse Heterogeneities in One-shot Federated   Learning,Eine einheitliche Lösung für unterschiedliche Heterogenitäten im one-shot-Federated Learning,一次性联邦学习中多样性的统一解决方案,http://arxiv.org/abs/2410.21119v3
88,"Federated fine-tuning (FedFT) provides an effective paradigm for fine-tuning large language models (LLMs) in privacy-sensitive scenarios. However, practical deployment remains challenging due to the limited resources on end devices. Existing methods typically utilize parameter-efficient fine-tuning (PEFT) techniques, such as Low-Rank Adaptation (LoRA), to substantially reduce communication overhead. Nevertheless, significant memory usage for activation storage and computational demands from full backpropagation remain major barriers to efficient deployment on resource-constrained end devices. Moreover, substantial resource heterogeneity across devices results in severe synchronization bottlenecks, diminishing the overall fine-tuning efficiency. To address these issues, we propose FedQuad, a novel LoRA-based FedFT framework that adaptively adjusts the LoRA depth (the number of consecutive tunable LoRA layers from the output) according to device computational capabilities, while employing activation quantization to reduce memory overhead, thereby enabling efficient deployment on resource-constrained devices. Specifically, FedQuad first identifies the feasible and efficient combinations of LoRA depth and the number of activation quantization layers based on device-specific resource constraints. Subsequently, FedQuad employs a greedy strategy to select the optimal configurations for each device, effectively accommodating system heterogeneity. Extensive experiments demonstrate that FedQuad achieves a 1.4-5.3x convergence acceleration compared to state-of-the-art baselines when reaching target accuracy, highlighting its efficiency and deployability in resource-constrained and heterogeneous end-device environments.","联邦微调(FedFT)为在对隐私敏感的情况下微调大型语言模型提供了有效的范例,但是,由于终端设备资源有限,实际部署仍具有挑战性。现有方法通常使用低兰克适应(LORA)等节能微调技术,以大幅降低通信管理费用。不过,在启动储存和计算全面反向转换产生的存储和计算需求方面大量使用记忆仍然是在资源限制的终端装置上有效部署的主要障碍。此外,各装置之间的资源差异性巨大,导致高度同步瓶颈,降低总体微调效率。为了解决这些问题,我们提议FedQuad(基于LORA的新型FedFTFT框架),根据LORA深度(与产出相接连的金枪鱼分级LORA层数量)进行适应性调整,以大大降低通信管理能力,同时利用快速存储存储能力以减少记忆性管理,从而有效部署资源限制装置。FedQuadalde首先确定LARA深度的可行和高效组合,以及基于具体设备目标性精调效率的平整层数量。我们建议,FedQaladadadadad,随后在优化资源配置的精准性基准中,在最佳配置中,实现最佳配置中实现最佳的精准性战略。","Rukuo Li, Jianchun Liu, Hongli Xu, Liusheng Huang",2025-06-01T13:13:20Z,FedQuad: Adaptive Layer-wise LoRA Deployment and Activation Quantization   for Federated Fine-Tuning,FedQuad: Adaptive Layer-weise LoRA Bereitstellung und Aktivierung Quantisierung für Federated Fine-Tuning,FedQuad:采用适应性图层方法的LORA 部署和激活联邦,http://arxiv.org/abs/2506.01001v1
89,"Fog computing significantly enhances the efficiency of IoT applications by providing computation, storage, and networking resources at the edge of the network. In this paper, we propose a federated fog computing framework designed to optimize resource management, minimize latency, and reduce energy consumption across distributed IoT environments. Our framework incorporates predictive scheduling, energy-aware resource allocation, and adaptive mobility management strategies. Experimental results obtained from extensive simulations using the OMNeT++ environment demonstrate that our federated approach outperforms traditional non-federated architectures in terms of resource utilization, latency, energy efficiency, task execution time, and scalability. These findings underline the suitability and effectiveness of the proposed framework for supporting sustainable and high-performance IoT services.","雾计算通过在网络边缘提供计算、储存和联网资源,大大提高了IOT应用的效率。在本文中,我们提议了一个联合雾计算框架,旨在优化资源管理、尽量减少延迟和减少分布式IOT环境中的能源消耗。我们的框架包括预测时间表、能源意识资源分配和适应性流动管理战略。利用OMNET+++环境进行的广泛模拟的实验结果表明,我们的联合方法在资源利用、延缓性、能效、任务执行时间和可扩缩性方面超过了传统的非联合结构。这些结论强调了支持可持续和高性能IOT服务的拟议框架的适宜性和有效性。","Syed Sarmad Shah, Anas Ali",2025-06-01T12:15:08Z,Optimizing Resource Allocation and Energy Efficiency in Federated Fog   Computing for IoT,Optimierung der Ressourcenallokation und Energieeffizienz im Federated Fog Computing für IoT,IoT的联雾计算器优化资源分配和能源效率,http://arxiv.org/abs/2504.00791v2
90,"Cloud data centres demand adaptive, efficient, and fair resource allocation techniques due to heterogeneous workloads with varying priorities. However, most existing approaches struggle to cope with dynamic traffic patterns, often resulting in suboptimal fairness, increased latency, and higher energy consumption. To overcome these limitations, we propose a novel method called Weighted Actor-Critic Deep Reinforcement Learning (WA3C). Unlike static rule-based schedulers, WA3C continuously learns from the environment, making it resilient to changing workload patterns and system dynamics. Furthermore, the algorithm incorporates a multi-objective reward structure that balances trade-offs among latency, throughput, energy consumption, and fairness. This adaptability makes WA3C well-suited for modern multi-tenant cloud infrastructures, where diverse applications often compete for limited resources. WA3C also supports online learning, allowing it to adapt in real time to shifting workload compositions without the need for retraining from scratch. The model's architecture is designed to be lightweight and scalable, ensuring feasibility even in large-scale deployments. Additionally, WA3C introduces a priority-aware advantage estimator that better captures the urgency of tasks, enhancing scheduling precision. As a result, WA3C achieves more effective convergence, lower latency, and balanced resource allocation among jobs. Extensive experiments using synthetic job traces demonstrate that WA3C consistently outperforms both traditional and reinforcement learning-based baselines, highlighting its potential for real-world deployment in large-scale cloud systems.","云中数据中心要求适应、高效和公平的资源分配技术,因为工作量不同,而且优先事项各异。然而,大多数现有方法都难以应对动态交通模式,往往导致不优化的公平性、更高的延缓度和更高的能源消耗。为了克服这些限制,我们提议了一种叫作“重力动作-临界深度强化学习”(WA3C)的新颖方法。与静态的基于规则的调度器不同,WA3C不断从环境中学习,使其适应不断变化的工作量模式和系统动态。此外,算法还包含一个多目标的奖励结构,平衡延缓、吞吐量、能源消耗和公平之间的平衡。这种适应性使WA3C非常适合现代多密度云基础设施,而各种应用往往争夺有限的资源。WA3C还支持在线学习,使其能够在实际时间上适应改变工作量构成,而无需从零开始再培训。模型结构的设计是轻度和可伸缩的,确保即使在大规模部署中也具有可行性。此外,WA3C在大规模部署中引入了一种大型的优先规模优势,即更精确地分配,能够更好地衡量工作,从而更好地衡量工作周期的进度,从而更好地衡量。","Suchi Kumari, Dhruv Mishra",2025-06-01T09:48:36Z,"Adaptive, Efficient and Fair Resource Allocation in Cloud Datacenters   leveraging Weighted A3C Deep Reinforcement Learning","Adaptive, effiziente und faire Ressourcenallokation in Cloud-Rechenzentren, die Gewichtetes A3C Deep Reinforcement Learning nutzen",利用重力A3C深度强化学习在云中数据中心进行适应性、高效和公平资源分配,http://arxiv.org/abs/2506.00929v1
91,"Personalized federated learning (PFL) offers a flexible framework for aggregating information across distributed clients with heterogeneous data. This work considers a personalized federated learning setting that simultaneously learns global and local models. While purely local training has no communication cost, collaborative learning among the clients can leverage shared knowledge to improve statistical accuracy, presenting an accuracy-communication trade-off in personalized federated learning. However, the theoretical analysis of how personalization quantitatively influences sample and algorithmic efficiency and their inherent trade-off is largely unexplored. This paper makes a contribution towards filling this gap, by providing a quantitative characterization of the personalization degree on the tradeoff. The results further offers theoretical insights for choosing the personalization degree. As a side contribution, we establish the minimax optimality in terms of statistical accuracy for a widely studied PFL formulation. The theoretical result is validated on both synthetic and real-world datasets and its generalizability is verified in a non-convex setting.","个人化联邦学习(PFL)提供了一个灵活的框架,用于汇集分布在分布在各地的客户中的信息,提供各种数据;这项工作考虑到一个个人化的联邦学习环境,同时学习全球和地方模式;虽然纯粹的当地培训没有通信成本,但客户之间的协作学习可以利用共享的知识,提高统计准确性,在个人化联邦学习中显示准确性-通信权衡;然而,关于个人化如何对抽样和算法效率及其内在权衡进行定量影响的理论分析基本上没有进行探讨;本文件通过对交易的个人化程度进行定量定性,为填补这一差距作出了贡献;结果进一步为选择个人化学位提供了理论见解;作为辅助贡献,我们为广泛研究的PFL的编制确定了统计准确性方面的微量最佳性;在合成和真实世界数据集及其通用性两方面的理论结果都得到验证,并在非碳化环境下验证。","Xin Yu, Zelin He, Ying Sun, Lingzhou Xue, Runze Li",2025-06-01T06:17:31Z,Understanding the Statistical Accuracy-Communication Trade-off in   Personalized Federated Learning with Minimax Guarantees,Das Verständnis der statistischen Genauigkeit-Kommunikation Trade-off in Personalized Federated Learning mit Minimax-Garantien,了解具有最低保障的个人化联邦学习中统计准确性-通信交易,http://arxiv.org/abs/2410.08934v4
92,"We introduce EvoGit, a decentralized multi-agent framework for collaborative software development driven by autonomous code evolution. EvoGit deploys a population of independent coding agents, each proposing edits to a shared codebase without centralized coordination, explicit message passing, or shared memory. Instead, all coordination emerges through a Git-based phylogenetic graph that tracks the full version lineage and enables agents to asynchronously read from and write to the evolving code repository. This graph-based structure supports fine-grained branching, implicit concurrency, and scalable agent interaction while preserving a consistent historical record. Human involvement is minimal but strategic: users define high-level goals, periodically review the graph, and provide lightweight feedback to promote promising directions or prune unproductive ones. Experiments demonstrate EvoGit's ability to autonomously produce functional and modular software artifacts across two real-world tasks: (1) building a web application from scratch using modern frameworks, and (2) constructing a meta-level system that evolves its own language-model-guided solver for the bin-packing optimization problem. Our results underscore EvoGit's potential to establish a new paradigm for decentralized, automated, and continual software development. EvoGit is open-sourced at https://github.com/BillHuang2001/evogit.","我们引入了由自主代码演变驱动的合作软件开发的分散式多试剂框架EvoGit。 EvoGit 配置了一组独立的编码代理人,每个都提议编辑到一个共用的代码库,而没有集中协调、明确传递信息或共享记忆。相反,所有协调都通过基于Git的植物基因图示出现,该图示跟踪整个版本线条,使代理商能够从正在演变的代码库中无动于衷地读写。这个基于图形的结构支持精细的分支、隐含的货币和可扩展的代理人互动,同时保存一致的历史记录。人类参与是最小的,但具有战略意义:用户定义高层次目标,定期审查图表,并提供轻量反馈,以促进有希望的方向或无益的记忆。实验表明EvoGit有能力在两种现实世界任务中自主地制作功能和模块软件工艺品:(1) 利用现代框架从零开始建立网络应用程序,(2) 构建一个元级系统,以发展自己的语言模范式/导导式解决方案,同时保存一致的历史记录。人类参与是最低限度的,但具有战略意义:用户定义、定期审查图表,并提供自动化的EvoGit/BIFormax。","Beichen Huang, Ran Cheng, Kay Chen Tan",2025-06-01T05:20:42Z,EvoGit: Decentralized Code Evolution via Git-Based Multi-Agent   Collaboration,EvoGit: Dezentralisierte Code-Evolution über die Git-basierte Multi-Agent-Kollaboration,"EvoGit:通过基建多机构协作,分散化代码演变",http://arxiv.org/abs/2506.02049v1
93,"Quantum computing has demonstrated potential for solving complex optimization problems; however, its application to spatial regionalization remains underexplored. Spatial contiguity, a fundamental constraint requiring spatial entities to form connected components, significantly increases the complexity of regionalization problems, which are typically challenging for quantum modeling. This paper proposes novel quantum formulations based on a flow model that enforces spatial contiguity constraints. Our scale-aware approach employs a Discrete Quadratic Model (DQM), solvable directly on quantum annealing hardware for small-scale datasets. In addition, it designs a hybrid quantum-classical approach to manage larger-scale problems within existing hardware limitations. This work establishes a foundational framework for integrating quantum methods into practical spatial optimization tasks.","量子计算已证明有可能解决复杂的优化问题;然而,它在空间区域化方面的应用仍未得到充分探讨。空间毗连是要求空间实体组成连接组件的一个根本制约因素,它大大增加了区域化问题的复杂性,而区域化问题通常对量子建模具有挑战性。本文件提议基于流动模型的新量子配方,以实施空间毗连限制。我们的量子认知方法采用了分立二次量子计算模型(DQM),该模型可直接用于小规模数据集的量子反射硬件。此外,它设计了一种混合量子古典方法,在现有硬件限制范围内管理较大规模的问题。这项工作为将量子方法纳入实际的空间优化任务奠定了一个基础框架。","Yunhan Chang, Amr Magdy, Federico M. Spedalieri",2025-06-01T04:36:19Z,Quantum Modeling of Spatial Contiguity Constraints,Quantenmodellierung von räumlichen Kontiguitätsbeschränkungen,空间相容制约量量量模型化,http://arxiv.org/abs/2505.12608v2
94,"With the rapid expansion in the scale of large language models (LLMs), enabling efficient distributed inference across multiple computing units has become increasingly critical. However, communication overheads from popular distributed inference techniques such as Tensor Parallelism pose a significant challenge to achieve scalability and low latency. Therefore, we introduce a novel optimization technique, Sync-Point Drop (SPD), to reduce communication overheads in tensor parallelism by selectively dropping synchronization on attention outputs. In detail, we first propose a block design that allows execution to proceed without communication through SPD. Second, we apply different SPD strategies to attention blocks based on their sensitivity to the model accuracy. The proposed methods effectively alleviate communication bottlenecks while minimizing accuracy degradation during LLM inference, offering a scalable solution for diverse distributed environments: SPD offered about 20% overall inference latency reduction with < 1% accuracy regression for LLaMA2-70B inference over 8 GPUs.","随着大型语言模型(LLMs)规模的迅速扩大,使多个计算单位之间高效分布的推论变得日益重要,但是,由流行的分布式推论技术(如Tensor平行主义)产生的通信间接费用对实现可缩放性和低延缓度提出了重大挑战。因此,我们引入了新型优化技术(同步点滴),通过有选择地降低关注输出的同步度,减少不同平行度的通信间接费用。详细而言,我们首先提出一个区块设计,允许在不通过 SPD进行沟通的情况下执行。第二,我们根据对模型准确度的敏感性,对关注区块采用不同的SPD战略。拟议方法有效地缓解通信瓶颈,同时在LLM 推导期间尽量减少准确度的退化,为多种分布环境提供了一种可缩放的解决方案:SPD提供了大约20%的总体推力拉拉马2-70B的精度下降率 < 1%的LLAMA2-70B推导力超过8 GPUs。","Han-Byul Kim, Duc Hoang, Arnav Kundu, Mohammad Samragh, Minsik Cho",2025-06-01T00:33:25Z,SPD: Sync-Point Drop for Efficient Tensor Parallelism of Large Language   Models,SPD: Sync-Point Drop für effiziente Tensor-Parallelität von großen Sprachmodellen,SPD: 高效大语言模式Tensor平行式同步点滴,http://arxiv.org/abs/2502.20727v4
95,"Diagnosing problems in deployed distributed applications continues to grow more challenging. A significant reason is the extreme mismatch between the powerful abstractions developers have available to build increasingly complex distributed applications versus the simple ones engineers have available to diagnose problems in them. To help, we present a novel abstraction, the workflow motif, instantiations of which represent characteristics of frequently-repeating patterns within and among request executions. We argue that workflow motifs will benefit many diagnosis tasks, formally define them, and use this definition to identify which frequent-subgraph-mining algorithms are good starting points for mining workflow motifs. We conclude by using an early version of workflow motifs to suggest performance-optimization points in HDFS.","所部署的分布式应用软件的诊断问题继续变得越来越具有挑战性,一个重大的原因是,强大的抽象开发者可以建立日益复杂的分布式应用软件,而简单的工程师可以诊断这些应用软件中的问题,两者之间的极端不匹配。为了提供帮助,我们提出了一个新颖的抽象,工作流程的模型,即即即时分析,它代表了请求处决中经常重复的模式的特点。我们争辩说,工作流程的模型将有利于许多诊断任务,正式界定它们,并使用这一定义来确定哪些频繁的谱式挖掘算法是开采工作流程模型的良好起点。我们最后用早期版本的工作流程模型来建议HDFS中的性能优化点。","Mania Abdi, Peter Desnoyers, Mark Crovella, Raja R. Sambasivan",2025-05-31T23:27:10Z,The workflow motif: a widely-userful performance diagnosis abstraction   for distributed applications,Das Workflow-Motiv: eine weit verbreitete Leistungsdiagnose Abstraktion für verteilte Anwendungen,工作流程图示:对分布式应用软件广泛使用的业绩诊断摘要,http://arxiv.org/abs/2506.00749v1
96,"Parameter Efficient Fine-Tuning (PEFT) has become the de-facto approach in adapting Large Language Models (LLMs) for downstream tasks in Natural Language Processing. However, its adoption in privacy-preserving distributed learning frameworks, such as Federated Learning (FL), remains relatively limited. This is mainly due to challenges specific to FL, such as resource-constrained devices and diverse data distributions among clients. In this paper, we propose an efficient method to perform PEFT within the FL framework for Multi-Head Attention (MHA) based language models. We address the challenges through head pruning, a novel head-specific weighted aggregation mechanism, and a client selection strategy. Head pruning minimizes training complexity within the clients, guided by the importance score computed based on the confidence of the attention head. Weighted aggregation of heads ensures the global model captures crucial updates from diverse clients complementing our client selection strategy. We show results on the MultiNLI benchmark along with 20 Newsgroups, XL-Sum, and E2E NLG datasets. We use the MultiNLI dataset and T5-small model with LoRA as our PEFT method, attaining sparsity levels of up to 90%, resulting in a communication advantage of up to 1.8x and a reduction in training OPs of 3.9x while maintaining the accuracy drop under 2%.","高效精密调制(PEFT)已成为调整大语言模式以适应自然语言处理下游任务的“大语言模型”的“脱法”方法,但该方法在保护隐私的分布式学习框架(如Federal Learning(FL))中的采用仍然相对有限,这主要是由于FL的具体挑战,例如资源限制装置和客户之间数据分布多样化。在本文件中,我们提出了在基于多种领导人注意语言模型的FL框架内实施“大语言模型”的有效方法。我们通过头部剪裁、新型的针对特定面的加权汇总机制和客户选择战略应对挑战。在根据关注头部信心计算的重要性分数的基础上,将客户内部培训的复杂性降到最低。经过仔细的整合,可以确保全球模型从不同客户中获取关键的最新信息,补充我们的客户选择战略。我们展示了“多语言框架”基准以及20个新闻组、XL-Sum和E2ENLG数据集。我们用多NLG数据集和T5-FSO的精确度数据设置和T5-FSOFSO的精确度提升为20,同时将PARA-SAL的精确度提升为PA-39级,而使PARIS的P-39级升级为PRIS的升级为0.90。","Yeshwanth Venkatesha, Souvik Kundu, Priyadarshini Panda",2025-05-31T23:09:26Z,Assortment of Attention Heads: Accelerating Federated PEFT with Head   Pruning and Strategic Client Selection,Auswahl von Aufmerksamkeitsköpfen: Beschleunigen von Federated PEFT mit Head Pruning und strategischer Kundenauswahl,"关注对象负责人组:快速联邦PEFT,由主管谨慎和战略客户选择",http://arxiv.org/abs/2506.00743v1
97,"Complex systems such as aircraft engines are continuously monitored by sensors. In predictive aircraft maintenance, the collected sensor measurements are used to estimate the health condition and the Remaining Useful Life (RUL) of such systems. However, a major challenge when developing prognostics is the limited number of run-to-failure data samples. This challenge could be overcome if multiple airlines would share their run-to-failure data samples such that sufficient learning can be achieved. Due to privacy concerns, however, airlines are reluctant to share their data in a centralized setting. In this paper, a collaborative federated learning framework is therefore developed instead. Here, several airlines cooperate to train a collective RUL prognostic machine learning model, without the need to centrally share their data. For this, a decentralized validation procedure is proposed to validate the prognostics model without sharing any data. Moreover, sensor data is often noisy and of low quality. This paper therefore proposes four novel methods to aggregate the parameters of the global prognostic model. These methods enhance the robustness of the FL framework against noisy data. The proposed framework is illustrated for training a collaborative RUL prognostic model for aircraft engines, using the N-CMAPSS dataset. Here, six airlines are considered, that collaborate in the FL framework to train a collective RUL prognostic model for their aircraft's engines. When comparing the proposed FL framework with the case where each airline independently develops their own prognostic model, the results show that FL leads to more accurate RUL prognostics for five out of the six airlines. Moreover, the novel robust aggregation methods render the FL framework robust to noisy data samples.","飞机引擎等复杂系统由传感器不断监测。在预测性飞机维修中,收集的传感器测量数据被用来估计这些系统的健康状况和剩余使用寿命(RUL),然而,在开发预测性数据样本时,一个重大挑战是运行至故障数据样本数量有限。如果多个航空公司共享运行至故障数据样本,从而能够实现足够的学习,这一挑战就可以克服。然而,出于隐私考虑,航空公司不愿意在中央环境中分享数据。因此,本文将开发一个合作性联合学习框架。在这里,一些航空公司合作培训一个集体的 RUL 预测性机器学习模型,而无需集中分享数据。为此,提议了一个分散的验证程序,在不分享任何数据的情况下验证预测性模型。此外,传感器数据往往很吵杂,质量也很低。因此,本文件提出了四种新颖的方法,用以汇总全球预测性模型的参数。这些方法加强了FL 常规框架的稳健性,以抵御暖动的数据数据框架。 拟议的框架是,在对一个名为 RUL ROL Pro-L Pro-L Pro-L IMAL 的A Airal Freportal Freal Climate Freal Creport Freal Frupal Freal Freal Freal 进行系统测试, 模型的每部模型框架, 。拟议的框架是用于对一个已考虑的FRUL Aircoal-real Creal Creal Creal Creal Creal Creal Creal Creal Creal deal decoal deco decutor 。","Diogo Landau, Ingeborg de Pater, Mihaela Mitici, Nishant Saurabh",2025-05-31T10:32:51Z,Federated learning framework for collaborative remaining useful life   prognostics: an aircraft engine case study,Föderierter Lernrahmen für kollaborative Überlebensprognostik: eine Fallstudie für Flugzeugmotoren,协作剩余使用寿命预测:飞机发动机个案研究的联邦学习框架,http://arxiv.org/abs/2506.00499v1
98,"We address the self-stabilizing exact majority problem in the population protocol model, introduced by Angluin, Aspnes, Diamadi, Fischer, and Peralta (2004). In this model, there are $n$ state machines, called agents, which form a network. At each time step, only two agents interact with each other, and update their states. In the self-stabilizing exact majority problem, each agent has a fixed opinion, $\mathtt{A}$ or $\mathtt{B}$, and stabilizes to a safe configuration in which all agents output the majority opinion from any initial configuration.   In this paper, we show the impossibility of solving the self-stabilizing exact majority problem without knowledge of $n$ in any protocol. We propose a silent self-stabilizing exact majority protocol, which stabilizes within $O(n)$ parallel time in expectation and within $O(n \log n)$ parallel time with high probability, using $O(n)$ states, with knowledge of $n$. Here, a silent protocol means that, after stabilization, the state of each agent does not change. We establish lower bounds, proving that any silent protocol requires $\Omega(n)$ states, $\Omega(n)$ parallel time in expectation, and $\Omega(n \log n)$ parallel time with high probability to reach a safe configuration. Thus, the proposed protocol is time- and space-optimal.","我们解决了安格鲁因、阿斯普内斯、迪亚马迪、菲舍尔和佩拉尔塔(2004年)提出的人口协议模式中的自我稳定多数问题。在这个模式中,有1美元的国家机器,称为代理人,它们组成了一个网络。在每一步中,只有2个代理人相互作用,并更新了各自的状态。在自我稳定多数问题中,每个代理人都有固定的意见,$matht{A}美元或$matht{B}美元,并稳定在一个安全配置中所有代理人从任何初始配置中输出多数意见的安全配置。在这个模式中,我们显示不可能在任何协议中不知晓$n美元的情况下解决自我稳定多数问题。我们建议一个静态的自我稳定多数协议,在预期中稳定在$(n)美元(n)美元(log n)美元(r)美元(r)的平行时间,使用$(n)美元(n)美元(n)的国家,并了解美元(n)。在这里,一个沉默协议意味着,在稳定后,每个代理人的状态需要美元(n)时间(n)的概率(r)不改变。","Haruki Kanaya, Ryota Eguchi, Taisho Sasada, Fukuhito Ooshita, Michiko Inoue",2025-05-31T06:59:23Z,Time- and Space-Optimal Silent Self-Stabilizing Exact Majority in   Population Protocols,Zeit- und Raumoptimale Stille Selbststabilisierung Exakte Mehrheit in Bevölkerungsprotokollen,《人口议定书》中的时间和空间-最佳气候、自我稳定、具体多数,http://arxiv.org/abs/2503.17652v2
99,"Modern software systems face increasing runtime performance demands, particularly in emerging architectures like far memory, where local-memory misses incur significant latency. While machine learning (ML) has proven effective in offline systems optimization, its application to high-frequency, runtime-level problems remains limited due to strict performance, generalization, and integration constraints. We present FarSight, a Linux-based far-memory system that leverages deep learning (DL) to efficiently perform accurate data prefetching. FarSight separates application semantics from runtime memory layout, allowing offline-trained DL models to predict access patterns using a compact vocabulary of ordinal possibilities, resolved at runtime through lightweight mapping structures. By combining asynchronous inference, lookahead prediction, and a cache-resident DL model, FarSight achieves high prediction accuracy with low runtime overhead. Our evaluation of FarSight on four data-intensive workloads shows that it outperforms the state-of-the-art far-memory system by up to 3.6 times. Overall, this work demonstrates the feasibility and advantages of applying modern ML techniques to complex, performance-critical software runtime problems.","现代软件系统面临越来越多的运行时间性能要求,特别是在远记忆等新兴结构中,当地-模拟失灵率很高。虽然机器学习(ML)在离线系统优化方面证明是有效的,但由于严格的性能、一般化和集成限制,其对高频、运行时间问题的应用仍然有限。我们展示了FarSight,一个基于Linux的远模系统,它利用深入学习(DL)来高效地完成准确的数据前拉伸。远视觉将应用程序的语义与运行时记忆布局分开,允许经过离线训练的DL模型使用精密的词汇或极有可能的词汇预测访问模式,在运行时通过轻量的绘图结构加以解决。通过将非同步的推断、长头预测和缓存的DL模型结合起来,FarSight能够以低运行时间的间接费用实现高预测准确性。我们对四大数据密集型工作量的FarSight的评估显示,它比最先进的远智能系统快到3.6倍的时间,这证明了将现代ML软件应用到复杂的可行性和优势。","Yutong Huang, Zhiyuan Guo, Yiying Zhang",2025-05-31T04:27:22Z,Deep-Learning-Driven Prefetching for Far Memory,Deep-Learning-Driven Prefetching für Fernes Gedächtnis,远记忆深深学习开发预展,http://arxiv.org/abs/2506.00384v1
100,"Many large enterprises that operate highly governed and complex ICT environments have no efficient and effective way to support their Data and AI teams in rapidly spinning up and tearing down self-service data and compute infrastructure, to experiment with new data analytic tools, and deploy data products into operational use. This paper proposes a key piece of the solution to the overall problem, in the form of an on-demand self-service data-platform infrastructure to empower de-centralised data teams to build data products on top of centralised templates, policies and governance. The core innovation is an efficient method to leverage immutable container operating systems and infrastructure-as-code methodologies for creating, from scratch, vendor-neutral and short-lived Kubernetes clusters on-premises and in any cloud environment. Our proposed approach can serve as a repeatable, portable and cost-efficient alternative or complement to commercial Platform-as-a-Service (PaaS) offerings, and this is particularly important in supporting interoperability in complex data mesh environments with a mix of modern and legacy compute infrastructure.","许多经营高度受管治和复杂的信通技术环境的大型企业没有切实有效的方法支持其数据和AI小组迅速转换和摧毁自助数据并计算基础设施,试验新的数据分析工具,将数据产品投入实际使用,本文件提出了解决总体问题的一个关键部分,其形式为即时自用自用数据平台基础设施,以授权分散的数据小组在中央化模板、政策和治理的基础上建立数据产品;核心创新是一种高效方法,用以利用不可改变的集装箱操作系统和基础设施编码方法,从零星、供应商中性和寿命短寿命的库伯涅茨集群到任何云层环境中和任何云层环境中。我们提议的办法可以作为商业平台-服务(Pa-S)的重复性、可移植和成本效益高的替代或补充,这对于支持复杂数据网状环境中的互操作性以及现代和遗留的基础设施组合特别重要。","Chinkit Patel, Kee Siong Ng",2025-05-31T02:30:22Z,Enabling Secure and Ephemeral AI Workloads in Data Mesh Environments,Sichere und ephemere KI-Workloads in Data Mesh-Umgebungen aktivieren,数据网状环境中的安全和短期 AI 工作负荷,http://arxiv.org/abs/2506.00352v1
101,"Implementing correct distributed systems is an error-prone task. Runtime Verification (RV) offers a lightweight formal method to improve reliability by monitoring system executions against correctness properties. However, applying RV in distributed settings - where no process has global knowledge - poses fundamental challenges, particularly under full asynchrony and fault tolerance. This paper addresses the Distributed Runtime Verification (DRV) problem under such conditions. In our model, each process in a distributed monitor receives a fragment of the input word describing system behavior and must decide whether this word belongs to the language representing the correctness property being verified. Hence, the goal is to decide languages in a distributed fault-tolerant manner. We propose several decidability definitions, study the relations among them, and prove possibility and impossibility results. One of our main results is a characterization of the correctness properties that can be decided asynchronously. Remarkably, it applies to any language decidability definition. Intuitively, the characterization is that only properties with no real-time order constraints can be decided in asynchronous fault-tolerant settings. These results expose the expressive limits of DRV in realistic systems, as several properties of practical interest rely on reasoning about real-time order of events in executions. To overcome these limitations, we introduce a weaker model where the system under inspection is verified indirectly. Under this weaker model we define predictive decidability, a decidability definition that turn some real-time sensitive correctness properties verifiable. Our framework unifies and extends existing DRV theory and sharpens the boundary of runtime monitorability under different assumptions.","执行正确的分布式系统是一个容易出错的任务。 运行时核查( RV) 提供了一个轻量级的正式方法, 用来通过监测系统执行是否正确性能来提高可靠性。 但是, 在分布式环境中( 没有任何程序具备全球知识)应用 RV 带来了根本性的挑战, 特别是在完全不同步和差分容忍的情况下。 本文涉及在这种条件下分配的运行时核查( DRV) 问题。 在我们的模型中, 分布式监视器的每个程序都接收一个描述系统行为的输入词的碎片, 并且必须决定这个词是否属于代表正在核实的正确性属性的语言。 因此, 目标是以分布式的不正确性能方式决定语言。 我们提出了几种默认性定义, 研究它们之间的关系, 并证明存在可能性和不可能的结果。 我们的主要结果之一是描述在这样的条件下, 分布式的分布式核查( DRV) 正确性能特性的正确性能特性被描述为任何语言的变异性定义。 直观的模型是, 只有没有实时约束的特性, 才能决定不精确性框架的边界环境。 因此, 我们的不精确性环境的不精确性会暴露地决定。 这些结果会暴露的精确性判断系统在现实性规则下, 在现实的精确性评估中, 下, 我们的精确性能的精确性能上, 的精确性判断下, 我们的精确性能的精确性能的精确性能的精确性判断性 。","Armando Castañeda, Gilde Valeria Rodríguez",2025-05-31T00:07:11Z,Asynchronous Fault-Tolerant Language Decidability for Runtime   Verification of Distributed Systems,Asynchrone Fehler-Tolerante Sprachentscheidung für die Laufzeitverifizierung von verteilten Systemen,分布式系统运行时核查的 Al- 同步错失容忍语言,http://arxiv.org/abs/2502.00191v3
102,"Large language models (LLMs) have proven to be very capable, but access to frontier models currently relies on inference providers. This introduces trust challenges: how can we be sure that the provider is using the model configuration they claim? We propose TOPLOC, a novel method for verifiable inference that addresses this problem. TOPLOC leverages a compact locality-sensitive hashing mechanism for intermediate activations, which can detect unauthorized modifications to models, prompts, or precision with 100% accuracy, achieving no false positives or negatives in our empirical evaluations. Our approach is robust across diverse hardware configurations, GPU types, and algebraic reorderings, which allows for validation speeds significantly faster than the original inference. By introducing a polynomial encoding scheme, TOPLOC minimizes the memory overhead of the generated proofs by $1000\times$, requiring only 258 bytes of storage per 32 new tokens, compared to the 262 KB requirement of storing the token embeddings directly for Llama 3.1-8B-Instruct. Our method empowers users to verify LLM inference computations efficiently, fostering greater trust and transparency in open ecosystems and laying a foundation for decentralized, verifiable and trustless AI services.","大型语言模型(LLMS)已被证明非常有能力,但目前对前沿模型的访问取决于推断提供者。 这带来了信任的挑战: 我们如何能确定提供者使用他们声称的模型配置? 我们提出TOIPOLC,这是解决这一问题的可核查推论新办法。 TOPOLC利用一个对地点敏感的紧凑散列机制来进行中间引爆,它可以以100%的准确度探测对模型、提示或精确度的未经授权的修改、提示或精确度,在我们的实验性评估中没有虚假的正数或负数。 我们的方法在各种硬件配置、 GPU 类型和代数重新排序中都很健全,使得验证速度大大快于最初的推断。 我们的方法通过引入一个多数值编码计划,将生成证据的记忆管理量最小化为1 000美元,每32个新代号只需要258字的存储量,而262KB要求直接存储Llama 3.1-8B-Instrustruct。 我们的方法使用户能够核查LLM Inference 和透明化基础的开放性,促进更大的信任和透明。","Jack Min Ong, Matthew Di Ferrante, Aaron Pazdera, Ryan Garner, Sami Jaghouar, Manveer Basra, Max Ryabinin, Johannes Hagemann",2025-05-30T23:07:40Z,TOPLOC: A Locality Sensitive Hashing Scheme for Trustless Verifiable   Inference,TOPLOC: Ein lokales Sensitiv-Hashing-Schema für vertrauenslose überprüfbare Schlussfolgerungen,TOPLC:无信托可核实推断的当地敏感散列计划,http://arxiv.org/abs/2501.16007v2
103,"Effective resource utilization and decreased makespan in heterogeneous High Performance Computing (HPC) environments are key benefits of workload mapping and scheduling. Tools such as Snakemake, a workflow management solution, employ Integer Linear Programming (ILP) and heuristic techniques to deploy workflows in various HPC environments like SLURM (Simple Linux Utility for Resource Management) or Kubernetes. Its scheduler factors in workflow task dependencies, resource requirements, and individual task data sizes before system deployment. ILP offers optimal solutions respecting constraints, but only for smaller workflows. Meanwhile, meta-heuristics and heuristics offer faster, though suboptimal, makespan. As problem sizes, system constraints, and complexities evolve, maintaining these schedulers becomes challenging. In this study, we propose a novel solution that integrates Graph Neural Network (GNN) and Reinforcement Learning (RL) to flexibly handle workflows, dynamic constraints, and heterogeneous resources while providing quick responses. GNN manages dependencies and resource requirements, and RL optimizes scheduling decision-making via a learned policy, overcoming the need for a comprehensive global search. Experimental results with different datasets demonstrate that this method effectively adapts to different workflows, adheres to HPC constraints, and offers optimal solutions akin to ILP but with drastically reduced execution times (76 percent faster), comparable to heuristic methods (only 3.85 times slower than OLB). Our contribution is to provide a robust yet scalable mapping and scheduling solution that can handle changing constraints, as well as workload sizes and complexities in a heterogeneous HPC Compute Continuum system landscape.","高效的资源利用和在各种高性能计算(HPC)环境中的高效资源利用和减少是工作量绘图和时间安排的关键好处。 Snakmake、工作流程管理解决方案、采用Integer Linear 编程(ILP)和超常技术等工具,以便在各种高常方案环境中部署工作流程,如SLURM(Soper Linux 资源管理工具)或Kubernetes。其工作流程依赖性、所需资源和系统部署前单个任务数据大小的排程因素。 ILP为制约提供了最佳解决方案,但只适用于较小的工作流程。与此同时,超常量和超常量管理提供了更快的解决方案。随着问题规模、系统制约和复杂程度的演变,超常提供更快的流程管理,随着问题的规模变化和超常的系统化,维持这些排程技术变得困难重重。在本研究中,我们提出了一个新的解决方案,将SGNUR网络(GNN)和加固学习(RL)纳入灵活处理工作流程、动态制约和混杂资源,同时提供较慢的反应。 GLNNNM管理可靠性和资源需求,同时通过学习政策优化决策安排决策安排,尽管不精准,但更优性系统化的系统进行更优化的系统化的系统化的流程要求。 克服可变缩缩缩缩缩缩缩缩缩化的流程的流程要求,从而显示不同的数据流压缩缩压压压压压式的流程的流程,为不同的流程的流程的流程为不同的流程,从而展示的流程,从而展示的流程向不同的流程可以向不同的流程向不同的流程。","Aasish Kumar Sharma, Julian Kunkel",2025-05-30T21:50:28Z,GrapheonRL: A Graph Neural Network and Reinforcement Learning Framework   for Constraint and Data-Aware Workflow Mapping and Scheduling in   Heterogeneous HPC Systems,GrapheonRL: Graph Neural Network and Reinforcement Learning Framework for Constraint and Data-Aware Workflow Mapping and Scheduling in Heterogenous HPC Systems,GrapheonRL: 用于限制和数据-软件工作流量绘图和不同多源高PC系统中的排程的图表神经网络和强化学习框架,http://arxiv.org/abs/2506.00260v1
104,"The widespread adoption of cloud-based proprietary large language models (LLMs) has introduced significant challenges, including operational dependencies, privacy concerns, and the necessity of continuous internet connectivity. In this work, we introduce an LLMOps pipeline, ""LlamaDuo"", for the seamless migration of knowledge and abilities from service-oriented LLMs to smaller, locally manageable models. This pipeline is crucial for ensuring service continuity in the presence of operational failures, strict privacy policies, or offline requirements. Our LlamaDuo involves fine-tuning a small language model against the service LLM using a synthetic dataset generated by the latter. If the performance of the fine-tuned model falls short of expectations, it is automatically improved through additional fine-tuning using extra similar data generated by the service LLM. This multi-turn process guarantees that the smaller model can eventually match or even surpass the service LLM's capabilities in specific downstream tasks, offering a practical and scalable solution for managing AI deployments in constrained environments. Extensive experiments with leading-edge LLMs are conducted to demonstrate the effectiveness, adaptability, and affordability of LlamaDuo across various downstream tasks. Our pipeline implementation is available at https://github.com/deep-diver/llamaduo.","广泛采用基于云的专有性大型语言模型(LLMS)带来了重大挑战,包括操作依赖性、隐私问题和持续互联网连接的必要性。在这项工作中,我们引入了LLMOps管道“LlamaDuo”,用于将知识和能力从服务导向的LLMS无缝地从服务导向的LLMS迁移到较小的、本地可操作的模式。这一管道对于确保服务在运行失败、严格的隐私政策或离线要求情况下的连续性至关重要。我们的LlamaDuu 使用由后者产生的合成数据集对服务LLMM小语言模型进行微调。如果微调模型的性能达不到预期,则通过使用服务LLMM产生的额外类似数据进行进一步的微调而自动改进。这一多转过程保证了小型模型最终能够匹配甚至超过服务LM在具体的下游任务方面的能力,为管理受限制环境中的AI部署提供了实用和可扩展的解决办法。与领先的LMMS进行广泛的实验,以展示LlamaDvo公司在各种下游任务中的有效性、适应性和可承受性。我们的输管可在http://s/dqubqual/s/s。","Chansung Park, Juyong Jiang, Fan Wang, Sayak Paul, Jing Tang",2025-05-30T17:53:07Z,LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to   Small-Scale Local LLMs,LlamaDuo: LLMOps-Pipeline für nahtlose Migration von Service-LLMs zu kleinen lokalen LLMs,LlamaDuo:无缝移徙从服务LLMs到小型地方LMs的LLMOps管道,http://arxiv.org/abs/2408.13467v3
105,"3D Gaussian Splatting (3DGS) renders pixels by rasterizing Gaussian primitives, where conditional alpha-blending dominates the time cost in the rendering pipeline. This paper proposes TC-GS, an algorithm-independent universal module that expands Tensor Core (TCU) applicability for 3DGS, leading to substantial speedups and seamless integration into existing 3DGS optimization frameworks. The key innovation lies in mapping alpha computation to matrix multiplication, fully utilizing otherwise idle TCUs in existing 3DGS implementations. TC-GS provides plug-and-play acceleration for existing top-tier acceleration algorithms tightly coupled with rendering pipeline designs, like Gaussian compression and redundancy elimination algorithms. Additionally, we introduce a global-to-local coordinate transformation to mitigate rounding errors from quadratic terms of pixel coordinates caused by Tensor Core half-precision computation. Extensive experiments demonstrate that our method maintains rendering quality while providing an additional 2.18x speedup over existing Gaussian acceleration algorithms, thus reaching up to a total 5.6x acceleration. The code is currently available at anonymous \href{https://github.com/TensorCore3DGS/3DGSTensorCore}","3D Gaussian Splatting (3DGS) 通过对 Gaussian 原始元素进行光化处理, 使像素像素化, 有条件的甲型混合法在管道铺设时间成本中占据主导地位。 本文建议 TC- GS , 是一个扩展 Tansor Core 应用 3DGS 3D Gaussian Splatting (TCGS) 的自算通用模块, 扩大 Tensor Core (TCCUS) 3DGS 的可应用性, 导致大量加速和无缝地融入现有的 3DGS 优化框架。 关键创新在于将阿尔法计算成矩阵倍增殖, 充分利用现有的 3DGS 执行过程中的闲置 TCUS 。 TC- GS 为现有的顶级加速算法提供插接和播放加速率加速, 并同时提供管道设计, 如高斯压缩和冗余清除算法 。 此外, 我们可在 STDGDGM/ 3/Cofor 进行全球 3 DGAGAGASG/3 3 上查到该代码。","Zimu Liao, Jifeng Ding, Rong Fu, Siwei Cui, Ruixuan Gong, Li Wang, Boni Hu, Yi Wang, Hengjie Li, XIngcheng Zhang, Hui Wang",2025-05-30T16:58:18Z,TC-GS: A Faster Gaussian Splatting Module Utilizing Tensor Cores,TC-GS: Ein schnelleres Gaussian Splatting Modul zur Verwendung von Tensorkernen,"TC-GS:一个更快的高山喷洒模块,利用天线核心",http://arxiv.org/abs/2505.24796v1
106,"Graph databases have become essential tools for managing complex and interconnected data, which is common in areas like social networks, bioinformatics, and recommendation systems. Unlike traditional relational databases, graph databases offer a more natural way to model and query intricate relationships, making them particularly effective for applications that demand flexibility and efficiency in handling interconnected data.   Despite their increasing use, graph databases face notable challenges. One significant issue is the irregular nature of graph data, often marked by structural sparsity, such as in its adjacency matrix representation, which can lead to inefficiencies in data read and write operations. Other obstacles include the high computational demands of traversal-based queries, especially within large-scale networks, and complexities in managing transactions in distributed graph environments. Additionally, the reliance on traditional centralized architectures limits the scalability of Online Transaction Processing (OLTP), creating bottlenecks due to contention, CPU overhead, and network bandwidth constraints.   This paper presents a thorough survey of graph databases. It begins by examining property models, query languages, and storage architectures, outlining the foundational aspects that users and developers typically engage with. Following this, it provides a detailed analysis of recent advancements in graph database technologies, evaluating these in the context of key aspects such as architecture, deployment, usage, and development, which collectively define the capabilities of graph database solutions.","图表数据库已成为管理复杂和相互关联的数据的基本工具,这些数据在社会网络、生物信息学和建议系统等领域是常见的。与传统的关联数据库不同,图表数据库提供了一种更自然的模型和查询复杂关系的方法,使这些数据库对要求灵活和高效处理相互关联数据的应用程序特别有效。尽管使用量日益增加,图表数据库面临显著的挑战。一个重要问题是图表数据不规则的性质,其特点是结构过于分散,例如其相近矩阵表征,这可能导致数据读写操作效率低下。其他障碍包括基于跨轨查询的计算需求很高,特别是在大型网络内部,以及管理分布式图表环境中交易的复杂性。此外,依赖传统的中央结构限制了在线交易处理(OLTP)的可缩放性,造成争议、CPU间接费用和网络带宽限制等瓶颈。本文对图表数据库进行了彻底调查,首先是研究财产模型、查询语言和存储结构,概述用户和开发商通常参与的基本方面。随后,它详细分析了最新进展的中央结构,从而界定了关键图表数据库中的关键部署能力,评估了这些方面。","Miguel E. Coimbra, Lucie Svitáková, Alexandre P. Francisco, Luís Veiga",2025-05-30T16:18:58Z,Survey: Graph Databases,Erhebung: Graphische Datenbanken,调查:图表数据库,http://arxiv.org/abs/2505.24758v1
107,"Federated Learning (FL) is a promising paradigm for realizing edge intelligence, allowing collaborative learning among distributed edge devices by sharing models instead of raw data. However, the shared models are often assumed to be ideal, which would be inevitably violated in practice due to various perturbations, leading to significant performance degradation. To overcome this challenge, we propose a novel method, termed Sharpness-Aware Minimization-based Robust Federated Learning (SMRFL), which aims to improve model robustness against perturbations by exploring the geometrical property of the model landscape. Specifically, SMRFL solves a min-max optimization problem that promotes model convergence towards a flat minimum by minimizing the maximum loss within a neighborhood of the model parameters. In this way, model sensitivity to perturbations is reduced, and robustness is enhanced since models in the neighborhood of the flat minimum also enjoy low loss values. The theoretical result proves that SMRFL can converge at the same rate as FL without perturbations. Extensive experimental results show that SMRFL significantly enhances robustness against perturbations compared to three baseline methods on two real-world datasets under three perturbation scenarios.","联邦学习(FL)是实现边缘智能的一个很有希望的范例,通过共享模型而不是原始数据,允许分布边缘设备之间通过共享模型进行协作学习;然而,通常认为共享模型是理想的,由于各种扰动,在实际中不可避免地会由于各种干扰而违反,从而导致显著的性能退化;为克服这一挑战,我们提议了一种新颖的方法,称为Sharpness-Aware最小化-基于硬质的联邦学习(SMRFL),其目的是通过探索模型景观的几何属性来改进模型对扰动的强度。具体地说,SMRFL解决了一个微最大优化问题,通过在模型参数附近尽可能减少最大损失,促进模型趋同到最小的最小值。这样,模型对扰动的敏感度就会降低,并且由于在平面最小值附近的模型也享有低的损失值。理论结果证明,SMRFL可以在不受扰动的情况下与FL相同的速度趋同。广泛的实验结果表明,SMRFL在三次的假设下,与两个实际世界数据设置的三种基线方法相比,大大加强了对扰动的强度。","Dongzi Jin, Yong Xiao, Yingyu Li",2025-05-30T15:52:05Z,Robust Federated Learning against Model Perturbation in Edge Networks,Robustes Federated Learning gegen Modellstörungen in Edge Networks,"在边缘网络中开展强有力的联邦学习,防止模型扰动",http://arxiv.org/abs/2505.24728v1
108,"The Computing Continuum (CC) is an emerging Internet-based computing paradigm that spans from local Internet of Things sensors and constrained edge devices to large-scale cloud data centers. Its goal is to orchestrate a vast array of diverse and distributed computing resources to support the next generation of Internet-based applications. However, the distributed, heterogeneous, and dynamic nature of CC platforms demands distributed intelligence for adaptive and resilient service management. This article introduces a distributed stream processing pipeline as a CC use case, where each service is managed by an Active Inference (AIF) agent. These agents collaborate to fulfill service needs specified by SLOiDs, a term we introduce to denote Service Level Objectives that are aware of its deployed devices, meaning that non-functional requirements must consider the characteristics of the hosting device. We demonstrate how AIF agents can be modeled and deployed alongside distributed services to manage them autonomously. Our experiments show that AIF agents achieve over 90% SLOiD fulfillment when using tested transition models, and around 80% when learning the models during deployment. We compare their performance to a multi-agent reinforcement learning algorithm, finding that while both approaches yield similar results, MARL requires extensive training, whereas AIF agents can operate effectively from the start. Additionally, we evaluate the behavior of AIF agents in offloading scenarios, observing a strong capacity for adaptation. Finally, we outline key research directions to advance AIF integration in CC platforms.","Econtinuum (CC) 是一个新兴的基于互联网的计算模式,它从当地互联网的事物传感器和受限边缘装置传感器和受限边缘装置到大型云层数据中心,从当地互联网到大型云层数据中心,其宗旨是安排各种各样的分布式和分布式计算资源,以支持下一代基于互联网的应用;然而,CC平台的分布式、多样性和动态性质要求分配适应性和弹性服务管理情报。本篇文章将分布式流处理管道作为CC使用案例,其中每项服务都由积极的推断(AIF)代理商管理。这些代理商合作满足SLOIDs规定的服务需求,这是我们介绍的术语,用来表示服务级目标了解其部署装置,这意味着非功能性要求必须考虑托管装置的特性。我们展示了如何将AIFA代理商的模型模型模型模型模型模型模型模型模型模型模型和大约80%的功能。我们将其业绩与多代理商强化学习算法进行比较,发现两种方法都能够产生类似的结果,这意味着非功能性要求考虑托管平台的特性特性特性特性。 我们要求对AIFFA的代理人进行广泛的演化,而后,我们从ARIFIFA的模型的模型的模型进行广泛的演算。","Victor Casamayor Pujol, Boris Sedlak, Tommaso Salvatori, Karl Friston, Schahram Dustdar",2025-05-30T14:10:33Z,Distributed Intelligence in the Computing Continuum with Active   Inference,Verteilte Intelligenz im Computing Continuum mit aktiver Schlussfolgerung,具有主动推断力的计算机连续体中传播的情报,http://arxiv.org/abs/2505.24618v1
109,"In stable matching, one must find a matching between two sets of agents, commonly men and women, or job applicants and job positions. Each agent has a preference ordering over who they want to be matched with. Moreover a matching is said to be stable if no pair of agents prefer each other over their current matching.   We consider solving stable matching in a distributed synchronous setting, where each agent is its own process. Moreover, we assume up to $t_L$ agents on one side and $t_R$ on the other side can be byzantine. After properly defining the stable matching problem in this setting, we study its solvability.   When there are as many agents on each side with fully-ordered preference lists, we give necessary and sufficient conditions for stable matching to be solvable in the synchronous setting. These conditions depend on the communication model used, i.e., if parties on the same side are allowed to communicate directly, and on the presence of a cryptographic setup, i.e., digital signatures.","在稳定匹配中,人们必须找到两组代理人(通常是男男女女)或求职者和工作职位之间的匹配。 每个代理人都优先排序他们想要与谁匹配。 此外,如果没有一对代理人相互偏向于当前匹配,那么匹配据说是稳定的。 我们考虑在分布式同步环境下解决稳定的匹配, 每一个代理人都是自己的过程。 此外, 我们假设一边的代理费用高达$t_L$, 另一边的代理费用为$t_R$。 在正确定义此设置的稳定匹配问题之后, 我们研究其可溶性。 当每边有同样多的代理人配齐了全顺序的优惠名单时, 我们为稳定的匹配提供了必要和充分的条件, 以便在同步环境下可以溶解。 这些条件取决于所使用的通信模式, 也就是说, 如果允许同一边的各方直接沟通, 并且存在加密设置, 即数字签名 。","Andrei Constantinescu, Marc Dufay, Diana Ghinea, Roger Wattenhofer",2025-05-30T13:58:26Z,Byzantine Stable Matching,Byzantinische stabile Übereinstimmung,拜占庭稳定匹配,http://arxiv.org/abs/2502.05889v2
110,"Modern serverless applications, often interactive with highly volatile traffic, challenge system scalability, demanding control planes that deliver low latency and cost efficiency. Analysis of production traces and existing systems reveals that current control plane designs (synchronous and asynchronous), particularly when built on conventional cluster managers like Kubernetes, struggle with this balance, often wasting significant CPU and memory resources on creating underutilized or idle instances. While clean-slate approaches like Dirigent offer performance gains, they sacrifice compatibility with established cluster management ecosystems.   We introduce WaveLink, a serverless system designed to achieve high performance and low cost while maintaining compatibility with conventional cluster managers. WaveLink employs a novel dual-track control plane. A standard asynchronous track manages long-lived, full-featured regular instances for handling predictable, sustainable traffic, preserving full compatibility and feature sets off the critical path. Concurrently, an expedited parallel track addresses excessive traffic bursts that trigger cold starts. This fast path utilizes node-local agents (Wavelets) to rapidly spawn short-lived Emergency Instances with a reduced feature set, critically bypassing the latency overhead of the main cluster manager.   Our experiments demonstrate that WaveLink, while remaining compatible with conventional managers for >98% invocation traffic, achieves 35% faster end-to-end performance at a comparable cost to the incompatible Dirigent system. WaveLink outperforms Kubernetes-compatible systems with synchronous control planes by 1.5-3.5x at 8-21% lower cost, and surpasses asynchronous counterparts by 1.7-3.5x at 3-33% lower cost.","无服务器的现代应用程序,往往与高度动荡的交通、挑战系统可变性、系统可变性、高要求控制机体,能提供低延迟和成本效率。对生产轨迹和现有系统的分析显示,当前的控制机设计(同步和不同步),特别是在Kubernetes等常规组群管理器上,与这种平衡作斗争时,往往浪费大量的CPU和记忆资源,造成利用不足或闲置的事例。虽然Diririgent等清洁的平流方法能够带来绩效收益,但它们牺牲了与既定的集束管理生态系统的兼容性。我们引入了WaveLink,这是一个没有服务器的系统,目的是实现高性能和低成本的低成本。WaveLink使用新型双轨控制机体设计了一个全新的双轨控制机体。一个标准的不同步轨道管理器管理器管理器管理器管理器长期运行、可持续交通、保持完全兼容性和特性的常规运行轨迹。同时,一个快速平行的轨道处理器系利用不易变的本地代理器(Waveletlex)快速生成短期紧急紧急情况,其功能设定了降低成本,紧绕下轨道运行运行运行运行运行运行运行,同时运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行运行,以比比比高的轨道运行运行比高的轨道运行运行比高。","Leonid Kondrashov, Lazar Cvetković, Hancheng Wang, Boxi Zhou, Dhairya Rungta, Dmitrii Ustiugov",2025-05-30T13:00:27Z,Melding the Serverless Control Plane with the Conventional Cluster   Manager for Speed and Compatibility,Verschmelzen des serverlosen Steuerplans mit dem konventionellen Clustermanager für Geschwindigkeit und Kompatibilität,与用于速度和兼容性的常规集管理器管理器熔化无服务器控制平面,http://arxiv.org/abs/2505.24551v1
111,"Emerging AI accelerators increasingly adopt wafer-scale manufacturing technologies, integrating hundreds of thousands of AI cores in a mesh architecture with large distributed on-chip memory (tens of GB in total) and ultra-high on-chip memory bandwidth (tens of PB/s). However, current LLM inference systems, optimized for shared memory architectures like GPUs, fail to exploit these accelerators fully.   We introduce WaferLLM, the first wafer-scale LLM inference system. WaferLLM is guided by a novel PLMR model (pronounced as ""Plummer"") that captures the unique hardware characteristics of wafer-scale architectures. Leveraging this model, WaferLLM pioneers wafer-scale LLM parallelism, optimizing the utilization of hundreds of thousands of on-chip cores. It also introduces MeshGEMM and MeshGEMV, the first GEMM and GEMV implementations designed to scale effectively on wafer-scale accelerators.   Evaluations show that WaferLLM achieves up to 200$\times$ higher accelerator utilization than state-of-the-art methods. Leveraging a wafer-scale accelerator (Cerebras WSE2), WaferLLM delivers GEMV operations 606$\times$ faster and 16$\times$ more energy-efficient than on an NVIDIA A100 GPU. For full LLM inference, WaferLLM achieves 10-20$\times$ speedups over A100 GPU clusters running SGLang and vLLM. These advantages are expected to grow as wafer-scale AI models, software, and hardware continue to mature. WaferLLM is open-sourced at https://github.com/MeshInfra/WaferLLM.","新兴的AI 加速器越来越多地采用卷轴制造技术,将数十万个AI核心纳入一个网状结构,在网状结构中大规模分布在芯状内存(总共10个GB)和超高的芯状内存带宽(10个PB/s)。然而,当前的LLLM推导系统,优化了像GPU这样的共享记忆结构,未能充分利用这些加速器。我们引入了WaferLLM(第一个千瓦氏级LLLM)系统。WaferMLM(将数十万个AI核心纳入网状结构中),将数十万个AI核心纳入其中,将WaferLLLMM(宣布为“平价”)模型,捕捉瓦LMM(宣布为“平价”)模型中独特的硬件特性。WaferLLLLM(将OLM)的运行速度提高到200美元,而GLLM(比GLLLL)的运行速度要快得多。","Congjie He, Yeqi Huang, Pei Mu, Ziming Miao, Jilong Xue, Lingxiao Ma, Fan Yang, Luo Mai",2025-05-30T12:10:19Z,WaferLLM: Large Language Model Inference at Wafer Scale,WaferLLM: Large Language Model Inferenz auf Wafer Scale,WaferLLM:Wafer规模上的大语言模型推断,http://arxiv.org/abs/2502.04563v3
112,"Blockchain protocols incentivize participation through monetary rewards, assuming rational actors behave honestly to maximize their gains. However, attackers may attempt to harm others even at personal cost. These denial of profit attacks aim to reduce the rewards of honest participants, potentially forcing them out of the system. While existing work has largely focused on the profitability of attacks, they often neglect the potential harm inflicted on the victim, which can be significant even when the attacker gains little or nothing.   This paper introduces a framework to quantify denial of profit attacks by measuring both attacker cost and victim loss. We model these attacks as a game and introduce relevant metrics to quantify these attacks. We then focus on committee-based blockchains and model vote collection as a game. We show that in the vote collection game, disincentivizing one denial of profit attack will make another attack more appealing, and therefore, attacks have to be balanced. We apply our framework to analyze real-world reward mechanisms in Ethereum and Cosmos. Our framework reveals imbalances in Cosmos that can make correct behavior suboptimal in practice. While Ethereum provides stronger protections, our framework shows that it is also not complete, and we propose alternative parameter settings to improve the balance between attacks. Our findings highlight the need for better-balanced reward designs to defend against denial of profit attacks.","封锁协议通过货币奖励鼓励参与,假设理性的行为者诚实地行事,以最大限度地获得其收益。然而,袭击者可能试图伤害他人,甚至以个人成本为代价。这些拒绝利润袭击的目的是减少诚实参与者的回报,可能迫使他们退出系统。虽然现有的工作主要侧重于袭击的盈利性,但他们往往忽视对受害者的潜在伤害,即使袭击者只获得很少或一无所获,这种伤害也可能是巨大的。本文提出了一个框架,通过衡量袭击者的成本和受害者损失来量化拒绝利润袭击。我们将这些袭击作为游戏来模拟,并引入相关指标来量化这些袭击。我们然后侧重于基于委员会的连锁和模式的收集选票游戏。我们显示,在收集选票的游戏中,不鼓励一次拒绝利润袭击将使另一次袭击更具吸引力,因此,袭击必须保持平衡。我们运用我们的框架来分析在Etheurum和宇宙中真实世界的奖赏机制。我们的框架揭示了宇宙中的不平衡,可以使行为在实际中变得不完美。我们的框架提供了更强有力的保护,同时我们的框架显示,它也表明,它不是完全的、我们提出更平衡的攻击计划,我们要改善我们的目标定义。","Arian Baloochestani, Leander Jehl",2025-05-30T11:32:47Z,Balancing incentives in committee-based blockchains,Ausgleich von Anreizen in Blockchains auf Ausschussbasis,平衡基于委员会的供应链中的奖励措施,http://arxiv.org/abs/2505.24482v1
113,"Federated fine-tuning for Large Language Models (LLMs) faces significant challenges due to the heavy communication overhead of transmitting large model updates. Although Low Rank Adaptation (LoRA) has been proposed as a solution, yet its application in federated learning is complicated by discordance in aggregation. Existing methods addressing this discordance often suffer from performance degradation at low ranks in heterogeneous data settings. In response, we introduce LoRA-A$^2$ (Low Rank Adaptation with Alternating freeze and Adaptive rank selection), which demonstrates robustness in challenging settings with low ranks and high data heterogeneity. Our experimental findings reveal that LoRA-A$^2$ maintains performance even under extreme heterogeneity and low rank conditions, achieving up to a significant reduction in uploaded parameters compared to full fine-tuning without compromising performance. This adaptive mechanism increases robustness and communication efficiency in federated fine-tuning, enabling the practical deployment of LLMs in resource-constrained environments.","大语言模型(LLMS)的联邦微调面临重大挑战,因为传送大模版更新的通信费用高昂。虽然低级别适应(LORA)已被提出来作为解决办法,但由于在汇总方面的不协调,在联邦学习中的应用更为复杂。解决这种差异的现有方法往往因不同数据环境中的低级别性能退化而受到影响。作为回应,我们引入LORA-A$2(低级别适应替代冻结和调适等级选择),这表明在低级别和高数据差异的富有挑战性环境中,适应能力很强。我们的实验结果表明,LORA-A$2 即使在极端异质和低级别条件下,LORA-A$仍然保持业绩,与完全微调而无损性能相比,上载参数大幅下降。这一适应机制提高了食品微调的稳健性和通信效率,使LMS能够在资源紧张的环境中实际部署。","Jabin Koo, Minwoo Jang, Jungseul Ok",2025-05-30T09:33:11Z,Towards Robust and Efficient Federated Low-Rank Adaptation with   Heterogeneous Clients,Hin zu robuster und effizienter Federated Low-Rank-Anpassung mit heterogenen Kunden,努力与异质客户进行强力和高效的联邦低碳适应,http://arxiv.org/abs/2410.22815v2
114,"To alleviate difficulties in writing smart contracts for distributed blockchain applications, as other research, we propose transformation of Business Process Model and Notation (BPMN) models into blockchain smart contracts. Unlike other research, we use Discrete Event Hierarchical State Machine (DE-HSM) multi-modal modeling to identify collaborative trade transactions that need to be supported by the smart contract and describe how the trade transactions, that may be nested, are supported by a transaction mechanism. We describe algorithms to (i) identify the nested trade transactions and to (ii) transform the BPMN model into blockchains smart contracts that include a transaction mechanism to enforce the transactional properties for the identified trade transactions. The developed proof of concept shows that our approach to automated transformation of BPMN models into smart contracts with the support of privacy and cross-chain interoperability is feasible. The thesis examines and evaluates automatically generated alternative transaction mechanisms to support such transactions using three use cases of varying degree of complexity, namely order processing, supply chain management, and a multi-faceted trade use case. The research enriches the academic dialogue on blockchain technology and smart contracts and proposes potential avenues for future research.","为了减轻在为分布式连锁应用程序签订智能合同方面的困难,作为其他研究,我们提议将业务流程模型和批注(BPMN)模型转换成链式智能合同;与其他研究不同,我们采用分立事件高度国家机器(DE-HSM)多模式模型,以确定需要智能合同支持的合作贸易交易,并描述可能嵌套的贸易交易如何得到交易机制的支持;我们将算法描述为(一) 查明嵌套式贸易交易,和(二) 将BPMN模型转换成包括执行已查明贸易交易交易性质交易机制在内的链式智能合同;我们开发的概念证明表明,我们在隐私和跨链互操作性的支持下,将BPMN模型自动转换为智能合同是可行的;研究研究利用三个复杂程度不同的使用案例,即订单处理、供应链管理和多面贸易使用案例,对支持这类交易自动产生替代交易机制,以利支持此类交易;研究丰富了关于链式技术和智能合同的学术对话,并提出未来研究的潜在途径。",Christian Gang Liu,2025-05-30T07:47:06Z,Supporting Long-term Transactions in Smart Contracts Generated from   Business Process Model and Notation (BPMN) Models,"Unterstützung langfristiger Transaktionen in Smart Contracts, die aus Geschäftsmodellen und Notationsmodellen (BPMN) generiert werden",支持从业务流程模型和标记模型生成的智能合同的长期交易,http://arxiv.org/abs/2505.24309v1
115,"Modern scientific discovery increasingly relies on high-performance computing for complex modeling and simulation. A key challenge in improving parallel program performance is efficiently mapping tasks to processors and data to memory, a process dictated by intricate, low-level system code known as mappers. Developing high-performance mappers demands days of manual tuning, posing a significant barrier for domain scientists without systems expertise. We introduce a framework that automates mapper development with generative optimization, leveraging richer feedback beyond scalar performance metrics. Our approach features the Agent-System Interface, which includes a Domain-Specific Language (DSL) to abstract away the low-level complexity of system code and define a structured search space, as well as AutoGuide, a mechanism that interprets raw execution output into actionable feedback. Unlike traditional reinforcement learning methods such as OpenTuner, which rely solely on scalar feedback, our method finds superior mappers in far fewer iterations. With just 10 iterations, it outperforms OpenTuner even after 1000 iterations, achieving 3.8X faster performance. Our approach finds mappers that surpass expert-written mappers by up to 1.34X speedup across nine benchmarks while reducing tuning time from days to minutes.","现代科学发现日益依赖高性能计算来进行复杂的建模和模拟。 改进平行程序性能的一个关键挑战是高效地绘制处理器和数据到记忆的处理器和数据的工作,这一过程由复杂、低层次的系统代码(即映射器)所决定。 开发高性能绘图师需要数日人工调整,这对没有系统专长的域科学家构成了巨大的障碍。 我们引入了一个框架,使成像开发自动成像,使其具有基因化优化,使更丰富的反馈超过缩微性能度量度尺度。 我们的方法特征是代理系统-系统界面,包括一个DSL(DSL)来抽取系统代码的低度复杂度,并定义结构搜索空间,以及AutoGuide(一个将原始执行输出解释为可操作反馈的机制) 。 与OpenTuner(OpenTuner)等传统的强化学习方法不同, 我们的方法仅依靠缩放反馈, 其发现高级地图师在更小得多的迭。 我们的方法在10次的外, 它比OpenTuster(OnTustry-TultalTustr)更接近于1000次后, 实现3.X更快的功能。 我们的方法从超过专家写地图数日,同时将速度调整到1.34时间调整至1.34时间到1xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx","Anjiang Wei, Allen Nie, Thiago S. F. X. Teixeira, Rohan Yadav, Wonchan Lee, Ke Wang, Alex Aiken",2025-05-30T03:34:55Z,Improving Parallel Program Performance with LLM Optimizers via   Agent-System Interfaces,Verbesserung der parallelen Programmleistung mit LLM-Optimierern über Agent-System-Schnittstellen,通过代理-系统接口改进与LLM优化器的平行方案绩效,http://arxiv.org/abs/2410.15625v4
116,"Serving Large Language Models (LLMs) efficiently in multi-region setups remains a challenge. Due to cost and GPU availability concerns, providers typically deploy LLMs in multiple regions using instance with long-term commitments, like reserved instances or on-premise clusters, which are often underutilized due to their region-local traffic handling and diurnal traffic variance. In this paper, we introduce SkyLB, a locality-aware multi-region load balancer for LLM inference that aggregates regional diurnal patterns through cross-region traffic handling. By doing so, SkyLB enables providers to reserve instances based on expected global demand, rather than peak demand in each individual region. Meanwhile, SkyLB preserves KV-Cache locality and a balanced load, ensuring cost efficiency without sacrificing performance. SkyLB achieves this with a cache-aware cross-region traffic handler and a selective pushing load balancing mechanism based on checking pending requests. Our evaluation on real-world workloads shows that it achieves 1.12-2.06x higher throughput and 1.74-6.30x lower latency compared to existing load balancers, while reducing total serving cost by 25%.","在多区域设置中,高效使用大型语言模型(LLMs)仍是一项挑战。由于成本和GPU的可用性顾虑,供应商通常在多个区域部署LMs,使用具有长期承诺的例子,如保留的情况或预设的集群,因为其区域-当地交通处理和二极分流交通差异,往往利用不足。在本文中,我们引入了SkyLB,这是LM中一个有地方认知的多区域负载平衡器,通过跨区域交通处理将区域二极模式汇总起来。通过这样做,SkyLB使供应商能够根据预期的全球需求而不是每个区域的高峰需求来保留案例。同时,SkyLB保存了KV-Cache地点和平衡的负荷,确保成本效率,同时又不牺牲了性能。SkyLB通过一个缓存的跨区域交通控制器和基于检查待决请求的有选择的推力负载平衡机制实现这一目标。我们对现实世界工作量的评估表明,与现有负载平衡器相比,它实现了1.12-2.06x的吞吐量和1.74-6.x低纬度,同时将总成本降低25%。","Tian Xia, Ziming Mao, Jamison Kerney, Ethan J. Jackson, Zhifei Li, Jiarong Xing, Scott Shenker, Ion Stoica",2025-05-30T00:46:18Z,SkyLB: A Locality-Aware Cross-Region Load Balancer for LLM Inference,SkyLB: Lokalitätsbewusster regionsübergreifender Lastausgleich für LLM-Inferenz,SkyLB: LLM 推理的局部- 软件交叉区域负载平衡器,http://arxiv.org/abs/2505.24095v1
117,"The inevitable presence of data heterogeneity has made federated learning very challenging. There are numerous methods to deal with this issue, such as local regularization, better model fusion techniques, and data sharing. Though effective, they lack a deep understanding of how data heterogeneity can affect the global decision boundary. In this paper, we bridge this gap by performing an experimental analysis of the learned decision boundary using a toy example. Our observations are surprising: (1) we find that the existing methods suffer from forgetting and clients forget the global decision boundary and only learn the perfect local one, and (2) this happens regardless of the initial weights, and clients forget the global decision boundary even starting from pre-trained optimal weights. In this paper, we present FedProj, a federated learning framework that robustly learns the global decision boundary and avoids its forgetting during local training. To achieve better ensemble knowledge fusion, we design a novel server-side ensemble knowledge transfer loss to further calibrate the learned global decision boundary. To alleviate the issue of learned global decision boundary forgetting, we further propose leveraging an episodic memory of average ensemble logits on a public unlabeled dataset to regulate the gradient updates at each step of local training. Experimental results demonstrate that FedProj outperforms state-of-the-art methods by a large margin.","不可避免的数据差异性的存在使得联盟间学习变得非常困难。 有很多方法可以解决这个问题, 比如本地规范化、更好的模型融合技术和数据共享。 虽然这些方法有效,但它们缺乏对数据差异性如何影响全球决策界限的深刻理解。 在本文中,我们通过使用一个玩具的例子对所学决定界限进行实验性分析来弥补这一差距。 我们的观察令人惊讶:(1) 我们发现,现有方法因忘记而受损,客户忘记了全球决策界限,只学会了完美的本地边界,(2) 不论初始重量如何, 客户都忘记了全球决策界限, 甚至从经过预先训练的最佳重量开始。 在本文中,我们介绍FedProj, 是一个能强有力地学习全球决策界限并避免在当地培训中遗忘的联邦化学习框架。 为了更好地实现共同的知识融合,我们设计了一个全新的服务器方知识转移损失,以进一步校准已学的全球决定界限。 为了减轻已学全球决定边界问题,我们进一步建议利用平均水平差值的记忆性记忆, 将每一步的FDP- Morealtial-lagial Adal fortial a ress a pretting a pretting a pretting a press press a press a press a press a maligilgaltiald progregal fal fal fal press a progal fal fal praldaldaldaldaldaldaldaldaldaldaldaldaldaldal uncald praldaldaldaldald un un praldaldaldaldaldaldaldaldald praldaldalds mas undaldaldaldaldaldal aps aps mas mas mas ap apdaldald praldaldaldaldal pral pral pral pral pral pral pral pral pral madaldaldaldaldaldaldaldaldaldaldaldaldal mas ap ma,我们,我们以在","Abhijit Chunduru, Majid Morafah, Mahdi Morafah, Vishnu Pandi Chellapandi, Ang Li",2025-05-29T22:56:25Z,Avoid Forgetting by Preserving Global Knowledge Gradients in Federated   Learning with Non-IID Data,"Vermeiden Sie das Vergessen, indem Sie globale Wissensgradienten im Föderierten Lernen mit nicht-ID-Daten bewahren",避免在使用非二二二维数据进行联邦学习时因保留全球知识进步而被遗忘,http://arxiv.org/abs/2505.20485v2
118,"Fine-tuning large language models (LLMs) with low-rank adaptations (LoRAs) has become common practice, often yielding numerous copies of the same LLM differing only in their LoRA updates. This paradigm presents challenges for systems that serve real-time responses to queries that each involve a different LoRA. Prior works optimize the design of such systems but still require continuous loading and offloading of LoRAs, as it is infeasible to store thousands of LoRAs in GPU memory. To mitigate this issue, we investigate the efficacy of compression when serving LoRAs. We propose a method for the joint compression of LoRAs into a shared basis paired with LoRA-specific scaling matrices. We extend our algorithm to learn clusters of LoRAs that are amenable to joint compression, allowing it to scale gracefully to large LoRA collections. Our experiments with up to 1000 LoRAs demonstrate that compressed LoRAs preserve performance while offering major throughput gains in realistic serving scenarios with over a thousand LoRAs, maintaining 80% of the throughput of serving a single LoRA.","微调大型语言模型(LLMs),其适应程度低(LORAs)已成为司空见惯的做法,经常产生许多相同的LLM(LLMs),其更新LORA时的LLM(LLMs)版本不同。这个范例为那些对每个系统都涉及不同的LORA的询问提供实时答复的系统提出了挑战。先前的工作优化了这些系统的设计,但仍需要不断装入和卸载LORAs,因为将数千个LORA存储在GPU记忆中是行不通的。为了减轻这一问题,我们研究了压缩压缩在为LORA服务时的功效。我们提出了一个方法,将LOLMs联合压缩成一个共享的基础,与LORA特定的缩放矩阵相匹配。我们扩展了我们的算法,以学习适合联合压缩的LORA群集,使其能够优于大型LORA的收藏。我们用1000个LORA的实验显示,压缩LRAs保存了工作绩效,同时在为一千多个实际服务情景提供大量投入,保持80%的耗量。","Rickard Brüel-Gabrielsson, Jiacheng Zhu, Onkar Bhardwaj, Leshem Choshen, Kristjan Greenewald, Mikhail Yurochkin, Justin Solomon",2025-05-29T20:47:12Z,Compress then Serve: Serving Thousands of LoRA Adapters with Little   Overhead,Komprimieren Sie dann Servieren: Tausende von LoRA-Adaptern mit wenig Overhead,"压缩后服务:为成千上万的LORA适应者服务,",http://arxiv.org/abs/2407.00066v4
119,"As large language models (LLMs) become widely used, their environmental impact$\unicode{x2014}$especially carbon emissions$\unicode{x2014}$has attracted more attention. Prior studies focus on compute-related carbon emissions. In this paper, we find that storage is another key contributor. LLM caching, which saves and reuses KV caches for repeated context, reduces operational carbon by avoiding redundant computation. However, this benefit comes at the cost of embodied carbon from high-capacity, high-speed SSDs. As LLMs scale, the embodied carbon of storage grows significantly.   To address this tradeoff, we present EmbAdvisor, a carbon-aware caching framework that selects the optimal cache size for LLM serving. EmbAdvisor profiles different LLM tasks and uses an Integer Linear Programming (ILP) solver to select cache sizes that meet SLOs while minimizing total carbon emissions. Overall, EmbAdvisor reduces the average carbon emissions of a Llama-3 70B model by 9.5% under various carbon intensities compared to a non-adaptive cache scenario, and can save up to 31.2% when the carbon intensity is low.","随着大型语言模型(LLM)被广泛使用,其环境影响$\uncode{x2014}$,特别是碳排放$\uncode{x2014}美元,特别是碳排放$\uncode{x2014}美元,引起了更多的关注。先前的研究侧重于计算相关的碳排放。在本文中,我们发现存储是另一个关键因素。LLM Caching为重复使用而节省和再利用 KV 缓存,通过避免冗余计算来减少操作性碳。然而,这一好处是以高容量、高速SSD的含碳成本为代价的。随着LLLMS规模的扩大,储存的含碳大幅增长。为了应对这一权衡,我们介绍了EmbAdvisor,一个碳意识缓存框架,为LLM服务选择最佳缓存规模。EmbAdvisor描述不同的LM任务,使用Integer Limar 计划(ILP)解答器选择符合SLOs的缓存大小,同时最大限度地减少碳排放总量。总体而言,EmbAdAvisor将Llama-370B的平均碳排放量减少9.5 %,在各种碳密度以下的碳密度下保存9.5至低碳密度。","Yuyang Tian, Desen Sun, Yi Ding, Sihang Liu",2025-05-29T19:52:44Z,EmbAdvisor: Adaptive Cache Management for Sustainable LLM Serving,EmbAdvisor: Adaptives Cache Management für nachhaltiges LLM Serving,执行顾问:可持续LLM服务适应性缓存管理,http://arxiv.org/abs/2505.23970v1
120,"This paper envisions 6G as a self-evolving telecom ecosystem, where AI-driven intelligence enables dynamic adaptation beyond static connectivity. We explore the key enablers of autonomous communication systems, spanning reconfigurable infrastructure, adaptive middleware, and intelligent network functions, alongside multi-agent collaboration for distributed decision-making. We explore how these methodologies align with emerging industrial IoT frameworks, ensuring seamless integration within digital manufacturing processes. Our findings emphasize the potential for improved real-time decision-making, optimizing efficiency, and reducing latency in networked control systems. The discussion addresses ethical challenges, research directions, and standardization efforts, concluding with a technology stack roadmap to guide future developments. By leveraging state-of-the-art 6G network management techniques, this research contributes to the next generation of intelligent automation solutions, bridging the gap between theoretical advancements and real-world industrial applications.","本文设想6G是一个自我演化的电信生态系统,AI驱动的智能使动态适应超越静态连接。我们探索了自主通信系统的关键促进因素,包括可重新配置的基础设施、适应性中器和智能网络功能,以及用于分配决策的多机构协作。我们探讨了这些方法如何与新兴工业互联网框架相协调,确保数字制造流程的无缝整合。我们的调查结果强调改进实时决策、优化效率和减少网络控制系统中的延迟的可能性。讨论涉及道德挑战、研究方向和标准化努力,并用技术堆叠路线图结束指导未来发展。通过利用最新的6G网络管理技术,这一研究有助于下一代智能自动化解决方案,弥合理论进步与现实世界工业应用之间的差距。","Zeinab Nezami, Syed Danial Ali Shah, Maryam Hafeez, Karim Djemame, Syed Ali Raza Zaidi",2025-05-29T17:45:02Z,From Connectivity to Autonomy: The Dawn of Self-Evolving Communication   Systems,Von der Konnektivität zur Autonomie: Die Morgenröte der sich selbst entwickelnden Kommunikationssysteme,从连接到自主:自我发展的通信系统的黎明,http://arxiv.org/abs/2505.23710v1
121,"In connected and autonomous vehicles, machine learning for safety message classification has become critical for detecting malicious or anomalous behavior. However, conventional approaches that rely on centralized data collection or purely local training face limitations due to the large scale, high mobility, and heterogeneous data distributions inherent in inter-vehicle networks. To overcome these challenges, this paper explores Distributed Federated Learning (DFL), whereby vehicles collaboratively train deep learning models by exchanging model updates among one-hop neighbors and propagating models over multiple hops. Using the Vehicular Reference Misbehavior (VeReMi) Extension Dataset, we show that DFL can significantly improve classification accuracy across all vehicles compared to learning strictly with local data. Notably, vehicles with low individual accuracy see substantial accuracy gains through DFL, illustrating the benefit of knowledge sharing across the network. We further show that local training data size and time-varying network connectivity correlate strongly with the model's overall accuracy. We investigate DFL's resilience and vulnerabilities under attacks in multiple domains, namely wireless jamming and training data poisoning attacks. Our results reveal important insights into the vulnerabilities of DFL when confronted with multi-domain attacks, underlining the need for more robust strategies to secure DFL in vehicular networks.","在连接和自主的车辆中,安全信息分类的机器学习对于发现恶意或异常行为至关重要。然而,依赖集中数据收集或纯本地培训的常规方法由于车辆间网络所固有的大规模、高度流动性和分散的数据分配而面临限制。为克服这些挑战,本文件探讨了分布式联邦学习(DFL),即车辆通过在单点邻居之间交换最新消息和在多个跳站上传播模型来合作培训深层次学习模式。我们利用通用参考Misbehavir(VeRemi)扩展数据集,表明DFL能够大大提高所有车辆的分类准确性,而严格使用当地数据来学习。值得注意的是,个人准确度低的车辆通过DFLL看到大量准确性收益,表明整个网络共享知识的好处。我们进一步表明,当地培训数据规模和时间变化式网络连接与模型的总体准确性密切相关。我们调查DFLL在多个领域(即无线干扰和培训数据中毒攻击)受到攻击时的复原力和脆弱性。我们的结果显示,当面对更强点攻击时,DFLL在更稳健的多点攻击战略时,需要对DFLL的弱点有重要了解。我们的结果。","Utku Demir, Yalin E. Sagduyu, Tugba Erpek, Hossein Jafari, Sastry Kompella, Mengran Xue",2025-05-29T17:41:02Z,Distributed Federated Learning for Vehicular Network Security: Anomaly   Detection Benefits and Multi-Domain Attack Threats,Verteiltes Federated Learning für die Sicherheit des Vehicular Network: Anomalieerkennungsvorteile und Multi-Domain-Angriffsbedrohungen,分布式联邦学习促进车辆网络安全:反常探测效益和多领域攻击威胁,http://arxiv.org/abs/2505.23706v1
122,"We introduce a parallel algorithm to construct a preconditioner for solving a large, sparse linear system where the coefficient matrix is a Laplacian matrix (a.k.a., graph Laplacian). Such a linear system arises from applications such as discretization of a partial differential equation, spectral graph partitioning, and learning problems on graphs. The preconditioner belongs to the family of incomplete factorizations and is purely algebraic. Unlike traditional incomplete factorizations, the new method employs randomization to determine whether or not to keep fill-ins, i.e., newly generated nonzero elements during Gaussian elimination. Since the sparsity pattern of the randomized factorization is unknown, computing such a factorization in parallel is extremely challenging, especially on many-core architectures such as GPUs. Our parallel algorithm dynamically computes the dependency among row/column indices of the Laplacian matrix to be factorized and processes the independent indices in parallel. Furthermore, unlike previous approaches, our method requires little pre-processing time. We implemented the parallel algorithm for multi-core CPUs and GPUs, and we compare their performance to other state-of-the-art methods.","我们引入了平行算法来构建一个解决大型、稀疏线性系统的先决条件,即系数矩阵是拉普拉西亚矩阵(a.k.a.a.a.,图Laplacian)的系数矩阵(a.k.a.a.a.a.,图Laplacian),这种线性系统来自部分差异方程式的离散化、光谱图形分割和图形学习问题等应用。这个前提属于不完全系数化的大家庭,是纯代数的。与传统的不完全的系数化不同,新的方法使用随机化来确定是否保持填充,即在高斯消除期间新产生的非零元素。由于随机化系数化的松散模式是未知的,因此平行计算这种系数化极具挑战性,特别是在诸如GPUs等多个核心结构中。我们平行的算法动态地将拉巴拉帕卡矩阵的行/柱性指数之间的依赖性进行系数化并同时处理独立指数。此外,我们的方法需要很少的预处理时间。我们为多核心的CPU和GPU-GPOs采用了平行的平行算算法,我们将其与其他状态进行比较。","Tianyu Liang, Chao Chen, Yotam Yaniv, Hengrui Luo, David Tench, Xiaoye S. Li, Aydin Buluc, James Demmel",2025-05-29T17:19:10Z,Parallel GPU-Accelerated Randomized Construction of Approximate Cholesky   Preconditioners,Parallele GPU-beschleunigte Randomisierte Konstruktion von ungefähren Cholesky-Vorkonditionen,平行的GPU-加速加速旋转式建造近焦天空预设装置,http://arxiv.org/abs/2505.02977v2
123,"We study the self-stabilizing leader election (SS-LE) problem in the population protocol model, assuming exact knowledge of the population size $n$. Burman, Chen, Chen, Doty, Nowak, Severson, and Xu (PODC 2021) showed that this problem can be solved in $O(n)$ expected time with $O(n)$ states. Recently, G\k{a}sieniec, Grodzicki, and Stachowiak (PODC 2025) proved that $n+O(\log n)$ states suffice to achieve $O(n \log n)$ time both in expectation and with high probability (w.h.p.). If substantially more states are available, sublinear time can be achieved. Burman~et~al.~(PODC 2021) presented a $2^{O(n^\rho\log n)}$-state SS-LE protocol with a parameter $\rho$: setting $\rho = \Theta(\log n)$ yields an optimal $O(\log n)$ time both in expectation and w.h.p., while $\rho = \Theta(1)$ results in $O(\rho\,n^{1/(\rho+1)})$ expected time. Very recently, Austin, Berenbrink, Friedetzky, G\""otte, and Hintze (PODC 2025) presented a novel SS-LE protocol parameterized by a positive integer $\rho$ with $1 \le \rho < n/2$ that solves SS-LE in $O(\frac{n}{\rho}\cdot\log n)$ time w.h.p.\ using $2^{O(\rho^2\log n)}$ states. This paper independently presents yet another time--space tradeoff of SS-LE: for any positive integer $\rho$ with $1 \le \rho \le \sqrt{n}$, SS-LE can be achieved within $O\left(\frac{n}{\rho}\cdot \log\rho\right)$ expected time using $2^{2\rho\lg\rho + O(\log n)}$ states. The proposed protocol uses significantly fewer states than the protocol of Austin~et~al.\ requires to achieve any expected stabilization time above $\Theta(\sqrt{n}\log n)$. When $\rho = \Theta\left(\frac{\log n}{\log \log n}\right)$,the proposed protocol is the first to achieve sublinear time while using only polynomially many states. A limitation of our protocol is that the constraint $\rho\le\sqrt{n}$ prevents achieving $o(\sqrt{n}\log n)$ time, whereas the protocol of Austin et~al.\ can surpass this bound.",,Yuichi Sudo,2025-05-29T16:58:51Z,Complementary Time-Space Tradeoff for Self-Stabilizing Leader Election:   Polynomial States Meet Sublinear Time,Komplementärer Zeit-Raum-Tradeoff für selbststabilisierende Leader-Wahl: Polynome Staaten treffen auf sublineare Zeit,自我稳定领导人选举的补充时间-空间权衡:多民族国家满足亚线性时间,http://arxiv.org/abs/2505.23649v1
124,"This paper explores second-order optimization methods in Federated Learning (FL), addressing the critical challenges of slow convergence and the excessive communication rounds required to achieve optimal performance from the global model. While existing surveys in FL primarily focus on challenges related to statistical and device label heterogeneity, as well as privacy and security concerns in first-order FL methods, less attention has been given to the issue of slow model training. This slow training often leads to the need for excessive communication rounds or increased communication costs, particularly when data across clients are highly heterogeneous. In this paper, we examine various FL methods that leverage second-order optimization to accelerate the training process. We provide a comprehensive categorization of state-of-the-art second-order FL methods and compare their performance based on convergence speed, computational cost, memory usage, transmission overhead, and generalization of the global model. Our findings show the potential of incorporating Hessian curvature through second-order optimization into FL and highlight key challenges, such as the efficient utilization of Hessian and its inverse in FL. This work lays the groundwork for future research aimed at developing scalable and efficient federated optimization methods for improving the training of the global model in FL.","本文探讨了联邦学习联合会(FL)的二级优化方法,探讨了缓慢趋同和为达到全球模式最佳业绩所需的过度通信周期等关键挑战。虽然FL的现有调查主要侧重于与统计和装置标签差异有关的挑战,以及一级FL方法的隐私和安全问题,但对模式培训缓慢问题的关注较少。这种缓慢的培训往往导致需要过多的通信回合或增加通信成本,特别是在客户数据高度差异的情况下。我们在本文件中审查了利用第二级优化来加快培训进程的多种FL方法。我们提供了第二级FL方法的全面分类,并根据趋同速度、计算成本、记忆使用、传承间接费用和全球模型的普及,比较其业绩。我们的调查结果显示,通过第二级优化将赫森曲线纳入FL的可能性,并突出了主要挑战,例如赫桑的有效利用及其在FL的反面。这项工作为今后旨在改进FC可升级和高效全球优化方法的示范研究奠定了基础。","Mrinmay Sen, Sidhant R Nair, C Krishna Mohan",2025-05-29T16:00:34Z,Accelerated Training of Federated Learning via Second-Order Methods,Beschleunigte Ausbildung des Föderierten Lernens über Methoden der zweiten Ordnung,通过二级方法加快联邦学习培训,http://arxiv.org/abs/2505.23588v1
125,"In recent years, Large Language Models (LLM) such as ChatGPT, CoPilot, and Gemini have been widely adopted in different areas. As the use of LLMs continues to grow, many efforts have focused on reducing the massive training overheads of these models. But it is the environmental impact of handling user requests to LLMs that is increasingly becoming a concern. Recent studies estimate that the costs of operating LLMs in their inference phase can exceed training costs by 25x per year. As LLMs are queried incessantly, the cumulative carbon footprint for the operational phase has been shown to far exceed the footprint during the training phase. Further, estimates indicate that 500 ml of fresh water is expended for every 20-50 requests to LLMs during inference. To address these important sustainability issues with LLMs, we propose a novel framework called SLIT to co-optimize LLM quality of service (time-to-first token), carbon emissions, water usage, and energy costs. The framework utilizes a machine learning (ML) based metaheuristic to enhance the sustainability of LLM hosting across geo-distributed cloud datacenters. Such a framework will become increasingly vital as LLMs proliferate.","近年来,大语言模型(LLM),如ChatGPT、CoPilot和Gemini等,在不同领域被广泛采用。随着LLMs的使用继续增加,许多努力集中于减少这些模型的大量培训间接费用。但是,处理用户对LLMs的要求对环境的影响日益引起关注。最近的研究估计,在推论阶段操作LMs的成本每年可超过培训成本25x。LMs不断被问及,运行阶段的累积碳足迹显示远远超过培训阶段的足迹。此外,估计表明,在推断过程中,每20-50个LMs提出的LMs申请中,就有500毫升的淡水花费。为了与LMS解决这些重要的可持续性问题,我们提出了一个名为SLIT的新框架,以共同优化LMs服务质量(时间到头等)、碳排放、水使用和能源成本。框架利用基于机器的MLAEuric来提高LM公司在地理分布式云中托管服务的可持续性。这一框架将日益成为至关重要的一个框架。","Hayden Moore, Sirui Qi, Ninad Hogade, Dejan Milojicic, Cullen Bash, Sudeep Pasricha",2025-05-29T15:31:28Z,Sustainable Carbon-Aware and Water-Efficient LLM Scheduling in   Geo-Distributed Cloud Datacenters,Nachhaltiges CO2-basiertes und wassereffizientes LLM-Scheeduling in Geo-verteilten Cloud-Rechenzentren,地球分布云数据中心的可持续碳软件和水效率高的LLM,http://arxiv.org/abs/2505.23554v1
126,"Distributed machine learning workloads use data and tensor parallelism for training and inference, both of which rely on the AllReduce collective to synchronize gradients or activations. However, bulk-synchronous AllReduce algorithms can be delayed by a persistent straggler that is slower to reach the synchronization barrier required to begin the collective. To address this challenge, we propose StragglAR: an AllReduce algorithm that accelerates distributed training and inference in the presence of persistent stragglers. StragglAR implements a ReduceScatter among the remaining GPUs during the straggler-induced delay, and then executes a novel collective algorithm to complete the AllReduce once the straggler reaches the synchronization barrier. StragglAR achieves a 2x theoretical speedup over popular bandwidth-efficient AllReduce algorithms (e.g., Ring) for large GPU clusters with persistent stragglers. On an 8-GPU server, our implementation of StragglAR yields a 22% speedup over state-of-the-art AllReduce algorithms.","分散的机器学习工作量在培训和推论方面使用数据和分解的平行法,两者都依靠 AllReduce 集体组合来同步梯度或激活。 但是, 散装同步的全Reduce 算法可能会被一个持久性的分解器延缓, 而这种分解速度要慢到启动集体所需的同步屏障。 为了应对这一挑战, 我们提议 StragglAR : 一种全Reduce 算法, 加速在持久性排挤者面前的分布式培训和推论。 StragglAR 在 strggler 引发的延缓期间, 在其余的 GPU 中实施一个减少分解器, 然后执行一种新的集体算法, 以在拖动器到达同步屏障后完成全Reduce 。 StragglAR 实现2x理论速度, 超过流行的带宽效率的全Reduce 算法( 如 Ring) 。 在 8- GGPU 服务器上, 我们的 StragglAR 将产生一个超过 22% 的全局-Art- allRuedudes 算法 。","Arjun Devraj, Eric Ding, Abhishek Vijaya Kumar, Robert Kleinberg, Rachee Singh",2025-05-29T15:03:56Z,Accelerating AllReduce with a Persistent Straggler,AllReduce mit einem persistenten Straggler beschleunigen,使用持久性斯特拉格驱动器加速全部拖动,http://arxiv.org/abs/2505.23523v1
127,"The exponential growth of data necessitates distributed storage models, such as peer-to-peer systems and data federations. While distributed storage can reduce costs and increase reliability, the heterogeneity in storage capacity, I/O performance, and failure rates of storage resources makes their efficient use a challenge. Further, node failures are common and can lead to data unavailability and even data loss. Erasure coding is a common resiliency strategy implemented in storage systems to mitigate failures by striping data across storage locations. However, erasure coding is computationally expensive and existing systems do not consider the heterogeneous resources and their varied capacity and performance when placing data chunks. We tackle the challenges of using erasure coding with distributed and heterogeneous nodes, aiming to store as much data as possible, minimize encoding and decoding time, and meeting user-defined reliability requirements for each data item. We propose two new dynamic scheduling algorithms, D-Rex LB and D-Rex SC, that adaptively choose erasure coding parameters and map chunks to heterogeneous nodes. D-Rex SC achieves robust performance for both storage utilization and throughput, at a higher computational cost, while D-Rex LB is faster but with slightly less competitive performance. In addition, we propose two greedy algorithms, GreedyMinStorage and GreedyLeastUsed, that optimize for storage utilization and load balancing, respectively. Our experimental evaluation shows that our dynamic schedulers store, on average, 45% more data items without significantly degrading I/O throughput compared to state-of-the-art algorithms, while GreedyLeastUsed is able to store 21% more data items while also increasing throughput.","数据指数增长需要分布式存储模型,如同行之间的存储系统和数据联合会。虽然分布式存储可以降低成本并增加可靠性,但存储能力、I/O性能和存储资源故障率的差异性使存储资源的高效使用成为一项挑战。此外,节点故障很常见,可能导致数据无法获取甚至数据丢失。偏差是在存储系统中实施的一种常见的弹性战略,目的是通过将数据从各个存储地点剥离数据来减轻故障。然而,压缩编码是计算成本昂贵,而现有系统在放置数据块时不考虑混杂资源及其不同的能力和性能。我们应对挑战,利用分布式和混杂节点对存储能力、I/O性能和性能进行超强的编码,目的是尽可能多存储数据,尽量减少编码和解码时间,满足每个数据项目的用户定义可靠性要求。我们提议了两种新的动态列表算法,即D-Rex LB和D-Rex SSC, 以适应性平衡性参数和地图块分解式的状态。D-RSC在存储利用和通货中都实现了稳度上的稳健性功能,但对比性使用和超度使用,在高度使用和超度使用率的计算中,同时提出高度数据运行成本数据项目,同时提出两种计算成本成本成本成本,同时提出,我们通过两种计算项目显示的计算项目,同时提出更低度数据,同时提出更低价值数据,我们使用。","Maxime Gonthier, Dante D. Sanchez-Gallegos, Haochen Pan, Bogdan Nicolae, Sicheng Zhou, Hai Duc Nguyen, Valerie Hayot-Sasson, J. Gregory Pauloski, Jesus Carretero, Kyle Chard, Ian Foster",2025-05-29T14:43:27Z,D-Rex: Heterogeneity-Aware Reliability Framework and Adaptive Algorithms   for Distributed Storage,D-Rex: Heterogenity-Aware Reliability Framework und adaptive Algorithmen für verteilte Speicherung,D-Rex:多样性-软件可靠性框架和分配储存的适应性比值,http://arxiv.org/abs/2506.02026v1
128,"High-Performance Computing (HPC) job scheduling involves balancing conflicting objectives such as minimizing makespan, reducing wait times, optimizing resource use, and ensuring fairness. Traditional methods, including heuristic-based (e.g., First-Come-First-Served) or intensive optimization techniques, often lack adaptability to dynamic workloads and heterogeneous HPC systems. To address this, we propose a novel Large Language Model (LLM)-based scheduler using a ReAct-style framework (Reason + Act), enabling iterative, interpretable decision-making. The system incorporates a scratchpad memory to track scheduling history and refine decisions via natural language feedback, while a constraint enforcement module ensures feasibility and safety. We evaluate our approach using OpenAI's O4-Mini and Anthropic's Claude 3.7 across seven real-world HPC workload scenarios, including heterogeneous mixes, bursty patterns, and adversarial cases. Comparisons against FCFS, Shortest Job First, and Google OR-Tools (on 10 to 100 jobs) reveal that LLM-based scheduling effectively balances multiple objectives while offering transparent reasoning through natural language traces. The method excels in constraint satisfaction and adapts to diverse workloads without domain-specific training. However, a trade-off between reasoning quality and computational overhead challenges real-time deployment. This work presents the first comprehensive study of reasoning-capable LLMs for HPC scheduling, demonstrating their potential to handle multiobjective optimization while highlighting limitations in computational efficiency. The findings provide insights into leveraging advanced language models for complex scheduling problems in dynamic HPC environments.","为了解决这个问题,我们提议采用“重新行动式框架”(Reason + Act),以新的大语言模型(LLM)为基础,采用“重新行动式框架”(Reason First + Act),使决策具有迭接和可解释性;该系统包含一个“刮痕式记忆”,以跟踪历史和通过自然语言反馈改进决定,而一个“执行限制”模块则确保可行性和安全;我们评估我们采用的方法,在七个现实世界的HPC工作量假设中,使用O4-Mini和Anthropic的Claude 3.7,包括混杂混、动荡模式和对抗性案例;与FCFS、“最短工作一”和Google OR-Tools(10至100个工作)的比较;该系统包含一个“抓痕式记忆”,用以跟踪历史和通过自然语言反馈改进决策,同时通过“先进执行”模块确保可行性和安全性;我们评估我们使用OpenAI的O4-Mini和Anthrodicic's Claude 3.7个“HPC”的工作量假设,包括混合混合混合混合混合组合、暴动模式以及对抗性计算方法,以适应和计算。","Prachi Jadhav, Hongwei Jin, Ewa Deelman, Prasanna Balaprakash",2025-05-29T14:25:29Z,Evaluating the Efficacy of LLM-Based Reasoning for Multiobjective HPC   Job Scheduling,Bewertung der Wirksamkeit von LLM-basierter Begründung für multiobjektive HPC-Arbeitsplanung,评估基于LLM的LLM理由对多重目标HPC工作时间安排的功效,http://arxiv.org/abs/2506.02025v1
129,"Large Language Models (LLMs) are playing a crucial role in latency-critical, high-throughput services like virtual assistants and code generation. While techniques such as continuous batching and paged attention address service-level objectives (SLOs), and quantization methods accelerate inference, the dynamic and efficient adaptation of precision at runtime remains a significant, largely underexplored challenge. The emergence of hardware support for FP8 arithmetic, offering up to 2x the throughput of FP16, presents an attractive opportunity for interactive LLM serving. However, current approaches like co-deploying FP8 and FP16 models suffer from increased storage overhead and fail to unlock FP8's full potential. To address these limitations, we introduce NestedFP, a novel precision-adaptive serving technique enabling seamless FP8 and FP16 inference from a single 16-bit model representation, thereby incurring no additional memory cost. NestedFP decomposes each FP16 weight into two 8-bit components, facilitating efficient FP8 execution while preserving full FP16 accuracy. We demonstrate the practical viability of our approach by implementing a custom CUTLASS-based GEMM kernel that reconstructs FP16 operands on-the-fly, integrated within the vLLM serving framework. Our evaluation shows that NestedFP delivers up to 1.55x throughput improvement in FP8 mode with negligible accuracy degradation compared to FP16 precision, while introducing only 3.9% performance overhead on average in FP16 mode across various models. NestedFP thus provides a flexible foundation for dynamic, SLO-aware precision selection, paving the way for more scalable and efficient LLM serving under bursty and heterogeneous workloads.","大型语言模型(LLMS)在诸如虚拟助理和代码生成等高通量服务中发挥着关键作用。尽管连续分批和按页关注等技术涉及服务级目标(SLOs)和量化方法,加快了推断,但运行时精确度的动态和高效调整仍是一个重大挑战,在很大程度上未得到充分探讨。FP8算术硬件支持的出现,提供了高达2x的FP16输液量,为互动式LM16服务提供了极好的机会。然而,目前的办法,如联合部署FP8和FP16模型,由于储存管理费增加,未能释放FP8的全部潜力。为了应对这些局限性,我们采用了Nested FPS、新颖的精度适应性服务技术,使FC8和FP16能够从单一的16位模型中无缝取回回,从而不增加记忆成本。Nested FP16的每个灵活度都将每个FP16的重量转换成两个8比方,既能高效执行FP16,同时又保持完全的准确性。我们通过S-S-LLLLFDFS的精确度框架,通过定制系统进行实际的升级,为S-S-S-S-S-S-S-S-SLLLFDFDS-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-SL-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-SL-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S","Haeun Lee, Omin Kwon, Yeonhong Park, Jae W. Lee",2025-05-29T11:05:26Z,"NestedFP: High-Performance, Memory-Efficient Dual-Precision Floating   Point Support for LLMs","NestedFP: Leistungsstarke, speichereffiziente Dual-Precision-Schwebepunktunterstützung für LLMs",NestedFP: 高性能、内存-有效双精度浮点支持LLMs,http://arxiv.org/abs/2506.02024v1
130,"As securities trading systems transition to a microservices architecture, optimizing system performance presents challenges such as inefficient resource scheduling and high service response delays. Existing container orchestration platforms lack tailored performance optimization mechanisms for trading scenarios, making it difficult to meet the stringent 50ms response time requirement imposed by exchanges. This paper introduces SealOS+, a Sealos-based performance optimization approach for securities trading, incorporating an adaptive resource scheduling algorithm leveraging deep reinforcement learning, a three-level caching mechanism for trading operations, and a Long Short-Term Memory (LSTM) based load prediction model. Real-world deployment at a securities exchange demonstrates that the optimized system achieves an average CPU utilization of 78\%, reduces transaction response time to 105ms, and reaches a peak processing capacity of 15,000 transactions per second, effectively meeting the rigorous performance and reliability demands of securities trading.","随着证券交易系统向微观服务结构过渡,优化系统绩效带来了诸如资源排期效率低和服务反应迟缓等挑战;现有集装箱管线平台缺乏针对交易情景的定制性优化性性性能机制,难以满足交易所规定的严格的50米反应时间要求;本文介绍了SealOS+(SealOS+)(证券交易基于Sealos的绩效优化性能优化办法),其中包括利用深度强化学习的适应性资源排期算法、3级贸易业务缓冲机制以及基于长期短期内存的负载预测模型;在证券交易所的实时部署表明,优化的系统实现了平均CPU使用78,将交易反应时间减少到105米,达到最高处理能力,即每秒处理15 000次交易,有效满足证券交易的严格业绩和可靠性要求。","Haojie Jia, Zhenhao Li, Gen Li, Minxian Xu, Kejiang Ye",2025-05-29T09:06:01Z,SealOS+: A Sealos-based Approach for Adaptive Resource Optimization   Under Dynamic Workloads for Securities Trading System,SealOS+: Ein Sealos-basierter Ansatz für adaptive Ressourcenoptimierung unter dynamischen Workloads für Securities Trading System,SealOS+:证券交易系统动态工作量下的适应性资源优化的以海路为基础的办法,http://arxiv.org/abs/2505.23258v1
131,"The rapid adoption of generative AI (GenAI), particularly Large Language Models (LLMs), has exposed critical limitations of cloud-centric deployments, including latency, cost, and privacy concerns. Meanwhile, Small Language Models (SLMs) are emerging as viable alternatives for resource-constrained edge environments, though they often lack the capabilities of their larger counterparts. This article explores the potential of collaborative inference systems that leverage both edge and cloud resources to address these challenges. By presenting distinct cooperation strategies alongside practical design principles and experimental insights, we offer actionable guidance for deploying GenAI across the computing continuum.","迅速采用基因化的AI(GenAI),特别是大语言模型(LLM),暴露了云中心部署的关键局限性,包括隐秘性、成本和隐私问题;与此同时,小型语言模型(SLM)正在成为受资源限制的边缘环境的可行替代物,尽管它们往往缺乏较大对应方的能力;这一条探讨了利用边际和云层资源来应对这些挑战的协作推论系统的潜力;通过提出不同的合作战略以及实际设计原则和实验洞察力,我们为在整个计算过程中部署GENAI提供了可行的指导。","Roberto Morabito, SiYoung Jang",2025-05-29T09:04:02Z,"Smaller, Smarter, Closer: The Edge of Collaborative Generative AI","Kleiner, intelligenter, enger: Der Rand der kollaborativen Generativen KI",较小、更聪明、更近:合作创造的边缘 AI,http://arxiv.org/abs/2505.16499v2
132,"Owing to the huge success of generative artificial intelligence (AI), large language models (LLMs) have emerged as a core subclass, underpinning applications such as question answering, text generation, and code completion. While fine-tuning these models on domain-specific data can yield significant performance gains, it also poses daunting computational challenges, especially for researchers and small organizations with limited hardware resources. Although SSD offloading (i.e., ZeRO-Infinity) has emerged as a viable strategy to overcome the GPU memory barrier via leveraging both system memory (i.e., CPU DRAM) and storage space (i.e., solid-state devices, SSDs), its design primarily targets model-centric performance issues. As a result, key system-level issues, including system memory fragmentation, inefficient pinned buffer allocation, peak CPU usage spikes, and file system overhead, remain unaddressed, stifling scalability and inflating costs. Such an observation motivates this paper to introduce MemAscend, a framework that systematically tackles the underexplored system memory bottlenecks in SSD-offloaded LLM training, with a focus on resource-constrained environments. By streamlining pinned-memory allocation, eradicating fragmentation, and mitigating peak overhead, MemAscend reclaims a substantial system memory budget, enabling larger models, longer context windows, and higher batch sizes without exceeding modest hardware limits. Across diverse LLM benchmarks, MemAscend reduces peak system-memory consumption by an average of 55.7% compared with standard SSD offloading techniques, lowering the hardware barrier for fine-tuning and unlocking new possibilities for cost-effective large-scale training on limited-resource machines.","由于基因化人工智能(AI)的巨大成功,大型语言模型(LLMS)已经成为一个核心小类,成为了核心小类,支持了问答、文本生成和代码完成等应用程序。虽然在具体领域数据上对这些模型进行微调可以产生显著的绩效收益,但也给研究人员和硬件资源有限的小型组织带来了巨大的计算挑战。尽管SSD卸载(即ZeRO-Infinity)已成为一项可行的战略,通过利用系统记忆(即,CPU DRA)和存储空间(即,固态设备、SSDSDs),克服了GPU的记忆障碍。 它的设计主要针对以模型为中心的绩效问题。因此,关键系统层面的问题,包括系统记忆破碎、低效率的缓冲分配、CUPUP使用峰值激增、缩缩放成本。 这样的观察促使本文引入了MemASDSBS的精细缩缩缩缩缩缩缩缩缩缩缩缩略缩略缩略微缩略微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩微缩缩缩缩缩缩缩缩缩缩缩缩缩微缩缩缩缩缩缩缩缩缩略缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩缩","Yong-Cheng Liaw, Shuo-Han Chen",2025-05-29T09:00:35Z,MemAscend: System Memory Optimization for SSD-Offloaded LLM Fine-Tuning,MemAscend: Systemspeicheroptimierung für SSD-Offloaded LLM Fine-Tuning,MemAscend: SSD- 卸载 LLM 精密调试的系统内存优化,http://arxiv.org/abs/2505.23254v1
133,"The widespread adoption of Language Models (LMs) across industries is driving interest in deploying these services across the computing continuum, from the cloud to the network edge. This shift aims to reduce costs, lower latency, and improve reliability and privacy. Small Language Models (SLMs), enabled by advances in model compression, are central to this shift, offering a path to on-device inference on resource-constrained edge platforms. This work examines the interplay between edge and cloud deployments, starting from detailed benchmarking of SLM capabilities on single edge devices, and extending to distributed edge clusters. We identify scenarios where edge inference offers comparable performance with lower costs, and others where cloud fallback becomes essential due to limits in scalability or model capacity. Rather than proposing a one-size-fits-all solution, we present platform-level comparisons and design insights for building efficient, adaptive LM inference systems across heterogeneous environments.","跨行业广泛采用语言模型(LMs)正在促使人们有兴趣在从云层到网络边缘的计算连续体中部署这些服务。这一转变旨在降低成本、降低潜伏度、提高可靠性和隐私性。由模型压缩进步促成的小型语言模型(SLMs)是这一转变的核心,为在资源紧缺的边缘平台上进行在线推论提供了一条路径。这项工作从对单一边缘装置的可持续土地管理能力进行详细基准设定开始,到分布边缘集群,审视了边缘和云层部署之间的相互作用。我们确定了边缘推论能够提供成本较低的可比性能,而由于可缩放性或模型能力的限制,云层回退变得至关重要的其他情况。我们提出的平台层面比较和设计洞察力,不是提出一刀切的解决办法,而是在各种环境中建立高效、适应性LM推力系统。","SiYoung Jang, Roberto Morabito",2025-05-29T08:56:27Z,"Edge-First Language Model Inference: Models, Metrics, and Tradeoffs","Edge-First Language Model Inferenz: Modelle, Metrics und Tradeoffs",边缘第一语言模式示范推论:模型、计量和权衡取舍,http://arxiv.org/abs/2505.16508v2
134,"In-situ LLM inference on end-user devices has gained significant interest due to its privacy benefits and reduced dependency on external infrastructure. However, as the decoding process is memory-bandwidth-bound, the diverse processing units in modern end-user devices cannot be fully exploited, resulting in slow LLM inference. This paper presents Ghidorah, a LLM inference system for end-user devices with the unified memory architecture. The key idea of Ghidorah can be summarized in two steps: 1) leveraging speculative decoding approaches to enhance parallelism, and 2) ingeniously distributing workloads across multiple heterogeneous processing units to maximize computing power utilization. Ghidorah includes the hetero-core model parallelism (HCMP) architecture and the architecture-aware profiling (ARCA) approach. The HCMP architecture guides partitioning by leveraging the unified memory design of end-user devices and adapting to the hybrid computational demands of speculative decoding. The ARCA approach is used to determine the optimal speculative strategy and partitioning strategy, balancing acceptance rate with parallel capability to maximize the speedup. Additionally, we optimize sparse computation on ARM CPUs. Experimental results show that Ghidorah can achieve up to 7.6x speedup in the dominant LLM decoding phase compared to the sequential decoding approach in NVIDIA Jetson NX.","Ghidorah的主要概念可以归纳为两步:1)利用投机性解码方法加强平行关系,2)在多个不同处理单位之间巧妙地分配工作量,以最大限度地利用计算能力。Ghidorah包括了高核心模型平行结构(HCMP)和结构质量分析(ARCA)方法。Hidorah结构通过利用终端用户设备统一记忆设计并适应投机解码的混合计算要求来指导分割。ARCA方法用于确定最佳投机性策略和分解战略,平衡接受率和平行能力以最大限度地实现加速使用。此外,我们在GMASER中可以优化对GMASIM的Slational-DRAS级计算结果,在GARSISBSA中可以对GSIM的SLSAS级进行最高级的升级。","Jinhui Wei, Ye Huang, Yuhui Zhou, Jiazhi Jiang, Jiangsu Du",2025-05-29T08:03:43Z,Ghidorah: Fast LLM Inference on Edge with Speculative Decoding and   Hetero-Core Parallelism,Ghidorah: Schnelle LLM-Inferenz am Rand mit spekulativer Dekodierung und Hetero-Core-Parallelität,Ghidorah:快速LLM,http://arxiv.org/abs/2505.23219v1
135,"To improve the training efficiency of federated learning (FL), previous research has employed low-rank decomposition techniques to reduce communication overhead. In this paper, we seek to enhance the performance of these low-rank decomposition methods. Specifically, we focus on three key issues related to decomposition in FL: what to decompose, how to decompose, and how to aggregate. Subsequently, we introduce three novel techniques: Model Update Decomposition (MUD), Block-wise Kronecker Decomposition (BKD), and Aggregation-Aware Decomposition (AAD), each targeting a specific issue. These techniques are complementary and can be applied simultaneously to achieve optimal performance. Additionally, we provide a rigorous theoretical analysis to ensure the convergence of the proposed MUD. Extensive experimental results show that our approach achieves faster convergence and superior accuracy compared to relevant baseline methods. The code is available at https://github.com/Leopold1423/fedmud-icml25.","为了提高联邦学习的培训效率,先前的研究采用了低级分解技术,以减少通信管理费用。在本文中,我们力求提高这些低级分解方法的绩效。具体地说,我们侧重于与FL分解有关的三个关键问题:分解什么,如何分解,如何分解,以及如何综合。随后,我们引入了三种新颖技术:模范更新分解技术(MUD),布洛克-中克罗内克分解技术(BKD),以及聚合-Aware分解技术(AAAD),这些技术都是针对一个具体问题的。这些技术是相辅相成的,可以同时应用,以实现最佳绩效。此外,我们提供了严格的理论分析,以确保拟议的MUD的趋同。广泛的实验结果表明,我们的方法与相关的基线方法相比,更快地趋同和更加精确。该代码可在https://github.com/Leopold1423Fedmud-icml25上查阅。","Shiwei Li, Xiandi Luo, Haozhao Wang, Xing Tang, Shijie Xu, Weihong Luo, Yuhua Li, Xiuqiang He, Ruixuan Li",2025-05-29T07:14:32Z,The Panaceas for Improving Low-Rank Decomposition in   Communication-Efficient Federated Learning,Die Panaceas zur Verbesserung der Zersetzung mit geringem Rank im kommunikativ-effizienten Federated Learning,改善通信-高效联邦学习中低-兰克分解的全景,http://arxiv.org/abs/2505.23176v1
136,"We study the problem of assigning operations in a dataflow graph to devices to minimize execution time in a work-conserving system, with emphasis on complex machine learning workloads. Prior learning-based methods often struggle due to three key limitations: (1) reliance on bulk-synchronous systems like TensorFlow, which under-utilize devices due to barrier synchronization; (2) lack of awareness of the scheduling mechanism of underlying systems when designing learning-based methods; and (3) exclusive dependence on reinforcement learning, ignoring the structure of effective heuristics designed by experts. In this paper, we propose \textsc{Doppler}, a three-stage framework for training dual-policy networks consisting of 1) a $\mathsf{SEL}$ policy for selecting operations and 2) a $\mathsf{PLC}$ policy for placing chosen operations on devices. Our experiments show that \textsc{Doppler} outperforms all baseline methods across tasks by reducing system execution time and additionally demonstrates sampling efficiency by reducing per-episode training time.","我们研究在数据流图中将操作分配到在工作保护系统中最大限度地减少执行时间的设备上的问题,重点是复杂的机器学习工作量。先前的学习方法往往由于三个关键限制而困难重重:(1) 依赖诸如TensorFlow这样的散装同步系统,这些系统由于障碍同步而未充分利用设备;(2) 在设计学习方法时对基础系统的时间安排机制缺乏认识;(3) 完全依赖强化学习,忽视专家设计的有效超常结构。在本文中,我们提议为培训双政策网络建立一个三阶段框架,包括:1) $\mathsf{SEL} 业务选择政策;和(2) 将选定操作安装在设备上的政策。我们的实验表明, ktextsc{Doppler} 通过减少系统执行时间和通过减少人均培训时间来进一步展示取样效率,从而超越了所有任务的基线方法。","Xinyu Yao, Daniel Bourgeois, Abhinav Jain, Yuxin Tang, Jiawen Yao, Zhimin Ding, Arlei Silva, Chris Jermaine",2025-05-29T06:04:32Z,DOPPLER: Dual-Policy Learning for Device Assignment in Asynchronous   Dataflow Graphs,DOPPLER: Dual-Policy-Lernen für die Gerätezuordnung in asynchronen Datenflussgraphen,DOPPLER: 同步数据流图表中设备分配的双政策学习,http://arxiv.org/abs/2505.23131v1
137,"Mixture-of-Experts (MoE) architectures offer the promise of larger model capacity without the prohibitive costs of fully dense designs. However, in real-world inference serving, load skew across experts often leads to suboptimal device utilization and excessive synchronization overheads. This paper introduces Asynchronous Expert Parallelism (AEP), a new paradigm that decouples layer execution from barrier-style synchronization. By dynamically queuing tokens at each layer (referred to as $\mu$-queuing) and adaptively re-batching them on demand, GPUs avoid waiting for straggling experts and instead continuously process whichever layer is ready. This asynchronous approach mitigates two major inefficiencies in traditional expert-parallel systems: (1) idle GPU time while waiting for the hottest expert, and (2) small-batch executions on colder experts that waste memory bandwidth.   We implement these ideas in a serving system called AMoE, which disaggregates attention from expert layers and uses a defragging scheduler to reduce batch fragmentation. Evaluations on prototype MoE models show that AMoE improves throughput by up to 2.7x compared to state-of-the-art baselines, incurring a manageable latency penalty and providing a cost-effective operating point. Furthermore, experiments demonstrate nearly linear scalability to multi-node settings, whereas the baseline system shows no throughput increase even when the number of GPUs is doubled.","模拟专家(MoE)架构提供了更大的模型能力,而没有完全稠密的设计的高昂成本。 然而,在现实世界的推论服务中,专家之间负重力往往导致设备使用不优化和过度同步管理。 本文介绍了Asyncronous 专家平行主义(AEP),这是一个将层执行与屏障式同步脱钩的新范例。 通过在每一层(称为双倍递增)以适应性方式对标志进行重新比对,GPUs避免等待悬浮专家,而是持续处理任何已经准备好的层。 这种不协调的做法减轻了传统专家平行系统中两大低效率:(1) 闲置的GPU值时间等待最热的专家,(2) 对浪费记忆带的冷藏专家进行小规模处决。 我们在一个名为AMoOuti的系统里实施这些想法,该系统从专家层中分解关注,并使用分批的表单来减少分批的分解分解。 在模范中,对MoEx原型的模型的精确性操作性几乎显示APO-x的可控性基线, 显示AWO- dal-xxxxxx的可控性运行成本。","Shaoyu Wang, Guangrong He, Geon-Woo Kim, Yanqi Zhou, Seo Jin Park",2025-05-29T05:20:04Z,Toward Cost-Efficient Serving of Mixture-of-Experts with Asynchrony,Auf dem Weg zu einem kosteneffizienten Servieren von Mixture-of-Experts mit Asynchrony,争取以成本低效益高的方式服务专家与非同步混合服务,http://arxiv.org/abs/2505.08944v2
138,"Expert parallelism has emerged as a key strategy for distributing the computational workload of sparsely-gated mixture-of-experts (MoE) models across multiple devices, enabling the processing of increasingly large-scale models. However, the All-to-All communication inherent to expert parallelism poses a significant bottleneck, limiting the efficiency of MoE models. Although existing optimization methods partially mitigate this issue, they remain constrained by the sequential dependency between communication and computation operations. To address this challenge, we propose ScMoE, a novel shortcut-connected MoE architecture integrated with an overlapping parallelization strategy. ScMoE decouples communication from its conventional sequential ordering, enabling up to 100% overlap with computation. Compared to the prevalent top-2 MoE baseline, ScMoE achieves speedups of 1.49 times in training and 1.82 times in inference. Moreover, our experiments and analyses indicate that ScMoE not only achieves comparable but in some instances surpasses the model quality of existing approaches.","专家的平行性已成为一种关键战略,用于通过多种装置分配分散的分散专家混合模型的计算工作量,从而能够处理越来越大规模的模型。然而,专家平行性所固有的 "" 人人交流 "" 构成了一个很大的瓶颈,限制了教育部模式的效率。虽然现有的优化方法在一定程度上缓解了这一问题,但它们仍然受到通信和计算操作之间依次依赖的制约。为了应对这一挑战,我们提议ScMoE,这是一个与重叠的平行战略相结合的新颖的、与捷径相连的教育部结构。ScMoE从常规顺序排序中解析通信,使计算重叠率达到100%。与普遍的上层-2教育部基线相比,ScMoE在培训中实现了1.49倍的加速率,在推断中实现了1.82倍的加速率。此外,我们的实验和分析表明,ScMoE不仅取得了可比较的结果,而且在某些情况下超过了现有方法的模型质量。","Weilin Cai, Juyong Jiang, Le Qin, Junwei Cui, Sunghun Kim, Jiayi Huang",2025-05-29T04:25:16Z,Shortcut-connected Expert Parallelism for Accelerating   Mixture-of-Experts,Shortcut-verbundene Experten-Parallelität für die Beschleunigung von Mixture-of-Experts,加速混合专家专家专家平行专家,http://arxiv.org/abs/2404.05019v3
139,"The rapid increases in model parameter sizes introduces new challenges in pre-trained model loading. Currently, machine learning code often deserializes each parameter as a tensor object in host memory before copying it to device memory. We found that this approach underutilized storage throughput and significantly slowed down loading large models with a widely-used model file formats, safetensors. In this work, we present fastsafetensors, a Python library designed to optimize the deserialization of tensors in safetensors files. Our approach first copies groups of on-disk parameters to device memory, where they are directly instantiated as tensor objects. This design enables further optimization in low-level I/O and high-level tensor preprocessing, including parallelized copying, peer-to-peer DMA, and GPU offloading. Experimental results show performance improvements of 4.8x to 7.5x in loading models such as Llama (7, 13, and 70 billion parameters), Falcon (40 billion parameters), and the Bloom (176 billion parameters).","模型参数大小的快速增加给经过训练的模型装入带来了新的挑战。 目前, 机器学习代码通常在复制到设备内存之前, 将每个参数作为主机内存的 发光对象进行消散。 我们发现, 这种方法未充分利用存储输送量, 并大大减慢了以广泛使用的模型文件格式、 安全加速器装载大型模型。 在这项工作中, 我们展示了快速安全器, 即一个旨在优化安全加速器文件中的发光器的发光的Python 图书馆。 我们的方法首先复制了设备内存的显示器参数组, 在那里, 它们被直接作为发光对象即时。 这个设计可以进一步优化低级 I/ O 和高水平的发光预处理, 包括平行复制、 同行对等DMA 和 GPUP 卸载。 实验结果表明, Llama ( 7、 13 和 700 参数)、 Falcon( 400 参数) 和 Bloom( 760亿 参数) 。","Takeshi Yoshimura, Tatsuhiro Chiba, Manish Sethi, Daniel Waddington, Swaminathan Sundararaman",2025-05-29T04:24:56Z,Speeding up Model Loading with fastsafetensors,Beschleunigen des Modells Beladung mit Schnellsicherern,加速装有快速保障装置的模型加载速度,http://arxiv.org/abs/2505.23072v1
