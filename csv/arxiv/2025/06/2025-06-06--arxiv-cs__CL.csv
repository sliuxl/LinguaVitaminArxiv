,abstract,abstract-zh,authors,date,title,title-de,title-zh,url
0,"Inference-time scaling trades efficiency for increased reasoning accuracy by generating longer or more parallel sequences. However, in Transformer LLMs, generation cost is bottlenecked by the size of the key-value (KV) cache, rather than the number of generated tokens. Hence, we explore inference-time hyper-scaling: by compressing the KV cache, we can generate more tokens within the same compute budget and further improve the accuracy of scaled inference. The success of this approach, however, hinges on the ability of compression methods to preserve accuracy even at high compression ratios. To make hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a novel method for sparsifying KV caches that only requires 1K training steps to achieve 8$\times$ compression, while maintaining better accuracy than training-free sparse attention. Instead of prematurely discarding cached tokens, DMS delays token eviction, implicitly merging representations and preserving critical information. We demonstrate the effectiveness of inference-time hyper-scaling with DMS on multiple families of LLMs, showing that it boosts accuracy for comparable inference runtime and memory load. For instance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on GPQA, and 9.6 on LiveCodeBench across compute budgets.","通过生成较长或更多平行序列来提高推理精确度,推算时间缩放交易效率提高了推理精确度。然而,在变换器LLMS中,生成成本被关键值缓冲的大小(KV)缓冲器而不是生成的象征性物品的数量所抑制。因此,我们探索推论时间超标:通过压缩 KV缓冲器,我们可以在同一计算预算内生成更多符号,进一步提高按比例缩放的精确度。然而,这一方法的成功取决于压缩方法是否有能力保持准确性,即使压缩比率高。要做到超缩,我们引入动态内存简化(DMS)成本,这是一种创新的方法,用来压缩KV缓冲的缓冲器的规模,而不需要1K培训步骤来实现8美元的时间压缩,同时保持比没有培训的微薄关注更精确度。而不是过早丢弃缓存的缓冲牌、DMS延迟信号驱逐、隐含合并的表达方式和保存关键信息。我们展示了与DMS的超时超缩度缩度缩缩放能力,我们引入了LMS多个家族的快速存储器预算,显示我们平均在IMSIMSIMS的准确度上提升了比重度。","Adrian Łańcucki, Konrad Staniszewski, Piotr Nawrot, Edoardo M. Ponti",2025-06-05T17:59:55Z,Inference-Time Hyper-Scaling with KV Cache Compression,Inferenz-Zeit-Hyper-Skalierung mit KV-Cache-Kompression,与 KV 缓存压缩一起的推断时间超大缩放,http://arxiv.org/abs/2506.05345v1
1,"Recent advancements in large language models (LLMs) have underscored their vulnerability to safety alignment jailbreaks, particularly when subjected to downstream fine-tuning. However, existing mitigation strategies primarily focus on reactively addressing jailbreak incidents after safety guardrails have been compromised, removing harmful gradients during fine-tuning, or continuously reinforcing safety alignment throughout fine-tuning. As such, they tend to overlook a critical upstream factor: the role of the original safety-alignment data. This paper therefore investigates the degradation of safety guardrails through the lens of representation similarity between upstream alignment datasets and downstream fine-tuning tasks. Our experiments demonstrate that high similarity between these datasets significantly weakens safety guardrails, making models more susceptible to jailbreaks. Conversely, low similarity between these two types of datasets yields substantially more robust models and thus reduces harmfulness score by up to 10.33%. By highlighting the importance of upstream dataset design in the building of durable safety guardrails and reducing real-world vulnerability to jailbreak attacks, these findings offer actionable insights for fine-tuning service providers.","大型语言模型(LLMs)最近的进展突显了他们容易陷入安全调整监狱的漏洞,特别是在下游进行微调时。然而,现有的缓解战略主要侧重于在安全保护装置受损后被动地处理越狱事件,在微调期间消除有害的梯度,或在整个微调期间不断加强安全调整。因此,它们往往忽视一个关键的上游因素:原有安全调整数据的作用。因此,本文件从代表上游调整数据集和下游微调任务之间的相似性的角度,调查了安全保护装置的退化。我们的实验表明,这些数据集之间的高度相似性大大削弱了安全保护装置,使模型更容易发生越狱情况。相反,这两种数据集之间的低相似性使模型产生更强得多的模型,从而降低了危害性得分,降幅高达10.33%。通过强调上游数据集设计在建设耐久的安全保护装置和降低真实世界对破狱攻击的脆弱性的重要性,这些发现为微调服务供应商提供了可操作的洞察力。","Lei Hsiung, Tianyu Pang, Yung-Chen Tang, Linyue Song, Tsung-Yi Ho, Pin-Yu Chen, Yaoqing Yang",2025-06-05T17:59:55Z,Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity   Analysis Between Alignment and Fine-tuning Datasets,Warum LLM Sicherheits-Guardrails nach Feinabstimmung zusammenbrechen: Eine Ähnlichkeitsanalyse zwischen Alignment- und Feinabstimmungs-Datensätzen,为何LLM 安全护卫车在微调后倒塌:对准和微调数据集之间的相似性分析,http://arxiv.org/abs/2506.05346v1
2,"Language models serve as proxies for human preference judgements in alignment and evaluation, yet they exhibit systematic miscalibration, prioritizing superficial patterns over substantive qualities. This bias manifests as overreliance on features like length, structure, and style, leading to issues like reward hacking and unreliable evaluations. Evidence suggests these biases originate in artifacts in human training data. In this work, we systematically investigate the relationship between training data biases and preference model miscalibration across five idiosyncratic features of language model generations: length, structure, jargon, sycophancy and vagueness. Using controlled counterfactual pairs, we first quantify the extent to which preference models favor responses with magnified biases (skew), finding this preference occurs in >60% of instances, and model preferences show high miscalibration (~40%) compared to human preferences. Notably, bias features only show mild negative correlations to human preference labels (mean r_human = -0.12) but show moderately strong positive correlations with labels from a strong reward model (mean r_model = +0.36), suggesting that models may overrely on spurious cues. To mitigate these issues, we propose a simple post-training method based on counterfactual data augmentation (CDA) using synthesized contrastive examples. Finetuning models with CDA reduces average miscalibration from 39.4% to 32.5% and average absolute skew difference from 20.5% to 10.0%, while maintaining overall RewardBench performance, showing that targeted debiasing is effective for building reliable preference models.","语言模型在调整和评估中充当人类偏好判断的代理物,然而,它们却表现出系统性的误差,将表面模式置于实质品质之上。这种偏差表现为过度依赖长度、结构和风格等特征,导致奖励黑客和不可靠的评价等问题。有证据表明这些偏差起源于人类培训数据中的人工制品。在这项工作中,我们系统地调查培训数据偏差和偏差模式模式在五个语言模型世代的特异性特征之间的关系:长度、结构、术语、偏差和模糊性。5 使用受控制的反事实配方,我们首先量化偏好模式偏爱以放大的偏差(skew)回应的程度,发现这种偏好出现在超过60%的事例中,而模型显示的偏差(~40 % ) 源自于人类偏好标签( or_ human =-0. 12) , 但也显示与可靠奖赏模式( r_ model=+0. 36) 的标签有适度的正反正正正正正比关系。 表明,模型可能过度调整为基于直观的硬性 CD CD 的平反向模型,同时显示以正反向的平反向的平反向的平反向的平反向的平反向 模化 模化 模化 模化 模化 模化 模化 样 样 样 样 样 模 样 模 样 样 模 样 样 模 样 样 样 模 样 样 。","Anirudh Bharadwaj, Chaitanya Malaviya, Nitish Joshi, Mark Yatskar",2025-06-05T17:59:32Z,"Flattery, Fluff, and Fog: Diagnosing and Mitigating Idiosyncratic Biases   in Preference Models","Flattery, Fluff und Fog: Diagnostizieren und Abmildern von idiosynkratischen Biasen in Präferenzmodellen",Flattery、Fluff和Fog:在首选模式中诊断和缓解非典型的两面主义,http://arxiv.org/abs/2506.05339v1
3,"Search-augmented language models combine web search with Large Language Models (LLMs) to improve response groundedness and freshness. However, analyzing these systems remains challenging: existing datasets are limited in scale and narrow in scope, often constrained to static, single-turn, fact-checking questions. In this work, we introduce Search Arena, a crowd-sourced, large-scale, human-preference dataset of over 24,000 paired multi-turn user interactions with search-augmented LLMs. The dataset spans diverse intents and languages, and contains full system traces with around 12,000 human preference votes. Our analysis reveals that user preferences are influenced by the number of citations, even when the cited content does not directly support the attributed claims, uncovering a gap between perceived and actual credibility. Furthermore, user preferences vary across cited sources, revealing that community-driven platforms are generally preferred and static encyclopedic sources are not always appropriate and reliable. To assess performance across different settings, we conduct cross-arena analyses by testing search-augmented LLMs in a general-purpose chat environment and conventional LLMs in search-intensive settings. We find that web search does not degrade and may even improve performance in non-search settings; however, the quality in search settings is significantly affected if solely relying on the model's parametric knowledge. We open-sourced the dataset to support future research in this direction. Our dataset and code are available at: https://github.com/lmarena/search-arena.","搜索强化语言模型将网络搜索与大语言模型(LLMS)相结合,以改善反应基础和新鲜度。然而,分析这些系统仍然具有挑战性:现有数据集的规模有限,范围狭窄,往往局限于静态、单向、事实核对问题。在这项工作中,我们引入了Search Arena,这是一个众源、大尺度、人文偏好的多端用户与搜索强化LMS互动的24 000多个对齐多端多端用户数据集。该数据集涵盖不同的意图和语言,包含全系统跟踪,并有大约12 000张人类偏好票。我们的分析显示,用户偏好受引用数量的影响,即使所引用的内容并不直接支持所归属的主张,也往往局限于于静态的、由众源、大型的多端的多端用户偏好数据数据集。为了评估不同环境的性能,我们通过测试普通聊天环境中的搜索和常规LMS支持度来进行跨端分析。我们的分析显示用户偏好受引用数量的影响,即使所引用的内容并不直接支持被引用,但搜索/基础数据环境的搜索质量也会影响。","Mihran Miroyan, Tsung-Han Wu, Logan King, Tianle Li, Jiayi Pan, Xinyan Hu, Wei-Lin Chiang, Anastasios N. Angelopoulos, Trevor Darrell, Narges Norouzi, Joseph E. Gonzalez",2025-06-05T17:59:26Z,Search Arena: Analyzing Search-Augmented LLMs,Sucharena: Analyse von Such-Augmentierten LLMs,搜索竞技场: 分析搜索推荐LMS,http://arxiv.org/abs/2506.05334v1
4,"We rethink test-time scaling laws from a practical efficiency perspective, revealing that the effectiveness of smaller models is significantly overestimated. Prior work, grounded in compute-optimality, overlooks critical memory access bottlenecks introduced by inference-time strategies (e.g., Best-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to 32B parameters, reveals a new Kinetics Scaling Law that better guides resource allocation by incorporating both computation and memory access costs. Kinetics Scaling Law suggests that test-time compute is more effective when used on models above a threshold than smaller ones. A key reason is that in TTS, attention, rather than parameter count, emerges as the dominant cost factor. Motivated by this, we propose a new scaling paradigm centered on sparse attention, which lowers per-token cost and enables longer generations and more parallel samples within the same resource budget. Empirically, we show that sparse attention models consistently outperform dense counterparts, achieving over 60 points gains in low-cost regimes and over 5 points gains in high-cost regimes for problem-solving accuracy on AIME, encompassing evaluations on state-of-the-art MoEs. These results suggest that sparse attention is essential for realizing the full potential of test-time scaling because, unlike training, where parameter scaling saturates, test-time accuracy continues to improve through increased generation. The code is available at https://github.com/Infini-AI-Lab/Kinetics.","我们从实际效率角度重新思考测试-时间缩放法,从实际效率角度重新思考测试-时间缩放法,揭示了小模型的有效性被大大高估了。先前的工作基于计算最佳性,忽略了由推论-时间战略(例如,美元-美元-美元-美元-美元-美元-长期COTs)引入的关键记忆存取瓶颈。我们从0.6B到32B参数的全方位分析揭示了一种新的《动因-缩放法》,它通过纳入计算和记忆存取成本来更好地指导资源分配。动因-缩放法表明,测试-时间计算当用于超过阈值的模型时更为有效。一个关键的原因是,在TTS中,注意力而不是参数计数,成为主要成本因素。我们为此提出一个新的缩放模式,其中心是关注程度低,即降低人均成本成本成本-成本-存取成本-存取成本-存取的模型。我们发现,微量模型在低成本制度和高成本-成本-系统上超过60点的增益。一个关键-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低-低-低-低-低-低-低-低度-低-低-低-低-低-低度-低度-低-低-低-低度-低-低-低度-低度-低度-低-低-低-低度-低-低-低度-低度-低度-低度-低度-低度-低度-低度-低度-低度-低-","Ranajoy Sadhukhan, Zhuoming Chen, Haizhong Zheng, Yang Zhou, Emma Strubell, Beidi Chen",2025-06-05T17:59:24Z,Kinetics: Rethinking Test-Time Scaling Laws,Kinetik: Überdenken von Test-Zeit-Skalierungsgesetzen,动因:重新思考试验时间扩增法,http://arxiv.org/abs/2506.05333v1
5,"Despite recent progress in large-scale reinforcement learning (RL) for reasoning, the training recipe for building high-performing reasoning models remains elusive. Key implementation details of frontier models, such as DeepSeek-R1, including data curation strategies and RL training recipe, are often omitted. Moreover, recent research indicates distillation remains more effective than RL for smaller models. In this work, we demonstrate that large-scale RL can significantly enhance the reasoning capabilities of strong, small- and mid-sized models, achieving results that surpass those of state-of-the-art distillation-based models. We systematically study the RL training process through extensive ablations and propose a simple yet effective approach: first training on math-only prompts, then on code-only prompts. Notably, we find that math-only RL not only significantly enhances the performance of strong distilled models on math benchmarks (e.g., +14.6% / +17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks (e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition, extended code-only RL iterations further improve performance on code benchmarks with minimal or no degradation in math results. We develop a robust data curation pipeline to collect challenging prompts with high-quality, verifiable answers and test cases to enable verification-based RL across both domains. Finally, we identify key experimental insights, including curriculum learning with progressively increasing response lengths and the stabilizing effect of on-policy parameter updates. We find that RL not only elicits the foundational reasoning capabilities acquired during pretraining and supervised fine-tuning (e.g., distillation), but also pushes the limits of the model's reasoning ability, enabling it to solve problems that were previously unsolvable.","尽管在大规模强化学习(RL)推理方面最近有所进展,但建立高性能推理模型的培训配方仍然难以实现。我们通过广泛的推算系统系统研究RL培训过程,并提出简单而有效的方法:首先进行数学提示培训,然后进行代码提示。此外,我们发现,仅数学的蒸馏不仅显著提高数学基准(例如,+14.6%/+17.2 % ) 强性、中小型和中型模型的推理能力,取得超过最先进的精度精度推理模型(RL)的推理能力,在7B/14B模型中取得超过最新水平的推理模型的推理能力,在7B/稳定度的推理模型中,我们发现超值的精度推理学流程,包括超值推理/+5.8 % 。我们发现,数学基准的精度不仅大大提升(例如,+14.66%/+17.2%)的推理模型的推理能力,在7B/14B模型的推理学模型中,我们发现超前的推理学能力。","Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping",2025-06-05T17:59:12Z,AceReason-Nemotron: Advancing Math and Code Reasoning through   Reinforcement Learning,AceReason-Nemotron: Mathematische und Code-Reasonierung durch Stärkungslernen,AceReson-Nepron:通过强化学习推进数学和守则,http://arxiv.org/abs/2505.16400v3
6,"Recent long-form video-language understanding benchmarks have driven progress in video large multimodal models (Video-LMMs). However, the scarcity of well-annotated long videos has left the training of hour-long Video-LLMs underexplored. To close this gap, we present VideoMarathon, a large-scale hour-long video instruction-following dataset. This dataset includes around 9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60 minutes per video. Specifically, it contains 3.3M high-quality QA pairs, spanning six fundamental topics: temporality, spatiality, object, action, scene, and event. Compared to existing video instruction datasets, VideoMarathon significantly extends training video durations up to 1 hour, and supports 22 diverse tasks requiring both short- and long-term video comprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and efficient Video-LMM for hour-scale video-language modeling. It enables hour-long video training and inference at 1-FPS sampling by leveraging a memory augmentation module, which adaptively integrates user question-relevant and spatiotemporal-informative semantics from a cached full video context. In our experiments, Hour-LLaVA achieves the best performance on multiple long video-language benchmarks, demonstrating the high quality of the VideoMarathon dataset and the superiority of the Hour-LLaVA model.","近期长式视频理解基准促成了视频大型多式联运模型(Video-LMMs)的进步。然而,高注释长视频的稀缺使得对小时长视频LLMs的培训没有得到充分探索。为了缩小这一差距,我们展示了一个大型小时长的视频指导视频数据集,即视频Marathon,这是一个大型小时长的视频指导数据集。该数据集包括大约9 700小时长的视频,来自不同领域,每部视频3至60分钟不等。具体来说,该数据集包含3.3M高质量QA配对,涵盖六个基本主题:时间性、空间性、目标、动作、场景和事件。与现有的视频教学数据集相比,视频马拉松将培训视频持续时间大幅延长至1小时,支持22项需要短期和长期视频理解的不同任务。在视频马拉松上,我们提出“小时-LLLVAVAVA,一个强大而高效的视频-LMMM模型,用于小时级视频-LMMM,一个高档视频培训,在1-FPS取样时段进行视频访问,利用一个记忆增强力的多级模块,该模块,该模块将我们机级高级的高级数据库的高级数据库的高级用户端级高级图像-图像-图像-图像-日历图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-上,从一个背景中,该模型从一个可转换到一个图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-图像-","Jingyang Lin, Jialian Wu, Ximeng Sun, Ze Wang, Jiang Liu, Yusheng Su, Xiaodong Yu, Hao Chen, Jiebo Luo, Zicheng Liu, Emad Barsoum",2025-06-05T17:59:04Z,Unleashing Hour-Scale Video Training for Long Video-Language   Understanding,Unleashing Stunden-Scale Video-Training für lange Video-Sprache verstehen,为了解长视视频语言而开放的时空视频培训,http://arxiv.org/abs/2506.05332v1
7,"The ability of language models to learn a task from a few examples in context has generated substantial interest. Here, we provide a perspective that situates this type of supervised few-shot learning within a much broader spectrum of meta-learned in-context learning. Indeed, we suggest that any distribution of sequences in which context non-trivially decreases loss on subsequent predictions can be interpreted as eliciting a kind of in-context learning. We suggest that this perspective helps to unify the broad set of in-context abilities that language models exhibit -- such as adapting to tasks from instructions or role play, or extrapolating time series. This perspective also sheds light on potential roots of in-context learning in lower-level processing of linguistic dependencies (e.g. coreference or parallel structures). Finally, taking this perspective highlights the importance of generalization, which we suggest can be studied along several dimensions: not only the ability to learn something novel, but also flexibility in learning from different presentations, and in applying what is learned. We discuss broader connections to past literature in meta-learning and goal-conditioned agents, and other perspectives on learning and adaptation. We close by suggesting that research on in-context learning should consider this broader spectrum of in-context capabilities and types of generalization.","语言模型从几个实例中学习任务的能力引起了极大的兴趣。在这里,我们提供了一个观点,将这种受监督的微小学习置于更广泛的超常学习中,从中学习。事实上,我们建议,在非边际地减少后续预测损失的顺序分配中,可以被解释为产生某种自通性学习。我们建议,这种观点有助于统一语言模型所展示的广泛的自通能力 -- -- 例如适应从指令或角色扮演或外推时间序列中产生的任务。这种观点还揭示了在语言依赖性低层次处理(例如共同参照或平行结构)中进行自通性学习的潜在根源。最后,我们从这一角度强调普遍化的重要性,我们建议可以从几个方面加以研究:不仅能够学习新颖的东西,而且能够灵活地从不同的介绍中学习,以及应用所学的东西。我们讨论了在元学习和目标调整代理人中与过去文学的更广泛的联系,以及学习和适应性的其他观点。我们建议,在广义的学习和适应能力方面,应当从广义的角度考虑。","Andrew Kyle Lampinen, Stephanie C. Y. Chan, Aaditya K. Singh, Murray Shanahan",2025-06-05T17:58:57Z,The broader spectrum of in-context learning,Das breitere Spektrum des In-Context-Lernens,广义的内通学习,http://arxiv.org/abs/2412.03782v3
8,"Reinforcement learning (RL) has become an effective approach for fine-tuning large language models (LLMs), particularly to enhance their reasoning capabilities. However, RL fine-tuning remains highly resource-intensive, and existing work has largely overlooked the problem of data efficiency. In this paper, we propose two techniques to improve data efficiency in LLM RL fine-tuning: difficulty-targeted online data selection and rollout replay. We introduce the notion of adaptive difficulty to guide online data selection, prioritizing questions of moderate difficulty that are more likely to yield informative learning signals. To estimate adaptive difficulty efficiently, we develop an attention-based framework that requires rollouts for only a small reference set of questions. The adaptive difficulty of the remaining questions is then estimated based on their similarity to this set. To further reduce rollout cost, we introduce a rollout replay mechanism that reuses recent rollouts, lowering per-step computation while maintaining stable updates. Extensive experiments across 6 LLM-dataset combinations show that our method reduces RL fine-tuning time by 25% to 65% to reach the same level of performance as the original GRPO algorithm.","强化学习(LLL)已成为微调大型语言模型(LLM)的有效方法,特别是为了加强其推理能力。然而,RL微调仍高度耗费资源,现有工作在很大程度上忽视了数据效率问题。在本文件中,我们提出了提高LLMR微调数据效率的两种技术:针对困难的在线数据选择和推出重播。我们引入了适应性困难的概念来指导在线数据选择,优先处理较易产生信息学习信号的中度困难问题。为了高效地估计适应性困难,我们开发了一个只要求少量参考问题推出的基于关注的框架。剩余问题的适应性困难随后根据它们与这套问题的相似性估算。为了进一步降低推出成本,我们引入一个推出性重现机制,重新使用最近的推出,降低每步计算,同时保持稳定的更新。在6 LLM-数据组合中进行的广泛实验表明,我们的方法将RL微调时间减少25%至65%,以达到与GROPO原始算法相同的业绩水平。","Yifan Sun, Jingyan Shen, Yibin Wang, Tianyu Chen, Zhendong Wang, Mingyuan Zhou, Huan Zhang",2025-06-05T17:55:43Z,Improving Data Efficiency for LLM Reinforcement Fine-tuning Through   Difficulty-targeted Online Data Selection and Rollout Replay,Verbesserung der Dateneffizienz für LLM-Verstärkung Feinabstimmung durch problemorientierte Online-Datenauswahl und Rollout-Replay,"提高LLLM数据效率,通过困难目标在线数据选择和推出重播提高LLM强化微调的数据效率",http://arxiv.org/abs/2506.05316v1
9,"Large Language Models (LLMs) deployed in real-world settings increasingly face the need to unlearn sensitive, outdated, or proprietary information. Existing unlearning methods typically formulate forgetting and retention as a regularized trade-off, combining both objectives into a single scalarized loss. This often leads to unstable optimization and degraded performance on retained data, especially under aggressive forgetting. We propose a new formulation of LLM unlearning as a constrained optimization problem: forgetting is enforced via a novel logit-margin flattening loss that explicitly drives the output distribution toward uniformity on a designated forget set, while retention is preserved through a hard constraint on a separate retain set. Compared to entropy-based objectives, our loss is softmax-free, numerically stable, and maintains non-vanishing gradients, enabling more efficient and robust optimization. We solve the constrained problem using a scalable primal-dual algorithm that exposes the trade-off between forgetting and retention through the dynamics of the dual variable. Evaluations on the TOFU and MUSE benchmarks across diverse LLM architectures demonstrate that our approach consistently matches or exceeds state-of-the-art baselines, effectively removing targeted information while preserving downstream utility.","在现实环境中部署的大语言模型(LLMs)日益面临需要解开敏感、过时或专有信息。现有的未学习方法通常会将遗忘和保留作为一种常规交易,将两个目标合并成单一的缩放损失。这往往导致保留数据上的不稳优化和性能退化,特别是在激进地遗忘的情况下。我们提出一种新的LLM不学习公式,认为这是一个限制优化的问题:忘记是通过一种新颖的对账-边平流平流损失强制执行的,它明确驱动产出分配走向指定废旧数据集的统一,而留存则通过对单独成套保值的硬性限制加以保存。与基于恒星的目标相比,我们的损失是软式的、数字稳定的,并且保持非加速的梯度,从而使得更有效和稳健的优化。我们用一种可缩放的原始的算法来解决受限问题,它通过双重变量的动态暴露了遗忘和留存之间的交易。不同LM结构对TOF和ME基准的评价表明,我们的方法始终匹配或超过州际通用基线,同时有效地维护了标准化的下游基线。","Taha Entesari, Arman Hatami, Rinat Khaziev, Anil Ramakrishna, Mahyar Fazlyab",2025-06-05T17:55:23Z,Constrained Entropic Unlearning: A Primal-Dual Framework for Large   Language Models,Engropisches Lernen eingeschränkt: Ein primäres Rahmenwerk für große Sprachmodelle,未学习:大语言模式的原始-双重框架,http://arxiv.org/abs/2506.05314v1
10,"Low-Resource Languages (LRLs) present significant challenges in natural language processing due to their limited linguistic resources and underrepresentation in standard datasets. While recent advances in Large Language Models (LLMs) and Neural Machine Translation have substantially improved translation capabilities for high-resource languages, performance disparities persist for LRLs, particularly impacting privacy-sensitive and resource-constrained scenarios. This paper systematically evaluates current LLMs in 200 languages using the FLORES-200 benchmark and demonstrates their limitations in LRL translation capability. We also explore alternative data sources, including news articles and bilingual dictionaries, and demonstrate how knowledge distillation from large pre-trained teacher models can significantly improve the performance of small LLMs on LRL translation tasks. For example, this approach increases EN->LB with the LLM-as-a-Judge score on the validation set from 0.36 to 0.89 for Llama-3.2-3B. Furthermore, we examine different fine-tuning configurations, providing practical insights on optimal data scale, training efficiency, and the preservation of generalization capabilities of models under study.","低资源语言(LLL)由于语言资源有限,在标准数据集中的代表性不足,在自然语言处理方面提出了重大挑战。虽然在大语言模型和神经机器翻译方面最近的进展大大改善了高资源语言的翻译能力,但LLL语言的绩效差距依然存在,特别是影响到隐私敏感和资源紧张的假设情况。本文件利用FLORES-200基准,系统地评估200种语言的当前LLMM, 并表明其在LLLL翻译能力方面的局限性。我们还探索了其他数据来源,包括新闻文章和双语词典,并展示了从大型预先培训的教师模型中提取知识如何显著改善LLLMM在LL翻译任务方面的绩效。例如,这一方法提高了EN-LB,LLM-as-as-a-Judge对Llama-3.2-3B的认证评分从0.36到0.89。此外,我们研究了不同的微调配置,就最佳数据规模、培训效率以及维护所研究模型的普及能力提供了实用见解。","Yewei Song, Lujun Li, Cedric Lothritz, Saad Ezzini, Lama Sleem, Niccolo Gentile, Radu State, Tegawendé F. Bissyandé, Jacques Klein",2025-06-05T17:55:07Z,Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?,Ist LLM die Silver Bullet zu Low-Resource Sprachen Maschinelle Übersetzung?,LLM 银弹到低资源语言机器翻译吗?,http://arxiv.org/abs/2503.24102v2
11,"LLMs are used predominantly in synchronous communication, where a human user and a model communicate in alternating turns. In contrast, many real-world settings are inherently asynchronous. For example, in group chats, online team meetings, or social games, there is no inherent notion of turns; therefore, the decision of when to speak forms a crucial part of the participant's decision making. In this work, we develop an adaptive asynchronous LLM-agent which, in addition to determining what to say, also decides when to say it. To evaluate our agent, we collect a unique dataset of online Mafia games, including both human participants, as well as our asynchronous agent. Overall, our agent performs on par with human players, both in game performance, as well as in its ability to blend in with the other human players. Our analysis shows that the agent's behavior in deciding when to speak closely mirrors human patterns, although differences emerge in message content. We release all our data and code to support and encourage further research for more realistic asynchronous communication between LLM agents. This work paves the way for integration of LLMs into realistic human group settings, from assistance in team discussions to educational and professional environments where complex social dynamics must be navigated.","LLMS主要用于同步通信, 即人类用户和模式交替交流。 相反, 许多真实世界的设置本质上是非同步的。 例如, 在集体聊天、 在线团队会议或社交游戏中, 不存在固有的旋转概念; 因此, 何时发言的决定是参与者决策的关键部分 。 在这项工作中, 我们开发了一个适应性和非同步的LM- 代理, 除了决定要说什么外, 还要决定何时说什么。 为了评估我们的代理, 我们收集了一个独特的网上黑手党游戏数据集, 包括人类参与者, 以及我们的不同步代理。 总的来说, 我们的代理在游戏表演中, 以及与其他人类玩家融合的能力上, 都不存在固有的转折概念。 我们的分析表明, 代理决定何时发言时的行为反映了人类模式, 尽管在信息内容上出现差异。 我们发布所有的数据和代码, 支持并鼓励进一步开展研究, 以便让LMPM 代理进行更现实的同步通信。 这项工作为将LMS 整合到现实的团队的复杂动态环境提供了帮助。","Niv Eckhaus, Uri Berger, Gabriel Stanovsky",2025-06-05T17:53:44Z,Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia   Games,Time to Talk: LLM-Agenten für asynchrone Gruppenkommunikation in Mafia-Spielen,讨论时间:黑手党运动会Asynconomic Group通讯的LLM代理商,http://arxiv.org/abs/2506.05309v1
12,"Agentic workflows, where multiple AI agents collaborate to accomplish complex tasks like reasoning or planning, are becoming increasingly prevalent. However, these workflows often suffer from error propagation and sub-optimal performance, largely due to poorly designed prompts that fail to effectively guide individual agents. This is a critical problem because it limits the reliability and scalability of these powerful systems. We introduce ProRefine, an innovative inference-time prompt optimization method that leverages textual feedback from large language models (LLMs) to address this challenge. ProRefine dynamically refines prompts for multi-step reasoning tasks without additional training or ground truth labels. Evaluated on five benchmark mathematical reasoning datasets, ProRefine significantly surpasses zero-shot Chain-of-Thought baselines by 3 to 37 percentage points. This approach not only boosts accuracy but also allows smaller models to match the performance of larger ones, highlighting its potential for efficient and scalable AI deployment, and democratizing access to high-performing AI.","在多个AI代理机构合作完成推理或规划等复杂任务的情况下,代理工作流程正在变得日益普遍;然而,这些工作流程往往受到错误传播和低最佳性能的影响,这主要是因为设计不当,未能有效指导单个代理机构。这是一个关键问题,因为它限制了这些强大系统的可靠性和可缩放性。我们引入了ProRefine,这是一种创新的推论时间快速优化方法,利用大语言模型的文字反馈来应对这一挑战。ProRefine 动态改进了多步骤推理任务的提示,而没有额外的培训或地面真相标签。根据五个基准数学推理数据集,ProRefine 大大超过零速链-Thought基线3至37个百分点。这种方法不仅提高了准确性,而且还使较小模型能够匹配较大系统的性能,突出其高效和可缩放的AI部署潜力,并使高性AI进入民主化。","Deepak Pandita, Tharindu Cyril Weerasooriya, Ankit Parag Shah, Christopher M. Homan, Wei Wei",2025-06-05T17:52:30Z,ProRefine: Inference-time Prompt Refinement with Textual Feedback,ProRefine: Inferenz-Zeit Prompt Verfeinerung mit Text-Feedback,ProRefine: 用文字反馈迅速改进推论-时间,http://arxiv.org/abs/2506.05305v1
13,"Although chain-of-thought reasoning and reinforcement learning (RL) have driven breakthroughs in NLP, their integration into generative vision models remains underexplored. We introduce ReasonGen-R1, a two-stage framework that first imbues an autoregressive image generator with explicit text-based ""thinking"" skills via supervised fine-tuning on a newly generated reasoning dataset of written rationales, and then refines its outputs using Group Relative Policy Optimization. To enable the model to reason through text before generating images, We automatically generate and release a corpus of model crafted rationales paired with visual prompts, enabling controlled planning of object layouts, styles, and scene compositions. Our GRPO algorithm uses reward signals from a pretrained vision language model to assess overall visual quality, optimizing the policy in each update. Evaluations on GenEval, DPG, and the T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong baselines and prior state-of-the-art models. More: aka.ms/reasongen.","虽然思维链推理和强化学习(RL)推动了NLP的突破,但将其纳入基因化愿景模型的工作仍未得到充分探讨。我们引入了“理性”R1,这是一个两阶段框架,首先通过监督对新生成的书面原理推理数据集进行精细调整,引入一个具有明确文本“思考”技能的自动递减图像生成器,然后利用集体相对政策优化来改进其产出。为了使模型在生成图像之前能够通过文本进行推理,我们自动生成并发布一套模型设计原理,配以视觉提示,使对象布局、风格和场景构成得到控制规划。我们GROP的算法使用预先培训的视觉语言模型的奖励信号来评估总体视觉质量,在每次更新中优化政策。对GenEval、DPG和T2I基准的评价表明,“理性”始终超越强的基线和以前的先进模型。更多: aka.ms/ricgen。","Yu Zhang, Yunqi Li, Yifan Yang, Rui Wang, Yuqing Yang, Dai Qi, Jianmin Bao, Dongdong Chen, Chong Luo, Lili Qiu",2025-06-05T17:51:58Z,ReasonGen-R1: CoT for Autoregressive Image generation models through SFT   and RL,ReasonGen-R1: CoT für Autoregressive Imagegenerierungsmodelle durch SFT und RL,理由Gen-R1:通过SFT和RL自动递减图像生成模型的 CoT,http://arxiv.org/abs/2505.24875v2
14,"Pre-trained Language Models (PLMs) have shown remarkable performances in recent years, setting a new paradigm for NLP research and industry. The legal domain has received some attention from the NLP community partly due to its textual nature. Some tasks from this domain are represented by question-answering (QA) tasks. This work explores the legal domain Multiple-Choice QA (MCQA) for a low-resource language. The contribution of this work is multi-fold. We first introduce JuRO, the first openly available Romanian legal MCQA dataset, comprising three different examinations and a number of 10,836 total questions. Along with this dataset, we introduce CROL, an organized corpus of laws that has a total of 93 distinct documents with their modifications from 763 time spans, that we leveraged in this work for Information Retrieval (IR) techniques. Moreover, we are the first to propose Law-RoG, a Knowledge Graph (KG) for the Romanian language, and this KG is derived from the aforementioned corpus. Lastly, we propose a novel approach for MCQA, Graph Retrieval Augmented by Facts (GRAF), which achieves competitive results with generally accepted SOTA methods and even exceeds them in most settings.","培训前语言模型(PLM)近年来表现出了显著的成绩,为NLP的研究和行业树立了一个新的范例,法律领域部分因其文本性质而得到NLP社区的一些关注,该领域的一些任务由问答任务代表。这项工作探索了一种低资源语言的法律领域多选择 QA(MCQA) 。这项工作的贡献是多方面的。我们首先引入了JuRO,这是罗马尼亚第一个公开提供的法律MCQA数据集,包括三个不同的考试和10 836个总问题。除了这一数据集外,我们引入了CROL,这是一套有组织的法律,共有93个不同的文件,与763个时间段的问答(QA)相比,我们在这项工作中利用了信息检索技术。此外,我们首先为罗马尼亚语提出了法律-ROG,一个知识图(KG),而这一KG是来自上述的。最后,我们建议对MCQA、图像检索(GG)的最具竞争力的方法,在一般情况下超过了SARVA上。","Cristian-George Crăciun, Răzvan-Alexandru Smădu, Dumitru-Clementin Cercel, Mihaela-Claudia Cercel",2025-06-05T17:37:25Z,GRAF: Graph Retrieval Augmented by Facts for Romanian Legal Multi-Choice   Question Answering,GRAF: Graph Retrieval Augmented by Facts for Rumänian Legal Multi-Choice Question Answering,GRAF: 罗马尼亚多种选择法律问题解答事实加增图,http://arxiv.org/abs/2412.04119v3
15,"Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge Conflicts, where retrieved external knowledge contradicts the inherent, parametric knowledge of large language models (LLMs). It adversely affects performance on downstream tasks such as question answering (QA). Existing approaches often attempt to mitigate conflicts by directly comparing two knowledge sources in a side-by-side manner, but this can overwhelm LLMs with extraneous or lengthy contexts, ultimately hindering their ability to identify and mitigate inconsistencies. To address this issue, we propose Micro-Act a framework with a hierarchical action space that automatically perceives context complexity and adaptively decomposes each knowledge source into a sequence of fine-grained comparisons. These comparisons are represented as actionable steps, enabling reasoning beyond the superficial context. Through extensive experiments on five benchmark datasets, Micro-Act consistently achieves significant increase in QA accuracy over state-of-the-art baselines across all 5 datasets and 3 conflict types, especially in temporal and semantic types where all baselines fail significantly. More importantly, Micro-Act exhibits robust performance on non-conflict questions simultaneously, highlighting its practical value in real-world RAG applications.","获得的外部知识与大型语言模型(LLMs)的内在的参数性知识相矛盾,对下游任务(如问答)的绩效产生不利影响。 现有方法往往试图通过直接对两个知识来源进行平行比较来缓解冲突,但这种做法可能会在外或长情况下使LMs过度渗透,最终妨碍其识别和减少不一致现象的能力。为解决这一问题,我们提议了一个具有等级行动空间的微行动框架,自动地将背景复杂性和适应性地将每个知识来源分解成细微的比较序列。这些比较是可操作的步骤,有利于超越表面背景的推理。通过对五个基准数据集的广泛试验,MicAcs始终在所有5个数据集和3个冲突类型中,特别是在所有基线都严重失灵的时间和语义类型中,使QA的准确性大大提高。更重要的是,微行动在非冲突问题上同时展示了稳健的绩效,突出其实际价值。","Nan Huo, Jinyang Li, Bowen Qin, Ge Qu, Xiaolong Li, Xiaodong Li, Chenhao Ma, Reynold Cheng",2025-06-05T17:33:02Z,Micro-Act: Mitigate Knowledge Conflict in Question Answering via   Actionable Self-Reasoning,Micro-Act: Wissenskonflikt bei der Fragebeantwortung durch handlungsfähige Selbstbesinnung abmildern,微行动:通过可采取行动的自觉反应解决问题时减少知识冲突,http://arxiv.org/abs/2506.05278v1
16,"Current studies have exposed the risk of Large Language Models (LLMs) generating harmful content by jailbreak attacks. However, they overlook that the direct generation of harmful content from scratch is more difficult than inducing LLM to calibrate benign content into harmful forms. In our study, we introduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR) to induce the LLM to calibrate malicious metaphors for jailbreaking. Specifically, to answer harmful queries, AVATAR adaptively identifies a set of benign but logically related metaphors as the initial seed. Then, driven by these metaphors, the target LLM is induced to reason and calibrate about the metaphorical content, thus jailbroken by either directly outputting harmful responses or calibrating residuals between metaphorical and professional harmful content. Experimental results demonstrate that AVATAR can effectively and transferable jailbreak LLMs and achieve a state-of-the-art attack success rate across multiple advanced LLMs.","目前的研究暴露了大语言模型(LLMs)通过越狱袭击产生有害内容的风险,然而,这些研究忽略了直接从零开始产生有害内容比促使LLM将无害内容校准成有害形式更为困难。在我们的研究中,我们引入了一个新的攻击框架,利用AdVersArial meTaphoR(AVATAR)来引导LLM校准恶意隐喻以进行破狱。具体地说,为了回答有害问题,AVATAR适应性地确定了一套无害但逻辑相关的隐喻作为最初的种子。然后,在这些隐喻的驱使下,目标LM(LM)被引向对隐喻内容的理性和校准,从而通过直接输出有害反应或校准隐喻性和专业有害内容之间的残留而陷入牢狱中。实验结果表明,AVATAR可以有效地和可转移的破狱LMs,并在多个先进的LMS中实现最新的攻击成功率。","Yu Yan, Sheng Sun, Zenghao Duan, Teli Liu, Min Liu, Zhiyi Yin, Jiangyu Lei, Qi Li",2025-06-05T17:10:34Z,From Benign import Toxic: Jailbreaking the Language Model via   Adversarial Metaphors,Von Benign Import Giftic: Jailbreaking the Language Model via Adversarial Metaphors,毒物:通过反versarial Metaphors打破语言模式的监狱,http://arxiv.org/abs/2503.00038v2
17,"The proliferation of disinformation demands reliable and scalable fact-checking solutions. We present Dynamic Evidence-based FAct-checking with Multimodal Experts (DEFAME), a modular, zero-shot MLLM pipeline for open-domain, text-image claim verification. DEFAME operates in a six-stage process, dynamically selecting the tools and search depth to extract and evaluate textual and visual evidence. Unlike prior approaches that are text-only, lack explainability, or rely solely on parametric knowledge, DEFAME performs end-to-end verification, accounting for images in claims and evidence while generating structured, multimodal reports. Evaluation on the popular benchmarks VERITE, AVerITeC, and MOCHEG shows that DEFAME surpasses all previous methods, establishing itself as the new state-of-the-art fact-checking system for uni- and multimodal fact-checking. Moreover, we introduce a new multimodal benchmark, ClaimReview2024+, featuring claims after the knowledge cutoff of GPT-4o, avoiding data leakage. Here, DEFAME drastically outperforms the GPT-4o baselines, showing temporal generalizability and the potential for real-time fact-checking.","虚假信息的扩散要求可靠和可扩缩的事实检查解决方案。 我们向多模式专家(DEFAME)展示基于证据的动态FA-CLC- Checking,这是一个模块化、零发MLLM管道,用于公开域名、文本图像索赔核查。 DeFAME在六阶段过程中运作,动态地选择工具,搜索深度以提取和评价文本和视觉证据。不同于以往只使用文本、缺乏解释性或完全依赖参数知识的方法,DEFAME进行端到端核查,在生成结构化、多式报告的同时,对索赔和证据中的图像进行会计核算。对流行基准VERITTE、AVerITec和MOCHEG的评价显示,DEFAM超越了以往所有方法,确立了自己作为新的单式和多式事实检查的最新数据核对系统。此外,我们引入了新的多式联运基准,即索赔审查2024+,在GPT-4o知识关闭后进行索赔,避免数据泄漏。在这里,DEFAM大大地超越了GPT-4的实际情况基线,显示了时间性检查的可能性。","Tobias Braun, Mark Rothermel, Marcus Rohrbach, Anna Rohrbach",2025-06-05T17:10:20Z,DEFAME: Dynamic Evidence-based FAct-checking with Multimodal Experts,DEFAME: Dynamic Evidence-based FAct-Checking mit multimodalen Experten,DFAME: 与多式联运专家进行动态证据法检查,http://arxiv.org/abs/2412.10510v3
18,"A common approach to hallucination detection casts it as a natural language inference (NLI) task, often using LLMs to classify whether the generated text is entailed by corresponding reference texts. Since entailment classification is a complex reasoning task, one would expect that LLMs could benefit from generating an explicit reasoning process, as in CoT reasoning or the explicit ``thinking'' of recent reasoning models. In this work, we propose that guiding such models to perform a systematic and comprehensive reasoning process -- one that both decomposes the text into smaller facts and also finds evidence in the source for each fact -- allows models to execute much finer-grained and accurate entailment decisions, leading to increased performance. To that end, we define a 3-step reasoning process, consisting of (i) claim decomposition, (ii) sub-claim attribution and entailment classification, and (iii) aggregated classification, showing that such guided reasoning indeed yields improved hallucination detection. Following this reasoning framework, we introduce an analysis scheme, consisting of several metrics that measure the quality of the intermediate reasoning steps, which provided additional empirical evidence for the improved quality of our guided reasoning scheme.","一种常见的幻觉检测方法将它作为一种自然语言推理(NLI)任务,常常使用LLMs来对产生的文本是否由相应的参考文本产生进行分类。由于隐含分类是一项复杂的推理任务,人们期望LLMs能够从产生一个明确的推理过程中获益,如在COT推理或最近推理模型的明确的“思考”中。在这项工作中,我们建议指导这些模型来进行系统和全面的推理过程 -- -- 既将案文分解成较小的事实,又在每种事实的来源中找到证据 -- -- 使模型能够执行更精细和准确的引理决定,从而导致提高绩效。为此,我们界定了一个三步推理过程,包括(一) 索赔分解,(二) 次级索赔归属和要求分类,以及(三) 综合分类,表明这种指导推理确实提高了幻觉的检测。根据这一推理框架,我们引入了一种分析方法,由若干衡量中间推理步骤质量的指标组成,为改进我们的引理学计划的质量提供了更多的经验证据。","Ron Eliav, Arie Cattan, Eran Hirsch, Shahaf Bassan, Elias Stengel-Eskin, Mohit Bansal, Ido Dagan",2025-06-05T17:02:52Z,CLATTER: Comprehensive Entailment Reasoning for Hallucination Detection,CLATT: Umfassende Verschweigung Grund für Halluzination Detection,CLATTER: 用于探测幻觉的全面成像理由,http://arxiv.org/abs/2506.05243v1
19,"Syntactic discontinuity is a grammatical phenomenon in which a constituent is split into more than one part because of the insertion of an element which is not part of the constituent. This is observed in many languages across the world such as Turkish, Russian, Japanese, Warlpiri, Navajo, Hopi, Dyirbal, Yidiny etc. Different formalisms/frameworks in current linguistic theory approach the problem of discontinuous structures in different ways. Each framework/formalism has widely been viewed as an independent and non-converging system of analysis. In this paper, we propose a unified system of representation for both continuity and discontinuity in structures of natural languages by taking into account three formalisms, in particular, Phrase Structure Grammar (PSG) for its widely used notion of constituency, Dependency Grammar (DG) for its head-dependent relations, and Categorial Grammar (CG) for its focus on functor-argument relations. We attempt to show that discontinuous expressions as well as continuous structures can be analysed through a unified mathematical derivation incorporating the representations of linguistic structure in these three grammar formalisms.","协同不连续是一种语法现象,其中将某一组成部分分成一个以上部分,因为插入了不属于该组成部分的一部分的一个要素,这在世界上许多语言中都观察到,例如土耳其语、俄语、日语、瓦尔皮里语、纳瓦霍、霍皮语、迪瓦尔语、伊迪尼语等。目前语言理论中不同的形式主义/框架以不同方式处理不连续结构问题。每个框架/形式主义都被广泛视为一种独立和不相互对立的分析系统。在本文件中,我们提议一种统一的自然语言结构的连续性和不连续性代表制度,特别是考虑到三种形式主义,特别是其广泛使用的选区概念的语法结构(PSG),其头部依赖关系的依赖语法(DG),以及其侧重于杀菌者与配方关系的Categorial Grammar(CG)。我们试图通过将语言结构的表述纳入这三种形式形式中的语言结构的表达方式加以分析。","Ratna Kandala, Prakash Mondal",2025-06-05T16:54:41Z,Towards a Unified System of Representation for Continuity and   Discontinuity in Natural Language,Hin zu einem einheitlichen System der Repräsentation für Kontinuität und Diskontinuität in der Natursprache,迈向一个统一的自然语言连续和中断代表制,http://arxiv.org/abs/2506.05235v1
20,"Sequence modeling is currently dominated by causal transformer architectures that use softmax self-attention. Although widely adopted, transformers require scaling memory and compute linearly during inference. A recent stream of work linearized the softmax operation, resulting in powerful recurrent neural network (RNN) models with constant memory and compute costs such as DeltaNet, Mamba or xLSTM. These models can be unified by noting that their recurrent layer dynamics can all be derived from an in-context regression objective, approximately optimized through an online learning rule. Here, we join this line of work and introduce a numerically stable, chunkwise parallelizable version of the recently proposed Mesa layer (von Oswald et al., 2024), and study it in language modeling at the billion-parameter scale. This layer again stems from an in-context loss, but which is now minimized to optimality at every time point using a fast conjugate gradient solver. Through an extensive suite of experiments, we show that optimal test-time training enables reaching lower language modeling perplexity and higher downstream benchmark performance than previous RNNs, especially on tasks requiring long context understanding. This performance gain comes at the cost of additional flops spent during inference time. Our results are therefore intriguingly related to recent trends of increasing test-time compute to improve performance -- here by spending compute to solve sequential optimization problems within the neural network itself.","序列模型目前由因果变压器结构支配,这些变压器使用软负自我注意。 虽然被广泛采用, 变压器需要缩放内存和在推论期间进行线性计算。 最近一连串的工作将软模操作线直线化, 导致产生强大的经常性神经网络模型, 并产生恒定内存和计算成本, 如 DeltaNet、 Mamba 或 xLSTM。 这些模型可以统一起来, 指出其经常性的层动态都可以通过一个在线学习规则来优化。 在这里, 我们加入这个工作线, 并引入一个数字稳定、 粗略的可平行计算版本。 最近提出的Mesa层( von Oswald et al., 2024) , 并用语言模型来模拟软模范神经网络( RNNN) , 这个层再次源于内损耗值, 但是现在可以最小化为每个时刻的最佳性能。 通过一个广泛的实验组合, 我们显示最佳的测试时间训练能够达到更低的语言模型化的解析度, 和更高的下游线性版版本版本版本版本版本版本的Mesasasasax 。 在前的不断的运行周期里, 的运行中需要增加的运行中, 更新的运行中, 更新的运行中, 更新的测试成本。","Johannes von Oswald, Nino Scherrer, Seijin Kobayashi, Luca Versari, Songlin Yang, Maximilian Schlegel, Kaitlin Maile, Yanick Schimpf, Oliver Sieberling, Alexander Meulemans, Rif A. Saurous, Guillaume Lajoie, Charlotte Frenkel, Razvan Pascanu, Blaise Agüera y Arcas, João Sacramento",2025-06-05T16:50:23Z,MesaNet: Sequence Modeling by Locally Optimal Test-Time Training,MesaNet: Sequenzmodellierung durch lokal optimale Test-Time-Schulung,MesaNet:通过当地最佳试验时间培训进行序列建模,http://arxiv.org/abs/2506.05233v1
21,"Transformer models struggle with long-context inference due to their quadratic time and linear memory complexity. Recurrent Memory Transformers (RMTs) offer a solution by reducing the asymptotic cost to linear time and constant memory usage. However, their memory update mechanism leads to sequential execution, causing a performance bottleneck.   We introduce Diagonal Batching, a scheduling scheme that unlocks parallelism across segments in RMTs while preserving exact recurrence. This approach eliminates the sequential constraint, enabling efficient GPU inference even for single long-context inputs without complex batching and pipelining techniques. Because the technique is purely a run-time computation reordering, existing RMT models adopt it with no retraining.   Applied to a LLaMA-1B ARMT model, Diagonal Batching yields a 3.3x speedup over standard full-attention LLaMA-1B and a 1.8x speedup over the sequential RMT implementation on 131,072-token sequences. By removing sequential bottleneck, Diagonal Batching reduces inference cost and latency, thereby strengthening RMTs as a practical solution for real-world, long-context applications.","常规内存变异器(RMT)通过降低线性时间和恒定内存使用时间的无现时成本,提供了一个解决方案。然而,它们的记忆更新机制导致连续执行,造成性能瓶颈。我们引入了Diagonal Batching,这是在保持精确重现的同时释放RMT中各部分平行现象的排期办法。这种方法消除了顺序限制,使得即使在没有复杂分批和管线技术的情况下单项长文本输入的高效 GPU 受挫。由于该技术纯粹是一种运行时间的计算重新排序,现有的RMT 模型在没有再培训的情况下采用它。对LLLAMA-1B ARMT模型应用, Diagonal Batching在131,072-token 序列的 RMMT 实施中,在标准全注意 LLMA-1B 和1.8x 速度上产生3.3x的加速度。通过清除连续的瓶壳、对角粘合物成本和衬底衬,从而加强RMTs,从而将RMT作为现实世界、长文本应用的实际解决办法。","Danil Sivtsov, Ivan Rodkin, Gleb Kuzmin, Yuri Kuratov, Ivan Oseledets",2025-06-05T16:43:48Z,Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers   for Long Contexts,Diagonales Batching löst Parallelismus in recurrenten Speichertransformatoren für lange Kontexte auf,对角对角拔 解锁长期常用内存变换器中的平行主义,http://arxiv.org/abs/2506.05229v1
22,"Self-supervised objectives have driven major advances in NLP by leveraging large-scale unlabeled data, but such resources are scarce for many of the world's languages. Surprisingly, they have not been explored much for character-level tasks, where smaller amounts of data have the potential to be beneficial. We investigate the effectiveness of self-supervised auxiliary tasks for morphological inflection -- a character-level task highly relevant for language documentation -- in extremely low-resource settings, training encoder-decoder transformers for 19 languages and 13 auxiliary objectives. Autoencoding yields the best performance when unlabeled data is very limited, while character masked language modeling (CMLM) becomes more effective as data availability increases. Though objectives with stronger inductive biases influence model predictions intuitively, they rarely outperform standard CMLM. However, sampling masks based on known morpheme boundaries consistently improves performance, highlighting a promising direction for low-resource morphological modeling.","自我监督的目标通过利用大规模无标签数据推动了国家语言方案的重大进展,但对于世界上的许多语言来说,这种资源却很少。 令人惊讶的是,在性格层面的任务中,并没有对这些资源进行多少探索,因为较少的数据可能是有益的。 我们调查了在极低的资源环境中,自监督的形态变化辅助任务 -- -- 一种与语言文件高度相关的性能层面任务 -- -- 的有效性,为19种语言和13个辅助目标培训了编码变压器。在无标签数据非常有限的情况下,自动编码产生最佳的性能,而特性蒙面语言模型(CMLM)则随着数据提供量的增加而变得更加有效。虽然具有更强烈的感性偏见模型预测的目标直觉地说,它们很少超过标准CMLM。 但是,基于已知的单字边界的抽样口罩不断提高性能,突出低资源形态模型的有希望的方向。","Adam Wiemerslage, Katharina von der Wense",2025-06-05T16:42:45Z,Improving Low-Resource Morphological Inflection via Self-Supervised   Objectives,Verbesserung der ressourcenarmen morphologischen Beugung durch selbstüberwachte Ziele,通过自我监督目标改进低资源资源 道德影响,http://arxiv.org/abs/2506.05227v1
23,"Although existing unified models achieve strong performance in vision-language understanding and text-to-image generation, they remain limited in addressing image perception and manipulation -- capabilities increasingly demanded in practical applications. Recently, OpenAI introduced the powerful GPT-4o-Image model, which showcases advanced capabilities in comprehensive image perception and manipulation, sparking widespread interest. Through carefully designed experiments, we observe that GPT-4o-Image likely relies on semantic encoders rather than VAEs for feature extraction, despite VAEs being commonly regarded as crucial for image manipulation tasks. Inspired by this insight, we propose UniWorld-V1, a unified generative framework built upon semantic features extracted from powerful multimodal large language models and contrastive semantic encoders. Using only 2.7M training data, UniWorld-V1 achieves impressive performance across diverse tasks, including image understanding, generation, manipulation, and perception. We fully open-source the UniWorld-V1 framework, including model weights, training and evaluation scripts, and datasets to promote reproducibility and further research.","虽然现有的统一模型在视觉语言理解和文字到图像生成方面取得了很强的业绩,但在处理图像感知和操纵方面仍然有限 -- -- 实际应用中日益要求的能力。最近,开放国际公司引入了强大的GPT-4o-图像模型,该模型展示了全面图像感知和操纵方面的先进能力,引起了广泛的兴趣。我们观察到,通过精心设计的实验,GPT-4o-图像可能依赖语义解码器而不是VAEs进行特征提取,尽管VAEs通常被视为对图像处理任务至关重要。我们建议UniWorld-V1在这种洞察的启发下,建立一个基于从强大的多式联运大型语言模型和对比性语义编码中提取的语义特征的统一基因化框架。仅使用2.7M培训数据,UnityWorld-V1在包括图像理解、生成、操纵和感知觉等不同任务中取得了令人印象深刻的成绩。我们完全开放地将Unwn-V1框架,包括模型重量、培训和评价脚本以及数据集,以促进再生和进一步研究。","Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, Yatian Pang, Li Yuan",2025-06-05T16:41:40Z,UniWorld-V1: High-Resolution Semantic Encoders for Unified Visual   Understanding and Generation,UniWorld-V1: Semantische Encoder mit hoher Auflösung für einheitliches visuelles Verständnis und Generation,Uni-World-V1:用于统一视觉理解和生成的高分辨率语义编码器,http://arxiv.org/abs/2506.03147v3
24,"Process Reward Models (PRMs) emerge as a promising approach for process supervision in mathematical reasoning of Large Language Models (LLMs), which aim to identify and mitigate intermediate errors in the reasoning processes. However, the development of effective PRMs faces significant challenges, particularly in data annotation and evaluation methodologies. In this paper, through extensive experiments, we demonstrate that commonly used Monte Carlo (MC) estimation-based data synthesis for PRMs typically yields inferior performance and generalization compared to LLM-as-a-judge and human annotation methods. MC estimation relies on completion models to evaluate current-step correctness, leading to inaccurate step verification. Furthermore, we identify potential biases in conventional Best-of-N (BoN) evaluation strategies for PRMs: (1) The unreliable policy models generate responses with correct answers but flawed processes, leading to a misalignment between the evaluation criteria of BoN and the PRM objectives of process verification. (2) The tolerance of PRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a significant proportion of minimum scores concentrated on the final answer steps, revealing the shift from process to outcome-based assessment in BoN Optimized PRMs. To address these challenges, we develop a consensus filtering mechanism that effectively integrates MC estimation with LLM-as-a-judge and advocates a more comprehensive evaluation framework that combines response-level and step-level metrics. Based on the mechanisms, we significantly improve both model performance and data efficiency in the BoN evaluation and the step-wise error identification task. Finally, we release a new state-of-the-art PRM that outperforms existing open-source alternatives and provides practical guidelines for future research in building process supervision models.","在大语言模型数学推理中,流程回归模型(RPRM)的制定是大语言模型数学推理中程序监督的一个很有希望的方法,目的是查明和减少推理过程中的中间错误;然而,有效的PRMS的制定面临重大挑战,特别是在数据说明和评价方法方面;在本文件中,我们通过广泛的实验表明,通常使用的基于Monte Carlo(MC)的PRM估算数据合成结果,与LLLM-a-a-判断和人文批注方法相比,其性能和概括性通常差强。 MC的估算依靠完成模型来评价当前步骤的正确性,从而导致步骤核查的不准确性;此外,我们查明了传统最佳语言模型(BoN-BAN)评价战略评估战略的潜在偏差:(1) 政策模型产生正确的答复,但程序有缺陷,导致该模型的评价标准与PRM核查进程的目标之间有误差。","Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, Junyang Lin",2025-06-05T16:34:24Z,The Lessons of Developing Process Reward Models in Mathematical   Reasoning,Die Lehren aus der Entwicklung von Prozess-Reward-Modellen in mathematischer Reasoning,数学理由中发展进程奖励模型的经验教训,http://arxiv.org/abs/2501.07301v2
25,"Graph Neural Networks (GNNs) often suffer from degree bias in node classification tasks, where prediction performance varies across nodes with different degrees. Several approaches, which adopt Graph Contrastive Learning (GCL), have been proposed to mitigate this bias. However, the limited number of positive pairs and the equal weighting of all positives and negatives in GCL still lead to low-degree nodes acquiring insufficient and noisy information. This paper proposes the Hardness Adaptive Reweighted (HAR) contrastive loss to mitigate degree bias. It adds more positive pairs by leveraging node labels and adaptively weights positive and negative pairs based on their learning hardness. In addition, we develop an experimental framework named SHARP to extend HAR to a broader range of scenarios. Both our theoretical analysis and experiments validate the effectiveness of SHARP. The experimental results across four datasets show that SHARP achieves better performance against baselines at both global and degree levels.","在节点分类任务中,神经网络(GNNs)往往受到程度偏差的影响,不同节点的预测性能不同,不同程度的预测性能不同。为了减轻这种偏差,建议采取几种方法,采用图表对比学习(GCL),但是,在GCL中,对正对数量有限,所有正对和负对等权重仍然导致低度节点获得不足和吵闹的信息。本文建议“硬性适应性重力(HAR)对比性损失,以降低偏差。通过利用节点标签和适应性重量的正对比增加双对。此外,我们开发了一个名为SHARP的实验框架,将HARP扩展到更广泛的情景。我们的理论分析和实验都验证了SHARP的有效性。四个数据集的实验结果显示,SHARP在全球和程度的基线上都取得了更好的业绩。","Jingyu Hu, Hongbo Bo, Jun Hong, Xiaowei Liu, Weiru Liu",2025-06-05T16:28:12Z,Mitigating Degree Bias Adaptively with Hard-to-Learn Nodes in Graph   Contrastive Learning,Degree Bias im Graph Contrastive Learning adaptiv mit schwer zu erlernenden Knoten abmildern,与图表对比学习中难以识别的节点相适应,http://arxiv.org/abs/2506.05214v1
26,"Large Language Models (LLMs) have demonstrated remarkable improvements in reasoning and planning through increased test-time compute, often by framing problem-solving as a search process. While methods like Monte Carlo Tree Search (MCTS) have proven effective in some domains, their reliance on fixed exploration hyperparameters limits their adaptability across tasks of varying difficulty, rendering them impractical or expensive in certain settings. In this paper, we propose \textbf{LLM-First Search (LFS)}, a novel \textit{LLM Self-Guided Search} method that removes the need for pre-defined search strategies by empowering the LLM to autonomously control the search process via self-guided exploration. Rather than relying on external heuristics or hardcoded policies, the LLM evaluates whether to pursue the current search path or explore alternative branches based on its internal scoring mechanisms. This enables more flexible and context-sensitive reasoning without requiring manual tuning or task-specific adaptation. We evaluate LFS on Countdown and Sudoku against three classic widely-used search algorithms, Tree-of-Thoughts' Breadth First Search (ToT-BFS), Best First Search (BestFS), and MCTS, each of which have been used to achieve SotA results on a range of challenging reasoning tasks. We found that LFS (1) performs better on more challenging tasks without additional tuning, (2) is more computationally efficient compared to the other methods, especially when powered by a stronger model, (3) scales better with stronger models, due to its LLM-First design, and (4) scales better with increased compute budget. Our code is publicly available at \href{https://github.com/NathanHerr/LLM-First-Search}{LLM-First-Search}.","大型语言模型(LLMS) 通过提高测试时间的计算显示在推理和规划方面的显著改进, 通常通过将问题设计成一个搜索过程。 蒙特卡洛树搜索( MCTS) 等方法在某些领域证明是有效的, 但它们依赖固定的超参数限制其在不同困难的任务中的适应性, 使得它们不切实际或不切实际, 在某些环境里, 使这些模型变得不切实际或昂贵。 在本文中, 我们建议了\ textbf{LLLLM- First Search( LFS) , 一种新型的\ textitle{LLLMM- Guded Search} 方法, 通过授权LLLMMMM(MTH) 来消除预先定义搜索战略的需要, 使LLMM( MLTS) 能够通过自我引导的探索自主控制搜索过程。 而不是依赖外部的超值或硬码政策, , LMLMLM(TFS-FS) 能够更灵活地进行更具有挑战性的工作。","Nathan Herr, Tim Rocktäschel, Roberta Raileanu",2025-06-05T16:27:49Z,LLM-First Search: Self-Guided Exploration of the Solution Space,LLM-First Search: Selbstgeführte Erkundung des Lösungsraums,LLM-第一次搜索:自导探索解决办法空间,http://arxiv.org/abs/2506.05213v1
27,"Large Language Models (LLMs) are transforming a wide range of domains, yet verifying their outputs remains a significant challenge, especially for complex open-ended tasks such as consolidation, summarization, and knowledge extraction. To address this, we introduce CheckEmbed (CE): a simple, scalable, and accurate verification method. CE reduces each LLM answer to a single embedding vector using powerful modern embedding LLM models like SFR-Embedding-Mistral. Prior methods such as BERTScore and SelfCheckGPT relied on weaker encoders like BERT, forcing them to operate at token or sentence granularity. In contrast, CE performs fast, semantically rich comparisons directly at the whole-answer level, overcoming key limitations in both accuracy and scalability. We conduct a comprehensive design and time complexity analysis across 13 verification baselines, including classical text scorers (e.g., BLEU), stability-based methods (e.g., SelfCheckGPT), and generative evaluators (e.g., LLM-as-a-Judge), which highlights the effectiveness, efficiency, versatility, and simplicity of CE. Empirical results show that CE reliably detects hallucinations in both closed and open-ended tasks. We further present evidence that CE generalizes beyond text to other modalities such as vision, establishing it as a practical and versatile verification framework.","大型语言模型(LLMS)正在改变一系列广泛的领域,但核实其产出仍是一项重大挑战,特别是对于合并、汇总和知识提取等复杂的开放性任务而言。为了解决这个问题,我们引入了CheckEmbed(CE):一个简单、可缩放和准确的核查方法。CE使用SFR-Embedding-Mistral等强大的现代嵌入式LLM模型,将每个LLM的答案降低为单一的嵌入矢量。BERTScore和Self CheckGPT等先前的方法,如BERT等较弱的编码器和Self CheckerGPT, 迫使它们以象征性或句号颗粒方式运作。相比之下,CE直接在整个答题一级进行快速、语义丰富的比较,克服准确性和可缩放度两方面的关键限制。我们通过13个核查基线,包括古典文本评分(e.BLEUE)、基于稳定性的方法(eselfecklyGPT)和基因化评价员(e.gralM-asal-ating judialjudal)等方法,以进一步显示C-listral-viewdal 和C.","Maciej Besta, Lorenzo Paleari, Marcin Copik, Robert Gerstenberger, Ales Kubicek, Piotr Nyczyk, Patrick Iff, Eric Schreiber, Tanja Srindran, Tomasz Lehmann, Hubert Niewiadomski, Torsten Hoefler",2025-06-05T16:22:36Z,CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks,CheckEmbed: Effektive Überprüfung von LLM-Lösungen auf offene Aufgaben,复选对象:有效核查对不限名额任务LLM解决方案的有效核查,http://arxiv.org/abs/2406.02524v4
28,"Large language models (LLMs) are typically trained on enormous quantities of unlicensed text, a practice that has led to scrutiny due to possible intellectual property infringement and ethical concerns. Training LLMs on openly licensed text presents a first step towards addressing these issues, but prior data collection efforts have yielded datasets too small or low-quality to produce performant LLMs. To address this gap, we collect, curate, and release the Common Pile v0.1, an eight terabyte collection of openly licensed text designed for LLM pretraining. The Common Pile comprises content from 30 sources that span diverse domains including research papers, code, books, encyclopedias, educational materials, audio transcripts, and more. Crucially, we validate our efforts by training two 7 billion parameter LLMs on text from the Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion tokens respectively. Both models attain competitive performance to LLMs trained on unlicensed text with similar computational budgets, such as Llama 1 and 2 7B. In addition to releasing the Common Pile v0.1 itself, we also release the code used in its creation as well as the training mixture and checkpoints for the Comma v0.1 models.","大型语言模型(LLMS)通常在大量无许可证文本方面接受培训,这种做法导致对可能发生的知识产权侵犯和伦理问题进行详细审查,公开许可文本培训LLMS是解决这些问题的第一步,但先前的数据收集工作已经产生了数据组,数据组太小或质量太低,无法产生有性能的LMS。为了解决这一差距,我们收集、整理和发行了共同Pile v0.1和8兆图文集,这是为LLMM预先培训设计的公开许可文本的8兆字节集。共同版由30个来源的内容组成,包括研究论文、代码、书籍、百科全书、教育材料、录音誊本等不同领域的30个来源。我们确认我们的努力,通过对共同Pile的文本的70亿个参数LLMS进行了培训:Comma v.1-11T和Comma v.1-2T,分别进行了1万亿图示的培训。两种模式都具有竞争性,通过类似的计算预算(如Llama 1和2 7B)。除了公布共同Pile V01模型的培训外,我们还发布了共同版本,作为用于建立共同检查站的培训。","Nikhil Kandpal, Brian Lester, Colin Raffel, Sebastian Majstorovic, Stella Biderman, Baber Abbasi, Luca Soldaini, Enrico Shippole, A. Feder Cooper, Aviya Skowron, John Kirchenbauer, Shayne Longpre, Lintang Sutawika, Alon Albalak, Zhenlin Xu, Guilherme Penedo, Loubna Ben Allal, Elie Bakouch, John David Pressman, Honglu Fan, Dashiell Stander, Guangyu Song, Aaron Gokaslan, Tom Goldstein, Brian R. Bartoldson, Bhavya Kailkhura, Tyler Murray",2025-06-05T16:21:30Z,The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly   Licensed Text,The Common Pile v0.1: Ein 8TB-Datensatz von Public Domain und Openly Licensed Text,Pile v0.1:公共域和公开许可文本的 8TB数据集,http://arxiv.org/abs/2506.05209v1
29,"In recent years, multimodal large language models (MLLMs) have made significant progress but continue to face inherent challenges in multimodal reasoning, which requires multi-level (e.g., perception, reasoning) and multi-granular (e.g., multi-step reasoning chain) advanced inferencing. Prior work on estimating model confidence tends to focus on the overall response for training and calibration, but fails to assess confidence in each reasoning step, leading to undesirable hallucination snowballing. In this work, we present MMBoundary, a novel framework that advances the knowledge boundary awareness of MLLMs through reasoning step confidence calibration. To achieve this, we propose to incorporate complementary textual and cross-modal self-rewarding signals to estimate confidence at each step of the MLLM reasoning process. In addition to supervised fine-tuning MLLM on this set of self-rewarded confidence estimation signal for initial confidence expression warm-up, we introduce a reinforcement learning stage with multiple reward functions for further aligning model knowledge and calibrating confidence at each reasoning step, enhancing reasoning chain self-correction. Empirical results show that MMBoundary significantly outperforms existing methods across diverse domain datasets and metrics, achieving an average of 7.5% reduction in multimodal confidence calibration errors and up to 8.3% improvement in task performance.","近年来,多式联运大型语言模型(MLLM)取得了长足进展,但继续面临多式联运推理的内在挑战,这需要多层次(例如认识、推理)和多层次自我调整(例如多步推理链)的先进推理。先前关于模型信心估计的工作往往侧重于培训和校准的总体反应,但未能评估对每个推理步骤的信心,导致不可取的幻觉雪球。在这项工作中,我们介绍了MMMMMoundary,这是一个通过推理步骤信心校准提高MLLM知识边界意识的新框架。为此,我们建议纳入补充文本和跨模式自我调整信号,以估计MLLM推理过程每一步骤的信心。除了监督对最初信心表示暖化的自评信心估计信号的调整外,我们还引入了一个强化学习阶段,并赋予多重奖励功能,以进一步调整模型知识,调整每推理步骤的信心,加强推理链的自我校正。为了实现这一目标,我们建议纳入补充文本和跨模式自我调整信号,以估计MLLLMARM过程的每一步骤的每个步骤,在MMBMMMMBM大大超越了现有标准的改进度上,在降低数据格式和调整中,在标准中大大改进了标准方面,在标准方面,在改进了标准调整了标准调整了标准调整了标准。","Zhitao He, Sandeep Polisetty, Zhiyuan Fan, Yuchen Huang, Shujin Wu, Yi R. Fung",2025-06-05T16:19:56Z,MMBoundary: Advancing MLLM Knowledge Boundary Awareness through   Reasoning Step Confidence Calibration,MMBoundary: MLLM-Wissensgrenzen-Bewusstsein durch vernünftige Schritt-Vertrauens-Kalibrierung,MMMMMMMMMM MMMMMMMM:通过合理步骤信任校准提高MLLM知识边界认识,http://arxiv.org/abs/2505.23224v2
30,"Large language models (LLMs) are increasingly expected to perform tasks based only on a specification of the task provided in context, without examples of inputs and outputs; this ability is referred to as instruction following. We introduce the Recognition of Languages In-Context (RELIC) framework to evaluate instruction following using language recognition: the task of determining if a string is generated by formal grammar. Unlike many standard evaluations of LLMs' ability to use their context, this task requires composing together a large number of instructions (grammar productions) retrieved from the context. Because the languages are synthetic, the task can be increased in complexity as LLMs' skills improve, and new instances can be automatically generated, mitigating data contamination. We evaluate state-of-the-art LLMs on RELIC and find that their accuracy can be reliably predicted from the complexity of the grammar and the individual example strings, and that even the most advanced LLMs currently available show near-chance performance on more complex grammars and samples, in line with theoretical expectations. We also use RELIC to diagnose how LLMs attempt to solve increasingly difficult reasoning tasks, finding that as the complexity of the language recognition task increases, models switch to relying on shallow heuristics instead of following complex instructions.","大型语言模型(LLMS)越来越期望仅仅根据在背景中提供的具体任务来执行任务,而没有投入和产出的例子;这种能力被称作以下指示。我们引入了承认语言内流(RELIC)框架,在使用语言识别之后对教学进行评价:确定字符串是否由正式语法生成的任务。与许多关于LLMS使用其背景能力的标准评价不同,这项任务要求将从背景中检索的大量指令(语法制作)合并在一起。由于语言是合成的,随着LLOMS技能的提高,任务的复杂性会增加,而且可以自动生成新的实例,从而减轻数据污染。我们评估了RELLMS的最新艺术 LLMS(LLMS)框架,发现其准确性可以可靠地从语法和单个示例链的复杂性中预测,而且即使现有最先进的LMS也显示根据理论期望,在更复杂的语法图和样本上接近成绩。我们还使用RELICS来判断LMS如何试图解决日益困难的推理任务,发现作为复杂的浅语言识别模型的复杂程度,而不是依赖浅语言识别模型。","Jackson Petty, Michael Y. Hu, Wentao Wang, Shauli Ravfogel, William Merrill, Tal Linzen",2025-06-05T16:17:24Z,RELIC: Evaluating Compositional Instruction Following via Language   Recognition,relIC: Bewertung der kompositorischen Instruktion über Spracherkennung,RELIC:评估通过语言承认进行的组成说明,http://arxiv.org/abs/2506.05205v1
31,"Multimodal Large Language Models (MLLMs) pose unique safety challenges due to their integration of visual and textual data, thereby introducing new dimensions of potential attacks and complex risk combinations. In this paper, we begin with a detailed analysis aimed at disentangling risks through step-by-step reasoning within multimodal inputs. We find that systematic multimodal risk disentanglement substantially enhances the risk awareness of MLLMs. Via leveraging the strong discriminative abilities of multimodal risk disentanglement, we further introduce \textbf{DREAM} (\textit{\textbf{D}isentangling \textbf{R}isks to \textbf{E}nhance Safety \textbf{A}lignment in \textbf{M}LLMs}), a novel approach that enhances safety alignment in MLLMs through supervised fine-tuning and iterative Reinforcement Learning from AI Feedback (RLAIF). Experimental results show that DREAM significantly boosts safety during both inference and training phases without compromising performance on normal tasks (namely oversafety), achieving a 16.17\% improvement in the SIUO safe\&effective score compared to GPT-4V. The data and code are available at https://github.com/Kizna1ver/DREAM.","多式大语言模型(MLLMM)由于整合了视觉和文字数据,从而引入了潜在攻击和复杂风险组合的新层面,因此构成了独特的安全挑战。在本文件中,我们首先进行详细分析,目的是通过在多式联运投入中逐步推理分散风险。我们发现,系统性多式联运风险分解极大地提高了MLLMS的风险意识。 利用多式联运风险分解的很强的区别能力,我们进一步引入了\ textbf{DREAM} (Textit hutbf{D}) (texthbf{R}isks to\ textbf{E}nhance Security\ textbf{A}亮度,目的是通过监管微调和从AI 反馈中反复强化学习,使MLLMMMs的安全更加一致。 实验结果显示,DREAM在判断和培训阶段,在不损害正常任务(即超越安全性)的绩效的情况下,大大提升了安全性安全性/安全性。 在SIMAV/DR1中,可实现有效的数据评分数。","Jianyu Liu, Hangyu Guo, Ranjie Duan, Xingyuan Bu, Yancheng He, Shilong Li, Hui Huang, Jiaheng Liu, Yucheng Wang, Chenchen Jing, Xingwei Qu, Xiao Zhang, Yingshui Tan, Yanan Wu, Jihao Gu, Yangguang Li, Jianke Zhu",2025-06-05T16:13:05Z,DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal   Large Language Models,DREAM: Entwirren von Risiken zur Verbesserung der Sicherheitsausrichtung in multimodalen großen Sprachmodellen,"DREAM:消除风险,加强多式联运大语言模式中的安全协调",http://arxiv.org/abs/2504.18053v2
32,"Large-scale neural language models (LMs) exhibit remarkable performance in in-context learning: the ability to learn and reason the input context on the fly without parameter update. This work studies in-context counterfactual reasoning in language models, that is, to predict the consequences of changes under hypothetical scenarios. We focus on studying a well-defined synthetic setup: a linear regression task that requires noise abduction, where accurate prediction is based on inferring and copying the contextual noise from factual observations. We show that language models are capable of counterfactual reasoning in this controlled setup and provide insights that counterfactual reasoning for a broad class of functions can be reduced to a transformation on in-context observations; we find self-attention, model depth, and data diversity in pre-training drive performance in Transformers. More interestingly, our findings extend beyond regression tasks and show that Transformers can perform noise abduction on sequential data, providing preliminary evidence on the potential for counterfactual story generation. Our code is available under https://github.com/moXmiller/counterfactual-reasoning.git .","大型神经语言模型(LMS)在理论内学习方面表现显著:能够学习并解释飞碟上的投入背景,而没有更新参数。这项工作研究语言模型中的反事实推理,即预测假设情景下变化的后果。我们侧重于研究一个明确界定的合成结构:需要噪音绑架的线性回归任务,准确预测的依据是从事实观察中推断和复制背景噪音。我们显示语言模型能够在这个控制的结构中进行反事实推理,并提供洞察力,将广义功能类别的反事实推理减少到文内观察的转变;我们在变换器的训练前驱动性表现中发现自我意识、模型深度和数据多样性。更有趣的是,我们的调查结果超越了倒退任务,并表明变换器可以在顺序数据上进行噪音绑架,提供关于反事实故事生成潜力的初步证据。我们的代码可在https://github.com/moXmiller/contifactal-rication.git下查阅。","Moritz Miller, Bernhard Schölkopf, Siyuan Guo",2025-06-05T16:02:07Z,Counterfactual reasoning: an analysis of in-context emergence,Gegenfaktische Argumentation: eine Analyse der Entstehung von Inkontexten,反事实推理:对内源出现的分析,http://arxiv.org/abs/2506.05188v1
33,"Retrieval Augmented Generation (RAG) enhances the abilities of Large Language Models (LLMs) by enabling the retrieval of documents into the LLM context to provide more accurate and relevant responses. Existing RAG solutions do not focus on queries that may require fetching multiple documents with substantially different contents. Such queries occur frequently, but are challenging because the embeddings of these documents may be distant in the embedding space, making it hard to retrieve them all. This paper introduces Multi-Head RAG (MRAG), a novel scheme designed to address this gap with a simple yet powerful idea: leveraging activations of Transformer's multi-head attention layer, instead of the decoder layer, as keys for fetching multi-aspect documents. The driving observation is that different attention heads learn to capture different data aspects. Harnessing the corresponding activations results in embeddings that represent various facets of data items and queries, improving the retrieval accuracy for complex queries. We provide an evaluation methodology and metrics, multi-aspect datasets, and real-world use cases to demonstrate MRAG's effectiveness. We show MRAG's design advantages over 18 RAG baselines, empirical improvements of up to 20% in retrieval success ratios, and benefits for downstream LLM generation. MRAG can be seamlessly integrated with existing RAG frameworks and benchmarks.","重新获取增强的一代(RAG)能够将文件检索到LLM环境中,从而提供更准确和更相关的答复,从而增强大语言模型(LLMS)的能力。现有的RAG解决方案并不侧重于可能需要获取内容大相径庭的多份文件的查询。这类查询经常发生,但具有挑战性,因为这些文件的嵌入在嵌入空间中可能距离遥远,难以全部检索。本文介绍了多主RAG(MRAG),这是一个旨在解决这一差距的新办法,其简单而有力的理念是:利用变换器多头注意层的启动,而不是拆解层,作为获取多层文件的关键。驱动意见是,不同的注意力负责人学会捕捉不同的数据方面。利用相应的启动成果,将数据项目和查询的各个方面嵌入其中,提高复杂查询的检索准确性。我们提供了一种评价和衡量方法,多层数据集,以及真实世界使用案例,以展示MRAG的有效性:利用变换器多头注意层的启动,而不是拆解码层,作为获取多层文件的钥匙。我们发现,不同的关注对象会学会会学会学会学会如何捕捉取不同的数据方面,从而获得数据,从而获得现有RAGMAGMLMB基准的20的创建率基准。","Maciej Besta, Ales Kubicek, Robert Gerstenberger, Marcin Chrapek, Roman Niggli, Patrik Okanovic, Yi Zhu, Patrick Iff, Michal Podstawski, Lucas Weitzendorf, Mingyuan Chi, Joanna Gajda, Piotr Nyczyk, Jürgen Müller, Hubert Niewiadomski, Torsten Hoefler",2025-06-05T15:57:36Z,Multi-Head RAG: Solving Multi-Aspect Problems with LLMs,Multi-Head RAG: Lösung von Multi-Aspect-Problemen mit LLMs,多方主管RAG:解决多领域问题与LLM,http://arxiv.org/abs/2406.05085v3
34,"In this work, we introduce the Qwen3 Embedding series, a significant advancement over its predecessor, the GTE-Qwen series, in text embedding and reranking capabilities, built upon the Qwen3 foundation models. Leveraging the Qwen3 LLMs' robust capabilities in multilingual text understanding and generation, our innovative multi-stage training pipeline combines large-scale unsupervised pre-training with supervised fine-tuning on high-quality datasets. Effective model merging strategies further ensure the robustness and adaptability of the Qwen3 Embedding series. During the training process, the Qwen3 LLMs serve not only as backbone models but also play a crucial role in synthesizing high-quality, rich, and diverse training data across multiple domains and languages, thus enhancing the training pipeline. The Qwen3 Embedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both embedding and reranking tasks, addressing diverse deployment scenarios where users can optimize for either efficiency or effectiveness. Empirical evaluations demonstrate that the Qwen3 Embedding series achieves state-of-the-art results across diverse benchmarks. Notably, it excels on the multilingual evaluation benchmark MTEB for text embedding, as well as in various retrieval tasks, including code retrieval, cross-lingual retrieval and multilingual retrieval. To facilitate reproducibility and promote community-driven research and development, the Qwen3 Embedding models are publicly available under the Apache 2.0 license.","在这项工作中,我们引入了Quen3 嵌入系列,这是与其前一系列GTE-Quen系列相比的一个重大进步,其案文嵌入和重新排序能力以Quwen3基础模型为基础。利用Quen3 LLM在多语种文本理解和生成方面的强大能力,我们创新的多阶段培训管道将大规模未经监督的预培训与高质量数据集的监管微调相结合。有效的合并示范战略进一步确保了“253嵌入系列”的稳健性和适应性。在培训过程中,Quen3 LLM不仅作为骨干模型,而且还在综合高质量、丰富和多样化的培训数据方面发挥着关键作用,从而加强了培训管道。“Quen3 嵌入式系列”提供了一系列模型规模(0.6B、4B、8B),用于嵌入和重新排列任务,用户可以在其中优化效率或效果。在各种部署情景评估过程中,根据“基础”3 CEB-RO3 系列实现了高质量、丰富和多样化的跨语言版本的检索,包括升级的检索。","Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, Jingren Zhou",2025-06-05T15:49:48Z,Qwen3 Embedding: Advancing Text Embedding and Reranking Through   Foundation Models,Qwen3 Embedding: Advancing Text Embedding and Reranking by Foundation Models,wen3 嵌入式:通过基础模型推进文本嵌入和重新排位,http://arxiv.org/abs/2506.05176v1
35,"Intermediate Representations (IRs) play a critical role in compiler design and program analysis, yet their comprehension by Large Language Models (LLMs) remains underexplored. In this paper, we present an explorative empirical study evaluating the capabilities of six state-of-the-art LLMs: GPT-4, GPT-3, DeepSeek, Gemma 2, Llama 3, and Code Llama, in understanding IRs. Specifically, we assess model performance across four core tasks: control flow graph reconstruction, decompilation, code summarization, and execution reasoning. While LLMs exhibit competence in parsing IR syntax and identifying high-level structures, they consistently struggle with instruction-level reasoning, especially in control flow reasoning, loop handling, and dynamic execution. Common failure modes include misinterpreting branching instructions, omitting critical operations, and relying on heuristic reasoning rather than precise instruction-level logic. Our findings highlight the need for IR-specific enhancements in LLM design. We recommend fine-tuning on structured IR datasets and integrating control-flow-sensitive architectures to improve model effectiveness. All experimental data and source code are publicly available at","中间代表机构(IRs)在汇编者设计和程序分析方面发挥着关键作用,然而,大语言模型(LLMs)对其理解的程度仍未得到充分探讨。在本文件中,我们介绍了一项探索性经验研究,评价六种最先进的LMs的能力:GPT-4、GPT-3、DeepSeek、Gemma 2、Llama 3和代码Llama。具体地说,我们评估了四种核心任务的模型性能:控制流程图重建、解析、代码总和和执行推理。虽然LLMs在区分IR合成法和确定高级结构方面表现出了能力,但是它们始终在与指导层面的推理,特别是在控制流程推理、循环处理和动态执行方面,挣扎。常见的失败模式包括曲解的分支指令、省略关键操作,以及依赖超理论性推理而不是精确的指令级逻辑。我们的调查结果强调,需要具体地改进LLM设计中的IR系统设计。我们建议对结构化的IR数据集进行微调整,并整合控制流动敏感结构架构,以提高模型的有效性。所有实验数据和源代码都是公开提供的。","Hailong Jiang, Jianfeng Zhu, Yao Wan, Bo Fang, Hongyu Zhang, Ruoming Jin, Qiang Guan",2025-06-05T15:48:54Z,Can Large Language Models Understand Intermediate Representations in   Compilers?,Können große Sprachmodelle Zwischendarstellungen in Compilern verstehen?,大语言模式能理解《汇编者》的中间代表吗?,http://arxiv.org/abs/2502.06854v2
36,"Event Detection (ED) -- the task of identifying event mentions from natural language text -- is critical for enabling reasoning in highly specialized domains such as biomedicine, law, and epidemiology. Data generation has proven to be effective in broadening its utility to wider applications without requiring expensive expert annotations. However, when existing generation approaches are applied to specialized domains, they struggle with label noise, where annotations are incorrect, and domain drift, characterized by a distributional mismatch between generated sentences and the target domain. To address these issues, we introduce SNaRe, a domain-aware synthetic data generation framework composed of three components: Scout, Narrator, and Refiner. Scout extracts triggers from unlabeled target domain data and curates a high-quality domain-specific trigger list using corpus-level statistics to mitigate domain drift. Narrator, conditioned on these triggers, generates high-quality domain-aligned sentences, and Refiner identifies additional event mentions, ensuring high annotation quality. Experimentation on three diverse domain ED datasets reveals how SNaRe outperforms the best baseline, achieving average F1 gains of 3-7% in the zero-shot/few-shot settings and 4-20% F1 improvement for multilingual generation. Analyzing the generated trigger hit rate and human evaluation substantiates SNaRe's stronger annotation quality and reduced domain drift.","事件探测(ED) -- -- 查明自然语言文本中提到的事件的任务 -- -- 发现自然语言文本中提及的事件的任务 -- -- 对于生物医学、法律和流行病学等高度专门领域的有利推理至关重要。数据生成已证明能够有效地扩大其用途,使其在不需要昂贵专家说明的情况下适用于更广泛的应用。然而,当现有的新一代方法应用于专门领域时,它们会与标签噪音(其说明不正确)和以生成的句子和目标领域之间分配不匹配为特征的域流作斗争。为了解决这些问题,我们引入了域觉合成数据生成框架SNaRe,这是一个域-觉识合成数据生成框架,由三个部分组成:Scout、Narrator和Refineer。 Scout 提取数据从未贴标签的目标域数据中触发了它的实用性,并运用了高品质的域域域内触发清单,以降低域内漂移的特性。在触发点上,产生了高质量的域内校准,并确定了额外的事件,确保了高度认知质量。在三个不同的域内ED数据集的实验显示SNaReRe refref 如何改进了最佳基线,在零比基准中实现了平均F1至7%的平均收益,在零摄氏/触发率中,在更强烈的碎片质量和更强烈的生成中,在更精确的冲击质量上。","Tanmay Parekh, Yuxuan Dong, Lucas Bandarkar, Artin Kim, I-Hung Hsu, Kai-Wei Chang, Nanyun Peng",2025-06-05T15:45:00Z,SNaRe: Domain-aware Data Generation for Low-Resource Event Detection,SNARe: Domain-aware Datengenerierung für Low-Resource-Erkennung,SNaRe: 用于低资源事件探测的域觉数据生成,http://arxiv.org/abs/2502.17394v2
37,"Large Language Models (LLMs) have shown remarkable performance in Open-Domain Question Answering (ODQA) by leveraging external documents through Retrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer context, context compression is necessary. However, prior compression methods do not focus on filtering out non-evidential information, which limit the performance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or \textbf{ECoRAG} framework. ECoRAG improves LLM performance by compressing retrieved documents based on evidentiality, ensuring whether answer generation is supported by the correct evidence. As an additional step, ECoRAG reflects whether the compressed content provides sufficient evidence, and if not, retrieves more until sufficient. Experiments show that ECoRAG improves LLM performance on ODQA tasks, outperforming existing compression methods. Furthermore, ECoRAG is highly cost-efficient, as it not only reduces latency but also minimizes token usage by retaining only the necessary information to generate the correct answer. Code is available at https://github.com/ldilab/ECoRAG.","大型语言模型(LLMS)在开放域问答(ODQA)框架中表现出了显著的性能,通过检索启动生成(RAG)来利用外部文档。为了从更长远的上下文中减少RAG间接费用,必须进行上下文压缩。然而,先前的压缩方法并不侧重于过滤不明显的信息,这种信息限制了以LLM为基础的RAG的性能。因此,我们提议了身份证明制导RAG,或\textbf{ECoRAG}框架。 ECoRAG通过根据证据压缩检索到的文件,确保答案生成是否得到正确证据的支持,改进了LMM的性能。作为补充步骤,ECoRAG反映压缩内容是否提供了充分的证据,如果不是,则在足够之前检索更多。实验显示ECRAG改进了ODQA任务中的LM性能,超过了现有的压缩方法。此外,ECRAG不仅降低了延迟性,而且还通过只保留正确答案所需的信息来尽量减少象征性的使用。","Yeonseok Jeong, Jinsu Kim, Dohyeon Lee, Seung-won Hwang",2025-06-05T15:43:49Z,ECoRAG: Evidentiality-guided Compression for Long Context RAG,ECoRAG: Evidentialitätsgeführte Kompression für lange RAG-Kontext,ECORAG: 长期RAG的证据制导压缩,http://arxiv.org/abs/2506.05167v1
38,"Large Language Models (LLMs) are known to exhibit social, demographic, and gender biases, often as a consequence of the data on which they are trained. In this work, we adopt a mechanistic interpretability approach to analyze how such biases are structurally represented within models such as GPT-2 and Llama2. Focusing on demographic and gender biases, we explore different metrics to identify the internal edges responsible for biased behavior. We then assess the stability, localization, and generalizability of these components across dataset and linguistic variations. Through systematic ablations, we demonstrate that bias-related computations are highly localized, often concentrated in a small subset of layers. Moreover, the identified components change across fine-tuning settings, including those unrelated to bias. Finally, we show that removing these components not only reduces biased outputs but also affects other NLP tasks, such as named entity recognition and linguistic acceptability judgment because of the sharing of important components with these tasks.","大型语言模型(LLMS)通常因其所培训的数据而表现出社会、人口和性别偏见。在这项工作中,我们采用一种机械的解释方法,分析这些偏见在GPT-2和Llama2等模型中的结构代表性,重点是人口和性别偏见,我们探索不同的衡量标准,以确定对偏向行为负责的内部边缘。然后我们评估这些组成部分在数据集和语言差异之间的稳定性、本地化和可概括性。我们通过系统推介,表明与偏见有关的计算高度本地化,往往集中在一小部分层次上。此外,我们发现,在微调环境中,包括那些与偏见无关的方面,这些组成部分的变化不仅减少了偏向性产出,而且影响到其他国家语言方案的任务,例如,由于与这些任务共享重要组成部分,而命名实体的承认和语言上的可接受性判断。","Bhavik Chandna, Zubair Bashir, Procheta Sen",2025-06-05T15:43:34Z,Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective,Desecting Bias in LLMs: Eine mechanistische Interpretationsperspektive,在LLMM中分解偏见:机械解释视角,http://arxiv.org/abs/2506.05166v1
39,"As Large Language Models (LLMs) continue to exhibit increasingly human-like capabilities, aligning them with human values has become critically important. Contemporary advanced techniques, such as prompt learning and reinforcement learning, are being deployed to better align LLMs with human values. However, while these approaches address broad ethical considerations and helpfulness, they rarely focus on simulating individualized human value systems. To address this gap, we present ValueSim, a framework that simulates individual values through the generation of personal backstories reflecting past experiences and demographic information. ValueSim converts structured individual data into narrative backstories and employs a multi-module architecture inspired by the Cognitive-Affective Personality System to simulate individual values based on these narratives. Testing ValueSim on a self-constructed benchmark derived from the World Values Survey demonstrates an improvement in top-1 accuracy by over 10% compared to retrieval-augmented generation methods. Further analysis reveals that performance enhances as additional user interaction history becomes available, indicating the model's ability to refine its persona simulation capabilities over time.","由于大型语言模型(LLMS)继续表现出日益人性化的能力,使其与人类价值观相适应,这一点变得至关重要。现代先进技术,例如迅速学习和强化学习,正在被运用,以更好地使LMS与人类价值观相协调;然而,虽然这些方法涉及广泛的道德考虑和帮助,但很少侧重于模拟个性化人类价值系统。为了缩小这一差距,我们介绍了SvalueSim,这是一个通过生成反映过去经验和人口信息的个人后台来模拟个人价值的框架。价值Sim将结构化的个人数据转换成叙事后台,并使用由Cognitive-affective人格系统启发的多模块结构来模拟基于这些叙述的个人价值。根据世界价值调查得出的自建基准测试价值Sim显示,与检索的生成方法相比,头一的准确率提高了10%以上。进一步分析显示,随着用户的更多互动历史的出现,业绩会增强,表明模型有能力随着时间的推移改进其人性模拟能力。","Bangde Du, Ziyi Ye, Zhijing Wu, Jankowska Monika, Shuqi Zhu, Qingyao Ai, Yujia Zhou, Yiqun Liu",2025-06-05T15:41:26Z,ValueSim: Generating Backstories to Model Individual Value Systems,ValueSim: Erzeugung von Backstories zum Modell individueller Wertsysteme,价值模拟: 生成个人价值系统模型的备份,http://arxiv.org/abs/2505.23827v2
40,"Natural Language Processing (NLP) has become a cornerstone in many critical sectors, including healthcare, finance, and customer relationship management. This is especially true with the development and use of advanced models such as GPT-based architectures and BERT, which are widely used in decision-making processes. However, the black-box nature of these advanced NLP models has created an urgent need for transparency and explainability. This review explores explainable NLP (XNLP) with a focus on its practical deployment and real-world applications, examining its implementation and the challenges faced in domain-specific contexts. The paper underscores the importance of explainability in NLP and provides a comprehensive perspective on how XNLP can be designed to meet the unique demands of various sectors, from healthcare's need for clear insights to finance's emphasis on fraud detection and risk assessment. Additionally, this review aims to bridge the knowledge gap in XNLP literature by offering a domain-specific exploration and discussing underrepresented areas such as real-world applicability, metric evaluation, and the role of human interaction in model assessment. The paper concludes by suggesting future research directions that could enhance the understanding and broader application of XNLP.","自然语言处理(NLP)已成为许多关键部门的基石,包括保健、金融和客户关系管理。这尤其适用于开发和使用先进的模型,如在决策过程中广泛使用的基于GPT的建筑和BERT。然而,这些先进的NLP模型的黑箱性质造成了透明度和解释的迫切需要。本审查报告探索了可解释的NLP(XNLP),重点是其实际部署和实际应用,审查了其执行情况和在具体领域环境中面临的挑战。本文件强调了NLP的解释性的重要性,并全面阐述了XNLP如何设计以满足各部门的独特需求,从保健需要明确洞察力,为强调欺诈检测和风险评估提供资金。此外,本审查报告的目的是通过提供具体领域的探索,并讨论代表性不足的领域,如真实世界适用性、指标评估以及人类互动在模型评估中的作用,从而缩小XLP的知识差距。文件最后提出未来研究方向,可增进对XLP的理解和更广泛的应用。","Hadi Mohammadi, Ayoub Bagheri, Anastasia Giachanou, Daniel L. Oberski",2025-06-05T15:41:25Z,Explainability in Practice: A Survey of Explainable NLP Across Various   Domains,Erklärbarkeit in der Praxis: Eine Übersicht über erklärbare NLP über verschiedene Domains,实践中的可解释性:关于各种领域可解释的《国家劳工调查》的调查,http://arxiv.org/abs/2502.00837v2
41,"Retrieval-augmented generation (RAG) is a mainstream method for improving performance on knowledge-intensive tasks. However,current RAG systems often place too much emphasis on retrieved contexts. This can lead to reliance on inaccurate sources and overlook the model's inherent knowledge, especially when dealing with misleading or excessive information. To resolve this imbalance, we propose Knowledgeable-r1 that using joint sampling and define multi policy distributions in knowledge capability exploration to stimulate large language models'self-integrated utilization of parametric and contextual knowledge. Experiments show that Knowledgeable-r1 significantly enhances robustness and reasoning accuracy in both parameters and contextual conflict tasks and general RAG tasks, especially outperforming baselines by 17.07% in counterfactual scenarios and demonstrating consistent gains across RAG tasks. Our code are available at https://github.com/lcy80366872/ knowledgeable-r1.","为了解决这一不平衡现象,我们建议采用知识-r1,在知识能力探索中采用联合抽样和界定多种政策分布,以刺激大型语言模型自我综合利用参数和背景冲突任务及一般区域咨询小组任务,特别是反事实假设中17.07%的业绩基线超过17.7%,并显示整个区域咨询小组任务的一致成果。 我们的代码可在https://github.com/cy80366872/nown-r1上查阅。","Chenyu Lin, Yilin Wen, Du Su, Fei Sun, Muhan Chen, Chenfu Bao, Zhonghou Lv",2025-06-05T15:34:15Z,Knowledgeable-r1: Policy Optimization for Knowledge Exploration in   Retrieval-Augmented Generation,Knowledgeable-r1: Politikoptimierung für Wissensexploration in der retrieval-generierten Generation,可知识-r1:在回溯性回溯性养殖中知识探索的政策优化,http://arxiv.org/abs/2506.05154v1
42,"While Vision-Language Models (VLMs) have achieved competitive performance in various tasks, their comprehension of the underlying structure and semantics of a scene remains understudied. To investigate the understanding of VLMs, we study their capability regarding object properties and relations in a controlled and interpretable manner. To this scope, we introduce CIVET, a novel and extensible framework for systematiC evaluatIon Via controllEd sTimuli. CIVET addresses the lack of standardized systematic evaluation for assessing VLMs' understanding, enabling researchers to test hypotheses with statistical rigor. With CIVET, we evaluate five state-of-the-art VLMs on exhaustive sets of stimuli, free from annotation noise, dataset-specific biases, and uncontrolled scene complexity. Our findings reveal that 1) current VLMs can accurately recognize only a limited set of basic object properties; 2) their performance heavily depends on the position of the object in the scene; 3) they struggle to understand basic relations among objects. Furthermore, a comparative evaluation with human annotators reveals that VLMs still fall short of achieving human-level accuracy.","虽然视觉-语言模型(VLMS)在各种任务中取得了有竞争力的业绩,但它们对一个场景的基本结构和语义的理解仍然不足。为了调查对VLMs的理解,我们以可控制和可解释的方式研究它们关于物体属性和关系的能力。为了这一范围,我们引入了CEVIVT,这是系统化C 蒸发Ion ViacrectiveEd stimuli(VLMS) 的新和可扩展的框架。CIVIVT 解决了缺乏标准化的系统评价来评估VLMs的理解,使研究人员能够用统计的严谨度来测试各种假设。与CIVET一起,我们评估了五套最先进的VLMS(VLMS)关于全局性刺激的全局性,没有说明性噪音、特定数据偏差和不受控制的场景复杂性。我们的调查结果显示:(1) 目前的VLMS只能准确识别有限的一套基本物体属性;(2)其性能在很大程度上取决于物体在现场的位置;(3)他们难以理解物体之间的基本关系。此外,与人类的比较评估显示VLMs仍然没有达到人类水平的准确性。","Massimo Rizzoli, Simone Alghisi, Olha Khomyn, Gabriel Roccabruna, Seyed Mahed Mousavi, Giuseppe Riccardi",2025-06-05T15:27:16Z,CIVET: Systematic Evaluation of Understanding in VLMs,CIVET: Systematische Bewertung des Verständnisses in VLMs,CIVET: 系统评估对脆弱、危险、危险和,http://arxiv.org/abs/2506.05146v1
43,"Large Language Models (LLMs) are increasingly used as automated evaluators in natural language generation, yet it remains unclear whether they can accurately replicate human judgments of error severity. In this study, we systematically compare human and LLM assessments of image descriptions containing controlled semantic errors. We extend the experimental framework of van Miltenburg et al. (2020) to both unimodal (text-only) and multimodal (text + image) settings, evaluating four error types: age, gender, clothing type, and clothing colour. Our findings reveal that humans assign varying levels of severity to different error types, with visual context significantly amplifying perceived severity for colour and type errors. Notably, most LLMs assign low scores to gender errors but disproportionately high scores to colour errors, unlike humans, who judge both as highly severe but for different reasons. This suggests that these models may have internalised social norms influencing gender judgments but lack the perceptual grounding to emulate human sensitivity to colour, which is shaped by distinct neural mechanisms. Only one of the evaluated LLMs, Doubao, replicates the human-like ranking of error severity, but it fails to distinguish between error types as clearly as humans. Surprisingly, DeepSeek-V3, a unimodal LLM, achieves the highest alignment with human judgments across both unimodal and multimodal conditions, outperforming even state-of-the-art multimodal models.","大型语言模型(LLMS)在自然语言生成中越来越多地被用作自动评估员,但人们仍不清楚它们能否准确地复制人类对错误严重程度的判断。在本研究中,我们系统地比较人类和LLM对含有受控语义错误的图像描述的评估。我们把van Miltenburg 等人(2020年)的实验框架扩大到单式(文字版)和多式(文本+图像)设置,评价四种错误类型:年龄、性别、服装类型和服装颜色。我们的调查结果显示,人类对不同类型的错误有不同的严重程度,视觉环境大大扩大了对颜色和类型错误的认知严重程度。值得注意的是,大多数LLMS对性别错误的评分很低,但对颜色错误的评分过高,不同于人类,因为人类的评分都非常严重,但原因不同。这表明,这些模型可能已经内部化的社会规范影响性别判断,但缺乏模仿人类对颜色的敏感性的认知基础。只有一种经过评估的LMMoubao,复制了与人类相似的错误严重程度排序,但是它未能将错误类型明确区分为最高级的摩体型模型。","Diege Sun, Guanyi Chen, Fan Zhao, Xiaorong Cheng, Tingting He",2025-06-05T15:24:33Z,Do Large Language Models Judge Error Severity Like Humans?,Urteilen große Sprachmodelle über Schwerelosigkeit wie Menschen?,大语言模型法官 误差严重像人类吗?,http://arxiv.org/abs/2506.05142v1
44,"Understanding the internal mechanisms of large audio-language models (LALMs) is crucial for interpreting their behavior and improving performance. This work presents the first in-depth analysis of how LALMs internally perceive and recognize auditory attributes. By applying vocabulary projection on three state-of-the-art LALMs, we track how attribute information evolves across layers and token positions. We find that attribute information generally decreases with layer depth when recognition fails, and that resolving attributes at earlier layers correlates with better accuracy. Moreover, LALMs heavily rely on querying auditory inputs for predicting attributes instead of aggregating necessary information in hidden states at attribute-mentioning positions. Based on our findings, we demonstrate a method to enhance LALMs. Our results offer insights into auditory attribute processing, paving the way for future improvements.","了解大型音频模型的内部机制(LALMs)对于解释其行为和改进性能至关重要,这项工作首次深入分析了LALMs内部如何看待和承认听觉属性。我们通过对三个最先进的LALMs应用词汇预测,追踪信息如何跨层次和象征性位置的属性。我们发现,识别失败时,属性信息通常随着层深而减少,而先前层的属性的解决则更加准确。此外,LALMs严重依赖询问性能投入来预测属性,而不是在属性定位位置的隐藏状态中汇总必要信息。根据我们的调查结果,我们展示了一种加强LALMs的方法。我们的结果为听力属性处理提供了深刻的洞见,为未来的改进铺平了道路。","Chih-Kai Yang, Neo Ho, Yi-Jyun Lee, Hung-yi Lee",2025-06-05T15:22:47Z,AudioLens: A Closer Look at Auditory Attribute Perception of Large   Audio-Language Models,AudioLens: Ein genauerer Blick auf auditory Attribut Wahrnehmung von großen Audio-Sprachen-Modellen,音频路程:更仔细地审视大型音频语言模型的听觉特性,http://arxiv.org/abs/2506.05140v1
45,"Inductive biases are inherent in every machine learning system, shaping how models generalize from finite data. In the case of neural language models (LMs), debates persist as to whether these biases align with or diverge from human processing constraints. To address this issue, we propose a quantitative framework that allows for controlled investigations into the nature of these biases. Within our framework, we introduce $m$-local entropy$\unicode{x2013}$an information-theoretic measure derived from average lossy-context surprisal$\unicode{x2013}$that captures the local uncertainty of a language by quantifying how effectively the $m-1$ preceding symbols disambiguate the next symbol. In experiments on both perturbed natural language corpora and languages defined by probabilistic finite-state automata (PFSAs), we show that languages with higher $m$-local entropy are more difficult for Transformer and LSTM LMs to learn. These results suggest that neural LMs, much like humans, are highly sensitive to the local statistical structure of a language.","在每个机器学习系统中,都有内在的感知偏差,这决定了模型如何从有限数据中概括。在神经语言模型(LMs)中,关于这些偏差是否与人的处理限制一致或不同的辩论持续存在。为了解决这一问题,我们提议了一个量化框架,以便能够对这些偏差的性质进行有控制的调查。在我们的框架内,我们引入了来自平均丢失的单价超值=unicode{x2013}的信息-理论计量,通过量化先前的1美元符号如何有效地使下一个符号脱钩而捕捉到一种语言的本地不确定性。在关于近乎自然语言的实验中,我们发现,高于1美元本地的酶语言对于变换器和LSTMLMS来说更加困难。这些结果表明,神经LMs(与人类非常相似)对于一种语言的本地统计结构非常敏感。","Taiga Someya, Anej Svete, Brian DuSell, Timothy J. O'Donnell, Mario Giulianelli, Ryan Cotterell",2025-06-05T15:21:05Z,Information Locality as an Inductive Bias for Neural Language Models,Informationslokalität als induktive Bias für neurale Sprachmodelle,信息地点作为神经语言模型的感性偏见,http://arxiv.org/abs/2506.05136v1
46,"A hallmark of human innovation is the process of recombination -- creating original ideas by integrating elements of existing mechanisms and concepts. In this work, we automatically mine the scientific literature and build CHIMERA: a large-scale knowledge base (KB) of recombination examples. CHIMERA can be used to empirically explore at scale how scientists recombine concepts and take inspiration from different areas, or to train supervised machine learning models that learn to predict new creative cross-domain directions. To build this KB, we present a novel information extraction task of extracting recombination from scientific paper abstracts, collect a high-quality corpus of hundreds of manually annotated abstracts, and use it to train an LLM-based extraction model. The model is applied to a large corpus of papers in the AI domain, yielding a KB of over 28K recombination examples. We analyze CHIMERA to explore the properties of recombination in different subareas of AI. Finally, we train a scientific hypothesis generation model using the KB, which predicts new recombination directions that real-world researchers find inspiring. Our data and code are available at https://github.com/noy-sternlicht/CHIMERA-KB","人类创新的一个标志是重组过程 -- -- 通过整合现有机制和概念的元素,创造原始想法。在这项工作中,我们自动地将科学文献埋设为地雷,并建造CHIMERA:一个大规模再融合范例的知识库。CHIMERA可以用来在经验上大规模探索科学家再融合概念和从不同领域获得灵感的方式,或者用来培训监督的机器学习模型,以学会预测新的创造性跨主题方向。为了构建这个KB,我们提出了一个新颖的信息提取任务,从科学论文摘要中提取再融合,收集了数百个人工注释摘要的高品质材料,并用来培训一个基于LLM的提取模型。该模型适用于AI领域的大量论文,产生了28K再融合范例的KB。我们分析了CHIMERA,以探索AI不同子领域再融合的特性。最后,我们用KB培训了一个科学假设生成模型,预测了现实世界研究人员发现的新的再融合方向。我们的数据和代码可以在 https://HIA/CIRA。","Noy Sternlicht, Tom Hope",2025-06-05T15:20:59Z,CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature,CHIMERA: Eine Wissensbasis der Ideenrekombination in der wissenschaftlichen Literatur,CHIMERA:科学文献中思想再融合的知识库,http://arxiv.org/abs/2505.20779v3
47,"Zero-shot Event Detection (ED), the task of identifying event mentions in natural language text without any training data, is critical for document understanding in specialized domains. Understanding the complex event ontology, extracting domain-specific triggers from the passage, and structuring them appropriately overloads and limits the utility of Large Language Models (LLMs) for zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent reasoning framework that decouples the task of ED using Dreamer and Grounder. Dreamer encourages divergent reasoning through open-ended event discovery, which helps to boost event coverage. Conversely, Grounder introduces convergent reasoning to align the free-form predictions with the task-specific instructions using finite-state machine guided constrained decoding. Additionally, an LLM-Judge verifies the final outputs to ensure high precision. Through extensive experiments on six datasets across five domains and nine LLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot, transfer-learning, and reasoning baselines, achieving 4-7% average F1 gains over the best baseline -- establishing DiCoRe as a strong zero-shot ED framework.","“零点事件探测”(ED)是查明自然语言文本中提到的事件的任务,没有任何培训数据,这是在自然语言文本中发现事件的任务,对于在专门领域理解文件至关重要。理解复杂的事件本体学,从通道中提取特定域的触发器,并适当地安排它们超载和限制大语言模型(LLMs)对零点 ED的效用。为此,我们提议DicoRe,这是一个使用Dreamer和Loorer来拆分ED任务的不同一致的推理框架。Dreater鼓励通过开放式事件发现进行不同的推理,这有助于提高事件覆盖面。相反,Gloorer采用趋同推理法,利用固定状态的机器制导解码,使自由形式预测与任务特定指示相一致。此外,LLMM-Judge核查最后产出以确保高度精确性。通过对五个领域和九个LMMS的六个数据集进行广泛的实验,我们证明DocoRe在零点、转移-学习和推理基线上始终超越前的精确度,在最佳基线上取得了4-7%的平均F1收益 -- -- 将DCO作为强有力的零点框架。","Tanmay Parekh, Kartik Mehta, Ninareh Mehrabi, Kai-Wei Chang, Nanyun Peng",2025-06-05T15:16:14Z,DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM   Reasoning,DiCoRe: Erweitern der Null-Shot-Erkennung durch Divergent-Convergent LLM Reasoning,"DiCore: 通过差异-说服者LLM 合理性,加强零射事件探测",http://arxiv.org/abs/2506.05128v1
48,"Accurate and verifiable large language model (LLM) simulations of human research subjects promise an accessible data source for understanding human behavior and training new AI systems. However, results to date have been limited, and few social scientists have adopted this method. In this position paper, we argue that the promise of LLM social simulations can be achieved by addressing five tractable challenges. We ground our argument in a review of empirical comparisons between LLMs and human research subjects, commentaries on the topic, and related work. We identify promising directions, including context-rich prompting and fine-tuning with social science datasets. We believe that LLM social simulations can already be used for pilot and exploratory studies, and more widespread use may soon be possible with rapidly advancing LLM capabilities. Researchers should prioritize developing conceptual models and iterative evaluations to make the best use of new AI systems.","精确和可核查的大型语言模型(LLM)模拟人类研究科目,为了解人类行为和培训新的AI系统提供了可获取的数据源;然而,迄今为止,结果有限,社会科学家很少采用这一方法;在本立场文件中,我们认为,通过应对五个可移动的挑战,可以实现LLM社会模拟的许诺;我们在审查LLMS与人类研究课题、关于该专题的评注和相关工作的经验性比较、我们的论点中,可以找到我们的论点;我们确定了有希望的方向,包括环境丰富的促进和与社会科学数据集的微调;我们认为,LLM社会模拟已经可用于试点和探索性研究,而且随着LM能力的迅速提高,可能很快可以更广泛地使用。研究人员应该优先发展概念模型和迭代评价,以便最佳地利用新的AI系统。","Jacy Reese Anthis, Ryan Liu, Sean M. Richardson, Austin C. Kozlowski, Bernard Koch, James Evans, Erik Brynjolfsson, Michael Bernstein",2025-06-05T15:12:55Z,LLM Social Simulations Are a Promising Research Method,LLM Sozialsimulationen sind eine vielversprechende Forschungsmethode,LLM LLM 社会模拟是一种有希望的研究方法,http://arxiv.org/abs/2504.02234v2
49,"A recent line of research on spoken language assessment (SLA) employs neural models such as BERT and wav2vec 2.0 (W2V) to evaluate speaking proficiency across linguistic and acoustic modalities. Although both models effectively capture features relevant to oral competence, each exhibits modality-specific limitations. BERT-based methods rely on ASR transcripts, which often fail to capture prosodic and phonetic cues for SLA. In contrast, W2V-based methods excel at modeling acoustic features but lack semantic interpretability. To overcome these limitations, we propose a system that integrates W2V with Phi-4 multimodal large language model (MLLM) through a score fusion strategy. The proposed system achieves a root mean square error (RMSE) of 0.375 on the official test set of the Speak & Improve Challenge 2025, securing second place in the competition. For comparison, the RMSEs of the top-ranked, third-ranked, and official baseline systems are 0.364, 0.384, and 0.444, respectively.","最近关于口语评估的研究线采用了诸如BERT和wav2vec 2.0(W2V)等神经模型,以评价语言和声学模式的熟练程度。虽然这两种模型都有效地抓住了与口述能力有关的特征,但每个展品模式都有其局限性。基于语言评估的研究线依据ASR的笔录,这些笔录往往未能捕捉到对语言评估的预感和语音提示。相比之下,基于W2V的方法在模拟声学特征方面十分出色,但缺乏语义解释性。为了克服这些局限性,我们提议采用一个系统,通过得分组合战略,将W2V与Phi-4多式大语言模型(MLMLM)结合起来。拟议系统在2025年话语与改进挑战的正式测试组中,在获得竞争第二位后,在2025年话与改进挑战的正式测试中,出现了0.375的根正方差(RMSE)。相比之下,最高级、排位和官方基线系统的RMSE分别是0.364、0.384和0.4和0.444。","Hong-Yun Lin, Tien-Hong Lo, Yu-Hsuan Fang, Jhen-Ke Lin, Chung-Chun Wang, Hao-Chien Lu, Berlin Chen",2025-06-05T15:09:23Z,The NTNU System at the S&I Challenge 2025 SLA Open Track,Das NTNU-System bei der S&I Challenge 2025 SLA Open Track,S&I挑战2025年S&I挑战的NNNU系统,http://arxiv.org/abs/2506.05121v1
50,"Role-Playing Agents (RPAs), benefiting from large language models, is an emerging interactive AI system that simulates roles or characters with diverse personalities. However, existing methods primarily focus on mimicking dialogues among roles in textual form, neglecting the role's voice traits (e.g., voice style and emotions) as playing a crucial effect in interaction, which tends to be more immersive experiences in realistic scenarios. Towards this goal, we propose OmniCharacter, a first seamless speech-language personality interaction model to achieve immersive RPAs with low latency. Specifically, OmniCharacter enables agents to consistently exhibit role-specific personality traits and vocal traits throughout the interaction, enabling a mixture of speech and language responses. To align the model with speech-language scenarios, we construct a dataset named OmniCharacter-10K, which involves more distinctive characters (20), richly contextualized multi-round dialogue (10K), and dynamic speech response (135K). Experimental results showcase that our method yields better responses in terms of both content and style compared to existing RPAs and mainstream speech-language models, with a response latency as low as 289ms. Code and dataset are available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/OmniCharacter.","角色扮演代理机构(RPA)受益于大型语言模型,是一个新兴的交互式AI系统,它模拟不同人格的角色或角色,但是,现有方法主要侧重于模拟角色之间在文本形式上的对话,忽视角色的语音特征(例如,语音风格和情绪)在互动中发挥着关键作用,这往往在现实情景中更显眼。为了实现这一目标,我们建议OmniCharacter(OmniCharacter),这是第一个无缝的语音-语言人格互动模式,目的是在低长的状态下实现隐蔽的RPA。具体地说,OmniCharacter使代理人能够在整个互动中持续展示特定角色的个性特征和发声特征,使言论和语言反应的混合。为了使该模式与语言情景相一致,我们构建了一个名为OmniCharacter-10K的数据集,其中涉及更独特的字符(20个)、内容丰富的背景化多轮对话(10K)和动态的语音反应(135K)。实验结果显示,我们的方法在内容和风格上与现有的RPA/RAPA/CRASASA/ASyalASyalASyal反应模式相比,在可使用的低读/CRAA/CRASyalexA/ASyalex/ASyal/ASyal/C/ASyalcommexemmalex/A/ASyalSyalex/A/ADRA/CRASyalemss/CRASmex反应方面都有更好的反应。","Haonan Zhang, Run Luo, Xiong Liu, Yuchuan Wu, Ting-En Lin, Pengpeng Zeng, Qiang Qu, Feiteng Fang, Min Yang, Lianli Gao, Jingkuan Song, Fei Huang, Yongbin Li",2025-06-05T14:53:28Z,OmniCharacter: Towards Immersive Role-Playing Agents with Seamless   Speech-Language Personality Interaction,OmniCharacter: Auf dem Weg zu immersiven Rollenspiel-Agenten mit nahtloser Sprach-Persönlichkeits-Interaktion,OmniCharacter:争取用无缝无言语-语言个性交互作用来模拟角色扮演剂,http://arxiv.org/abs/2505.20277v2
51,"Misleading text detection on social media platforms is a critical research area, as these texts can lead to public misunderstanding, social panic and even economic losses. This paper proposes a novel framework - CL-ISR (Contrastive Learning and Implicit Stance Reasoning), which combines contrastive learning and implicit stance reasoning, to improve the detection accuracy of misleading texts on social media. First, we use the contrastive learning algorithm to improve the model's learning ability of semantic differences between truthful and misleading texts. Contrastive learning could help the model to better capture the distinguishing features between different categories by constructing positive and negative sample pairs. This approach enables the model to capture distinguishing features more effectively, particularly in linguistically complicated situations. Second, we introduce the implicit stance reasoning module, to explore the potential stance tendencies in the text and their relationships with related topics. This method is effective for identifying content that misleads through stance shifting or emotional manipulation, because it can capture the implicit information behind the text. Finally, we integrate these two algorithms together to form a new framework, CL-ISR, which leverages the discriminative power of contrastive learning and the interpretive depth of stance reasoning to significantly improve detection effect.","在社交媒体平台上误差的文本检测是一个关键的研究领域,因为这些文本可能导致公众误解、社会恐慌甚至经济损失。本文件提出一个新的框架----CL-ISR(学习和隐含原因),将对比学习和隐含立场推理结合起来,以提高社交媒体误导文本的检测准确性。首先,我们使用对比学习算法来提高模式在真实文本和误导文本之间的语义差异方面的学习能力。相反的学习有助于模型通过建立正对和负抽样来更好地捕捉不同类别之间的不同特征。这一方法使模型能够更有效地捕捉显著特征,特别是在语言复杂的情况下。第二,我们引入隐含的立场推理模块,以探索文本中的潜在立场倾向及其与相关主题的关系。这种方法对于通过立场转移或情感操纵来识别误导内容是有效的,因为它可以捕捉到文本背后的隐含信息。最后,我们将这两种算法结合起来,形成一个新的框架,即CL-ISR,利用对比学习和解释深度的判断力,大大改进了理性思维力。","Tianyi Huang, Zikun Cui, Cuiqianhe Du, Chia-En Chiang",2025-06-05T14:52:28Z,CL-ISR: A Contrastive Learning and Implicit Stance Reasoning Framework   for Misleading Text Detection on Social Media,CL-ISR: Ein kontrastives Lern- und Implizit-Stance-Reasoning-Framework für irreführende Texterkennung in sozialen Medien,CL-ISR: 社交媒体错误领导层文字探测错误文本的矛盾学习和隐含理由依据框架,http://arxiv.org/abs/2506.05107v1
52,"The rise of general-purpose artificial intelligence (AI) systems, particularly large language models (LLMs), has raised pressing moral questions about how to reduce bias and ensure fairness at scale. Researchers have documented a sort of ""bias"" in the significant correlations between demographics (e.g., race, gender) in LLM prompts and responses, but it remains unclear how LLM fairness could be evaluated with more rigorous definitions, such as group fairness or fair representations. We analyze a variety of technical fairness frameworks and find inherent challenges in each that make the development of a fair LLM intractable. We show that each framework either does not logically extend to the general-purpose AI context or is infeasible in practice, primarily due to the large amounts of unstructured training data and the many potential combinations of human populations, use cases, and sensitive attributes. These inherent challenges would persist for general-purpose AI, including LLMs, even if empirical challenges, such as limited participatory input and limited measurement methods, were overcome. Nonetheless, fairness will remain an important type of model evaluation, and there are still promising research directions, particularly the development of standards for the responsibility of LLM developers, context-specific evaluations, and methods of iterative, participatory, and AI-assisted evaluation that could scale fairness across the diverse contexts of modern human-AI interaction.","通用人工智能(AI)系统,特别是大型语言模型(LLMS)的兴起,提出了关于如何减少偏见和确保规模公平化的紧迫道德问题;研究人员在LLM的提示和反应中记录了在人口统计(例如种族、性别)之间重要相互关系中的某种“偏差”,但在LLM的提示和反应中仍然不清楚如何用更严格的定义来评价LLM公平,例如群体公平或公平代表等;我们分析各种技术公平框架,发现每个框架都存在内在挑战,使得公平的LLM难以发展;我们表明,每个框架在逻辑上并不延伸至通用的AI背景或在实践中不可行,主要是因为大量没有结构的培训数据以及人类人口、使用案例和敏感属性的许多潜在组合;即使克服了经验性挑战,例如参与性投入有限和计量方法有限,但LLMMM的公平性将持续存在。","Jacy Anthis, Kristian Lum, Michael Ekstrand, Avi Feller, Chenhao Tan",2025-06-05T14:35:42Z,The Impossibility of Fair LLMs,Die Unmöglichkeit fairer LLMs,公平专利Ms的不可行性,http://arxiv.org/abs/2406.03198v2
53,"Standard benchmarks of bias and fairness in large language models (LLMs) measure the association between the user attributes stated or implied by a prompt and the LLM's short text response, but human-AI interaction increasingly requires long-form and context-specific system output to solve real-world tasks. In the commonly studied domain of gender-occupation bias, we test whether these benchmarks are robust to lengthening the LLM responses as a measure of Realistic Use and Tangible Effects (i.e., RUTEd evaluations). From the current literature, we adapt three standard bias metrics (neutrality, skew, and stereotype) and develop analogous RUTEd evaluations from three contexts of real-world use: children's bedtime stories, user personas, and English language learning exercises. We find that standard bias metrics have no significant correlation with the more realistic bias metrics. For example, selecting the least biased model based on the standard ""trick tests"" coincides with selecting the least biased model as measured in more realistic use no more than random chance. We suggest that there is not yet evidence to justify standard benchmarks as reliable proxies of real-world AI biases, and we encourage further development of evaluations grounded in particular contexts.","在大型语言模型(LLMs)中,标准偏向和公平基准的标准基准衡量用户特征之间的关联性,这些属性由迅速和LLM的简短文本反应所说明或暗示,但人类-AI的互动日益需要长式和因地制宜的系统产出来解决现实世界的任务。在通常研究的性别-职业偏见领域,我们测试这些基准是否足以延长LLM的响应时间,作为衡量现实使用和有形影响的一种尺度(即REUTEd评价)。从目前的文献中,我们调整了三种标准偏向指标(中性、扭曲和刻板化),并从现实世界使用的三种情况(儿童床边故事、用户和英语语言学习练习)中制定类似的RUTEd评价。我们发现,标准的偏向性指标与更现实的偏差指标没有重大关联。例如,根据标准的“Trick测试”选择最不偏差的模式与选择最接近于更现实使用衡量的偏差模式,而不是随机机会。我们指出,尚没有证据表明标准基准是真实的,在现实世界的偏差背景下,我们鼓励进一步评估。","Kristian Lum, Jacy Reese Anthis, Kevin Robinson, Chirag Nagpal, Alexander D'Amour",2025-06-05T14:35:00Z,Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation,Bias in Language Models: Jenseits von Tricktests und hin zu RUTEd-Evaluierung,语言模式中的偏见:超越欺骗性测试和争取公正评价,http://arxiv.org/abs/2402.12649v3
54,"While objective street metrics derived from imagery or GIS have become standard in urban analytics, they remain insufficient to capture subjective perceptions essential to inclusive urban design. This study introduces a novel Multimodal Street Evaluation Framework (MSEF) that fuses a vision transformer (VisualGLM-6B) with a large language model (GPT-4), enabling interpretable dual-output assessment of streetscapes. Leveraging over 15,000 annotated street-view images from Harbin, China, we fine-tune the framework using LoRA and P-Tuning v2 for parameter-efficient adaptation. The model achieves an F1 score of 0.84 on objective features and 89.3 percent agreement with aggregated resident perceptions, validated across stratified socioeconomic geographies. Beyond classification accuracy, MSEF captures context-dependent contradictions: for instance, informal commerce boosts perceived vibrancy while simultaneously reducing pedestrian comfort. It also identifies nonlinear and semantically contingent patterns -- such as the divergent perceptual effects of architectural transparency across residential and commercial zones -- revealing the limits of universal spatial heuristics. By generating natural-language rationales grounded in attention mechanisms, the framework bridges sensory data with socio-affective inference, enabling transparent diagnostics aligned with SDG 11. This work offers both methodological innovation in urban perception modeling and practical utility for planning systems seeking to reconcile infrastructural precision with lived experience.","虽然来自图像或地理信息系统的客观街道指标已成为城市分析的标准,但仍不足以反映包容性城市设计所必需的主观认识。本研究推出一个新的多式街道评价框架(MSEF),将具有大语言模型(GPT-4)的愿景变压器(VisualGLM-6B)结合成一个大语言模型(GPT-4),便于对街头景色进行可解释的双输出评估。我们利用中国哈尔滨的15 000多张附加说明的街头景象图像,对使用LORA和P-Tuning v2的参数高效适应框架进行微调。模型在客观特征和89.3%协议方面达到F1分0.84和综合居民认知的89.3%的F1分。除了分类准确性外,MSEF还捕捉了背景上的矛盾:例如,非正规商业刺激了人们所感觉到的振动性,同时降低了行人舒适感。它还确定了非线性和内和内线性应急模式,例如建筑透明度在住宅区和商业区之间产生的视觉影响 -- 揭示了普遍空间重复的局限性。通过在建筑结构分析机制中创造出自然语言的精确性概念,从而寻求关注,从而形成具有可持续性的精确性定义的诊断性的定义框架。",HaoTian Lan,2025-06-05T14:34:04Z,Interpretable Multimodal Framework for Human-Centered Street Assessment:   Integrating Visual-Language Models for Perceptual Urban Diagnostics,Interpretable Multimodal Framework for Human-Centered Street Assessment: Integrating Visual-Language Models for Perceptual Urban Diagnostics,以人为中心的街头评估可解释的多模式框架:将视觉语言模型纳入感知城市诊断,http://arxiv.org/abs/2506.05087v1
55,"The commercial vitality of community-scale streets in Chinese cities is shaped by complex interactions between vehicular accessibility, environmental quality, and pedestrian perception. This study proposes an interpretable, image-based framework to examine how street-level features -- including parked vehicle density, greenery, cleanliness, and street width -- impact retail performance and user satisfaction in Harbin, China. Leveraging street view imagery and a multimodal large language model (VisualGLM-6B), we construct a Community Commercial Vitality Index (CCVI) from Meituan and Dianping data and analyze its relationship with spatial attributes extracted via GPT-4-based perception modeling. Our findings reveal that while moderate vehicle presence may enhance commercial access, excessive on-street parking -- especially in narrow streets -- erodes walkability and reduces both satisfaction and shop-level pricing. In contrast, streets with higher perceived greenery and cleanliness show significantly greater satisfaction scores but only weak associations with pricing. Street width moderates the effects of vehicle presence, underscoring the importance of spatial configuration. These results demonstrate the value of integrating AI-assisted perception with urban morphological analysis to capture non-linear and context-sensitive drivers of commercial success. This study advances both theoretical and methodological frontiers by highlighting the conditional role of vehicle activity in neighborhood commerce and demonstrating the feasibility of multimodal AI for perceptual urban diagnostics. The implications extend to urban design, parking management, and scalable planning tools for community revitalization.","中国城市社区街道的商业活力取决于车辆无障碍性、环境质量和行人观念之间的复杂互动关系。本研究报告提出一个可解释的、基于图像的框架,以审查街道层面的特点 -- -- 包括停放车辆密度、绿绿绿、清洁和街道宽度 -- -- 如何影响中国哈尔滨的零售业绩和用户满意度。利用街头观景图像和多式大型语言模型(VisualGLM-6B),我们从Meituan和Dianping数据中构建了社区商业生命指数(CCVI),并分析其与通过GPT-4基于观点的振兴模型得出的空间属性之间的关系。我们的调查结果显示,虽然中度车辆的存在可以加强商业准入,过度的街道停车场停车场停车场停车场停车场停放 -- -- 特别是在狭窄的街道上 -- -- 削弱行驶能力,降低满意度和商店一级定价。相比之下,认为绿色和清洁度较高的街道景色图像展示了车辆存在的效果,强调了空间配置的重要性。这些结果表明,将人工辅助的观念与城市形态化分析结合起来,可以加强商用车辆管理工具的不线上和背景分析,从而展示了机动性机动性车辆管理工具,从而展示了机动性地展示了机动性管理工具,从而展示了机动性地展示了机动性管理工具,从而展示了机动性地展示了机动性管理。",HaoTian Lan,2025-06-05T14:28:48Z,"Parking, Perception, and Retail: Street-Level Determinants of Community   Vitality in Harbin","Parken, Wahrnehmung und Einzelhandel: Street-Level Determinanten der gemeinschaftlichen Vitalität in Harbin",泊车、观感和零售:哈尔滨社区生命的街道一级决定因素,http://arxiv.org/abs/2506.05080v1
56,"Self-harm detection on social media is critical for early intervention and mental health support, yet remains challenging due to the subtle, context-dependent nature of such expressions. Identifying self-harm intent aids suicide prevention by enabling timely responses, but current large language models (LLMs) struggle to interpret implicit cues in casual language and emojis. This work enhances LLMs' comprehension of self-harm by distinguishing intent through nuanced language-emoji interplay. We present the Centennial Emoji Sensitivity Matrix (CESM-100), a curated set of 100 emojis with contextual self-harm interpretations and the Self-Harm Identification aNd intent Extraction with Supportive emoji sensitivity (SHINES) dataset, offering detailed annotations for self-harm labels, casual mentions (CMs), and serious intents (SIs). Our unified framework: a) enriches inputs using CESM-100; b) fine-tunes LLMs for multi-task learning: self-harm detection (primary) and CM/SI span detection (auxiliary); c) generates explainable rationales for self-harm predictions. We evaluate the framework on three state-of-the-art LLMs-Llama 3, Mental-Alpaca, and MentalLlama, across zero-shot, few-shot, and fine-tuned scenarios. By coupling intent differentiation with contextual cues, our approach commendably enhances LLM performance in both detection and explanation tasks, effectively addressing the inherent ambiguity in self-harm signals. The SHINES dataset, CESM-100 and codebase are publicly available at: https://www.iitp.ac.in/~ai-nlp-ml/resources.html#SHINES .","在社交媒体上自我伤害检测对于早期干预和心理健康支持至关重要,但由于这些表达方式的微妙和因地制宜的性质,仍然具有挑战性。确定自我伤害意图有助于通过及时反应进行自杀预防,但目前大型语言模型(LLMS)在解释临时语言和感化语言的隐含提示方面挣扎。这项工作通过细微的语言-感化互动来区分意图,提高了LLMS对自我伤害的理解。我们展示了百年Emoji感知母体(CESM-100),一套由100个具有背景自我伤害解释的感官和100个内部识别(SHIM)内在识别意图有助于进行自杀预防,通过支持感化感应(SHINES)数据集,为自我伤害标签、随意提及(CM)和严肃意图(SI)提供详细的说明。我们的统一框架:(a)利用CESM-100来丰富投入;(b)微调LMLMLML学习:自我伤害检测(初级)和CM-SHAR-S-SQ-C-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SQ-SLLLLAR-S-SQ-SD-SD-SD-SD-s-S-S-SD-SD-C-S-S-S-S-S-S-S-S-S-S-S-S-S-S-SQ-SQ-SQ-SQ-SQ-SD-SL-S-SL-SL-SL-SL-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S","Soumitra Ghosh, Gopendra Vikram Singh, Shambhavi, Sabarna Choudhury, Asif Ekbal",2025-06-05T14:19:48Z,Just a Scratch: Enhancing LLM Capabilities for Self-harm Detection   through Intent Differentiation and Emoji Interpretation,Nur ein Kratzer: Verbesserung der LLM-Fähigkeiten für die Selbst-Schaden-Erkennung durch Intent Differentiation und Emoji-Interpretation,"仅仅一个缩略图:通过内在差别和Emoji解释,提高自残检测的LLM能力",http://arxiv.org/abs/2506.05073v1
57,"Large language models (LLMs) possess strong multilingual capabilities, and combining Reinforcement Learning from Human Feedback (RLHF) with translation tasks has shown great potential. However, we observe that this paradigm performs unexpectedly poorly when applied to colloquial subtitle translation tasks. In this work, we investigate this issue and find that the offline reward model (RM) gradually diverges from the online LLM due to distributional shift, ultimately leading to undesirable training outcomes. To address this, we propose RIVAL, an adversarial training framework that formulates the process as a min-max game between the RM and the LLM. RIVAL iteratively updates the both models, with the RM trained to distinguish strong from weak translations (qualitative preference reward), and the LLM trained to enhance its translation for closing this gap. To stabilize training and improve generalizability, we also incorporate quantitative preference reward (e.g., BLEU) into the RM, enabling reference-free quality modeling aligned with human evaluation. Through extensive experiments, we demonstrate that the proposed adversarial training framework significantly improves upon translation baselines.","大型语言模式(LLMS)拥有强大的多语种能力,并将人文反馈强化学习(RLHF)与翻译任务相结合,显示出巨大的潜力。然而,我们注意到,这一模式在应用到口头字幕翻译任务时出人意料地表现不佳。在这项工作中,我们调查这一问题,发现离线奖励模式(RM)由于分配性转移而逐渐与在线LLM脱钩,最终导致不良的培训结果。为此,我们建议RIVAL,这是一个对抗性培训框架,将这一过程发展为RM与LLM之间的一个微积分游戏。RIVAL反复更新这两个模式,而RM受过培训,以区别翻译能力强和微的翻译(Qaliplipecal progy),LLM则受过培训,以加强其翻译,以弥合这一差距。为了稳定培训和提高通用性,我们还将数量优惠奖励(例如BLEU)纳入RMM,使无参考质量模型与人类评价相一致。通过广泛的实验,我们证明拟议的对抗性培训框架在翻译基线上大大改进了。","Tianjiao Li, Mengran Yu, Chenyu Shi, Yanjun Zhao, Xiaojing Liu, Qiang Zhang, Qi Zhang, Xuanjing Huang, Jiayin Wang",2025-06-05T14:18:21Z,RIVAL: Reinforcement Learning with Iterative and Adversarial   Optimization for Machine Translation,RIVAL: Verstärktes Lernen mit iterativer und adversarieller Optimierung für maschinelle Übersetzung,RIV: 机械翻译的循环和逆向优化强化学习,http://arxiv.org/abs/2506.05070v1
58,"Large Language Models (LLMs) perform exceedingly well in Natural Language Understanding (NLU) tasks for many languages including English. However, despite being the fifth most-spoken language globally, Grammatical Error Correction (GEC) in Bangla remains underdeveloped. In this work, we investigate how LLMs can be leveraged for improving Bangla GEC. For that, we first do an extensive categorization of 12 error classes in Bangla, and take a survey of native Bangla speakers to collect real-world errors. We next devise a rule-based noise injection method to create grammatically incorrect sentences corresponding to correct ones. The Vaiyakarana dataset, thus created, consists of 5,67,422 sentences of which 2,27,119 are erroneous. This dataset is then used to instruction-tune LLMs for the task of GEC in Bangla. Evaluations show that instruction-tuning with \name improves GEC performance of LLMs by 3-7 percentage points as compared to the zero-shot setting, and makes them achieve human-like performance in grammatical error identification. Humans, though, remain superior in error correction.","大型语言模型(LLMS)在包括英语在内的许多语言的自然语言理解任务方面表现极好。然而,尽管是全球第五种最常用语言,但孟加拉语的语法错误校正(GEC)仍然不发达。我们调查如何利用LLMS改进孟加拉语GEC。为此,我们首先对孟加拉语的12个错误类别进行广泛分类,对孟加拉语本地语使用者进行调查,以收集真实世界错误。我们接下来设计一种基于规则的噪音注入方法,以创建与正确语言对应的语法错误句。Vaiyakarana数据集因此创建,由5,67,422个句组成,其中2,27,119个为错误。该数据集随后用于为孟加拉语的GEC任务提供指导-调LMS。评价显示,与零分位设置相比,对 mLMSGEC的教学调整提高了3-7个百分点,使其在校正错误识别方面达到人性化的功能。虽然人类在错误校正中仍然处于优势。","Pramit Bhattacharyya, Arnab Bhattacharya",2025-06-05T14:17:05Z,"Leveraging LLMs for Bangla Grammar Error Correction:Error   Categorization, Synthetic Data, and Model Evaluation","Leveraging LLMs für Bangla Grammatik Fehlerkorrektur:Error Kategorisierung, Synthetische Daten und Modellbewertung",Bangla语法错误校正:错误分类、合成数据和模型评价,http://arxiv.org/abs/2406.14284v2
59,"Low-Rank Adaptation (LoRA) is a crucial method for efficiently fine-tuning large language models (LLMs), with its effectiveness influenced by two key factors: rank selection and weight initialization. While numerous LoRA variants have been proposed to improve performance by addressing one of these aspects, they often compromise usability or computational efficiency. In this paper, we analyze and identify the core limitations of existing approaches and propose a novel framework -- GoRA (Gradient-driven Adaptive Low Rank Adaptation) -- that simultaneously adapts both the rank and initialization strategy within a unified framework. GoRA leverages gradient information during training to dynamically assign optimal ranks and initialize low-rank adapter weights in an adaptive manner. To our knowledge, GoRA is the first method that not only addresses the limitations of prior approaches -- which often focus on either rank selection or initialization in isolation -- but also unifies both aspects within a single framework, enabling more effective and efficient adaptation. Extensive experiments across various architectures and modalities show that GoRA consistently outperforms existing LoRA-based methods while preserving the efficiency of vanilla LoRA. For example, when fine-tuning Llama3.1-8B-Base for mathematical reasoning, GoRA achieves a 5.13-point improvement over standard LoRA and even outperforms full fine-tuning by 2.05 points under high-rank settings.","低兰克适应(LORA)是高效微调大型语言模型(LLMS)的关键方法,其有效性受到两个关键因素的影响:排名选择和权重初始化。虽然提出了许多LORA变量,通过处理其中的一个方面来提高绩效,但往往会损害可用性或计算效率。在本文件中,我们分析和确定现有方法的核心局限性,并提议一个创新框架 -- -- Gora(由较强驱动的适应性低级适应) -- -- 在统一框架内同时调整级别和初始化战略。GORA在培训期间利用梯度信息,动态地分配最佳等级,并以适应方式初始化低级适应器重量。根据我们的知识,GORA是不仅解决以往方法局限性的第一个方法 -- -- 这些方法往往侧重于排在排在排序或初始化方面 -- -- 而且还在单一框架内统一了两个方面 -- -- GORA(由较强的适应力驱动的适应性低级适应) -- -- 各种结构和模式的广泛实验表明,GORA在保持VIRA的优化等级和低级调整低级调整中的效率的同时,甚至以高调低级调整了LRA的调整了LARA-13(GOA)-B)的升级为全调制。","Haonan He, Peng Ye, Yuchen Ren, Yuan Yuan, Luyang Zhou, Shucun Ju, Lei Chen",2025-06-05T14:16:46Z,GoRA: Gradient-driven Adaptive Low Rank Adaptation,GoRA: Gradient-getriebene Adaptive Low-Rank-Anpassung,GARA:逐步驱动的适应性低级别适应,http://arxiv.org/abs/2502.12171v2
60,"Large language models (LLMs) exhibit compelling linguistic behaviour, and sometimes offer self-reports, that is to say statements about their own nature, inner workings, or behaviour. In humans, such reports are often attributed to a faculty of introspection and are typically linked to consciousness. This raises the question of how to interpret self-reports produced by LLMs, given their increasing linguistic fluency and cognitive capabilities. To what extent (if any) can the concept of introspection be meaningfully applied to LLMs? Here, we present and critique two examples of apparent introspective self-report from LLMs. In the first example, an LLM attempts to describe the process behind its own ``creative'' writing, and we argue this is not a valid example of introspection. In the second example, an LLM correctly infers the value of its own temperature parameter, and we argue that this can be legitimately considered a minimal example of introspection, albeit one that is (presumably) not accompanied by conscious experience.","大型语言模型(LLMs)表现出令人信服的语言行为,有时还提供自我报告,即关于其自身性质、内部工作或行为的陈述。在人类中,这类报告往往被归咎于内省力,通常与意识相关。这提出了如何解释LLMs自报的问题,因为语言流度和认知能力不断提高。内省概念在多大程度上(如果有的话)可以有意义地适用于LLMs?在这里,我们介绍并批评两个关于LMs的明显反省自我报告的例子。在第一个例子中,LLM试图描述自己的“内省性”书写过程,我们认为这是不正确的内省力的例子。在第二个例子中,LLMM正确地推断了其自身温度参数的价值,我们争辩说,这可以被合理视为一个最小的内省,尽管(大概)没有意识经验。","Iulia Comşa, Murray Shanahan",2025-06-05T14:13:54Z,Does It Make Sense to Speak of Introspection in Large Language Models?,"Macht es Sinn, von Introspektion in großen Sprachmodellen zu sprechen?",它是否让人们想到在大语言模型中谈论反省?,http://arxiv.org/abs/2506.05068v1
61,"We introduce Debate Speech Evaluation as a novel and challenging benchmark for assessing LLM judges. Evaluating debate speeches requires a deep understanding of the speech at multiple levels, including argument strength and relevance, the coherence and organization of the speech, the appropriateness of its style and tone, and so on. This task involves a unique set of cognitive abilities that have previously received limited attention in systematic LLM benchmarking. To explore such skills, we leverage a dataset of over 600 meticulously annotated debate speeches and present the first in-depth analysis of how state-of-the-art LLMs compare to human judges on this task. Our findings reveal a nuanced picture: while larger models can approximate individual human judgments in some respects, they differ substantially in their overall judgment behavior. We also investigate the ability of frontier LLMs to generate persuasive, opinionated speeches, showing that models may perform at a human level on this task.","我们把辩论演说评价作为评估法学硕士法官的新颖和具有挑战性的基准。评价辩论演讲要求深入了解多层次的演讲,包括辩论的力度和相关性、演讲的连贯性和组织安排、其风格和语调的恰当性等等。这项任务涉及一套独特的认知能力,这些能力以前在有系统的法学硕士基准中得到有限关注。为了探索这些技能,我们利用了600多份精心注解的辩论演讲的数据集,并首次深入分析了在这项任务上如何与人类法官进行比较。我们的调查结果揭示了一种细微的图景:虽然较大的模型可以在某些方面接近个人判断,但它们在总体判断行为方面差异很大。我们还调查了前沿法学硕士学位协会生成有说服力、有见解的演讲的能力,表明模型可以在人类层面开展这项工作。","Noy Sternlicht, Ariel Gera, Roy Bar-Haim, Tom Hope, Noam Slonim",2025-06-05T14:06:51Z,Debatable Intelligence: Benchmarking LLM Judges via Debate Speech   Evaluation,Debattierbare Intelligenz: Benchmarking der LLM-Richter durch Debatte Sprachbewertung,可辩驳的情报:通过辩论演说评价确定LLM法官的基准,http://arxiv.org/abs/2506.05062v1
62,"Large Language Models (LLMs) have revolutionized various Natural Language Generation (NLG) tasks, including Argument Summarization (ArgSum), a key subfield of Argument Mining (AM). This paper investigates the integration of state-of-the-art LLMs into ArgSum, including for its evaluation. In particular, we propose a novel prompt-based evaluation scheme, and validate it through a novel human benchmark dataset. Our work makes three main contributions: (i) the integration of LLMs into existing ArgSum frameworks, (ii) the development of a new LLM-based ArgSum system, benchmarked against prior methods, and (iii) the introduction of an advanced LLM-based evaluation scheme. We demonstrate that the use of LLMs substantially improves both the generation and evaluation of argument summaries, achieving state-of-the-art results and advancing the field of ArgSum. We also show that among the four LLMs integrated in (i) and (ii), Qwen-3-32B, despite having the fewest parameters, performs best, even surpassing GPT-4o, while LLaMA-3.3-70B consistently underperforms.","大型语言模型(LLMS)使各种自然语言生成(NLG)任务发生了革命性的变化,包括标语采矿(AM)的一个重要子领域(ArgSum)的标语总和(ArgSum)系统。本文调查了将最先进的LLMs纳入ArgSum(ArgSum)的工作,包括对其进行评估。我们特别提出一个新的快速评估计划,并通过新的人类基准数据集加以验证。我们的工作主要有三个主要贡献:(一) 将LLLMs纳入现有的ArgSum框架;(二) 开发一个新的以LLM为基础的ARgSum系统(ArgSum),以以前的方法为基准;(三) 采用先进的LLMM(LM)评价计划。我们证明,使用LLMMs大大改进了论据摘要的生成和评价,实现了最新成果,并推进了ArgSum领域。我们还表明,在纳入(一)和(二)的4个LLMMMS(Quen-3-3B)中,尽管其参数最少,但表现最佳,甚至超过了GPTTT-4-MAD-MAxx。","Moritz Altemeyer, Steffen Eger, Johannes Daxenberger, Yanran Chen, Tim Altendorf, Philipp Cimiano, Benjamin Schiller",2025-06-05T14:03:53Z,Argument Summarization and its Evaluation in the Era of Large Language   Models,Argumentationszusammenfassung und ihre Bewertung im Zeitalter der großen Sprachmodelle,"在 "" 大语言模式时代 "" 中的论点概述及其评价",http://arxiv.org/abs/2503.00847v3
63,"Large Language Models (LLMs) excel in high-resource languages but struggle with low-resource languages due to limited training data. This paper presents TALL (Trainable Architecture for Enhancing LLM Performance in Low-Resource Languages), which integrates an LLM with two bilingual translation models. TALL transforms low-resource inputs into high-resource representations, leveraging the LLM's capabilities while preserving linguistic features through dimension alignment layers and custom transformers. Our experiments on Hebrew demonstrate significant improvements over several baselines, including direct use, naive translation, and fine-tuning approaches. The architecture employs a parameter-efficient strategy, freezing pre-trained components while training only lightweight adapter modules, balancing computational efficiency with performance gains.","大型语言模型(LLMS)在高资源语言方面十分出色,但由于培训数据有限,与低资源语言抗争。本文介绍了TALL(提高低资源语言LLM绩效的可培训架构),它将一个LLM(LLM)与两个双语翻译模型相结合。TALL将低资源投入转化为高资源代表,利用LLM(LLM)的能力,同时通过维度校正层和定制变压器来保持语言特征。我们在希伯来语上的实验显示,在几个基线上,包括直接使用、天真翻译和微调方法,都取得了显著的改进。建筑采用了一个节能战略,冻结了预先培训的组件,同时只培训轻量适应模块,平衡计算效率和绩效收益。","Moshe Ofer, Orel Zamler, Amos Azaria",2025-06-05T14:02:12Z,TALL -- A Trainable Architecture for Enhancing LLM Performance in   Low-Resource Languages,TALL -- Eine praktikable Architektur zur Verbesserung der LLM-Leistung in ressourcenarmen Sprachen,TALL -- -- 提高低资源语言LLM性能的训练建筑,http://arxiv.org/abs/2506.05057v1
64,"Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present a novel framework, AnytimeReasoner, to optimize anytime reasoning performance, which aims to improve token efficiency and the flexibility of reasoning under varying token budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, Budget Relative Policy Optimization (BRPO), to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency.","缩放测试时间计算对于提高大型语言模型(LLMs)的推理能力至关重要。 现有方法通常采用强化学习(RL),以最大限度地实现推理结果的尾端获得的可核查的奖励;然而,这些方法只优化在大型固定象征性预算下的最后业绩,这妨碍了培训和部署的效率。在这项工作中,我们提出了一个新的框架,即“随时更新”,以优化时间推理业绩,目的是提高象征性的效率和在不同象征性预算限制下推理的灵活性。为了做到这一点,我们调整了完整的思维过程,以将先前分配的抽样象征性预算纳入其中,迫使模型总结每一种细化的核查思维的最佳答案。这在推理过程中引入了可核查的密集奖励,便利了在REL优化中更有效的信用分配。然后,我们以分解的方式优化思维和汇总政策,以最大限度地增加累积的奖励。此外,我们引入了新的差异减少技术,即预算相对政策优化(BROPO),以便在加强思维政策时加强学习过程的稳健和效率。在数学推理学过程中,在数学推理工作中得出的结果显示我们以往的推理学方法都持续超越了所有方法。","Penghui Qi, Zichen Liu, Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin",2025-06-05T13:55:06Z,Optimizing Anytime Reasoning via Budget Relative Policy Optimization,Optimierung jederzeit über Budget-Relational-Policy-Optimierung,"通过预算相对政策优化优化,优化任何时间的理由",http://arxiv.org/abs/2505.13438v2
65,"Large language models (LLMs) have achieved distinguished performance on various reasoning-intensive tasks. However, LLMs might still face the challenges of robustness issues and fail unexpectedly in some simple reasoning tasks. Previous works evaluate the LLM robustness with hand-crafted templates or a limited set of perturbation rules, indicating potential data contamination in pre-training or fine-tuning datasets. In this work, inspired by stress testing in software engineering, we propose a novel framework, Automatic Robustness Checker (AR-Checker), to generate mathematical problem variants that maintain the semantic meanings of the original one but might fail the LLMs. The AR-Checker framework generates mathematical problem variants through multi-round parallel streams of LLM-based rewriting and verification. Our framework can generate benchmark variants dynamically for each LLM, thus minimizing the risk of data contamination. Experiments on GSM8K and MATH-500 demonstrate the strong performance of AR-Checker on mathematical tasks. We also evaluate AR-Checker on benchmarks beyond mathematics, including MMLU, MMLU-Pro, and CommonsenseQA, where it also achieves strong performance, further proving the effectiveness of AR-Checker.","大型语言模型(LLMS)在各种推理密集型任务中取得了不同成绩。然而,LLMS可能仍面临稳健性问题的挑战,在某些简单推理任务中出乎意料地失败。以前的作品用手工制作的模板或有限的扰动规则评估LLMS的稳健性,表明培训前或微调数据集中潜在的数据污染。在这项工作中,在软件工程压力测试的启发下,我们提出了一个新颖的框架,即自动强力检查器(AR-Cryer),以产生数学问题变异,保持原始的语义含义,但可能使LMMS失败。AR-Cer 框架通过基于LMM的多轮平行重写和核查流程产生数学问题变异。我们的框架可以动态地为每个LLMM生成基准变体,从而将数据污染风险降到最低。GSM8K和MATH-500的实验表明AR-Cer在数学任务方面的强性表现。我们还评估了数学基准的AR-CRE,包括MLU、MLU-ProPro和ComsenseQA,在其中也取得了强有力的业绩。","Yutao Hou, Zeguan Xiao, Fei Yu, Yihan Jiang, Xuetao Wei, Hailiang Huang, Yun Chen, Guanhua Chen",2025-06-05T13:42:39Z,Automatic Robustness Stress Testing of LLMs as Mathematical Problem   Solvers,Automatische Robustheits-Stress-Prüfung von LLMs als mathematische Problemlöser,对作为数学问题解答器的LLMS进行自动强力压力测试,http://arxiv.org/abs/2506.05038v1
66,"Large language model (LLM) unlearning has demonstrated its essential role in removing privacy and copyright-related responses, crucial for their legal and safe applications. However, the pursuit of complete unlearning often comes with substantial costs due to its compromises in their general functionality, leading to a notorious trade-off between unlearning and retention. It motivates this paper to explore enhanced unlearning schemes that can mitigate this trade-off. Specifically, we propose Gradient Rectified Unlearning (GRU), an improved framework that regulates the directions of gradient updates during the unlearning procedure such that their side impacts on other, unrelated responses can be minimized. GRU is easy and general to implement, demonstrating practical effectiveness across a variety of well-established unlearning benchmarks.","大型语言模式(LLM)不学习已经表明其在消除隐私和版权相关反应方面的关键作用,这些反应对于其法律和安全应用至关重要,然而,追求完全不学习往往由于在一般功能方面的妥协而付出巨大代价,导致在不学习和保留之间发生臭名昭著的权衡;它激励本文件探索能够减轻这种权衡的强化的不学习计划;具体地说,我们提议逐步校正不学习(GRU),这是一个经改进的框架,在不学习程序期间规范梯度更新方向,以便尽可能减少其对其他互不相干的反应的影响。","Yue Wang, Qizhou Wang, Feng Liu, Wei Huang, Yali Du, Xiaojiang Du, Bo Han",2025-06-05T13:34:42Z,GRU: Mitigating the Trade-off between Unlearning and Retention for LLMs,GRU: Abbau des Kompromisses zwischen Unlearning und Retention für LLMs,GRU:减少LLMM的不学习与保留之间的取舍,http://arxiv.org/abs/2503.09117v3
67,"While Vision-Language Models (VLMs) have shown remarkable abilities in visual and language reasoning tasks, they invariably generate flawed responses. Self-correction that instructs models to refine their outputs presents a promising solution to this issue. Previous studies have mainly concentrated on Large Language Models (LLMs), while the self-correction abilities of VLMs, particularly concerning both visual and linguistic information, remain largely unexamined. This study investigates the self-correction capabilities of VLMs during both inference and fine-tuning stages. We introduce a Self-Correction Learning (SCL) approach that enables VLMs to learn from their self-generated self-correction data through Direct Preference Optimization (DPO) without relying on external feedback, facilitating self-improvement. Specifically, we collect preferred and disfavored samples based on the correctness of initial and refined responses, which are obtained by two-turn self-correction with VLMs during the inference stage. Experimental results demonstrate that although VLMs struggle to self-correct effectively during iterative inference without additional fine-tuning and external feedback, they can enhance their performance and avoid previous mistakes through preference fine-tuning when their self-generated self-correction data are categorized into preferred and disfavored samples. This study emphasizes that self-correction is not merely a refinement process; rather, it should enhance the reasoning abilities of models through additional training, enabling them to generate high-quality responses directly without further refinement.","虽然视觉语言模型(VLMS)在视觉和语言推理任务方面表现出非凡的能力,但它们总是会产生有缺陷的响应。自我校正指导模型完善其产出的自我校正是一个很好的解决办法。以前的研究主要集中于大语言模型(LLMS),而VLM的自我校正能力,特别是视觉和语言信息的自我校正能力,基本上没有受到审查。这项研究调查了VLMS在推论和微调阶段的自我校正能力。我们引入了自我校正(SCLL)方法,使VLMS能够通过直接的自我校正(DPO)从自制自我校正数据中学习,而不依赖外部的反馈,促进自我改进。具体地说,我们收集了基于初步和语言信息的正确性而优异的样本,这些样本在推理和微调阶段与VLMS的自我校正能力进行两次自我校正,这些实验结果表明,虽然VLMS在不进一步精确的反复校正过程中努力争取自我校正,而无需再进行微和外部的自我校正的自我校正,但是它们通过自我校正的自我校正的自我校正过程可以提高自己的自我偏偏重的自我学习来提高自我偏重其自我偏重。他们自我的自我学习的自我偏重。","Jiayi He, Hehai Lin, Qingyun Wang, Yi Fung, Heng Ji",2025-06-05T13:25:43Z,Self-Correction is More than Refinement: A Learning Framework for Visual   and Language Reasoning Tasks,Selbstkorrektion ist mehr als Verfeinerung: Ein Lernrahmen für visuelle und sprachliche Aufgaben,自我校正不仅仅是改进:视觉和语言原因说明任务学习框架,http://arxiv.org/abs/2410.04055v3
68,"Controlling the length of generated text can be crucial in various text-generation tasks, including summarization. Existing methods often require complex model alterations, limiting compatibility with pre-trained models. We address these limitations by developing a simple approach for controlling the length of automatic text summaries by increasing the importance of correctly predicting the EOS token in the cross-entropy loss computation. The proposed methodology is agnostic to architecture and decoding algorithms and orthogonal to other inference-time techniques to control the generation length. We tested it with encoder-decoder and modern GPT-style LLMs, and show that this method can control generation length, often without affecting the quality of the summary.","控制生成文本的长度在包括摘要化在内的各种文本生成任务中可能至关重要。现有方法往往需要复杂的模型改变,限制与预先培训的模型的兼容性。我们通过在跨肾损失计算中提高正确预测 EOS 符号的重要性,为控制自动文本摘要的长度制定一种简单的方法来应对这些限制。拟议方法对建筑和解码算法具有不可知性,对控制生成长度的其他推论时间技术也具有可辨别性。我们用编码-解码器和现代的GPT-LMS 测试了这些限制,并表明这种方法可以控制生成长度,但往往不影响摘要的质量。","Zeno Belligoli, Emmanouil Stergiadis, Eran Fainman, Ilya Gusev",2025-06-05T13:25:28Z,Controlling Summarization Length Through EOS Token Weighting,Kontrolle der Zusammenfassung Länge durch EOS Token Gewichtung,控制通过 EOS 过 EOS 键权重加权的控控总和长度,http://arxiv.org/abs/2506.05017v1
69,"We introduce ComfyUI-Copilot, a large language model-powered plugin designed to enhance the usability and efficiency of ComfyUI, an open-source platform for AI-driven art creation. Despite its flexibility and user-friendly interface, ComfyUI can present challenges to newcomers, including limited documentation, model misconfigurations, and the complexity of workflow design. ComfyUI-Copilot addresses these challenges by offering intelligent node and model recommendations, along with automated one-click workflow construction. At its core, the system employs a hierarchical multi-agent framework comprising a central assistant agent for task delegation and specialized worker agents for different usages, supported by our curated ComfyUI knowledge bases to streamline debugging and deployment. We validate the effectiveness of ComfyUI-Copilot through both offline quantitative evaluations and online user feedback, showing that it accurately recommends nodes and accelerates workflow development. Additionally, use cases illustrate that ComfyUI-Copilot lowers entry barriers for beginners and enhances workflow efficiency for experienced users. The ComfyUI-Copilot installation package and a demo video are available at https://github.com/AIDC-AI/ComfyUI-Copilot.","我们引入了ComfyUI-Copilit,这是一个大型语言模型驱动的插件,目的是提高ComfyUI的可用性和效率,这是一个供AI驱动的艺术创作的开放源码平台。尽管ComfyUI具有灵活性和方便用户的界面,但可以对新来者提出挑战,包括有限的文件、模型配置错误和工作流程设计的复杂性。ComfyUI-Copilot通过提供智能节点和模型建议,同时进行自动的单击工作流程建设,来应对这些挑战。在系统的核心,该系统采用由任务授权的中央助理代理和不同用途的专门工人代理组成的分级多试办框架,并辅之以我们经校准的ComfyUI知识库,以简化调试和部署。我们通过离线定量评估和在线用户反馈验证ComfyUI-Coililit的有效性,表明它准确建议节点并加快工作流程开发。此外,使用案例说明ComfyUIIA-Copilit降低了初始用户的进入障碍,并提高了有经验的用户的工作流程效率。ComyUI-CoCompilital安装软件包和一个演示视频可在http://AIDUI/AY/AY/AIDUI/AY/AY/AY/AIDUI。","Zhenran Xu, Xue Yang, Yiyu Wang, Qingli Hu, Zijiao Wu, Longyue Wang, Weihua Luo, Kaifu Zhang, Baotian Hu, Min Zhang",2025-06-05T13:20:50Z,ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow   Development,ComfyUI-Copilot: Ein intelligenter Assistent für automatisierte Workflow-Entwicklung,ComfyUI-副驾驶员:一名负责自动工作流程开发的智能助理,http://arxiv.org/abs/2506.05010v1
70,"A range of recent works addresses the problem of compression of sequence of tokens into a shorter sequence of real-valued vectors to be used as inputs instead of token embeddings or key-value cache. These approaches are focused on reduction of the amount of compute in existing language models rather than minimization of number of bits needed to store text. Despite relying on powerful models as encoders, the maximum attainable lossless compression ratio is typically not higher than x10. This fact is highly intriguing because, in theory, the maximum information capacity of large real-valued vectors is far beyond the presented rates even for 16-bit precision and a modest vector size. In this work, we explore the limits of compression by replacing the encoder with a per-sample optimization procedure. We show that vectors with compression ratios up to x1500 exist, which highlights two orders of magnitude gap between existing and practically attainable solutions. Furthermore, we empirically show that the compression limits are determined not by the length of the input but by the amount of uncertainty to be reduced, namely, the cross-entropy loss on this sequence without any conditioning. The obtained limits highlight the substantial gap between the theoretical capacity of input embeddings and their practical utilization, suggesting significant room for optimization in model design.","最近一系列工程解决了将象征序列压缩成一个更短、更短、真正价值矢量序列以用作投入而不是象征性嵌入或关键值缓存的定点序列的问题。 这些方法侧重于减少现有语言模型中的计算量,而不是最大限度地减少存储文本所需的位数。 尽管依赖强大的模型作为编码器,但最大可实现的无损压缩比率一般不会高于x10。 这一事实非常令人着迷,因为从理论上讲,大型实际价值矢量的最大信息能力远远超出所提出的比率,即使16位精确度和适量矢量也远远超出所提出的比率。 在这项工作中,我们探索压缩的限度,用每个模版优化程序取代编码器。我们显示,压缩比率高达x1500的矢量存在,这突出了现有解决办法与实际可实现的解决办法之间的两个程度差距。此外,我们的经验显示,压缩限度不是由投入的长度决定,而是由需要减少的不确定性所决定的幅度所决定的,也就是说,这一顺序上的交叉损耗程度不高。在不作任何调节的情况下,我们探索压缩的限度是其理论性能力之间的重大差距。","Yuri Kuratov, Mikhail Arkhipov, Aydar Bulatov, Mikhail Burtsev",2025-06-05T13:20:09Z,Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the   Limits of Embedding Space Capacity,1568 Tokens in einen einzigen Vektor und wieder zurück krammen: Die Grenzen der Einbettung von Raumkapazität erkunden,将1568吨撞成单一矢量和后向:探索嵌入空间能力的极限,http://arxiv.org/abs/2502.13063v2
71,"System prompts in Large Language Models (LLMs) are predefined directives that guide model behaviour, taking precedence over user inputs in text processing and generation. LLM deployers increasingly use them to ensure consistent responses across contexts. While model providers set a foundation of system prompts, deployers and third-party developers can append additional prompts without visibility into others' additions, while this layered implementation remains entirely hidden from end-users. As system prompts become more complex, they can directly or indirectly introduce unaccounted for side effects. This lack of transparency raises fundamental questions about how the position of information in different directives shapes model outputs. As such, this work examines how the placement of information affects model behaviour. To this end, we compare how models process demographic information in system versus user prompts across six commercially available LLMs and 50 demographic groups. Our analysis reveals significant biases, manifesting in differences in user representation and decision-making scenarios. Since these variations stem from inaccessible and opaque system-level configurations, they risk representational, allocative and potential other biases and downstream harms beyond the user's ability to detect or correct. Our findings draw attention to these critical issues, which have the potential to perpetuate harms if left unexamined. Further, we argue that system prompt analysis must be incorporated into AI auditing processes, particularly as customisable system prompts become increasingly prevalent in commercial AI deployments.","大语言模型(LLMS)中的系统提示是指导示范行为,在文本处理和生成方面优先于用户投入的预先定义指令。LLMS部署者越来越多地使用这些指令,以确保在各种情况下作出一致的反应。虽然示范提供者建立了系统提示的基础,但部署者和第三方开发者可以将额外的提示附加在其他方面,而无需引起注意,而这种分层的实施仍然完全隐藏在终端用户身上。随着系统提示变得日益复杂,它们可以直接或间接地引入副作用的不明。这种缺乏透明度的问题提出了各种指令中的信息位置如何影响模型产出的根本问题。因此,这项工作审视了信息定位如何影响模式行为。为此,我们比较了系统内处理人口信息的模式与用户如何在6个商业上可用的LMS和50个人口组中进行提示。我们的分析揭示了重大偏差,在用户代表性和决策情景上存在差异。由于这些差异来自无法进入和不透明的系统配置,因此它们有代表性、偏向性和潜在的其他偏见和下游伤害,超出了用户的检测或纠正能力。我们发现的信息定位如何影响模型的模型。为此,我们比较系统中的这些模型是如何处理系统中的人口信息与用户如何进行普遍的、不断更新。","Anna Neumann, Elisabeth Kirsten, Muhammad Bilal Zafar, Jatinder Singh",2025-06-05T13:12:53Z,Position is Power: System Prompts as a Mechanism of Bias in Large   Language Models (LLMs),Position ist Macht: Systemprompts als Mechanismus von Bias in großen Sprachmodellen (LLMs),位置是电源:系统提示作为大语言模型比阿语机制(LLMs),http://arxiv.org/abs/2505.21091v2
72,"Despite the great potential of large language models(LLMs) in machine comprehension, it is still disturbing to fully count on them in real-world scenarios. This is probably because there is no rational explanation for whether the comprehension process of LLMs is aligned with that of experts. In this paper, we propose SCOP to carefully examine how LLMs perform during the comprehension process from a cognitive view. Specifically, it is equipped with a systematical definition of five requisite skills during the comprehension process, a strict framework to construct testing data for these skills, and a detailed analysis of advanced open-sourced and closed-sourced LLMs using the testing data. With SCOP, we find that it is still challenging for LLMs to perform an expert-level comprehension process. Even so, we notice that LLMs share some similarities with experts, e.g., performing better at comprehending local information than global information. Further analysis reveals that LLMs can be somewhat unreliable -- they might reach correct answers through flawed comprehension processes. Based on SCOP, we suggest that one direction for improving LLMs is to focus more on the comprehension process, ensuring all comprehension skills are thoroughly developed during training.","尽管在机能理解方面大型语言模型(LLMs)潜力巨大,但在现实世界情景中充分依赖这些模型仍然令人不安,这很可能是因为没有合理解释LLMs的理解过程是否与专家的理解过程相一致。在本文件中,我们建议SCOP仔细审查LLMs在理解过程中如何从认知角度分析LLMs。具体地说,它具备了在理解过程中对五项必要技能的系统定义,为这些技能建立测试数据的严格框架,以及利用测试数据对先进的开放源和封闭源LMs进行详细分析。关于SCOP,我们发现LLMs仍然难以进行专家级的理解过程。即使如此,我们注意到LMMs与专家有一些相似之处,例如,在理解当地信息方面的表现比全球信息要好。进一步的分析表明LMs可能有些不可靠 -- -- 它们可能通过错误的理解过程得到正确的答案。基于SCOP,我们建议改进LMs的一个方向是更加注重理解过程,确保培训中的所有理解技能得到彻底发展。","Yongjie Xiao, Hongru Liang, Peixin Qin, Yao Zhang, Wenqiang Lei",2025-06-05T13:10:24Z,SCOP: Evaluating the Comprehension Process of Large Language Models from   a Cognitive View,SCOP: Bewertung des Verständnisprozesses von großen Sprachmodellen aus einer kognitiven Ansicht,SCOP: 从认知的角度评估大语言模型的全纳进程,http://arxiv.org/abs/2506.05000v1
73,"Despite the strong performance of ColPali/ColQwen2 in Visualized Document Retrieval (VDR), it encodes each page into multiple patch-level embeddings and leads to excessive memory usage. This empirical study investigates methods to reduce patch embeddings per page at minimum performance degradation. We evaluate two token-reduction strategies: token pruning and token merging. Regarding token pruning, we surprisingly observe that a simple random strategy outperforms other sophisticated pruning methods, though still far from satisfactory. Further analysis reveals that pruning is inherently unsuitable for VDR as it requires removing certain page embeddings without query-specific information. Turning to token merging (more suitable for VDR), we search for the optimal combinations of merging strategy across three dimensions and develop Light-ColPali/ColQwen2. It maintains 98.2% of retrieval performance with only 11.8% of original memory usage, and preserves 94.6% effectiveness at 2.8% memory footprint. We expect our empirical findings and resulting Light-ColPali/ColQwen2 offer valuable insights and establish a competitive baseline for future research towards efficient VDR.","尽管ColPali/ColQuen2在可视化文档检索(VDR)中表现良好,但它将每页编码成多个补丁嵌入层,并导致过度使用内存。这项实证研究调查了在最低性能退化时减少每页补丁嵌入的方法。我们评估了两个象征性的削减战略:象征性的剪裁和象征性合并。关于象征性的剪裁,我们惊讶地发现,简单的随机战略比其他精密的剪裁方法要好得多,尽管还远远不令人满意。进一步的分析显示,对VDR来说,运行本身是不适合的,因为它需要删除某些页面嵌入点没有特定查询信息的嵌入部分。转向象征性合并(更适合VDR),我们寻求三个维度的合并战略的最佳组合,并开发光-ColPali/ColQwen2。它保持98.2%的检索性能,只有11.8%的原始记忆用量,在2.8%的记忆足迹上保留94.6%的效能。我们期望我们的经验发现和由此得出的轻-ColPali/Colwen2,为未来的研究提供了宝贵的洞察,并建立了有竞争力的基线。","Yubo Ma, Jinsong Li, Yuhang Zang, Xiaobao Wu, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Haodong Duan, Jiaqi Wang, Yixin Cao, Aixin Sun",2025-06-05T13:06:01Z,Towards Storage-Efficient Visual Document Retrieval: An Empirical Study   on Reducing Patch-Level Embeddings,Auf dem Weg zu einem effizienten visuellen Dokumenten-Retrieval: Eine empirische Studie zur Reduzierung von Patch-Level-Einbindungen,迈向储存-有效视觉文件检索:减少定级嵌入的经验研究,http://arxiv.org/abs/2506.04997v1
74,"Large Language Models (LLMs) have shown to be effective evaluators across various domains such as machine translations or the scientific domain. Current LLM-as-a-Judge approaches rely mostly on individual assessments or a single round of pairwise assessments, preventing the judge LLM from developing a global ranking perspective. To address this, we present Knockout Assessment, an LLM-asa Judge method using a knockout tournament system with iterative pairwise comparisons. Experiments across three LLMs on two datasets show that knockout assessment improves scoring accuracy, increasing Pearson correlation with expert evaluations by 0.07 on average for university-level exam scoring and machine translation evaluations, aligning LLM assessments more closely with human scoring.","大型语言模型(LLMs)在机器翻译或科学领域等不同领域证明是有效的评价者,目前的LLM-as-a-judge方法主要依靠个别评估或单轮双向评估,使法官LLM无法从全球排名的角度出发。为解决这一问题,我们介绍了LLM-asa法官使用击倒比赛系统、迭接对比的敲倒评估方法。在两个数据集上对三个LMs的实验显示,击倒评估提高了评分的准确性,使Pearson与专家评价的相互关系平均增加0.07分,用于大学一级的考试评分和机器翻译评价,使LMM评估与人类评分更加接近。","Isik Baran Sandan, Tu Anh Dinh, Jan Niehues",2025-06-05T13:01:33Z,Knockout LLM Assessment: Using Large Language Models for Evaluations   through Iterative Pairwise Comparisons,Knockout LLM Assessment: Verwendung großer Sprachmodelle für Bewertungen durch iterative Pairwise-Vergleiche,"LLLM 评估:利用大语言模式,通过迭接对等比较进行评估",http://arxiv.org/abs/2506.03785v2
75,"Fine-tuning pretrained ASR models for specific domains is challenging when labeled data is scarce. But unlabeled audio and labeled data from related domains are often available. We propose an incremental semi-supervised learning pipeline that first integrates a small in-domain labeled set and an auxiliary dataset from a closely related domain, achieving a relative improvement of 4% over no auxiliary data. Filtering based on multi-model consensus or named entity recognition (NER) is then applied to select and iteratively refine pseudo-labels, showing slower performance saturation compared to random selection. Evaluated on the multi-domain Wow call center and Fisher English corpora, it outperforms single-step fine-tuning. Consensus-based filtering outperforms other methods, providing up to 22.3% relative improvement on Wow and 24.8% on Fisher over single-step fine-tuning with random selection. NER is the second-best filter, providing competitive performance at a lower computational cost.","当标签数据稀少时,特定域的微调预先培训 ASR 模型将面临挑战。 但是,来自相关域的无标签音频和标签数据往往可以获得。 我们建议建立一个递增的半监督的学习管道,该管道首先整合一个小域内标签数据集和一个密切相关域的辅助数据集,相对改进率为4%,比无辅助数据要改进4%。然后根据多模式共识或名称实体识别(NER)进行过滤,以选择和迭代精细化假标签,显示比随机选择慢的性能饱和度。在多域Wow调用中心和Fisher Englian Corora上进行了评估,它优于单步微调。基于共识的过滤优于其他方法,在Wow上提供了22.3%的相对改进,而Fishercher则在随机选择的单步微调上提供了24.8%的相对改进。 NER是第二最好的过滤器,以较低的计算成本提供竞争性性表现。","Andres Carofilis, Pradeep Rangappa, Srikanth Madikeri, Shashi Kumar, Sergio Burdisso, Jeena Prakash, Esau Villatoro-Tello, Petr Motlicek, Bidisha Sharma, Kadri Hacioglu, Shankar Venkatesan, Saurabh Vyas, Andreas Stolcke",2025-06-05T12:53:20Z,Better Semi-supervised Learning for Multi-domain ASR Through Incremental   Retraining and Data Filtering,Besseres semi-überwachtes Lernen für Multi-Domain ASR durch inkrementelle Umschulung und Datenfilterung,"通过递增再培训和数据过滤,为多领域ASR更好地进行半监督的半监督学习",http://arxiv.org/abs/2506.04981v1
76,"We introduce EvaLearn, a pioneering benchmark designed to evaluate large language models (LLMs) on their learning capability and efficiency in challenging tasks, a critical, yet underexplored aspect of model potential. EvaLearn contains 648 challenging problems across six task types, grouped into 182 sequences, each sequence dedicated to one task type. Diverging from most existing benchmarks that evaluate models in parallel, EvaLearn requires models to solve problems sequentially, allowing them to leverage the experience gained from previous solutions. EvaLearn provides five comprehensive automated metrics to evaluate models and quantify their learning capability and efficiency. We extensively benchmark nine frontier models and observe varied performance profiles: some models, such as Claude-3.7-sonnet, start with moderate initial performance but exhibit strong learning ability, while some models struggle to benefit from experience and may even show negative transfer. Moreover, we investigate model performance under two learning settings and find that instance-level rubrics and teacher-model feedback further facilitate model learning. Importantly, we observe that current LLMs with stronger static abilities do not show a clear advantage in learning capability across all tasks, highlighting that EvaLearn evaluates a new dimension of model performance. We hope EvaLearn provides a novel evaluation perspective for assessing LLM potential and understanding the gap between models and human capabilities, promoting the development of deeper and more dynamic evaluation approaches. All datasets, the automatic evaluation framework, and the results studied in this paper are available at the GitHub repository.","我们引入了EvaLearn(EvaLearn),这是用来评价大型语言模型(LLMs)在具有挑战性的任务中的学习能力和效率的开创性基准,是模型潜力中一个关键但探索不足的方面。EvaLearn(EvaLearn)包含六种任务类型的648个具有挑战性的问题,分为182个序列,每个序列专用于一个任务类型。从大多数同时评价模型的现有基准中,EvaLearn(EvaLearn)要求采用各种模型,以便依次解决问题,从而使它们能够利用从以往解决方案中获得的经验。EvaLearn(EvaLearn)提供了五种全面的自动化指标,用以评价模型,并量化其学习能力和效率。我们广泛设定了九个前沿模型,并观察了不同的业绩特征:一些模型,如Claude-3.7-sonnet(Claude),以中等的初期业绩为起点,但展示了很强的学习能力,而有些模型则努力从经验中获益,甚至可能显示动态模型。我们在EvaL(Elab)评估中,我们更深入地评估了人类的动态模型和深度评估能力。","Shihan Dou, Ming Zhang, Chenhao Huang, Jiayi Chen, Feng Chen, Shichun Liu, Yan Liu, Chenxiao Liu, Cheng Zhong, Zongzhang Zhang, Tao Gui, Chao Xin, Wei Chengzhi, Lin Yan, Qi Zhang, Yonghui Wu, Xuanjing Huang",2025-06-05T12:44:51Z,EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via   Sequential Problem Solving,EvaLearn: Quantifizierung der Lernfähigkeit und Effizienz von LLMs durch sequentielle Problemlösung,"EvaLearn:通过按顺序解决问题,量化LLMs的学习能力和效率",http://arxiv.org/abs/2506.02672v2
77,"This paper presents a comprehensive evaluation of the performance of state-of-the-art Large Language Models (LLMs) on challenging university-level algorithms exams. By testing multiple models on both a Romanian exam and its high-quality English translation, we analyze LLMs' problem-solving capabilities, consistency, and multilingual performance. Our empirical study reveals that the most recent models not only achieve scores comparable to top-performing students but also demonstrate robust reasoning skills on complex, multi-step algorithmic challenges, even though difficulties remain with graph-based tasks. Building on these findings, we explore the potential of LLMs to support educational environments through the generation of high-quality editorial content, offering instructors a powerful tool to enhance student feedback. The insights and best practices discussed herein pave the way for further integration of generative AI in advanced algorithm education.","本文件全面评价了在具有挑战性的大学级算法考试方面最先进的大语言模型(LLMs)的绩效;通过测试罗马尼亚考试及其高质量英语翻译的多种模型,我们分析了LLMs的解决问题能力、一致性和多语种表现;我们的经验研究表明,最新模型不仅取得了与成绩优异的学生相当的分数,而且还展示了在复杂、多步算法挑战方面的有力推理技能,尽管基于图表的任务仍然存在困难。我们根据这些发现,探索LLMs通过生成高质量编辑内容支持教育环境的潜力,为教员提供了增强学生反馈的有力工具。这里讨论的见解和最佳做法为进一步将带有基因的AI纳入高级算法教育铺平了道路。","Adrian Marius Dumitran, Theodor-Pierre Moroianu, Vasile Paul Alexe",2025-06-05T12:41:20Z,From Struggle (06-2024) to Mastery (02-2025) LLMs Conquer Advanced   Algorithm Exams and Pave the Way for Editorial Generation,Vom Kampf (06-2024) zur Meisterschaft (02-2025) LLMs erobern fortgeschrittene Algorithmenprüfungen und ebnen den Weg für die redaktionelle Generation,从斗争(06-2024)到掌握(02-2025),http://arxiv.org/abs/2506.04965v1
78,"The rise of Large Language Models (LLMs) has revolutionized Graphical User Interface (GUI) automation through LLM-powered GUI agents, yet their ability to process sensitive data with limited human oversight raises significant privacy and security risks. This position paper identifies three key risks of GUI agents and examines how they differ from traditional GUI automation and general autonomous agents. Despite these risks, existing evaluations focus primarily on performance, leaving privacy and security assessments largely unexplored. We review current evaluation metrics for both GUI and general LLM agents and outline five key challenges in integrating human evaluators for GUI agent assessments. To address these gaps, we advocate for a human-centered evaluation framework that incorporates risk assessments, enhances user awareness through in-context consent, and embeds privacy and security considerations into GUI agent design and evaluation.","大语言模型(LLMs)的兴起使图形用户界面自动化通过LLM-GUI驱动的用户界面(GUI)代理实现革命化,然而,在有限人力监督的情况下处理敏感数据的能力带来了重大的隐私和安全风险。本立场文件确定了GUI代理商的三大风险,并审查了它们与传统的GUI自动化和一般自主代理商有何不同。尽管存在这些风险,但现有的评价主要侧重于业绩,使隐私和安全评估基本上未进行探讨。我们审查了目前对GUI和一般LM代理商的评价指标,并概述了在将人评价员纳入GUI代理商评估方面的五个关键挑战。为了弥补这些差距,我们倡导建立一个以人为本的评价框架,其中纳入风险评估,通过文本内同意提高用户的认识,并将隐私和安全考虑纳入GuIERM代理商的设计和评价。","Chaoran Chen, Zhiping Zhang, Ibrahim Khalilov, Bingcan Guo, Simret A Gebreegziabher, Yanfang Ye, Ziang Xiao, Yaxing Yao, Tianshi Li, Toby Jia-Jun Li",2025-06-05T12:40:53Z,Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered   GUI Agents,Auf dem Weg zu einem menschenzentrierten Bewertungsrahmen für vertrauenswürdige LLM-Powered-GUI-Agenten,争取为具有信誉的LLM授权的用户代表建立一个以人为中心的评价框架,http://arxiv.org/abs/2504.17934v2
79,"Adversarial attacks on Large Language Models (LLMs) via jailbreaking techniques-methods that circumvent their built-in safety and ethical constraints-have emerged as a critical challenge in AI security. These attacks compromise the reliability of LLMs by exploiting inherent weaknesses in their comprehension capabilities. This paper investigates the efficacy of jailbreaking strategies that are specifically adapted to the diverse levels of understanding exhibited by different LLMs. We propose the Adaptive Jailbreaking Strategies Based on the Semantic Understanding Capabilities of Large Language Models, a novel framework that classifies LLMs into Type I and Type II categories according to their semantic comprehension abilities. For each category, we design tailored jailbreaking strategies aimed at leveraging their vulnerabilities to facilitate successful attacks. Extensive experiments conducted on multiple LLMs demonstrate that our adaptive strategy markedly improves the success rate of jailbreaking. Notably, our approach achieves an exceptional 98.9% success rate in jailbreaking GPT-4o(29 May 2025 release)","通过绕过其内在安全和道德限制的侵入技术方法(LLMS),对大语言模型的反方攻击,绕过其内在安全和伦理限制,已成为AI安全中的一项重大挑战。这些攻击利用了LLMs内在的理解能力方面的弱点,损害了LLMs的可靠性。本文件调查了专门适应不同LMs所显示的不同理解程度的侵入战略的效力。我们提议了基于大语言模型的语义理解能力的适应性监狱破碎战略,这是一个新颖的框架,根据LMs的语义理解能力将其分为I类和II类。我们针对每一类别设计了有针对性的破门战略,旨在利用它们的脆弱性促进成功攻击。对多个LLMs进行的广泛实验表明,我们的适应性战略显著提高了破门的成功率。值得注意的是,我们的方法在GPT-4监狱破门的GPT-4o(2025年5月29日释放)中取得了非常高的98.9%的成功率。","Mingyu Yu, Wei Wang, Yanjie Wei, Sujuan Qin",2025-06-05T12:24:01Z,Adaptive Jailbreaking Strategies Based on the Semantic Understanding   Capabilities of Large Language Models,Adaptive Jailbreaking-Strategien basierend auf dem semantischen Verständnis von Fähigkeiten großer Sprachmodelle,基于大语言模型的语义理解能力,http://arxiv.org/abs/2505.23404v2
80,"Sustainability reports are key for evaluating companies' environmental, social and governance, ESG performance, but their content is increasingly obscured by greenwashing - sustainability claims that are misleading, exaggerated, and fabricated. Yet, existing NLP approaches for ESG analysis lack robustness against greenwashing risks, often extracting insights that reflect misleading or exaggerated sustainability claims rather than objective ESG performance. To bridge this gap, we introduce A3CG - Aspect-Action Analysis with Cross-Category Generalization, as a novel dataset to improve the robustness of ESG analysis amid the prevalence of greenwashing. By explicitly linking sustainability aspects with their associated actions, A3CG facilitates a more fine-grained and transparent evaluation of sustainability claims, ensuring that insights are grounded in verifiable actions rather than vague or misleading rhetoric. Additionally, A3CG emphasizes cross-category generalization. This ensures robust model performance in aspect-action analysis even when companies change their reports to selectively favor certain sustainability areas. Through experiments on A3CG, we analyze state-of-the-art supervised models and LLMs, uncovering their limitations and outlining key directions for future research.","可持续性报告是评估公司环境、社会和治理的关键,环境、社会和治理业绩,但其内容日益被绿色洗涤 — — 误导、夸大和编造的可持续性主张所掩盖。然而,现有的环境、社会和治理分析的NLP方法缺乏抵御绿色洗涤风险的稳健性,常常产生反映误导或夸大可持续性主张的洞察力,而不是反映环境、社会和治理的客观表现。为了缩小这一差距,我们引入了A3CG — — 跨类别概括性行动分析,作为在绿色洗涤盛行的情况下提高环境、SG分析的稳健性的新数据集。通过将可持续性方面与其相关行动明确联系起来,A3CG促进更精细、更透明地评价可持续性主张,确保以可核查的行动而不是模糊或误导性言论为基础。此外,A3CG强调跨类别概括性。这确保了在公司改变报告以选择性地偏向某些可持续性领域时的方方面行动分析的稳健模式绩效。我们通过对A3CG的实验,我们分析受监管的状态模型和LMS,揭示其局限性,并概述未来研究的关键方向。","Keane Ong, Rui Mao, Deeksha Varshney, Erik Cambria, Gianmarco Mengaldo",2025-06-05T12:05:19Z,Towards Robust ESG Analysis Against Greenwashing Risks: Aspect-Action   Analysis with Cross-Category Generalization,Auf dem Weg zu robuster ESG-Analyse gegen Greenwashing-Risiken: Aspekt-Action-Analyse mit Cross-Category-Verallgemeinerung,致力于对绿色清洗风险进行强有力的环境SG分析:跨类别通用化的外观行动分析,http://arxiv.org/abs/2502.15821v2
81,"Neural Machine Translation (NMT) has improved translation by using Transformer-based models, but it still struggles with word ambiguity and context. This problem is especially important in domain-specific applications, which often have problems with unclear sentences or poor data quality. Our research explores how adding information to models can improve translations in the context of e-commerce data. To this end we create ConECT -- a new Czech-to-Polish e-commerce product translation dataset coupled with images and product metadata consisting of 11,400 sentence pairs. We then investigate and compare different methods that are applicable to context-aware translation. We test a vision-language model (VLM), finding that visual context aids translation quality. Additionally, we explore the incorporation of contextual information into text-to-text models, such as the product's category path or image descriptions. The results of our study demonstrate that the incorporation of contextual information leads to an improvement in the quality of machine translation. We make the new dataset publicly available.","神经机器翻译(NMT)通过使用基于变换器的模型改进了翻译工作,但是它仍然在用词模糊和上下文进行挣扎。 这个问题在具体领域的应用中特别重要, 这些应用往往有不清楚的句子或数据质量差的问题。 我们的研究探讨了将信息添加到模型中如何改进电子商务数据方面的翻译。 为此,我们创建了Contlection -- -- 捷克到波兰的新的电子商务产品翻译数据集,配有11 400对句子的图像和产品元数据。 然后我们调查并比较了适用于背景认知翻译的不同方法。 我们测试了一个视觉语言模型(VLM),发现视觉背景背景有助于翻译质量。 此外,我们探索将背景信息纳入文本到文本模型中,例如产品分类路径或图像描述。 我们的研究结果表明,将背景信息整合导致机器翻译质量的改善。 我们将新的数据集公诸于众。","Mikołaj Pokrywka, Wojciech Kusa, Mieszko Rutkowski, Mikołaj Koszowski",2025-06-05T12:02:01Z,ConECT Dataset: Overcoming Data Scarcity in Context-Aware E-Commerce MT,ConECT-Datensatz: Überwindung von Datenknappheit im Kontext-Bewusst-E-Commerce MT,Conconect 数据集:克服上下文软件电子商务MT中的数据缺乏问题,http://arxiv.org/abs/2506.04929v1
82,"Counterfactual reasoning typically involves considering alternatives to actual events. While often applied to understand past events, a distinct form-forward counterfactual reasoning-focuses on anticipating plausible future developments. This type of reasoning is invaluable in dynamic financial markets, where anticipating market developments can powerfully unveil potential risks and opportunities for stakeholders, guiding their decision-making. However, performing this at scale is challenging due to the cognitive demands involved, underscoring the need for automated solutions. Large Language Models (LLMs) offer promise, but remain unexplored for this application. To address this gap, we introduce a novel benchmark, Fin-Force-FINancial FORward Counterfactual Evaluation. By curating financial news headlines and providing structured evaluation, Fin-Force supports LLM based forward counterfactual generation. This paves the way for scalable and automated solutions for exploring and anticipating future market developments, thereby providing structured insights for decision-making. Through experiments on Fin-Force, we evaluate state-of-the-art LLMs and counterfactual generation methods, analyzing their limitations and proposing insights for future research.","反事实推理通常涉及考虑实际事件的替代办法。虽然通常用于理解过去的事件,但有一种不同的反事实推理形式,注重预测未来可能发生的发展。这种推理在活跃的金融市场中非常宝贵,因为预测市场发展可以有力地暴露潜在风险和利益攸关方的机会,指导其决策。然而,由于认知需求,大规模地进行这一推理具有挑战性,突出了自动解决方案的必要性。大型语言模型(LLMS)提供了希望,但对于这一应用仍然尚未探索。为了弥补这一差距,我们采用了一个新的基准,即Fin-Fin-Fincial 反事实评估。通过调整金融新闻标题和提供结构评估,Fin-Financial-Fward Factal Compressment支持LM以前方反事实一代为基础。这为探索和预测未来市场发展的可缩放和自动化解决方案铺平了道路,从而为决策提供了有条理的见解。通过Fin-Force实验,我们评估最新LMs和反事实生成方法,分析其局限性并为未来研究提出见解。","Keane Ong, Rui Mao, Deeksha Varshney, Paul Pu Liang, Erik Cambria, Gianmarco Mengaldo",2025-06-05T11:59:20Z,Deriving Strategic Market Insights with Large Language Models: A   Benchmark for Forward Counterfactual Generation,Strategische Markteinblicke mit großen Sprachmodellen ableiten: Ein Benchmark für die vorausschauende kontrafaktische Generation,具有大语言模式的战略市场展望:前瞻性反实际生成基准,http://arxiv.org/abs/2505.19430v2
83,"Large language models (LLMs) have demonstrated the ability to generate formative feedback and instructional hints in English, making them increasingly relevant for AI-assisted education. However, their ability to provide effective instructional support across different languages, especially for mathematically grounded reasoning tasks, remains largely unexamined. In this work, we present the first large-scale simulation of multilingual tutor-student interactions using LLMs. A stronger model plays the role of the tutor, generating feedback in the form of hints, while a weaker model simulates the student. We explore 352 experimental settings across 11 typologically diverse languages, four state-of-the-art LLMs, and multiple prompting strategies to assess whether language-specific feedback leads to measurable learning gains. Our study examines how student input language, teacher feedback language, model choice, and language resource level jointly influence performance. Results show that multilingual hints can significantly improve learning outcomes, particularly in low-resource languages when feedback is aligned with the student's native language. These findings offer practical insights for developing multilingual, LLM-based educational tools that are both effective and inclusive.","大型语言模型(LLMS)展示了以英语生成成型反馈和教学提示的能力,使这些模型对AI辅助教育越来越具有相关性;然而,它们在不同语言中提供有效教学支持的能力,特别是基于数学的推理任务,在很大程度上仍未受到审查;在这项工作中,我们展示了第一次使用LLMS的多语种辅导学生互动的大规模模拟。一个更强大的模型扮演了教师的角色,以提示形式产生反馈,而一个较弱模型则模拟学生。我们探索了11种类型多样语言的352个实验环境,四种最先进的LMS,以及多种促进战略,以评估特定语言反馈是否带来可衡量的学习成果。我们的研究审查了学生投入语言、教师反馈语言、模式选择和语言资源水平如何共同影响业绩。结果显示,多语种提示可以大大改善学习成果,特别是在反馈与学生母语一致的情况下,在低资源语言方面。这些发现为开发有效和包容性的多种语言、基于LM的教育工具提供了实用的洞察力。","Junior Cedric Tonga, KV Aditya Srivatsa, Kaushal Kumar Maurya, Fajri Koto, Ekaterina Kochmar",2025-06-05T11:53:04Z,Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback,Simulation von LLM-zu-LLM-Tutoring für mehrsprachiges Mathe-Feedback,模拟多种语言数学反馈LLM至LLM教学,http://arxiv.org/abs/2506.04920v1
84,"An effective approach to the development of ASR systems for low-resource languages is to fine-tune an existing multilingual end-to-end model. When the original model has been trained on large quantities of data from many languages, fine-tuning can be effective with limited training data, even when the language in question was not present in the original training data. The fine-tuning approach has been encouraged by the availability of public-domain E2E models and is widely believed to lead to state-of-the-art results. This paper, however, challenges that belief. We show that an approach combining hybrid HMMs with self-supervised models can yield substantially better performance with limited training data. This combination allows better utilisation of all available speech and text data through continued self-supervised pre-training and semi-supervised training. We benchmark our approach on Scottish Gaelic, achieving WER reductions of 32% relative over our best fine-tuned Whisper model.","开发低资源语言的ASR系统的有效方法就是微调现有的多语言端到端模式。当原始模式已经就来自多种语言的大量数据进行了培训时,微调可以用有限的培训数据来有效,即使原始培训数据中不存在有关语言。微调方法因公共领域E2E模式的提供而得到鼓励,并被广泛认为会导致最先进的结果。然而,本文挑战了这一信念。我们表明,将混合的HMM与自监督的模式相结合的做法,如果培训数据有限,就能产生更好的效果。这种结合通过持续自我监督的培训前培训和半监督培训,可以更好地利用所有现有的语音和文本数据。我们用苏格兰盖尔语衡量我们的方法,比我们最精细调整的Wiper模式降低32%。","Ondřej Klejch, William Lamb, Peter Bell",2025-06-05T11:52:08Z,A Practitioner's Guide to Building ASR Models for Low-Resource   Languages: A Case Study on Scottish Gaelic,Leitfaden für Praktiker zum Aufbau von ASR-Modellen für Low-Resource-Sprachen: Eine Fallstudie zu schottischem Gälisch,《建立低资源语言ASR模式实践者指南:苏格兰盖尔语案例研究》,http://arxiv.org/abs/2506.04915v1
85,"With approximately 7,000 languages spoken worldwide, current large language models (LLMs) support only a small subset. Prior research indicates LLMs can learn new languages for certain tasks without supervised data. We extend this investigation to speech recognition, investigating whether LLMs can learn unseen, low-resource languages through in-context learning (ICL). With experiments on four diverse endangered languages that LLMs have not been trained on, we find that providing more relevant text samples enhances performance in both language modelling and Automatic Speech Recognition (ASR) tasks. Furthermore, we show that the probability-based approach outperforms the traditional instruction-based approach in language learning. Lastly, we show ICL enables LLMs to achieve ASR performance that is comparable to or even surpasses dedicated language models trained specifically for these languages, while preserving the original capabilities of the LLMs.","以往的研究显示,LLMS可以在某些任务中学习新语言,而无需监督数据。我们把调查扩大到语音识别,调查LLMs是否可以通过内文学习(ICL)学习隐蔽的、低资源的语言。我们发现,LLMS没有接受过培训的四种濒危语言实验,通过提供更相关的文本样本,可以提高语言建模和自动语音识别(ASR)任务两方面的绩效。此外,我们表明,基于概率的方法优于传统的语言学习教学方法。最后,我们展示ICLL使LMs能够取得与专门为这些语言培训的ASR模式相似甚至超过专门语言模式的功能,同时保持LMMs的原始能力。","Zhaolin Li, Jan Niehues",2025-06-05T11:49:15Z,In-context Language Learning for Endangered Languages in Speech   Recognition,Im Zusammenhang mit dem Sprachenlernen für gefährdete Sprachen in der Spracherkennung,在语音识别中为濒危语言进行内通语言学习,http://arxiv.org/abs/2505.20445v3
86,"We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional Multi-Token Prediction objective for enhanced performance and accelerated inference speed. During post-training, we curate a dataset of 130K verifiable mathematics and programming problems for reinforcement learning, integrating a test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and employing strategic data resampling to stabilize training. Extensive evaluations show that MiMo-7B-Base possesses exceptional reasoning potential, outperforming even much larger 32B models. The final RL-tuned model, MiMo-7B-RL, achieves superior performance on mathematics, code and general reasoning tasks, surpassing the performance of OpenAI o1-mini. The model checkpoints are available at https://github.com/xiaomimimo/MiMo.","我们介绍了MiMo-7B,这是为推理任务而诞生的大型语言模型,在培训前和培训后阶段都实现了优化;在培训前阶段,我们加强数据处理前管道,并采用三阶段数据混合战略,以加强基本模型的推理潜力;MiMo-7BBB在25万亿个象征性品上进行了预先培训,并增加了提高性能和加速推理速度的多功能预测目标;在培训后,我们整理了130K的可核实数学和编程问题数据集,用于强化学习,整合了由测试和难度驱动的代码调整计划,以缓解稀释问题,并利用战略数据重现来稳定培训;广泛的评价显示,MiMo-7BBB数据库拥有超乎寻常的推理潜力,其性能甚至超过32B模型;最后的RL调制模型MiM-7B-RL在数学、代码和一般推理学任务上取得了优异性,超过了OpenAI1-mini的性能;示范检查站见https://github.com/ximimimimo/Mimomo。","LLM-Core Xiaomi, :, Bingquan Xia, Bowen Shen, Cici, Dawei Zhu, Di Zhang, Gang Wang, Hailin Zhang, Huaqiu Liu, Jiebao Xiao, Jinhao Dong, Liang Zhao, Peidian Li, Peng Wang, Shihua Yu, Shimao Chen, Weikun Wang, Wenhan Ma, Xiangwei Deng, Yi Huang, Yifan Song, Zihan Jiang, Bowen Ye, Can Cai, Chenhong He, Dong Zhang, Duo Zhang, Guoan Wang, Hao Tian, Haochen Zhao, Heng Qu, Hongshen Xu, Jun Shi, Kainan Bao, Kai Fang, Kang Zhou, Kangyang Zhou, Lei Li, Menghang Zhu, Nuo Chen, Qiantong Wang, Shaohui Liu, Shicheng Li, Shuhao Gu, Shuhuai Ren, Shuo Liu, Sirui Deng, Weiji Zhuang, Weiwei Lv, Wenyu Yang, Xin Zhang, Xing Yong, Xing Zhang, Xingchen Song, Xinzhe Xu, Xu Wang, Yihan Yan, Yu Tu, Yuanyuan Tian, Yudong Wang, Yue Yu, Zhenru Lin, Zhichao Song, Zihao Yue",2025-06-05T11:49:09Z,MiMo: Unlocking the Reasoning Potential of Language Model -- From   Pretraining to Posttraining,MiMo: Entsperren des vernünftigen Potenzials des Sprachmodells -- Von der Vorschulung zur Nachschulung,米莫:释放语文模式的理性潜力 -- -- 从预培训到员额培训,http://arxiv.org/abs/2505.07608v2
87,"Despite recent progress in training long-context reasoning models via reinforcement learning (RL), several open questions and counterintuitive behaviors remain. This work focuses on three key aspects: (1) We systematically analyze the roles of positive and negative samples in RL, revealing that positive samples mainly facilitate data fitting, whereas negative samples significantly enhance generalization and robustness. Interestingly, training solely on negative samples can rival standard RL training performance. (2) We identify substantial data inefficiency in group relative policy optimization, where over half of the samples yield zero advantage. To address this, we explore two straightforward strategies, including relative length rewards and offline sample injection, to better leverage these data and enhance reasoning efficiency and capability. (3) We investigate unstable performance across various reasoning models and benchmarks, attributing instability to uncertain problems with ambiguous outcomes, and demonstrate that multiple evaluation runs mitigate this issue.","尽管最近在通过强化学习培训长文本推理模型方面取得了进展,但若干未决问题和反直觉行为依然存在,这项工作侧重于三个关键方面:(1) 我们系统地分析在强化学习中正反抽样的作用,表明正面样品主要有助于数据配置,而负面样品则大大加强了概括性和稳健性;有趣的是,仅仅进行负面样品培训可能与标准RL培训业绩相悖;(2) 我们发现,在群体相对优化政策方面,有一半以上的样本产生零优势,因此,我们发现了大量数据效率低下;为解决这一问题,我们探索了两种直接的战略,包括相对长度奖励和脱线抽样注射,以更好地利用这些数据,提高推理效率和能力。(3) 我们调查各种推理模型和基准的不稳定性表现,将不稳定因素归咎于不确定性问题,结果模糊不清,并表明多重评价会缓解这一问题。","Yongyu Mu, Jiali Zeng, Bei Li, Xinyan Guan, Fandong Meng, Jie Zhou, Tong Xiao, Jingbo Zhu",2025-06-05T11:47:10Z,Dissecting Long Reasoning Models: An Empirical Study,Modelle der langen Vernunft zu entschlüsseln: Eine empirische Studie,解析长期理由模型:经验研究,http://arxiv.org/abs/2506.04913v1
88,"The honesty of large language models (LLMs) is a critical alignment challenge, especially as advanced systems with chain-of-thought (CoT) reasoning may strategically deceive humans. Unlike traditional honesty issues on LLMs, which could be possibly explained as some kind of hallucination, those models' explicit thought paths enable us to study strategic deception--goal-driven, intentional misinformation where reasoning contradicts outputs. Using representation engineering, we systematically induce, detect, and control such deception in CoT-enabled LLMs, extracting ""deception vectors"" via Linear Artificial Tomography (LAT) for 89% detection accuracy. Through activation steering, we achieve a 40% success rate in eliciting context-appropriate deception without explicit prompts, unveiling the specific honesty-related issue of reasoning models and providing tools for trustworthy AI alignment.","大型语言模型(LLMs)的诚实性是一个至关重要的整合挑战,特别是具有思维链推理的先进系统可能在战略上欺骗人类。 与LLMs的传统诚实问题不同,LLMs的传统诚实性问题可能被解释为某种幻觉,这些模型的清晰思维路径使我们能够研究战略欺骗-目标驱动的故意错误信息,而其推理与产出相矛盾。 使用演示工程,我们系统地诱导、检测和控制COT驱动的LLMs中的这种欺骗性,通过线性人工成像法(LAT)提取“欺骗矢量 ” , 以89%的检测精确度。 通过启动指导,我们实现了40%的成功率,在没有明确提示的情况下诱发符合背景的欺骗,揭示了与诚实相关的具体推理模型问题,并为可信赖的AI匹配提供了工具。","Kai Wang, Yihao Zhang, Meng Sun",2025-06-05T11:44:19Z,When Thinking LLMs Lie: Unveiling the Strategic Deception in   Representations of Reasoning Models,Wenn LLMs denken lügen: Enthüllen der strategischen Täuschung in Repräsentationen von Vernunftmodellen,当思考LLLM Lie:在解释理由模型代表中消除战略欺骗时,http://arxiv.org/abs/2506.04909v1
89,"Large Language Models (LLMs), whilst great at extracting facts from text, struggle with nested narrative reasoning. Existing long context and multi-hop QA benchmarks inadequately test this, lacking realistic distractors or failing to decouple context length from reasoning complexity, masking a fundamental LLM limitation. We introduce Verbose ListOps, a novel benchmark that programmatically transposes ListOps computations into lengthy, coherent stories. This uniquely forces internal computation and state management of nested reasoning problems by withholding intermediate results, and offers fine-grained controls for both narrative size \emph{and} reasoning difficulty. Whilst benchmarks like LongReason (2025) advance approaches for synthetically expanding the context size of multi-hop QA problems, Verbose ListOps pinpoints a specific LLM vulnerability: difficulty in state management for nested sub-reasoning amongst semantically-relevant, distracting narrative. Our experiments show that leading LLMs (e.g., OpenAI o4, Gemini 2.5 Pro) collapse in performance on Verbose ListOps at modest (~10k token) narrative lengths, despite effortlessly solving raw ListOps equations. Addressing this failure is paramount for real-world text interpretation which requires identifying key reasoning points, tracking conceptual intermediate results, and filtering irrelevant information. Verbose ListOps, and its extensible generation framework thus enables targeted reasoning enhancements beyond mere context-window expansion; a critical step to automating the world's knowledge work.","大型语言模型(LLMS)虽然在从文本中提取事实方面非常出色,但与嵌入式叙事推理的推理力抗争。 现有的长背景和多点QA基准没有对此进行充分的测试, 没有现实的分流器, 或者没有将背景长度与推理复杂性脱钩, 掩盖基本的 LLMM 限制。 我们引入了Verbose ListOps 的新基准, 将ListOps 移植成冗长、 一致的故事。 这独特地迫使内部计算和州管理嵌入推理问题, 并给描述性大小的 \emph{和} 推理困难提供精细的控控。 虽然LongReason (2025) 等基准在合成方式上扩展多点QA 问题的背景大小, 或无法将背景长度与推导,  Verbose ListO 的LPM: 在将 ListO 嵌入的子序列中, 很难管理 。 我们的实验显示 Verbose LIMSM(e, Ominal o, 2.5 Pro 2.5 Prodealalal) lishalalal listral liver liver lading lish lish) lish lading lading lish","Alex Pan, Mary-Anne Williams",2025-06-05T11:41:05Z,Verbose ListOps (VLO): Beyond Long Context -- Unmasking LLM's Reasoning   Blind Spots,Verbose ListOps (VLO): Jenseits des langen Kontextes -- LLMs aufschlussreiche Blindspots entlarven,(VLO):超越长期范围 -- -- 揭开LLM的致盲点,http://arxiv.org/abs/2506.04907v1
90,"With the significant progress of large reasoning models in complex coding and reasoning tasks, existing benchmarks, like LiveCodeBench and CodeElo, are insufficient to evaluate the coding capabilities of large language models (LLMs) in real competition environments. Moreover, current evaluation metrics such as Pass@K fail to capture the reflective abilities of reasoning models. To address these challenges, we propose \textbf{ICPC-Eval}, a top-level competitive coding benchmark designed to probing the frontiers of LLM reasoning. ICPC-Eval includes 118 carefully curated problems from 11 recent ICPC contests held in various regions of the world, offering three key contributions: 1) A challenging realistic ICPC competition scenario, featuring a problem type and difficulty distribution consistent with actual contests. 2) A robust test case generation method and a corresponding local evaluation toolkit, enabling efficient and accurate local evaluation. 3) An effective test-time scaling evaluation metric, Refine@K, which allows iterative repair of solutions based on execution feedback. The results underscore the significant challenge in evaluating complex reasoning abilities: top-tier reasoning models like DeepSeek-R1 often rely on multi-turn code feedback to fully unlock their in-context reasoning potential when compared to non-reasoning counterparts. Furthermore, despite recent advancements in code generation, these models still lag behind top-performing human teams. We release the benchmark at: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs","由于在复杂的编码和推理任务中大型推理模型的重大进展,现有基准,如LiveCodeBench和CodeElo等,不足以评价在实际竞争环境中大型语言模型(LLMs)的编码能力;此外,目前诸如Pass@K等评价指标未能反映推理模型的反映能力;为了应对这些挑战,我们提议了用于检验LLLM推理前沿的顶级竞争性编码基准,即:Textbf{ICPC-Eval}。ICPC-Eval包括最近在世界不同区域举行的11次ICPC竞赛中经过仔细整理的118个问题,提供了三大贡献:1)具有挑战性的、现实的ICPC竞争设想方案竞争情况,其特点是问题类型和与实际竞争情况一致的难以分配。 2)一个强大的测试生成案例方法和相应的当地评价工具包,使高效和准确的地方评价成为有效的测试时间缩放评价基准,即Refine@K,该基准允许根据执行反馈对解决办法进行迭接补。","Shiyi Xu, Yiwen Hu, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Ji-Rong Wen",2025-06-05T11:20:37Z,ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive   Programming Contests,ICPC-Eval: Mit wettbewerbsfähigen Programmierwettbewerben die Grenzen der LLM-Vernunft ebnen,"ICPC-Eval:以竞争性方案拟订竞赛为理由,探索LLM的前沿",http://arxiv.org/abs/2506.04894v1
91,"In this technical report, we empirically investigate the relationship between linguistic fluency and domain knowledge in the context of continual learning with large language models (LLMs). Specifically, we enhance the linguistic fluency of the Gemma2 LLM for the Lithuanian language by autoregressively pretraining its full parameter set on the first 10\% of the Lithuanian language component of the CulturaX dataset. To prevent catastrophic forgetting of the model's existing domain knowledge, we apply Elastic Weight Consolidation (EWC), leveraging Fisher information estimated using data from the Massive Multitask Language Understanding (MMLU) benchmark. In the post-training evaluations, we assess linguistic fluency through perplexity and evaluate domain knowledge using accuracy on a suite of language understanding benchmarks, including ARC-Easy, Belebele, GSM8K, HellaSwag, MMLU, TruthfulQA, and Winogrande, in both English and Lithuanian. The empirical results demonstrate that EWC not only mitigates catastrophic forgetting by preserving the model's performance in terms of both linguistic fluency and domain knowledge but also improves or maintains these capabilities for the newly added Lithuanian language. These findings highlight the potential for more efficient adaptation of general-purpose LLMs to under-represented languages without requiring access to the original training data. The accompanying codebase is openly accessible at https://github.com/Neurotechnology/LLM_EWC.","在这份技术报告中,我们从经验上调查语言流利和领域知识在继续学习大语言模型的背景下的关系。具体地说,我们通过自动地对CulturaX数据集立陶宛语部分头10设置的完整参数进行自动提前培训,提高Gemma2LLM在立陶宛语的语言流利程度,提高Gemma2 LLM在立陶宛语的语言流利程度。为了防止灾难性地忘记该模式现有的域知识,我们应用Elatic Weight 集聚(EWC),利用利用Massive Mulationaltask语言理解(MMMLU)基准中的数据估算的渔业信息。在培训后评价中,我们利用一套语言理解基准的精度评估Gemma2LLM语言流利度并评估域知识,包括ARC-Easy、Beebleble、GSM8K、HellaSwag、MLMLU、Tyful QA和Wino grande, 实证结果表明,EWCWC不仅通过维护该模型在语言流流流利和域知识方面的业绩知识方面的性知识,而且通过通过透视能/流中提高RMUMLMRLM公司在新版本数据库中的原始数据中的潜力,这些能力,这些能力,这些能力是不需要数据库的更新数据库的原始数据。","Vytenis Šliogeris, Povilas Daniušis, Artūras Nakvosas",2025-06-05T11:13:42Z,Full-Parameter Continual Pretraining of Gemma2: Insights into Fluency   and Domain Knowledge,Full-Parameter Continual Pretraining von Gemma2: Einblicke in Fluency und Domain Knowledge,Gemma2:深入了解流能和广域知识,http://arxiv.org/abs/2505.05946v2
92,"Universal Dependencies (UD), while widely regarded as the most successful linguistic framework for cross-lingual syntactic representation, remains underexplored in terms of its effectiveness. This paper addresses this gap by integrating UD into pretrained language models and assesses if UD can improve their performance on a cross-lingual adversarial paraphrase identification task. Experimental results show that incorporation of UD yields significant improvements in accuracy and $F_1$ scores, with average gains of 3.85\% and 6.08\% respectively. These enhancements reduce the performance gap between pretrained models and large language models in some language pairs, and even outperform the latter in some others. Furthermore, the UD-based similarity score between a given language and English is positively correlated to the performance of models in that language. Both findings highlight the validity and potential of UD in out-of-domain tasks.","普遍依赖(UD)虽然被广泛认为是跨语文综合表述最成功的语言框架,但在效力方面仍未得到充分探讨,本文件通过将UD纳入预先培训的语文模式来弥补这一差距,并评估UD能否改善其在跨语文对抗性参数识别任务方面的表现,实验结果表明,采用UD在准确性和1美元分数方面大有改进,平均收益分别为3.85和6.08,这些改进缩小了某些语文对口的预先培训模式与大型语文模式之间的性能差距,在某些语文对口中甚至优于后者。此外,基于UD的某种语文与英文的相似性分数与该语文模式的性能有着积极的相关性。 两项结果都强调了UD在外部任务中的有效性和潜力。",Wenxi Li,2025-06-05T11:10:14Z,Evaluating the Effectiveness of Linguistic Knowledge in Pretrained   Language Models: A Case Study of Universal Dependencies,Bewertung der Wirksamkeit sprachlicher Kenntnisse in vorgebildeten Sprachmodellen: Eine Fallstudie universeller Abhängigkeiten,评价在未受过训练的语言模式中语言知识的有效性:对普遍依赖性的个案研究,http://arxiv.org/abs/2506.04887v1
93,"Inspired by the impressive capabilities of GPT-4o, there is growing interest in enabling speech language models (SLMs) to engage in natural, fluid spoken interactions with humans. Recent advancements have led to the development of several SLMs that demonstrate promising results in this area. However, current approaches have yet to fully exploit dual-channel speech data, which inherently captures the structure and dynamics of human conversation. In this work, we systematically explore the use of dual-channel speech data in the context of modern large language models, and introduce a novel generative modeling paradigm, Next-Token-Pair Prediction (NTPP), to enable speaker-independent dual-channel spoken dialogue learning using decoder-only architectures for the first time. We evaluate our approach on standard benchmarks, and empirical results show that our proposed method, NTPP, significantly improves the conversational abilities of SLMs in terms of turn-taking prediction, response coherence, and naturalness. Moreover, compared to existing methods, NTPP achieves substantially lower inference latency, highlighting its practical efficiency for real-time applications.","由于GPT-4o的惊人能力,人们越来越有兴趣使语言模型(SLMs)能够与人类进行自然的、流畅的口头互动,最近的进展导致若干可持续土地管理的发展,展示了这一领域的有希望的成果,然而,目前的办法尚未充分利用双通道语音数据,这些数据本身就捕捉了人类对话的结构和动态。在这项工作中,我们系统地探索在现代大型语言模型中使用双通道语音数据,并引入了新型的基因模型模式(Rext-Token-Pair 预测(NTPPPP)),以便首次利用只使用拆解器的架构,使具有独立性的双通道语音对话学习成为可能。我们评估标准基准的方法和实证结果显示,我们拟议的方法(NTPPP)大大提高了可持续土地管理在转手预测、反应一致性和自然性方面的谈话能力。此外,与现有方法相比,NTPPPP取得低得多的推导力,突出其实时应用的实际效率。","Qichao Wang, Ziqiao Meng, Wenqian Cui, Yifei Zhang, Pengcheng Wu, Bingzhe Wu, Irwin King, Liang Chen, Peilin Zhao",2025-06-05T11:09:58Z,NTPP: Generative Speech Language Modeling for Dual-Channel Spoken   Dialogue via Next-Token-Pair Prediction,NTPP: Generative Sprachmodellierung für Dual-Channel-Gesprochenen Dialog über Next-Token-Pair-Vorhersage,NTPP:通过下波波语预测为双声道口语对话生成语音模型,http://arxiv.org/abs/2506.00975v2
94,"This paper introduces NorEval, a new and comprehensive evaluation suite for large-scale standardized benchmarking of Norwegian generative language models (LMs). NorEval consists of 24 high-quality human-created datasets -- of which five are created from scratch. In contrast to existing benchmarks for Norwegian, NorEval covers a broad spectrum of task categories targeting Norwegian language understanding and generation, establishes human baselines, and focuses on both of the official written standards of the Norwegian language: Bokm{\aa}l and Nynorsk. All our datasets and a collection of over 100 human-written prompts are integrated into LM Evaluation Harness, ensuring flexible and reproducible evaluation. We describe the NorEval design and present the results of benchmarking 19 open-source pre-trained and instruction-tuned LMs for Norwegian in various scenarios. Our benchmark, evaluation framework, and annotation materials are publicly available.","本文介绍NorEval,这是挪威基因语言模型大规模标准化基准的新的综合评价套件,NorEval由24个高质量的人造数据集组成,其中5个是从零创造的,与挪威现有基准不同,NorEval涵盖以挪威语言理解和生成为目标的广泛任务类别,确立了人类基线,并侧重于挪威语言的正式书面标准:Bokm-aa}l和Nynorsk。我们的所有数据集和100多个人类写作提示集都纳入了LM 评估工具,确保灵活和可复制的评价。我们描述了NorEval的设计,并介绍了19个挪威语基准的公开源预培训和指导调整LM的结果。我们的基准、评价框架和说明材料可以公开查阅。","Vladislav Mikhailov, Tita Enstad, David Samuel, Hans Christian Farsethås, Andrey Kutuzov, Erik Velldal, Lilja Øvrelid",2025-06-05T10:51:26Z,NorEval: A Norwegian Language Understanding and Generation Evaluation   Benchmark,NorEval: Ein Benchmark für das Verständnis und die Bewertung der norwegischen Sprache,NorEval:挪威语言理解和世代评价基准,http://arxiv.org/abs/2504.07749v2
95,"Recent work suggests that large language models (LLMs) can improve performance of speech tasks compared to existing systems. To support their claims, results on LibriSpeech and Common Voice are often quoted. However, this work finds that a substantial amount of the LibriSpeech and Common Voice evaluation sets appear in public LLM pretraining corpora. This calls into question the reliability of findings drawn from these two datasets. To measure contamination impact, LLMs trained with/without contamination are compared. A contaminated LLM is more likely to generate test sentences it has seen during training. Then, speech recognisers based on LLMs are compared. They show only subtle error rate differences if the LLM is contaminated, but assign significantly higher probabilities to transcriptions seen during LLM training. Results show that LLM outputs can be biased by tiny amounts of data contamination, highlighting the importance of evaluating LLM-based speech systems with held-out data.","最近的工作表明,与现有系统相比,大型语言模型(LLMs)可以改善语言任务的业绩。为了支持他们的说法,经常引用LibriSpeech和共同声音的结果。然而,这项工作发现,大量的LibriSpeech和共同声音评价组出现在公开的LLM培训前公司中。这就令人质疑从这两个数据集中得出的调查结果的可靠性。为了测量污染影响,对受过训练/没有污染的LLM进行了比较。受污染的LLM更有可能产生在培训期间看到的测试性判决。然后,对基于LMs的语音识别器进行比较。如果LLM受到污染,它们只显示微妙的错误率差异,但给LLM培训期间看到的记录显示的概率要高得多。结果显示,LLM的产出可能因少量的数据污染而有偏差,这突出说明了对基于LM的语音系统进行锁定数据评价的重要性。","Yuan Tseng, Titouan Parcollet, Rogier van Dalen, Shucong Zhang, Sourav Bhattacharya",2025-06-05T10:40:05Z,Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in   Large Language Models for Speech Recognition,Bewertung von LLMs in Speech wird oft abgeflacht: Testset Kontaminierung in großen Sprachmodellen für die Spracherkennung,对演讲中LLMs的评价经常是片断的:在大语言语音识别模型中测试设置污染,http://arxiv.org/abs/2505.22251v2
96,"In this work, we explore a cost-effective framework for multilingual image generation. We find that, unlike models tuned on high-quality images with multilingual annotations, leveraging text encoders pre-trained on widely available, noisy Internet image-text pairs significantly enhances data efficiency in text-to-image (T2I) generation across multiple languages.Based on this insight, we introduce MuLan, Multi-Language adapter, a lightweight language adapter with fewer than 20M parameters, trained alongside a frozen text encoder and image diffusion model. Compared to previous multilingual T2I models, this framework offers: (1) Cost efficiency. Using readily accessible English data and off-the-shelf multilingual text encoders minimizes the training cost; (2) High performance. Achieving comparable generation capabilities in over 110 languages with CLIP similarity scores nearly matching those in English (39.57 for English vs. 39.61 for other languages); and (3) Broad applicability. Seamlessly integrating with compatible community tools like LoRA, LCM, ControlNet, and IP-Adapter, expanding its potential use cases.","在这项工作中,我们探索了多语种图像生成的成本效益框架。我们发现,与以具有多语种说明的高质量图像调整的模型不同,利用在广泛可用的情况下预先培训过的文本编码器,在以多种语言制作文本到图像(T2I)方面,噪音的互联网图像文本对配方大大提高了数据效率。基于这一认识,我们引入了Mulan, Mul-Language适配器,即一个小于20M参数的轻量语言适配器,与冻结文本编码器和图像传播模型一起接受培训。与以前的多语言T2I模型相比,该框架提供:(1) 成本效率,利用容易获得的英文数据和现成的多语言文本编码器,最大限度地降低了培训成本;(2) 高绩效。实现110多种语言的可比生成能力,而CLIP相似性分数几乎与英语相匹配(39.57用于英语,39.61用于其他语言);(3) 广泛适用性。与兼容的社区工具(如LRA、LCM、控制Net和IP-Adapter)一起,毫不含糊地整合,扩大其潜在使用案例。","Sen Xing, Muyan Zhong, Zeqiang Lai, Liangchen Li, Jiawen Liu, Yaohui Wang, Jifeng Dai, Wenhai Wang",2025-06-05T10:34:56Z,MuLan: Adapting Multilingual Diffusion Models for Hundreds of Languages   with Negligible Cost,MuLan: Multilinguale Diffusionsmodelle für Hunderte von Sprachen mit vernachlässigbaren Kosten anpassen,MuLan:为数百种具有可忽略费用的语言调整多语言传播模式,http://arxiv.org/abs/2412.01271v2
97,"In this study, we explore the effectiveness of isometric machine translation across multiple language pairs (En$\to$De, En$\to$Fr, and En$\to$Es) under the conditions of the IWSLT Isometric Shared Task 2022. Using eight open-source large language models (LLMs) of varying sizes, we investigate how different prompting strategies, varying numbers of few-shot examples, and demonstration selection influence translation quality and length control. We discover that the phrasing of instructions, when aligned with the properties of the provided demonstrations, plays a crucial role in controlling the output length. Our experiments show that LLMs tend to produce shorter translations only when presented with extreme examples, while isometric demonstrations often lead to the models disregarding length constraints. While few-shot prompting generally enhances translation quality, further improvements are marginal across 5, 10, and 20-shot settings. Finally, considering multiple outputs allows to notably improve overall tradeoff between the length and quality, yielding state-of-the-art performance for some language pairs.","在本研究中,我们探索了在IWSLT Isoricat Common 任务2022的条件下,多种语言对口(En$to$De, E$to$Fr, E$$to$to$Es)的等量机器翻译的有效性。我们使用8个不同大小的开放源码大语言模型(LLMs),我们调查了不同的快速战略、不同数量的小例子和演示选择如何影响翻译质量和长度控制。我们发现,在与所提供的演示的特性相一致的情况下,指令的写法在控制输出长度方面发挥着关键作用。我们的实验表明,LLMS只有在提供极端例子时,才倾向于制作较短的翻译,而非计量演示往往导致模型无视长度限制。虽然很少发光的提高翻译质量,但在5个、10个和20个截面的设置中,进一步的改进是微不足道的。最后,考虑到多种产出可以显著改善长度和质量之间的总体权衡,为一些语言配方带来最先进的性能。","Dávid Javorský, Ondřej Bojar, François Yvon",2025-06-05T10:24:08Z,Prompting LLMs: Length Control for Isometric Machine Translation,Prompting LLMs: Längensteuerung für isometrische maschinelle Übersetzung,LLM: 遥测机器翻译的长度控制,http://arxiv.org/abs/2506.04855v1
98,"Integrating Artificial Intelligence (AI) in educational settings has brought new learning approaches, transforming the practices of both students and educators. Among the various technologies driving this transformation, Large Language Models (LLMs) have emerged as powerful tools for creating educational materials and question answering, but there are still space for new applications. Educators commonly use Multiple-Choice Questions (MCQs) to assess student knowledge, but manually generating these questions is resource-intensive and requires significant time and cognitive effort. In our opinion, LLMs offer a promising solution to these challenges. This paper presents a novel comparative analysis of three widely known LLMs - Llama 2, Mistral, and GPT-3.5 - to explore their potential for creating informative and challenging MCQs. In our approach, we do not rely on the knowledge of the LLM, but we inject the knowledge into the prompt to contrast the hallucinations, giving the educators control over the test's source text, too. Our experiment involving 21 educators shows that GPT-3.5 generates the most effective MCQs across several known metrics. Additionally, it shows that there is still some reluctance to adopt AI in the educational field. This study sheds light on the potential of LLMs to generate MCQs and improve the educational experience, providing valuable insights for the future.","将人工智能(AI)纳入教育环境带来了新的学习方法,改变了学生和教育工作者的做法。在推动这一转变的各种技术中,大语言模型(LLMS)已成为创造教育材料和回答问题的有力工具,但是仍然有新的应用空间。教育者通常使用多选择问题(MCQs)来评估学生知识,但人工生成这些问题需要大量资源,需要大量的时间和认知努力。我们认为,LLMS为这些挑战提供了很有希望的解决方案。本文对三个广为人知的LLMS-Llama 2,Mistral和GPT-3.5 - 探索其创造信息和具有挑战性的MCQ的潜力进行了新颖的比较分析。在我们的方法中,我们并不依赖LMM的知识,但我们把知识注入了对幻觉的瞬间对比,使教育者对测试源文本的控制也十分紧张。我们21名教育工作者的实验表明,GPT-3.5在几个已知的计量标准中产生了最有效的MCQ。此外,它表明,在教育领域提供宝贵的深刻见解方面,目前还不太愿意利用AI。","Giorgio Biancini, Alessio Ferrato, Carla Limongelli",2025-06-05T10:21:49Z,Multiple-Choice Question Generation Using Large Language Models:   Methodology and Educator Insights,Multiple-Choice-Frage-Generierung mit großen Sprachmodellen: Methodologie und Erzieher-Insights,使用大语言模式生成多选择问题:方法和教育者透视,http://arxiv.org/abs/2506.04851v1
99,"Online misinformation remains a critical challenge, and fact-checkers increasingly rely on claim matching systems that use sentence embedding models to retrieve relevant fact-checks. However, as users interact with claims online, they often introduce edits, and it remains unclear whether current embedding models used in retrieval are robust to such edits. To investigate this, we introduce a perturbation framework that generates valid and natural claim variations, enabling us to assess the robustness of a wide-range of sentence embedding models in a multi-stage retrieval pipeline and evaluate the effectiveness of various mitigation approaches. Our evaluation reveals that standard embedding models exhibit notable performance drops on edited claims, while LLM-distilled embedding models offer improved robustness at a higher computational cost. Although a strong reranker helps to reduce the performance drop, it cannot fully compensate for first-stage retrieval gaps. To address these retrieval gaps, we evaluate train- and inference-time mitigation approaches, demonstrating that they can improve in-domain robustness by up to 17 percentage points and boost out-of-domain generalization by 10 percentage points. Overall, our findings provide practical improvements to claim-matching systems, enabling more reliable fact-checking of evolving misinformation. Code and data are available at https://github.com/JabezNzomo99/claim-matching-robustness.","在线错误信息仍然是一个重大挑战,而事实检查者越来越依赖使用嵌入模型的句子来检索相关事实检查的匹配系统。然而,随着用户在网上与索赔者互动,他们往往会引入编辑,而且仍然不清楚目前用于检索的嵌入模型是否对此类编辑十分健全。为了调查这一问题,我们引入了造成有效和自然索赔差异的扰动框架,使我们能够评估在多阶段检索管道中广泛嵌入判决模型的稳健性,并评价各种缓解方法的有效性。我们的评估显示,标准嵌入模型在编辑索赔中表现显著的绩效下降,而LLM留存的嵌入模型则以更高的计算成本提供更好的稳健性。虽然强大的重置器有助于减少绩效下降,但无法充分弥补第一阶段的回收差距。为了解决这些回收差距,我们评估培训和推断时间减缓方法,表明它们能够以17个百分点保持稳健性,并提升外向外扩展的概括性能。总体而言,我们的调查结果提供了对索赔/RBAS-BAS/CADRM/CADRBA/CADRismating的实用性改进,使现有数据系统更可靠。","Jabez Magomere, Emanuele La Malfa, Manuel Tonneau, Ashkan Kazemi, Scott Hale",2025-06-05T10:17:20Z,When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding   Models Against Misinformation Edits,Wenn Ansprüche entstehen: Bewertung und Verbesserung der Robustheit von Einbettungsmodellen gegen Fehlinformations-Edits,索赔演变时:评价和加强反错误信息嵌入模型的威力,http://arxiv.org/abs/2503.03417v3
100,"In simultaneous interpreting, an interpreter renders a source speech into another language with a very short lag, much sooner than sentences are finished. In order to understand and later reproduce this dynamic and complex task automatically, we need dedicated datasets and tools for analysis, monitoring, and evaluation, such as parallel speech corpora, and tools for their automatic annotation. Existing parallel corpora of translated texts and associated alignment algorithms hardly fill this gap, as they fail to model long-range interactions between speech segments or specific types of divergences (e.g., shortening, simplification, functional generalization) between the original and interpreted speeches. In this work, we introduce MockConf, a student interpreting dataset that was collected from Mock Conferences run as part of the students' curriculum. This dataset contains 7 hours of recordings in 5 European languages, transcribed and aligned at the level of spans and words. We further implement and release InterAlign, a modern web-based annotation tool for parallel word and span annotations on long inputs, suitable for aligning simultaneous interpreting. We propose metrics for the evaluation and a baseline for automatic alignment. Dataset and tools are released to the community.","翻译在同时翻译时,将源语言转换成另一种语言,时间短得多,比句子完成要早得多。为了理解和随后自动复制这一动态和复杂的任务,我们需要专门的数据集和工具,用于分析、监测和评价,例如平行演讲团,以及用于自动注解的工具。现有的翻译文本和相关校对算法平行体无法填补这一空白,因为他们无法模拟原言和释义之间语言部分或特定类型分歧(如缩短、简化、功能概括)之间的长距离互动(如缩短、简化、功能概括)。在这项工作中,我们引入了MockConf,这是一名学生解释作为学生课程的一部分从Mock会议收集的数据集。该数据集包含以5种欧洲语言制作的7小时录音,在词表和文字层面进行转录和校正。我们进一步实施和发布InterAlign,这是一个基于网络的现代注释工具,用于平行词和长文说明,适合同时进行解释。我们为评价提出了指标和自动校准基准。数据和工具被发布到社区。","Dávid Javorský, Ondřej Bojar, François Yvon",2025-06-05T10:16:15Z,"MockConf: A Student Interpretation Dataset: Analysis, Word- and   Span-level Alignment and Baselines","MockConf: A Student Interpretation Dataset: Analysis, Word- und Span-Level Alignment und Baselines",MockConf:学生解释数据集:分析、单词和泛层对齐和基线,http://arxiv.org/abs/2506.04848v1
101,"Large language models (LLMs) have demonstrated significant progress in various natural language generation and understanding tasks. However, their linguistic generalization capabilities remain questionable, raising doubts about whether these models learn language similarly to humans. While humans exhibit compositional generalization and linguistic creativity in language use, the extent to which LLMs replicate these abilities, particularly in morphology, is under-explored. In this work, we systematically investigate the morphological generalization abilities of LLMs through the lens of compositionality. We define morphemes as compositional primitives and design a novel suite of generative and discriminative tasks to assess morphological productivity and systematicity. Focusing on agglutinative languages such as Turkish and Finnish, we evaluate several state-of-the-art instruction-finetuned multilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs struggle with morphological compositional generalization particularly when applied to novel word roots, with performance declining sharply as morphological complexity increases. While models can identify individual morphological combinations better than chance, their performance lacks systematicity, leading to significant accuracy gaps compared to humans.","大型语言模型(LLMs)在各种自然语言的生成和理解任务方面显示出了显著的进步,然而,他们的语言概括能力仍然令人怀疑,使人怀疑这些模型是否学习与人类相似的语言。虽然人类在语言使用方面表现出了通用的构成和语言创造性,但LLMs复制这些能力的程度,特别是在形态学方面,探索不足。在这项工作中,我们系统地从组成性的角度来调查LLMs的形态概括能力。我们把模具定义为构成性原始,并设计了一套新型的基因化和歧视性任务来评估形态生产力和系统性。我们侧重于土耳其文和芬兰文等拼写语言,我们评估了若干最先进的多语种教学模型,包括GPT-4和Gemini。我们的分析表明,LMs在运用新词根时,其性能急剧下降,因为形态复杂性增加。虽然模型可以更好地识别个人形态组合,但其表现缺乏系统性,导致与人类相比的显著准确性差距。","Mete Ismayilzada, Defne Circi, Jonne Sälevä, Hale Sirin, Abdullatif Köksal, Bhuwan Dhingra, Antoine Bosselut, Duygu Ataman, Lonneke van der Plas",2025-06-05T10:08:39Z,Evaluating Morphological Compositional Generalization in Large Language   Models,Bewertung der morphologischen Kompositionsverallgemeinerung in großen Sprachmodellen,评价大语言模式中人种构成的概括化,http://arxiv.org/abs/2410.12656v4
102,"In recent years, protein-text models have gained significant attention for their potential in protein generation and understanding. Current approaches focus on integrating protein-related knowledge into large language models through continued pretraining and multi-modal alignment, enabling simultaneous comprehension of textual descriptions and protein sequences. Through a thorough analysis of existing model architectures and text-based protein understanding benchmarks, we identify significant data leakage issues present in current benchmarks. Moreover, conventional metrics derived from natural language processing fail to accurately assess the model's performance in this domain. To address these limitations, we reorganize existing datasets and introduce a novel evaluation framework based on biological entities. Motivated by our observation, we propose a retrieval-enhanced method, which significantly outperforms fine-tuned LLMs for protein-to-text generation and shows accuracy and efficiency in training-free scenarios. Our code and data can be seen at https://github.com/IDEA-XL/RAPM.","近年来,蛋白质文本模型在蛋白质的产生和理解方面的潜力已受到极大关注。目前的方法侧重于通过继续培训前和多模式调整,将蛋白质相关知识纳入大型语言模型,从而能够同时理解文本描述和蛋白序列。通过对现有模型结构和基于文本的蛋白质理解基准的透彻分析,我们确定了当前基准中存在的重要数据渗漏问题。此外,来自自然语言处理的常规指标未能准确评估该模型在这一领域的绩效。为解决这些局限性,我们调整了现有的数据集,并引入了基于生物实体的新的评价框架。根据我们的观察,我们提出了一种检索强化的方法,该方法大大超越了蛋白质到文字生成的微调LLMS,并显示了培训无培训情景的准确度和效率。我们的代码和数据可以在https://github.com/IDA-XL/RAPM上查看。","Juntong Wu, Zijing Liu, He Cao, Hao Li, Bin Feng, Zishan Shu, Ke Yu, Li Yuan, Yu Li",2025-06-05T09:59:09Z,Rethinking Text-based Protein Understanding: Retrieval or LLM?,Rethinking Text-basierte Protein-Verständnis: Retrieval oder LLM?,重新思考基于文本的蛋白质理解:检索还是LLM?,http://arxiv.org/abs/2505.20354v2
103,"Large Reasoning Models (LRMs) extend large language models with explicit, multi-step reasoning traces to enhance transparency and performance on complex tasks. However, these reasoning traces can be redundant or logically inconsistent, making them a new source of hallucination that is difficult to detect. Existing hallucination detection methods focus primarily on answer-level uncertainty and often fail to detect hallucinations or logical inconsistencies arising from the model's reasoning trace. This oversight is particularly problematic for LRMs, where the explicit thinking trace is not only an important support to the model's decision-making process but also a key source of potential hallucination. To this end, we propose RACE (Reasoning and Answer Consistency Evaluation), a novel framework specifically tailored for hallucination detection in LRMs. RACE operates by extracting essential reasoning steps and computing four diagnostic signals: inter-sample consistency of reasoning traces, entropy-based answer uncertainty, semantic alignment between reasoning and answers, and internal coherence of reasoning. This joint analysis enables fine-grained hallucination detection even when the final answer appears correct. Experiments across datasets and different LLMs demonstrate that RACE outperforms existing hallucination detection baselines, offering a robust and generalizable solution for evaluating LRMs. Our code is available at: https://github.com/bebr2/RACE.","大型理性模型(LRMs)扩展了大型语言模型,有明确、多步的推理痕迹,以提高复杂任务的透明度和绩效;然而,这些推理痕迹可能是多余的或逻辑上不一致的,使得它们成为难以检测的新的幻觉来源;现有幻觉检测方法主要侧重于答案级不确定性,往往无法检测出因模型推理痕量而产生的幻觉或逻辑不一致;这种监督对LRMs来说特别成问题,因为清晰的思维跟踪不仅是模型决策过程的重要支持,而且也是潜在幻觉的关键来源。为此,我们提议RACE(重新认识和回答协调评价),这是专门为LRMs的幻觉检测而设计的一个新框架。RACE通过提取基本推理步骤和计算四种诊断信号来运作:推理痕迹之间的一致性、基于加密的回答不确定性、推理和答案之间的语义性调整以及内部推理的一致性。这种联合分析使得即使在最后答案看来正确时也能进行精细的幻觉检测。在数据集和不同的LLMSMSA中进行实验和不同的LMSBs 显示,RCegregregalex a s regreal lamental laction sal laps laction surs lactions lactions lactions","Changyue Wang, Weihang Su, Qingyao Ai, Yiqun Liu",2025-06-05T09:54:04Z,Joint Evaluation of Answer and Reasoning Consistency for Hallucination   Detection in Large Reasoning Models,Gemeinsame Bewertung der Antwort- und Begründungskonsistenz für Halluzinationserkennung in großen Vernunftmodellen,对在大理由模型中发现幻幻幻剂的答案和理由一致性的联合评价,http://arxiv.org/abs/2506.04832v1
104,"Healthcare systems face significant challenges in managing and interpreting vast, heterogeneous patient data for personalized care. Existing approaches often focus on narrow use cases with a limited feature space, overlooking the complex, longitudinal interactions needed for a holistic understanding of patient health. In this work, we propose a novel approach to patient pathway modeling by transforming diverse electronic health record (EHR) data into a structured representation and designing a holistic pathway prediction model, EHR2Path, optimized to predict future health trajectories. Further, we introduce a novel summary mechanism that embeds long-term temporal context into topic-specific summary tokens, improving performance over text-only models, while being much more token-efficient. EHR2Path demonstrates strong performance in both next time-step prediction and longitudinal simulation, outperforming competitive baselines. It enables detailed simulations of patient trajectories, inherently targeting diverse evaluation tasks, such as forecasting vital signs, lab test results, or length-of-stay, opening a path towards predictive and personalized healthcare.","在管理和解释广泛的、多种多样的病人个人化护理数据方面,保健系统面临重大挑战。现有办法往往侧重于有限特点空间的狭隘使用案例,忽视全面了解病人健康所需的复杂、纵向互动。在这项工作中,我们提出一种新的办法,通过将多种电子健康记录(EHR)数据转换成结构化的代表性数据和设计一个整体路径预测模型EHR2Path,优化以预测未来的健康轨迹。此外,我们引入了一个新的摘要机制,将长期时间背景嵌入特定专题摘要符号中,改进仅文本模型的性能,同时提高象征性效率。 EHR2Path展示了下一个时间步骤预测和纵向模拟的强效业绩,超过了竞争基线。它使得能够对病人轨道进行详细的模拟,其目标必然是不同的评价任务,例如预测生命迹象、实验室测试结果或停留时间长度,为预测和个人化保健开辟了一条道路。","Chantal Pellegrini, Ege Özsoy, David Bani-Harouni, Matthias Keicher, Nassir Navab",2025-06-05T09:54:01Z,From EHRs to Patient Pathways: Scalable Modeling of Longitudinal Health   Trajectories with LLMs,Von EHRs zu Patientenpfaden: Skalierbare Modellierung von langitudinalen Gesundheits-Trajektorien mit LLMs,从EHRs到患者途径:可扩展的长纵向健康轨迹模型与LLMs,http://arxiv.org/abs/2506.04831v1
105,"Large vision-language models (LVLMs) excel at multimodal understanding but suffer from high computational costs due to redundant vision tokens. Existing pruning methods typically rely on single-layer attention scores to rank and prune redundant visual tokens to solve this inefficiency. However, as the interaction between tokens and layers is complicated, this raises a basic question: Is such a simple single-layer criterion sufficient to identify redundancy? To answer this question, we rethink the emergence of redundant visual tokens from a fundamental perspective: information flow, which models the interaction between tokens and layers by capturing how information moves between tokens across layers. We find (1) the CLS token acts as an information relay, which can simplify the complicated flow analysis; (2) the redundancy emerges progressively and dynamically via layer-wise attention concentration; and (3) relying solely on attention scores from single layers can lead to contradictory redundancy identification. Based on this, we propose FlowCut, an information-flow-aware pruning framework, mitigating the insufficiency of the current criterion for identifying redundant tokens and better aligning with the model's inherent behaviors. Extensive experiments show that FlowCut achieves superior results, outperforming SoTA by 1.6% on LLaVA-1.5-7B with 88.9% token reduction, and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, delivering 3.2x speed-up in the prefilling stage. Our code is available at https://github.com/TungChintao/FlowCut","大型视觉语言模型(LVLMS)在多式理解方面非常出色,但由于多余的视觉标志,其计算成本很高。现有的裁剪方法通常依赖于单层关注分数,将多余的视觉符号排到排位和纯化,以解决这种低效率问题。然而,由于符号和层之间的相互作用复杂,这就提出了一个基本问题:这样一个简单的单层标准是否足以识别冗余?为了回答这个问题,我们从一个基本的角度重新思考多余的视觉标志的出现:信息流,它通过捕捉不同层次之间信息标记的移动方式来模拟象征物与层之间的互动。我们发现:(1) CLS标牌是一个信息中继器,可以简化复杂的流程分析;(2) 冗余通过分层集中逐渐和动态地出现;(3) 仅仅依靠单层的注意分数可能导致矛盾的裁断。基于这一点,我们提议了FllowCut, 信息流-aware 调试框架, 缓解当前标准在确定冗余符号和更好地适应模型的固有行为。 广泛的实验显示FlickC- sulue-real-del-xxxxxxxxxxxxxxxxx","Jintao Tong, Wenwei Jin, Pengda Qin, Anqi Li, Yixiong Zou, Yuhong Li, Yuhua Li, Ruixuan Li",2025-06-05T09:50:13Z,FlowCut: Rethinking Redundancy via Information Flow for Efficient   Vision-Language Models,FlowCut: Redundanz über Informationsfluss für effiziente Vision-Sprachenmodelle neu denken,流程:通过信息流动重新思考通过信息流动实现高效愿景-语言模型的冗余,http://arxiv.org/abs/2505.19536v2
106,"Cryptic crossword clues are challenging language tasks for which new test sets are released daily by major newspapers on a global basis. Each cryptic clue contains both the definition of the answer to be placed in the crossword grid (in common with regular crosswords), and 'wordplay' that proves that the answer is correct (i.e. a human solver can be confident that an answer is correct without needing crossing words as confirmation). This work describes an LLM-based reasoning system built from open-licensed components that solves cryptic clues by (i) hypothesising answers; (ii) proposing wordplay explanations; and (iii) using a verifier system that operates on codified reasoning steps. Overall, this system establishes a new state-of-the-art performance on the challenging Cryptonite dataset of clues from The Times and The Telegraph newspapers in the UK. Because each proved solution is expressed in Python, interpretable wordplay reasoning for proven answers is available for inspection.","加密交叉字串线索具有挑战性的语言任务,主要报纸每天在全球发布新的测试集。 每一个加密线索都包含要放在填字格中答案的定义( 与常规填字字组相同 ) , 以及证明答案正确的“ 字节 ” ( 即人类解答者可以相信答案是正确的, 而不需要交叉词来确认 ) 。 这项工作描述了一个基于LLM 的推理系统, 该系统由开放许可的组件建立, 通过( 一) 假设答案解决加密线索;(二) 提出文字剧解释; 以及 (三) 使用一个根据编纂的推理步骤操作的校验系统。 总的来说, 该系统在具有挑战性的《 时报》 和英国电报报纸 线索的Cryptonite数据集上建立了新的状态性表现。 因为每个被证实的解决方案都用Python 来表达, 可以解释的经验证的答案的文字推理。","Martin Andrews, Sam Witteveen",2025-06-05T09:43:28Z,A Reasoning-Based Approach to Cryptic Crossword Clue Solving,Ein vernünftiger Ansatz zur kryptischen Kreuzworträtsellösung,以基于理性的方法解决加密口号的谜题,http://arxiv.org/abs/2506.04824v1
107,"Although vision-language and large language models (VLM and LLM) offer promising opportunities for AI-driven educational assessment, their effectiveness in real-world classroom settings, particularly in underrepresented educational contexts, remains underexplored. In this study, we evaluated the performance of a state-of-the-art VLM and several LLMs on 646 handwritten exam responses from grade 4 students in six Indonesian schools, covering two subjects: Mathematics and English. These sheets contain more than 14K student answers that span multiple choice, short answer, and essay questions. Assessment tasks include grading these responses and generating personalized feedback. Our findings show that the VLM often struggles to accurately recognize student handwriting, leading to error propagation in downstream LLM grading. Nevertheless, LLM-generated feedback retains some utility, even when derived from imperfect input, although limitations in personalization and contextual relevance persist.","虽然视觉语言和大型语言模式(VLM和LLM)为AI驱动的教育评估提供了充满希望的机会,但它们在现实世界课堂环境中的有效性,特别是在代表性不足的教育环境中的有效性仍未得到充分探讨,在这项研究中,我们评估了印度尼西亚六所学校四年级学生在646个手写考试答复中最先进的VLM和若干LM的成绩,涉及两个科目:数学和英语,这些教材包含超过14K的学生回答,涉及多种选择、简短回答和论文问题。评估任务包括对这些答复进行分级并产生个性化反馈。我们的调查结果显示,VLM常常努力准确地识别学生笔迹,导致下游LM等级的错误传播。然而,LLM产生的反馈保留了一些效用,即使来自不完善的投入,尽管个性化和背景关联性方面仍然存在限制。","Nurul Aisyah, Muhammad Dehan Al Kautsar, Arif Hidayat, Raqib Chowdhury, Fajri Koto",2025-06-05T09:41:09Z,Evaluating Vision-Language and Large Language Models for Automated   Student Assessment in Indonesian Classrooms,Bewertung von Vision-Sprachen und großen Sprachmodellen für die automatisierte Beurteilung von Studierenden in indonesischen Klassenräumen,评价印度尼西亚教室学生自动评估的视觉语言和大语言模式,http://arxiv.org/abs/2506.04822v1
108,"Since automatic translations can contain errors that require substantial human post-editing, machine translation proofreading is essential for improving quality. This paper proposes a novel hybrid approach for robust proofreading that combines convolutional neural networks (CNN) with Bidirectional Encoder Representations from Transformers (BERT). In order to extract semantic information from phrases and expressions, CNN uses a variety of convolution kernel filters to capture local n-gram patterns. In the meanwhile, BERT creates context-rich representations of whole sequences by utilizing stacked bidirectional transformer encoders. Using BERT's attention processes, the integrated error detection component relates tokens to spot translation irregularities including word order problems and omissions. The correction module then uses parallel English-German alignment and GRU decoder models in conjunction with translation memory to propose logical modifications that maintain original meaning. A unified end-to-end training process optimized for post-editing performance is applied to the whole pipeline. The multi-domain collection of WMT and the conversational dialogues of Open-Subtitles are two of the English-German parallel corpora used to train the model. Multiple loss functions supervise detection and correction capabilities. Experiments attain a 90% accuracy, 89.37% F1, and 16.24% MSE, exceeding recent proofreading techniques by over 10% overall. Comparative benchmarking demonstrates state-of-the-art performance in identifying and coherently rectifying mistranslations and omissions.","由于自动翻译可能包含需要大量人编辑后编辑的错误, 机器翻译校对对于提高质量至关重要 。 本文提出一种新型的稳健校对混合方法, 将进化神经网络( CNN) 和来自变换器的双向电解码演示( BERT) 结合起来。 为了从语句和表达式中提取语义信息, CNN 使用各种变动内核过滤器来捕捉本地 n 克模式。 与此同时, BERT 利用堆叠双向变压器编码器创建了内容丰富的整个序列的校正比例。 使用 BERT 的注意程序, 综合误差检测组件将符号与点翻译违规行为( 包括字序问题和疏漏) 相结合。 校正模块使用平行的英语- 德语校对和 GRURU 解调模型, 以提出符合原始含义的逻辑修改。 整个编审后表现最优化的端对端对端培训程序将适用于整个管道。 通过多处收集 WMTT 和 Opal- 字幕的对口对等对话, 在英语- ASy imal imal imal imal lavial laveil prilling train press press 校验中, 16 校验中, 校正 16 校正 校正 16 校正 校正 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校对 校","Feijun Liu, Huifeng Wang, Kun Wang, Yizhen Wang",2025-06-05T09:34:42Z,Design of intelligent proofreading system for English translation based   on CNN and BERT,Entwurf eines intelligenten Korrekturlesesystems für die englische Übersetzung basierend auf CNN und BERT,设计基于CNN和BERT的英译文智能校对系统,http://arxiv.org/abs/2506.04811v1
109,"Logical reasoning is a core capability for many applications of large language models (LLMs), yet existing benchmarks often rely solely on final-answer accuracy, failing to capture the quality and structure of the reasoning process. We propose FineLogic, a fine-grained evaluation framework that assesses logical reasoning across three dimensions: overall benchmark accuracy, stepwise soundness, and representation-level alignment. In addition, to better understand how reasoning capabilities emerge, we conduct a comprehensive study on the effects of supervision format during fine-tuning. We construct four supervision styles (one natural language and three symbolic variants) and train LLMs under each. Our findings reveal that natural language supervision yields strong generalization even on out-of-distribution and long-context tasks, while symbolic reasoning styles promote more structurally sound and atomic inference chains. Further, our representation-level probing shows that fine-tuning primarily improves reasoning behaviors through step-by-step generation, rather than enhancing shortcut prediction or internalized correctness. Together, our framework and analysis provide a more rigorous and interpretable lens for evaluating and improving logical reasoning in LLMs.","逻辑推理是许多应用大语言模型(LLMs)的核心能力,但现有基准往往完全依赖最后答案的准确性,无法反映推理过程的质量和结构。我们提议FinalLogic,这是一个精细的评价框架,评估逻辑推理,分三个方面:总体基准准确性、逐步完善性以及代表层次的一致。此外,为了更好地了解推理能力如何出现,我们还在微调过程中对监督格式的影响进行全面研究。我们构建了四种监督风格(一种自然语言和三种象征性变异物),并在每个方面对LMs进行培训。我们的研究结果表明,自然语言监督甚至对分配和长文本任务都产生了强烈的概括性,而象征性推理风格则促进了结构上更健全和原子上的推理链。此外,我们的代表性水平的推理表明,微调主要是通过逐步生成改进推理行为,而不是加强快捷的预测或内部化的正确性。我们的框架和分析为LMS的逻辑推理提供了更为严格和可解释的透视镜。","Yujun Zhou, Jiayi Ye, Zipeng Ling, Yufei Han, Yue Huang, Haomin Zhuang, Zhenwen Liang, Kehan Guo, Taicheng Guo, Xiangqi Wang, Xiangliang Zhang",2025-06-05T09:34:12Z,Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and   Supervision Study,Dissecting Logical Reasoning in LLMs: Eine feinkörnige Bewertungs- und Aufsichtsstudie,在LLMM中解剖逻辑理由:精细评价和监督研究,http://arxiv.org/abs/2506.04810v1
110,"Automatic Speech Recognition (ASR) error correction aims to correct recognition errors while preserving accurate text. Although traditional approaches demonstrate moderate effectiveness, LLMs offer a paradigm that eliminates the need for training and labeled data. However, directly using LLMs will encounter hallucinations problem, which may lead to the modification of the correct text. To address this problem, we propose the Reliable LLM Correction Framework (RLLM-CF), which consists of three stages: (1) error pre-detection, (2) chain-of-thought sub-tasks iterative correction, and (3) reasoning process verification. The advantage of our method is that it does not require additional information or fine-tuning of the model, and ensures the correctness of the LLM correction under multi-pass programming. Experiments on AISHELL-1, AISHELL-2, and Librispeech show that the GPT-4o model enhanced by our framework achieves 21%, 11%, 9%, and 11.4% relative reductions in CER/WER.","自动语音识别(ASR)错误纠正的目的是纠正识别错误,同时保留准确的文本。虽然传统方法显示出中等效果,但LLMs提供了一种范例,消除了培训和标签数据的需求。然而,直接使用LLMs将遇到幻觉问题,可能导致修改正确文本。为解决这一问题,我们提议了可靠的LLM校正框架,该框架由三个阶段组成:(1) 检测前的错误,(2) 思考中的子任务链迭代校正,(3) 推理过程核查。我们方法的优点是,它不需要额外的信息或对模型进行微调,并确保多路程序下LLM校正的正确性。关于ASHELL-1、AHISELL-2和Librispeech的实验表明,通过我们框架强化的GPT-4o模型在CER/WER中实现了21%、11%、9%和11.4%的相对削减。","Yangui Fang, Baixu Cheng, Jing Peng, Xu Li, Yu Xi, Chengwei Zhang, Guohui Zhong",2025-06-05T09:26:39Z,"Fewer Hallucinations, More Verification: A Three-Stage LLM-Based   Framework for ASR Error Correction","Weniger Halluzinationen, mehr Verifizierung: Ein dreistufiges LLM-basiertes Framework für die ASR-Fehlerkorrektur","较少的幻觉,更多的核查:基于ASR的三级LLM框架错误更正",http://arxiv.org/abs/2505.24347v2
111,"The rapid progress of Multimodal Large Language Models(MLLMs) has transformed the AI landscape. These models combine pre-trained LLMs with various modality encoders. This integration requires a systematic understanding of how different modalities connect to the language backbone. Our survey presents an LLM-centric analysis of current approaches. We examine methods for transforming and aligning diverse modal inputs into the language embedding space. This addresses a significant gap in existing literature. We propose a classification framework for MLLMs based on three key dimensions. First, we examine architectural strategies for modality integration. This includes both the specific integration mechanisms and the fusion level. Second, we categorize representation learning techniques as either joint or coordinate representations. Third, we analyze training paradigms, including training strategies and objective functions. By examining 125 MLLMs developed between 2021 and 2025, we identify emerging patterns in the field. Our taxonomy provides researchers with a structured overview of current integration techniques. These insights aim to guide the development of more robust multimodal integration strategies for future models built on pre-trained foundations.","多式大语言模型(MLLM)的快速进展改变了AI景观。这些模型将预先培训的LLMS与各种模式编码器结合起来。这种整合要求系统理解不同模式与语言主干线的联系。我们的调查对当前方法进行了以LLM为中心的分析。我们研究了将多种模式投入转换和调整到语言嵌入空间的方法。这解决了现有文献中的重大差距。我们基于三个关键方面为MLLMS提出了一个分类框架。首先,我们研究了模式整合的建筑战略。这包括具体的整合机制和融合水平。第二,我们将代表性学习技术分为联合或协调代表。第三,我们分析了培训模式,包括培训战略和客观功能。我们研究了2021年至2025年期间开发的125 MLLLMs,确定了外地的新模式。我们分类为研究人员提供了当前整合技术的结构化概览。这些洞察旨在指导为在培训前基础上建立的未来模型制定更稳健的多式联运战略。","Jisu An, Junseok Lee, Jeoungeun Lee, Yongseok Son",2025-06-05T09:14:41Z,Towards LLM-Centric Multimodal Fusion: A Survey on Integration   Strategies and Techniques,Auf dem Weg zur multimodalen Fusion LLM-Centric: Eine Studie über Integrationsstrategien und -techniken,走向LLM-Centric Multimods 融合:关于一体化战略和技术的调查,http://arxiv.org/abs/2506.04788v1
112,"Speech inherently contains rich acoustic information that extends far beyond the textual language. In real-world spoken language understanding, effective interpretation often requires integrating semantic meaning (e.g., content), paralinguistic features (e.g., emotions, speed, pitch) and phonological characteristics (e.g., prosody, intonation, rhythm), which are embedded in speech. While recent multimodal Speech Large Language Models (SpeechLLMs) have demonstrated remarkable capabilities in processing audio information, their ability to perform fine-grained perception and complex reasoning in natural speech remains largely unexplored. To address this gap, we introduce MMSU, a comprehensive benchmark designed specifically for understanding and reasoning in spoken language. MMSU comprises 5,000 meticulously curated audio-question-answer triplets across 47 distinct tasks. To ground our benchmark in linguistic theory, we systematically incorporate a wide range of linguistic phenomena, including phonetics, prosody, rhetoric, syntactics, semantics, and paralinguistics. Through a rigorous evaluation of 14 advanced SpeechLLMs, we identify substantial room for improvement in existing models, highlighting meaningful directions for future optimization. MMSU establishes a new standard for comprehensive assessment of spoken language understanding, providing valuable insights for developing more sophisticated human-AI speech interaction systems. MMSU benchmark is available at https://huggingface.co/datasets/ddwang2000/MMSU. Evaluation Code is available at https://github.com/dingdongwang/MMSU_Bench.","在现实世界口语理解中,有效解释往往需要结合语义含义(例如内容)、语言特征(例如情绪、速度、音调)和声学特征(例如流传、内交、节奏),这些特征都包含在语言中。虽然最近的多式语言大语言模型(SpeechLLLMS)在处理音频信息方面表现出了非凡的能力,但是它们在自然言语中进行精细感知和复杂推理的能力仍然在很大程度上没有得到解析。为了弥补这一差距,我们引入了MMSU,这是专门为口语理解和推理而设计的一个全面基准。MMSU包括了5 000个精心拼凑的口头问题答案,跨过47项不同的任务。为了以语言理论作为我们的基准,我们系统地纳入了广泛的语言现象,包括音调、流言、口语、语学、语调、语言流学、语言流传、语言流传、语言流传、语言流传和语言流传。通过对14个高级言语言语道/语言流传学的严格评估,我们确定改进现有模式中的重大改进空间空间空间空间空间,我们在现有模型中要改进空间-了解中,在现有模型中确定现有甚高调标准标准上现有语言/MIM标准,为未来评估提供有意义的理解标准。","Dingdong Wang, Jincenzi Wu, Junan Li, Dongchao Yang, Xueyuan Chen, Tianhua Zhang, Helen Meng",2025-06-05T09:09:36Z,MMSU: A Massive Multi-task Spoken Language Understanding and Reasoning   Benchmark,MMSU: Ein massiver Multi-Task Spoken Language Understanding Benchmark,MMSU: 大规模多任务口头语言理解和说明理由基准,http://arxiv.org/abs/2506.04779v1
113,"Studies of LLMs' political opinions mainly rely on evaluations of their open-ended responses. Recent work indicates that there is a misalignment between LLMs' responses and their internal intentions. This motivates us to probe LLMs' internal mechanisms and help uncover their internal political states. Additionally, we found that the analysis of LLMs' political opinions often relies on single-axis concepts, which can lead to concept confounds. In this work, we extend the single-axis to multi-dimensions and apply interpretable representation engineering techniques for more transparent LLM political concept learning. Specifically, we designed a four-dimensional political learning framework and constructed a corresponding dataset for fine-grained political concept vector learning. These vectors can be used to detect and intervene in LLM internals. Experiments are conducted on eight open-source LLMs with three representation engineering techniques. Results show these vectors can disentangle political concept confounds. Detection tasks validate the semantic meaning of the vectors and show good generalization and robustness in OOD settings. Intervention Experiments show these vectors can intervene in LLMs to generate responses with different political leanings.","对LLMs政治观点的研究主要依靠对其开放式反应的评价。最近的工作表明,LLMs的反应与其内部意图之间有不协调之处。这促使我们探究LMs的内部机制,并帮助发现其内部政治状态。此外,我们发现,LLMs政治观点的分析往往依赖单一轴概念,这可能导致概念混淆。在这项工作中,我们将单一轴扩展至多种分层,并应用可解释的代表性工程技术,以便更透明的LMM政治概念学习。具体地说,我们设计了一个四维政治学习框架,并为精细区分的政治概念矢量学习建立了相应的数据集。这些矢量可用于探测和干预LLMM的内部。用三种代表工程技术对8个开源的LMs进行了实验。结果显示这些矢量可以解开政治概念的纠结。检测任务证实矢量的语义,并显示OD环境中的良好概括性和坚固性。干预实验显示,LMMs中的这些矢量可以在不同的政治倾斜度中进行干预。","Jingyu Hu, Mengyue Yang, Mengnan Du, Weiru Liu",2025-06-05T09:06:59Z,Fine-Grained Interpretation of Political Opinions in Large Language   Models,Feinkörnige Interpretation politischer Meinungen in großen Sprachmodellen,以大语言模式对政治意见的精细解释,http://arxiv.org/abs/2506.04774v1
114,"Evaluating text revision in scientific writing remains a challenge, as traditional metrics such as ROUGE and BERTScore primarily focus on similarity rather than capturing meaningful improvements. In this work, we analyse and identify the limitations of these metrics and explore alternative evaluation methods that better align with human judgments. We first conduct a manual annotation study to assess the quality of different revisions. Then, we investigate reference-free evaluation metrics from related NLP domains. Additionally, we examine LLM-as-a-judge approaches, analysing their ability to assess revisions with and without a gold reference. Our results show that LLMs effectively assess instruction-following but struggle with correctness, while domain-specific metrics provide complementary insights. We find that a hybrid approach combining LLM-as-a-judge evaluation and task-specific metrics offers the most reliable assessment of revision quality.","在这项工作中,我们分析和确定这些指标的局限性,并探索更符合人类判断的替代评价方法;我们首先进行人工说明研究,以评估不同修订的质量;然后,我们调查相关国家实验室领域的无参考评价指标;此外,我们审查LLM-as-a-judge性方法,分析它们评估修订的能力,同时不参考黄金;我们的结果显示LLM有效评估教学的遵循情况,但以正确的方式挣扎,而具体领域指标则提供补充性见解;我们发现,将LLM-as-a-a-judge性评价和具体任务指标相结合的混合方法,提供了对修订质量的最可靠的评估。","Léane Jourdan, Florian Boudin, Richard Dufour, Nicolas Hernandez",2025-06-05T09:00:23Z,Identifying Reliable Evaluation Metrics for Scientific Text Revision,Identifizieren von verlässlichen Bewertungsmetrics für wissenschaftliche Textrevision,科学文本订正的可靠评价计量指标,http://arxiv.org/abs/2506.04772v1
115,"Distilling reasoning paths from teacher to student models via supervised fine-tuning (SFT) provides a shortcut for improving the reasoning ability of smaller Large Language Models (LLMs). However, the reasoning paths generated by teacher models often reflect only surface-level traces of their underlying authentic reasoning. Insights from cognitive neuroscience suggest that authentic reasoning involves a complex interweaving between meta-reasoning (which selects appropriate sub-problems from multiple candidates) and solving (which addresses the sub-problem). This implies authentic reasoning has an implicit multi-branch structure. Supervised fine-tuning collapses this rich structure into a flat sequence of token prediction in the teacher's reasoning path, preventing effective distillation of this structure to students. To address this limitation, we propose RLKD, a reinforcement learning (RL)-based distillation framework guided by a novel Generative Structure Reward Model (GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving steps and computes rewards to measure structural alignment between student and teacher reasoning. RLKD combines this reward with RL, enabling student LLMs to internalize the teacher's implicit multi-branch reasoning structure rather than merely mimicking fixed output paths. Experiments show RLKD surpasses standard SFT-RL pipelines even when trained on 0.1% of data under an RL-only regime, unlocking greater student reasoning potential than SFT-based distillation.","通过监管的微调(SFT)从教师到学生模型的蒸馏推理路径通过监管的微调(SFT)为改进小型大语言模型(LLMS)的推理能力提供了一个捷径。然而,教师模型产生的推理路径往往只反映其基本真实推理的表面痕迹。认知神经科学的透视表明,真实推理过程涉及在元推理(从多个候选人中选择适当的子问题)和解决(解决子问题)之间复杂的交织。这意味着真实推理有一个隐含的多处结构。监督微调将这一丰富的结构破碎成教师推理路径中象征性预测的一个平坦的序列,防止向学生有效淡化这一结构。为了解决这一局限性,我们建议RLKD,一个基于强化学习(RL)的提炼框架,由创新结构(从多个候选人中选择适当的子问题)和解析(解决子问题)和解决路径转换成多个基于元理的多处解决步骤,并计算奖赏测量师与师的推理的结构性调整。甚至RKD在教师的不断的RBR-RR(BA-L)中将这一奖励与隐定的S-RLL的S-R-R-R-R-R-L)的升级结构推理学中,使学生的内输出显示一个基础的S-R-R-RML的升级的内输出系统显示的升级的内制,使S-RMMMM-L的系统显示一个基础的升级为内部数据。","Shicheng Xu, Liang Pang, Yunchang Zhu, Jia Gu, Zihao Wei, Jingcheng Deng, Feiyang Pan, Huawei Shen, Xueqi Cheng",2025-06-05T08:57:42Z,Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via   Reinforcement Learning,Destillieren der impliziten Multi-Branch-Struktur in LLMs' Reasoning durch Verstärkungslernen,"通过强化学习,将LLMs的隐含多层结构提炼在“通过强化学习推理”中",http://arxiv.org/abs/2505.16142v2
116,"Large language models (LLMs)-based query expansion for information retrieval augments queries with generated hypothetical documents with LLMs. However, its performance relies heavily on the scale of the language models (LMs), necessitating larger, more advanced LLMs. This approach is costly, computationally intensive, and often has limited accessibility. To address these limitations, we introduce GOLFer - Smaller LMs-Generated Documents Hallucination Filter & Combiner - a novel method leveraging smaller open-source LMs for query expansion. GOLFer comprises two modules: a hallucination filter and a documents combiner. The former detects and removes non-factual and inconsistent sentences in generated documents, a common issue with smaller LMs, while the latter combines the filtered content with the query using a weight vector to balance their influence. We evaluate GOLFer alongside dominant LLM-based query expansion methods on three web search and ten low-resource datasets. Experimental results demonstrate that GOLFer consistently outperforms other methods using smaller LMs, and maintains competitive performance against methods using large-size LLMs, demonstrating its effectiveness.","在信息检索方面,基于大语言模型(LLMs)的查询扩展使与LLMs一起生成的假设文件的查询增加。然而,其性能在很大程度上依赖于语言模型的规模(LMs),这需要更大、更先进的LMs。这一方法成本高,计算密集,而且往往限制获取。为解决这些局限性,我们引入了GOLFer - 小LMs-Generaled Document Hallucilation Flucle and Guperation(LMs),这是利用较小的开放源LMs扩大查询的新方法。GOLFer由两个模块组成:幻觉过滤器和文件组合器。前者探测和删除生成文件中的非事实和不一致的句子,这是与较小的LMs(LMs)的一个共同问题,而后者则将过滤的内容与查询结合起来,使用重量矢量矢量来平衡其影响。我们评估GOLFer(GOLFer)在三个网络搜索和10个低资源数据集上的主要LM(LMs)查询扩展方法。实验结果表明,GOLFer 一直优于使用较小的LMs(LMs)与其他方法,并保持与使用较小型LM的其他方法相近于其他方法,并保持与使用大型LM方法的竞争性性性能,展示其效力。","Lingyuan Liu, Mengxiang Zhang",2025-06-05T08:45:48Z,GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner   for Query Expansion in Information Retrieval,GOLFer: Kleinere LM-generierte Dokumente Halluzination Filter & Combiner zur Abfrageerweiterung in der Informationsaufzeichnung,GOLFer: 用于信息检索中查询扩展的小型 LM - 光生文件 HALL 过滤器和组合器,http://arxiv.org/abs/2506.04762v1
117,"Large Language Models (LLMs) have shown potential in generating hypothetical documents for query expansion, thereby enhancing information retrieval performance. However, the efficacy of this method is highly dependent on the quality of the generated documents, which often requires complex prompt strategies and the integration of advanced dense retrieval techniques. This can be both costly and computationally intensive. To mitigate these limitations, we explore the use of zero-shot LLM-based query expansion to improve sparse retrieval, particularly for learned sparse retrievers. We introduce a novel fusion ranking framework, Exp4Fuse, which enhances the performance of sparse retrievers through an indirect application of zero-shot LLM-based query expansion. Exp4Fuse operates by simultaneously considering two retrieval routes-one based on the original query and the other on the LLM-augmented query. It then generates two ranked lists using a sparse retriever and fuses them using a modified reciprocal rank fusion method. We conduct extensive evaluations of Exp4Fuse against leading LLM-based query expansion methods and advanced retrieval techniques on three MS MARCO-related datasets and seven low-resource datasets. Experimental results reveal that Exp4Fuse not only surpasses existing LLM-based query expansion methods in enhancing sparse retrievers but also, when combined with advanced sparse retrievers, achieves SOTA results on several benchmarks. This highlights the superior performance and effectiveness of Exp4Fuse in improving query expansion for sparse retrieval.","大型语言模型(LLMS)在为扩大查询提供假设文件方面显示出了潜力,从而提高了信息检索性能;然而,这一方法的功效在很大程度上取决于生成文件的质量,这往往需要复杂的迅速战略和先进的密集检索技术的集成,这往往需要复杂的迅速战略和先进的密集检索技术的集成,费用高,而且计算密集。为了减轻这些限制,我们探索使用零射LLM为基础的查询扩大,以改进稀薄的检索,特别是对于知识稀少的检索者来说尤其如此。我们引入了一个新型的聚合排序框架(Exp4Fuse),通过间接应用基于零射LMM的查询扩展来提高稀少的检索者的业绩。Exp4F使用两种基于原始查询的检索路径之一,同时考虑基于LLM系统原始查询的检索技术;然后使用稀少的检索器制作两份排名清单,用修改的相互级融合方法来连接它们。我们广泛评价Exp4F利用主要LMM的查询扩展方法和高级检索技术,通过三个基于MS MARM的数据集和七个低资源数据集的扩展数据集来提高微缩检索技术。 实验性结果显示,在SOM4F的检索中改进了一些的先进分析结果。","Lingyuan Liu, Mengxiang Zhang",2025-06-05T08:44:34Z,Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using   Large Language Model-based Query Expansion,Exp4Fuse: Ein Rank-Fusion-Framework für verbesserte Sparse-Retrieval unter Verwendung einer großsprachigen modellbasierten Abfrageerweiterung,Exp4Fuse:使用基于大语言模型的查询扩展增强分散检索的排名组合框架,http://arxiv.org/abs/2506.04760v1
118,"Large Language Models (LLMs) have shown remarkable capabilities across various tasks, but their deployment in high-stake domains requires consistent and coherent behavior across multiple rounds of user interaction. This paper introduces a comprehensive framework for evaluating and improving LLM response consistency, making three key contributions. Code and data are available at: https://github.com/yubol-bobo/MT-Consistency. First, we introduce Position-Weighted Consistency (PWC), a metric designed to capture both the importance of early-stage stability and recovery patterns in multi-turn interactions. Second, we present MT-Consistency, a carefully curated benchmark dataset spanning diverse domains and difficulty levels, specifically designed to evaluate LLM consistency under various challenging follow-up scenarios. Third, we introduce Confidence-Aware Response Generation (CARG), a framework that significantly improves response stability by explicitly integrating internal model confidence scores during the generation process. Experimental results demonstrate that CARG significantly improves response stability without sacrificing accuracy, offering a practical path toward more dependable LLM behavior in critical, real-world deployments.","大型语言模型(LLMS)在各种任务中表现出非凡的能力,但在高占用域的部署需要多轮用户互动的一致和一致行为,本文件为评价和改进LLM反应一致性提出了全面框架,提出了三个主要贡献。代码和数据见https://github.com/yupol-bobo/MT-Consistence。首先,我们引入了定位-Weightd Consistence(PWC),该指标旨在反映早期稳定和恢复模式在多转式互动中的重要性。第二,我们介绍MT-Consity,这是一个仔细整理的基准数据集,涵盖不同的领域和困难级别,专门用来评估LLMM在各种具有挑战性的后续情景下的一致性。第三,我们引入了信任-软件反应生成(CARG),这是一个框架,通过在生成过程中明确整合内部模型信任分数,大大提高了响应稳定性。实验结果表明,CARG在不牺牲准确性的情况下大大改进了反应稳定性,为关键、现实部署中更可靠的LM行为提供了切实可行的途径。","Yubo Li, Yidi Miao, Xueying Ding, Ramayya Krishnan, Rema Padman",2025-06-05T08:39:20Z,Firm or Fickle? Evaluating Large Language Models Consistency in   Sequential Interactions,Firm oder Fickle? Bewertung großer Sprachmodelle Konsistenz in sequenziellen Interaktionen,公司或Fickle?评估大语言模型在序列相互作用中的一致性,http://arxiv.org/abs/2503.22353v4
119,"Despite the success of Instruction Tuning (IT) in training large language models (LLMs), such models often leverage spurious or biased features learnt from their training data and can become misaligned, leading to undesired behaviours. While existing techniques can steer model behaviour at inference-time, they are often post-hoc and do not embed steering as an intrinsic model feature. In this work, we introduce Focus Instruction Tuning (FIT), which trains LLMs to condition their responses by focusing on specific features whilst ignoring others, leading to different behaviours based on what features are specified. Across diverse benchmarks, we demonstrate that FIT: (i) successfully steers behaviour at inference time; (ii) increases robustness by amplifying core task signals and down-weighting spurious cues; (iii) mitigates social bias by suppressing demographic attributes; and (iv) generalises under distribution shifts and to previously unseen focus features. FIT therefore offers a lightweight, intrinsic mechanism for building more robust, fair, and easily controllable LLMs.","尽管在培训大型语言模式(LLMS)方面教学指导成功,但这类模式往往利用从培训数据中汲取的虚假或偏差特征,并可能导致不理想的行为;虽然现有技术可以在推论时指导示范行为,但往往是超常的,不作为内在模式特征嵌入指导;在这项工作中,我们引入Focus指导(FIT),通过注重具体特征而忽视其他特征来训练LMS的应对条件,导致基于特定特征的不同行为;在不同的基准中,我们证明FIT:(一) 成功地引导在推论时的行为;(二) 通过扩大核心任务信号和降低重量的虚假提示来增强稳健性;(三) 通过抑制人口属性来减少社会偏见;(四) 控制分布变化中的通则和以往看不见的重点特征。因此,FIT为建设更强大、公平和易于控制的LMS提供了轻量、内在机制。","Tom A. Lamb, Adam Davies, Alasdair Paren, Philip H. S. Torr, Francesco Pinto",2025-06-05T08:25:12Z,"Focus On This, Not That! Steering LLMs with Adaptive Feature   Specification","Fokus auf diese, nicht das! Lenkung LLMs mit adaptiven Feature Specification","聚焦于此, 不是那个! 指导性带适应性特征规格的LLMS",http://arxiv.org/abs/2410.22944v4
120,"In-context learning (ICL) is a crucial capability of current large language models (LLMs), where the selection of examples plays a key role in performance. While most existing approaches focus on selecting the most similar examples to the query, the impact of diversity in example selection remains underexplored. We systematically investigate the role of diversity in in-context example selection through experiments across a range of tasks, from sentiment classification to more challenging math and code problems. Experiments on Llama-3.1, Gemma-2, and Mistral-v0.3 families of models show that diversity-aware selection methods improve performance, particularly on complex tasks like math and code, and enhance robustness to out-of-distribution queries. To support these findings, we introduce a theoretical framework that explains the benefits of incorporating diversity in in-context example selection.","内文学习(ICL)是目前大型语言模式(LLM)的关键能力,在这种模式中,选择实例在业绩中起着关键作用。虽然大多数现有方法侧重于选择与查询最相似的例子,但多样性在样板选择中的影响仍未得到充分探讨。我们通过从情绪分类到更具挑战性的数学和代码问题等一系列任务的实验,系统地调查内文实例选择中的多样性的作用。Llama-3.1、Gemma-2和Mistral-v0.3模型系列的实验表明,多样性选择方法提高了业绩,特别是在数学和代码等复杂任务上,并提高了分配外查询的可靠性。为了支持这些研究结果,我们引入了一个理论框架,解释将多样性纳入文文本示例选择的好处。","Wenyang Xiao, Haoyu Zhao, Lingxiao Huang",2025-06-05T08:20:31Z,The Role of Diversity in In-Context Learning for Large Language Models,Die Rolle der Vielfalt im In-Context-Lernen für große Sprachmodelle,多样性在为大语言模式进行内文学习方面的作用,http://arxiv.org/abs/2505.19426v2
121,"The widespread dissemination of fake news on social media has significantly impacted society, resulting in serious consequences. Conventional deep learning methodologies employing small language models (SLMs) suffer from extensive supervised training requirements and difficulties adapting to evolving news environments due to data scarcity and distribution shifts. Large language models (LLMs), despite robust zero-shot capabilities, fall short in accurately detecting fake news owing to outdated knowledge and the absence of suitable demonstrations. In this paper, we propose a novel Continuous Collaborative Emergent Fake News Detection (C$^2$EFND) framework to address these challenges. The C$^2$EFND framework strategically leverages both LLMs' generalization power and SLMs' classification expertise via a multi-round collaborative learning framework. We further introduce a lifelong knowledge editing module based on a Mixture-of-Experts architecture to incrementally update LLMs and a replay-based continue learning method to ensure SLMs retain prior knowledge without retraining entirely. Extensive experiments on Pheme and Twitter16 datasets demonstrate that C$^2$EFND significantly outperforms existed methods, effectively improving detection accuracy and adaptability in continuous emergent fake news scenarios.","使用小型语言模式的常规深层次学习方法由于数据稀缺和分布变化而面临广泛的监管培训要求和困难,难以适应不断变化的新闻环境。大型语言模式尽管具有强健的零射能力,但由于知识过时和缺乏适当的演示,无法准确检测假消息。在本文件中,我们提出一个新的新型合作新兴假消息探测(C$2$EFND)框架,以应对这些挑战。C$2$EFND框架战略性地利用了LLMS的普及能力和可持续土地管理分类专门知识,通过一个多层次合作学习框架。我们还引入了一个基于Mixture-Explants结构的终身知识编辑模块,以逐步更新LLMS和基于再玩耍的持续学习方法,确保可持续土地管理保留先前的知识而无需再培训。关于Pheme和Twitter16数据集的广泛实验表明,C$2$EFND在战略上大大超越了外形,有效地改进了持续突发假新闻情景中的检测准确性和适应性。","Ziyi Zhou, Xiaoming Zhang, Litian Zhang, Yibo Zhang, Zhenyu Guan, Chaozhuo Li, Philip S. Yu",2025-06-05T08:17:55Z,Lifelong Evolution: Collaborative Learning between Large and Small   Language Models for Continuous Emergent Fake News Detection,Lebenslange Evolution: Kollaboratives Lernen zwischen großen und kleinen Sprachmodellen für kontinuierliche emergente Fake News Detection,"终身演进:大型和小型语文模式合作学习,以不断发现假冒假消息",http://arxiv.org/abs/2506.04739v1
122,"Increasing model size has unlocked a dazzling array of capabilities in modern language models. At the same time, even frontier models remain vulnerable to jailbreaks and prompt injections, despite concerted efforts to make them robust. As both attack and defense gain access to more compute, and as models become larger, what happens to robustness? We argue that to answer this question requires a \emph{scaling} approach, which we employ in an extensive study of language model robustness across several classification tasks, model families, and adversarial attacks. We find that in the absence of explicit safety training, larger models are not consistently more robust; however, scale improves sample efficiency in adversarial training, though it worsens compute efficiency. Further, we find that increasing attack compute smoothly improves attack success rate against both undefended and adversarially trained models. Finally, after exploring robustness transfer across attacks and threat models, we combine attack and defense scaling rates to study the offense-defense balance. We find that while attack scaling outpaces adversarial training across all models studied, larger adversarially trained models might give defense the advantage in the long run. These results underscore the utility of the scaling lens, and provide a paradigm for evaluating future attacks and defenses on frontier models.","在现代语言模式中,越来越多的模型规模已经释放出一系列惊人的现代语言模型能力。 同时,尽管各方一致努力使这些模型变得强大,但即使是前沿模型也仍然容易被破狱和迅速注射。随着攻击和防御都获得更精确的计算,随着模型的扩大,强性又会怎样呢?我们争辩说,要回答这个问题,我们需要一种对多种分类任务、模范家庭和对抗性攻击进行广泛的语言模型强健性研究,我们采用这种方法。我们发现,在缺乏明确的安全培训的情况下,较大的模型并不始终更加强大;然而,规模扩大的模型提高了对抗性培训的样本效率,尽管它使对立性培训的测试效率恶化了。此外,我们发现,随着攻击的不断增长,能够顺利地提高攻击率,同时针对不受干扰和受敌对性训练的模型。最后,在探索攻击和威胁性模型之间的稳健性转移之后,我们将攻击和防御性缩放速率结合起来,以研究防御平衡。我们发现,在所有所研究的模型中,攻击的比对立式培训速度越来越强,但经过对抗性培训的模型可能给长期防御攻击的优势。","Nikolaus Howe, Ian McKenzie, Oskar Hollinsworth, Michał Zajac, Tom Tseng, Aaron Tucker, Pierre-Luc Bacon, Adam Gleave",2025-06-05T08:11:43Z,Scaling Trends in Language Model Robustness,Skalierungstrends in der Robustheit von Sprachmodellen,语言模型强度的增缩趋势,http://arxiv.org/abs/2407.18213v5
123,"Speculative Decoding has gained popularity as an effective technique for accelerating the auto-regressive inference process of Large Language Models. However, Speculative Decoding entirely relies on the availability of efficient draft models, which are often lacking for many existing language models due to a stringent constraint of vocabulary compatibility. In this work we introduce FastDraft, a novel and efficient approach for pre-training and aligning a draft model to any large language model by incorporating efficient pre-training, followed by fine-tuning over synthetic datasets generated by the target model. We demonstrate FastDraft by training two highly parameter efficient drafts for the popular Phi-3-mini and Llama-3.1-8B models. Using FastDraft, we were able to produce a draft model with approximately 10 billion tokens on a single server with 8 Intel$^\circledR$ Gaudi$^\circledR$ 2 accelerators in under 24 hours. Our results show that the draft model achieves impressive results in key metrics of acceptance rate, block efficiency and up to 3x memory bound speed up when evaluated on code completion and up to 2x in summarization, text completion and instruction tasks. We validate our theoretical findings through benchmarking on the latest Intel$^\circledR$ Core$^{\tiny \text{TM}}$ Ultra, achieving a wall-clock time speedup of up to 2x, indicating a significant reduction in runtime. Due to its high quality, FastDraft unlocks large language models inference on AI-PC and other edge-devices.","作为加快大型语言模型自动递减推论进程的有效技术,投机下调已越来越受欢迎,但是,投机下调完全取决于能否提供高效的模型草案,许多现有语言模型往往缺乏这些模型,因为词汇兼容性受到严格的限制。在这项工作中,我们引入了快速草稿,这是在24小时内为培训前和将模型草案与任何大型语言模型统一起来的一种新颖而有效的方法,它包括了高效的训练前阶段,随后是对目标模型产生的合成数据集进行微调。我们通过培训为流行的Phi-3-mini和Llama-31-31-8B模型提供两个高参数高效的草案来展示快速草稿。我们利用快速草稿,能够制作出一个有大约100亿个符号的草案,一个有8个IntellecledR$ Gaudicled $\circurcledR$ 2 acerclergers。我们的成果显示,在对代码完成和高标准中达到2xrcal_rcalalalalalalalalal commax commax commax commaxal commax commax commax commax commax commax commax commax commax 完成完成一个高校标, 在完成一个大幅校正一个大幅校正完成一个高校正。","Ofir Zafrir, Igor Margulis, Dorin Shteyman, Shira Guskin, Guy Boudoukh",2025-06-05T08:09:22Z,FastDraft: How to Train Your Draft,FastDraft: Wie Sie Ihren Entwurf trainieren,快速草稿:如何训练您的草稿,http://arxiv.org/abs/2411.11055v3
124,"Reasoning models represented by the Deepseek-R1-Distill series have been widely adopted by the open-source community due to their strong performance in mathematics, science, programming, and other domains. However, our study reveals that their benchmark evaluation results are subject to significant fluctuations caused by various factors. Subtle differences in evaluation conditions can lead to substantial variations in results. Similar phenomena are observed in other open-source inference models fine-tuned based on the Deepseek-R1-Distill series, as well as in the QwQ-32B model, making their claimed performance improvements difficult to reproduce reliably. Therefore, we advocate for the establishment of a more rigorous paradigm for model performance evaluation and present our empirical assessments of the Deepseek-R1-Distill series models.","由Deepseek-R1-蒸馏系列所代表的理性模型已被开放源码社区广泛采用,因为它们在数学、科学、方案制定和其他领域的成绩优异,然而,我们的研究显示,它们的基准评价结果受到各种因素造成的巨大波动的影响,评价条件的细微差异可能导致结果的巨大差异,在根据Deepseek-R1-蒸馏系列和QwQ-32B模型进行微调调整的其他开放源码推断模型中也观察到类似现象,因此难以可靠地复制它们声称的业绩改进。 因此,我们主张为示范业绩评价建立一个更严格的模式,并介绍我们对Deepseek-R1-蒸馏系列模型的经验评估。","Lin Sun, Weihong Lin, Jinzhu Wu, Yongfu Zhu, Xiaoqi Jian, Guangxiang Zhao, Change Jia, Linglin Zhang, Sai-er Hu, Yuhan Wu, Xiangzheng Zhang",2025-06-05T08:09:11Z,Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning   Capabilities Through Evaluation Design,"Bewertung ist alles, was Sie brauchen: Strategisches Überfordern von LLM-Gründerfähigkeiten durch Evaluationsdesign","评价是全你需要的:通过评价设计,从战略上压低LLM能力,通过评价设计提高LLM能力",http://arxiv.org/abs/2506.04734v1
125,"The remarkable performance of large language models (LLMs) on complex linguistic tasks has sparked debate about their capabilities. Unlike humans, these models learn language solely from textual data without directly interacting with the world. Yet they generate seemingly meaningful text on diverse topics. This achievement has renewed interest in the classical `Symbol Grounding Problem' -- the question of whether the internal representations and outputs of symbolic AI systems can possess intrinsic meaning that is not parasitic on external interpretation. Although modern LLMs compute over vectors rather than symbols, an analogous problem arises for these systems, which we call the Vector Grounding Problem. This paper has two main goals. First, we distinguish five main notions of grounding that are often conflated in the literature, and argue that only one of them, which we call referential grounding, is relevant to the Vector Grounding Problem. Second, drawing on philosophical theories of representational content, we provide two arguments for the claim that LLMs and related systems can achieve referential grounding: (1) through preference fine-tuning methods that explicitly establish world-involving functions, and (2) through pre-training alone, which in limited domains may select for internal states with world-involving content, as mechanistic interpretability research suggests. Through these pathways, LLMs can establish connections to the world sufficient for intrinsic meaning. One potentially surprising implication of our discussion is that that multimodality and embodiment are neither necessary nor sufficient to overcome the Grounding Problem.","在复杂的语言任务方面,大型语言模型(LLMs)的出色表现引发了有关其能力的争论。与人类不同,这些模型只从文本数据中学习语言,而没有直接与世界互动。它们却在不同的专题上产生似乎有意义的文字。这一成就使人们对传统“共同基础问题”重新产生了兴趣,即象征性的AI系统的内部表述和产出是否具有内在含义,而不是对外部解释的寄生性。尽管现代LLMs的计算是针对矢量而不是符号的,但这些系统也出现了类似的问题,我们称之为矢量基问题。本文没有两个主要目标。首先,我们区分五大基础概念,这些概念往往在文献中混为一谈,并争论说其中只有一个概念,即我们称之为“优点基础问题 ” 。 第二,根据代表性内容的哲学理论,我们提出两个论点,即LLMs和相关系统可以实现偏向基础的偏向:(1) 通过优惠微调方法,明确确立世界的关联性,而这一方法又称为“矢量基点” ;以及(2) 仅通过培训前的讨论,在有限的地域上,可以选择一个具有足够内在含义的地理含义的自我解释,从而可以使我成为世界的内流研究。","Dimitri Coelho Mollo, Raphaël Millière",2025-06-05T07:55:56Z,The Vector Grounding Problem,Das Vector Grounding Problem,矢量基定问题,http://arxiv.org/abs/2304.01481v2
126,"We propose SPARTA ALIGNMENT, an algorithm to collectively align multiple LLMs through competition and combat. To complement a single model's lack of diversity in generation and biases in evaluation, multiple LLMs form a ""sparta tribe"" to compete against each other in fulfilling instructions while serving as judges for the competition of others. For each iteration, one instruction and two models are selected for a duel, the other models evaluate the two responses, and their evaluation scores are aggregated through a adapted elo-ranking based reputation system, where winners/losers of combat gain/lose weight in evaluating others. The peer-evaluated combat results then become preference pairs where the winning response is preferred over the losing one, and all models learn from these preferences at the end of each iteration. SPARTA ALIGNMENT enables the self-evolution of multiple LLMs in an iterative and collective competition process. Extensive experiments demonstrate that SPARTA ALIGNMENT outperforms initial models and 4 self-alignment baselines across 10 out of 12 tasks and datasets with 7.0% average improvement. Further analysis reveals that SPARTA ALIGNMENT generalizes more effectively to unseen tasks and leverages the expertise diversity of participating models to produce more logical, direct and informative outputs.","我们提出SPARTA 授标,这是通过竞争和战斗来集体协调多种LLMs的算法,它是一种通过竞争和战斗将多种LLMs统一起来的算法。为了补充单一模型在产生和评价方面缺乏多样性和偏见,多个LLMs组成了一个“Sparta部落”,在担任其他竞争的法官时相互竞争执行指令,相互竞争。对于每个迭代、一个指令和两个模型,选择决斗,其他模型评价两种反应,它们的评分通过一个经调整的以地标为基础的名誉系统加以汇总,这个系统将战斗得失/得分用于评价其他的得分/得分的得分/得分合并为4个自调基准,在其中12个任务和数据集中有10个是赢取的得分/得分,在评价中得分后成为优先的对等。 进一步的分析表明,SPARA AL ALINGENT在每一版本结束时从这些偏好中学习了这些偏好的选择。SPARA SPRATRAIALAL ALALALALALALAL 使多个连续和连续和集体产出更有效地产生更多的逻辑影响。","Yuru Jiang, Wenxuan Ding, Shangbin Feng, Greg Durrett, Yulia Tsvetkov",2025-06-05T07:51:23Z,SPARTA ALIGNMENT: Collectively Aligning Multiple Language Models through   Combat,SPARTA ALIGNMENT: Kollektive Ausrichtung mehrerer Sprachmodelle durch Kampf,SPARTA 签署:通过战斗集体调整多种语言模式,http://arxiv.org/abs/2506.04721v1
127,"This paper presents the submission of IIITH-BUT to the IWSLT 2025 shared task on speech translation for the low-resource Bhojpuri-Hindi language pair. We explored the impact of hyperparameter optimisation and data augmentation techniques on the performance of the SeamlessM4T model fine-tuned for this specific task. We systematically investigated a range of hyperparameters including learning rate schedules, number of update steps, warm-up steps, label smoothing, and batch sizes; and report their effect on translation quality. To address data scarcity, we applied speed perturbation and SpecAugment and studied their effect on translation quality. We also examined the use of cross-lingual signal through joint training with Marathi and Bhojpuri speech data. Our experiments reveal that careful selection of hyperparameters and the application of simple yet effective augmentation techniques significantly improve performance in low-resource settings. We also analysed the translation hypotheses to understand various kinds of errors that impacted the translation quality in terms of BLEU.","本文介绍了向IWSLT 2025年IWSLT 共同任务提交的三TH-BUT, 用于低资源 Bhojpuri-Hindi 语言配对的语音翻译。我们探讨了超参数优化和数据增强技术对缝合M4T 模型为这一具体任务进行微调的性能的影响。我们系统地调查了一系列超参数,包括学习进度表、更新步骤的数目、暖化步骤、标签平滑和批量大小;并报告了其对翻译质量的影响。为了解决数据稀缺的问题,我们应用了快速扰动和分解,并研究了其对翻译质量的影响。我们还通过与Marathi和Bhojpuri 语音数据进行联合培训,研究了跨语言信号的使用。我们的实验表明,仔细选择超参数和应用简单而有效的增强技术大大改善了低资源环境中的性能。我们还分析了翻译假设,以了解影响翻译质量的各种错误。","Bhavana Akkiraju, Aishwarya Pothula, Santosh Kesiraju, Anil Kumar Vuppala",2025-06-05T07:38:01Z,IIITH-BUT system for IWSLT 2025 low-resource Bhojpuri to Hindi speech   translation,IIITH-BUT System für IWSLT 2025 Low-Resource Bhojpuri zu Hindi Sprachübersetzung,IWSLT 2025 IWSLT 的三TH-BUT系统,http://arxiv.org/abs/2506.04714v1
128,"In automatic speech recognition (ASR), phoneme-based multilingual pre-training and crosslingual fine-tuning is attractive for its high data efficiency and competitive results compared to subword-based models. However, Weighted Finite State Transducer (WFST) based decoding is limited by its complex pipeline and inability to leverage large language models (LLMs). Therefore, we propose LLM-based phoneme-to-grapheme (LLM-P2G) decoding for phoneme-based ASR, consisting of speech-to-phoneme (S2P) and phoneme-to-grapheme (P2G). A challenge is that there seems to have information loss in cascading S2P and P2G. To address this challenge, we propose two training strategies: data augmentation with noisy phonemes (DANP), and randomized top-$K$ marginalized (TKM) training and decoding. Our experimental results show that LLM-P2G outperforms WFST-based systems in crosslingual ASR for Polish and German, by relative WER reductions of 3.6% and 6.9% respectively.","在自动语音识别(ASR)中,基于电话的多语种预先培训和跨语种微调对其高数据效率和与基于子词的模型相比的竞争性效果具有吸引力,然而,基于精密国家传输器(WFST)的编码因其复杂的管道而受到限制,无法利用大型语言模型(LLM),因此,我们提议为基于电话的ASR(LLM-P2G)解码,包括语音对话语(S2P)和电话对文字(P2G),一个难题是,在S2P和P2G的剖析中似乎会丢失信息。 为了应对这一挑战,我们提出了两项培训战略:用吵闹的电话(DANP)增强数据,以及随机拼凑的顶价(TKM)培训和解码。我们的实验结果表明,LLM-P2G在波兰语和德语交叉语言的ASR中比WST系统高出3.6%和6.9%。","Te Ma, Min Bi, Saierdaer Yusuyin, Hao Huang, Zhijian Ou",2025-06-05T07:35:55Z,LLM-based phoneme-to-grapheme for phoneme-based speech recognition,LLM-basiertes Phoneme-to-grapheme für Phoneme-basierte Spracherkennung,用于语音语音识别的LLM-基于LLM的电话机对电报,http://arxiv.org/abs/2506.04711v1
129,"Language models have demonstrated remarkable capabilities in reasoning tasks through test-time scaling techniques like best-of-N sampling and tree search. However, these approaches often demand substantial computational resources, creating a critical trade-off between performance and efficiency. We introduce STAND (STochastic Adaptive N-gram Drafting), a novel model-free speculative decoding approach that leverages the inherent redundancy in reasoning trajectories to achieve significant acceleration without compromising accuracy. Our analysis reveals that reasoning paths frequently reuse similar reasoning patterns, enabling efficient model-free token prediction without requiring separate draft models. By introducing stochastic drafting and preserving probabilistic information through a memory-efficient logit-based N-gram module, combined with optimized Gumbel-Top-K sampling and data-driven tree construction, STAND significantly improves token acceptance rates. Extensive evaluations across multiple models and reasoning tasks (AIME-2024, GPQA-Diamond, and LiveCodeBench) demonstrate that STAND reduces inference latency by 60-65% compared to standard autoregressive decoding while maintaining accuracy. Furthermore, STAND outperforms state-of-the-art speculative decoding methods by 14-28% in throughput and shows strong performance even in single-trajectory scenarios, reducing inference latency by 48-58%. As a model-free approach, STAND can be applied to any existing language model without additional training, being a powerful plug-and-play solution for accelerating language model reasoning.","语言模型通过测试-时间缩放技术,例如最佳N抽样和树木搜索,在推理任务方面表现出非凡的能力;然而,这些方法往往需要大量的计算资源,从而在业绩和效率之间形成关键的权衡。我们采用了STAND(STAND-STocastic Adeptical Aminive N-gram 起草),这是一种新型的没有模型的投机解码方法,它利用推理轨固有的冗余力来实现显著加速,同时又不损害准确性。我们的分析表明,推理路径经常重复类似的推理模式,使高效的无模型象征性预测无需单独的模型草案。通过采用基于记忆-高效的加速逻辑型N-gram模块,加上优化的Gumbel-Top-K采样和数据驱动的树的构造,引入了大量的能力信息。此外,STAND大大改进了象征性的接受率。 对多种模型和推理任务(AIME-2024、GPQA-Dimmond和LiveCodeBench)进行广泛的评价,表明,与标准的自动递解析模式相比,可以降低60-65%的推推推度,同时保持精确。此外,STAND-Defrofram-defrod-defrodustr-de-de-la-de-lades-lades-lades-lades-de-de-de-de-la-la-de-de-de-de-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-tra-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-","Woomin Song, Saket Dingliwal, Sai Muralidhar Jayanthi, Bhavana Ganesh, Jinwoo Shin, Aram Galstyan, Sravan Babu Bodapati",2025-06-05T07:31:18Z,Accelerated Test-Time Scaling with Model-Free Speculative Sampling,Beschleunigte Test-Zeit-Skalierung mit modellfreier Spekulativ-Sampling,"加速测试时间缩放,采用无投机示范抽样",http://arxiv.org/abs/2506.04708v1
130,"Leveraging Multi-modal Large Language Models (MLLMs) to create embodied agents offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have garnered substantial attention, MLLM-based embodied agents remain underexplored due to the lack of comprehensive evaluation frameworks. To bridge this gap, we introduce EmbodiedBench, an extensive benchmark designed to evaluate vision-driven embodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing tasks across four environments, ranging from high-level semantic tasks (e.g., household) to low-level tasks involving atomic actions (e.g., navigation and manipulation); and (2) six meticulously curated subsets evaluating essential agent capabilities like commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning. Through extensive experiments, we evaluated 24 leading proprietary and open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel at high-level tasks but struggle with low-level manipulation, with the best model, GPT-4o, scoring only 28.9\% on average. EmbodiedBench provides a multifaceted standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance MLLM-based embodied agents. Our code and dataset are available at https://embodiedbench.github.io.","利用多式大型语言模型(MLLMM)来创建具有代表性的代理商,为处理现实世界的任务提供了一个充满希望的渠道;虽然以语言为中心的代表商已获得大量关注,但是由于缺乏全面的评估框架,基于语言的体现商仍未得到充分探索;为弥合这一差距,我们引入了EmbodiedBench,这是一个旨在评价以愿景驱动的代理商的广泛基准;安博迪贝尼奇的特征:(1) 跨四个环境的一组不同的测试任务,从高层语义任务(如家庭)到涉及原子行动的低层次任务(如导航和操纵);以及(2) 六个精心调整的子集,对普通思维推理、复杂的教学理解、空间意识、视觉感知和长期规划等基本代理能力进行评价。我们通过广泛的实验,在EmbodiedBench内部评价了24个主要专利和开放源的MLLLMs。 我们的研究结果显示:MLLM在高级别任务中非常出色,但与低层次的操控力,采用最佳模型,即GPT-4o,只评评到我们现有的多层面见解平台。","Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, Heng Ji, Huan Zhang, Tong Zhang",2025-06-05T07:22:50Z,EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language   Models for Vision-Driven Embodied Agents,EmbodydBench: Umfassendes Benchmarking multimodaler Großsprachenmodelle für visionsgetriebene Embodyd-Agenten,"Embudied bench:为有远见的Embodied代理商制定综合基准综合基准,确定多模式大语言模式",http://arxiv.org/abs/2502.09560v3
131,"The internet has become a hotspot for hate speech (HS), threatening societal harmony and individual well-being. While automatic detection methods perform well in identifying explicit hate speech (ex-HS), they struggle with more subtle forms, such as implicit hate speech (im-HS). We tackle this problem by introducing a new taxonomy for im-HS detection, defining six encoding strategies named codetypes. We present two methods for integrating codetypes into im-HS detection: 1) prompting large language models (LLMs) directly to classify sentences based on generated responses, and 2) using LLMs as encoders with codetypes embedded during the encoding process. Experiments show that the use of codetypes improves im-HS detection in both Chinese and English datasets, validating the effectiveness of our approach across different languages.","互联网已成为仇恨言论(HS)的热点,威胁着社会和谐和个人福祉。自动检测方法在识别明确的仇恨言论(前HS)方面表现良好,但用隐含仇恨言论(im-HS)等更隐蔽的形式挣扎。我们通过引入新的隐含仇恨言论(im-HS)分类法来解决这一问题,定义了六种编码战略(代号型),我们提出了两种将代码型号纳入非HS检测的方法:(1)促使大型语言模型(LLMS)直接根据生成的响应进行分类,(2)使用LLMS作为编码过程中嵌入代码型的编码器。实验表明,代码型的使用可以改善中文和英文数据集中的In-HS检测,从而验证了我们在不同语言中的方法的有效性。","Lu Wei, Liangzhi Li, Tong Xiang, Xiao Liu, Noa Garcia",2025-06-05T07:15:21Z,Cracking the Code: Enhancing Implicit Hate Speech Detection through   Coding Classification,Den Code knacken: Implizite Hass-Spracherkennung durch Coding-Klassifikation verbessern,打破《守则》:通过编码分类加强隐性仇恨言论探测,http://arxiv.org/abs/2506.04693v1
132,"Large Language Models (LLMs) show remarkable proficiency in natural language tasks, yet their frequent overconfidence-misalignment between predicted confidence and true correctness-poses significant risks in critical decision-making applications. We present a comprehensive analysis on calibration in LLMs across nine LLMs and three factual Question-Answering (QA) datasets, systematically comparing standard free-generation settings against structured distractor-augmented prompts. Our evaluation reveals that explicitly incorporating distractors can substantially mitigate miscalibration, achieving relative accuracy improvements up to 460% and ECE reductions up to 90%. Despite general trends, we uncover nuanced findings: large RLHF-tuned models display inherent calibration strengths but can paradoxically suffer increased miscalibration on easier queries, whereas smaller models benefit disproportionately from distractor prompts but remain significantly miscalibrated. Through detailed analyses across question types, we identify persistent calibration failures, particularly in person-based queries. We conclude with concrete recommendations-targeted fine-tuning, structured prompting, and strategic model choice-to ensure reliable, trustworthy LLM deployments.","大型语言模型(LLMS)在自然语言任务方面表现出非凡的熟练程度,然而,在预测的信心和真实正确性之间经常出现过度的不信任和误差,给关键决策应用带来重大风险。我们全面分析了九大LMS和三套事实问题解答数据集的LLMS校准情况,系统地比较标准的自由生成环境与结构分散或增强的提示。我们的评价显示,明确纳入分散器可以大大减少调校错误,实现相对准确性提高高达46%,欧洲经委会削减到90%。尽管存在一般趋势,但我们发现有细微的发现:大型RLHF调制型模型显示出内在的校准强度,但矛盾的是,在较容易查询时可能会出现更大的误差,而较小的模型则从分散式的提示中获得不成比例的惠益,但仍然严重不协调。通过对问题类型进行详细分析,我们发现持续的校准失败,特别是在以个人为基础的查询中。我们最后提出了具体的建议,即有针对性的微调、结构化、迅速和战略模式选择,以确保可靠、可靠的LM部署。",Prateek Chhikara,2025-06-05T07:14:36Z,"Mind the Confidence Gap: Overconfidence, Calibration, and Distractor   Effects in Large Language Models","Mind the Confidence Gap: Überbewusstsein, Kalibrierung und Distraktor-Effekte in großen Sprachmodellen",牢记信心差距:在大语言模式中过度自信、校准和扰动效应,http://arxiv.org/abs/2502.11028v2
133,"Scaling laws predict that the performance of large language models improves with increasing model size and data size. In practice, pre-training has been relying on massive web crawls, using almost all data sources publicly available on the internet so far. However, this pool of natural data does not grow at the same rate as the compute supply. Furthermore, the availability of high-quality texts is even more limited: data filtering pipelines often remove up to 99% of the initial web scrapes to achieve state-of-the-art. To address the ""data wall"" of pre-training scaling, our work explores ways to transform and recycle data discarded in existing filtering processes. We propose REWIRE, REcycling the Web with guIded REwrite, a method to enrich low-quality documents so that they could become useful for training. This in turn allows us to increase the representation of synthetic data in the final pre-training set. Experiments at 1B, 3B and 7B scales of the DCLM benchmark show that mixing high-quality raw texts and our rewritten texts lead to 1.0, 1.3 and 2.5 percentage points improvement respectively across 22 diverse tasks, compared to training on only filtered web data. Training on the raw-synthetic data mix is also more effective than having access to 2x web data. Through further analysis, we demonstrate that about 82% of the mixed in texts come from transforming lower-quality documents that would otherwise be discarded. REWIRE also outperforms related approaches of generating synthetic data, including Wikipedia-style paraphrasing, question-answer synthesizing and knowledge extraction. These results suggest that recycling web texts holds the potential for being a simple and effective approach for scaling pre-training data.","缩放法律预测,随着模型规模和数据规模的增加,大型语言模型的性能会随着模型规模和数据规模的扩大而提高。在实践中,培训前一直依赖大规模网络爬行,使用互联网上迄今公开的几乎所有数据源。然而,这一自然数据库的生长速度与计算供应量的增长速度不同。此外,高质量文本的提供甚至更加有限:数据过滤管道往往会从初始网络废料中消除多达99%的绩效,以达到最新水平。为了解决培训前规模的“数据墙”问题,我们的工作一直在探索如何改造和再循环现有过滤过程中丢弃的数据。我们提议REWIRE, 将网络循环使用虚拟版的Rewret, 以这一方法来丰富低质量文件,这样就可以对培训有用。 数据过滤前的1B、 3B和 7B 测试显示,高质量的原始文本和我们重新编译文的文本将最终转化为1.0、 1.3 和 2.5 变异性指标将分别在22个不同的任务中进行, 培训数据将显示我们精选的精选前数据。","Thao Nguyen, Yang Li, Olga Golovneva, Luke Zettlemoyer, Sewoong Oh, Ludwig Schmidt, Xian Li",2025-06-05T07:12:12Z,Recycling the Web: A Method to Enhance Pre-training Data Quality and   Quantity for Language Models,Recycling the Web: Eine Methode zur Verbesserung der Vorschulung von Daten Qualität und Menge für Sprachmodelle,网上再循环:提高语文模式培训前数据质量和数量的方法,http://arxiv.org/abs/2506.04689v1
134,"This paper introduces MMRefine, a MultiModal Refinement benchmark designed to evaluate the error refinement capabilities of Multimodal Large Language Models (MLLMs). As the emphasis shifts toward enhancing reasoning during inference, MMRefine provides a framework that evaluates MLLMs' abilities to detect and correct errors across six distinct scenarios beyond just comparing final accuracy before and after refinement. Furthermore, the benchmark analyzes the refinement performance by categorizing errors into six error types. Experiments with various open and closed MLLMs reveal bottlenecks and factors impeding refinement performance, highlighting areas for improvement in effective reasoning enhancement. Our code and dataset are publicly available at https://github.com/naver-ai/MMRefine.","本文件介绍MMREFine,这是一个多式改进基准,旨在评价多式大语言模型(MLLM)的错误完善能力。随着重点转向在推论期间加强推理,MMREFine提供了一个框架,评估MMREFine除了在推理前和推理后仅仅比较最后准确性外,还能够发现和纠正六种不同情景中的错误。此外,该基准通过将错误分为六种错误来分析改进性能。与各种开放和封闭的MLLMS进行的实验揭示了阻碍改进工作的瓶颈和因素,突出了有效推理改进方面需要改进的领域。我们的代码和数据集可在https://github.com/naver-ai/MMNRefine上公开查阅。","Gio Paik, Geewook Kim, Jinbae Im",2025-06-05T07:11:36Z,MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal   Large Language Models,MMRefine: Enthüllen der Hindernisse zur robusten Veredelung in multimodalen großen Sprachmodellen,MMREFine:克服阻碍大力改进多模式大语言模式的障碍,http://arxiv.org/abs/2506.04688v1
135,"We introduce $Urania$, a novel framework for generating insights about LLM chatbot interactions with rigorous differential privacy (DP) guarantees. The framework employs a private clustering mechanism and innovative keyword extraction methods, including frequency-based, TF-IDF-based, and LLM-guided approaches. By leveraging DP tools such as clustering, partition selection, and histogram-based summarization, $Urania$ provides end-to-end privacy protection. Our evaluation assesses lexical and semantic content preservation, pair similarity, and LLM-based metrics, benchmarking against a non-private Clio-inspired pipeline (Tamkin et al., 2024). Moreover, we develop a simple empirical privacy evaluation that demonstrates the enhanced robustness of our DP pipeline. The results show the framework's ability to extract meaningful conversational insights while maintaining stringent user privacy, effectively balancing data utility with privacy preservation.","我们引入了“乌拉尼亚元”这一创新框架,以使人们深入了解LLM聊天室互动与严格的差异隐私保障(DP)的严格差异隐私保障(DP)的关联;该框架采用私人集群机制和创新性关键词提取方法,包括基于频率的、基于TF-IDF的和基于LLM的引导方法;通过利用集群、分区选择和基于直方图的组合等DP工具,美元提供端对端的隐私保护;我们的评估评估评估评估了词汇和语义内容保护、对等性以及基于LLM的衡量标准,对照非私营的Clio启发型管道(Tamkin等人,2024年),我们开发了简单的实证隐私评估,展示了我们DP管道的稳健性。结果显示,该框架有能力在保持严格的用户隐私的同时获取有意义的谈话见解,有效地平衡数据效用与隐私保护。","Daogao Liu, Edith Cohen, Badih Ghazi, Peter Kairouz, Pritish Kamath, Alexander Knop, Ravi Kumar, Pasin Manurangsi, Adam Sealfon, Da Yu, Chiyuan Zhang",2025-06-05T07:00:31Z,Urania: Differentially Private Insights into AI Use,Urania: Unterschiedliche private Einblicke in die KI-Nutzung,Urania: 差异私人透视转化为 AI 使用,http://arxiv.org/abs/2506.04681v1
136,"The progress of AI systems such as large language models (LLMs) raises increasingly pressing concerns about their safe deployment. This paper examines the value alignment problem for LLMs, arguing that current alignment strategies are fundamentally inadequate to prevent misuse. Despite ongoing efforts to instill norms such as helpfulness, honesty, and harmlessness in LLMs through fine-tuning based on human preferences, they remain vulnerable to adversarial attacks that exploit conflicts between these norms. I argue that this vulnerability reflects a fundamental limitation of existing alignment methods: they reinforce shallow behavioral dispositions rather than endowing LLMs with a genuine capacity for normative deliberation. Drawing from on research in moral psychology, I show how humans' ability to engage in deliberative reasoning enhances their resilience against similar adversarial tactics. LLMs, by contrast, lack a robust capacity to detect and rationally resolve normative conflicts, leaving them susceptible to manipulation; even recent advances in reasoning-focused LLMs have not addressed this vulnerability. This ``shallow alignment'' problem carries significant implications for AI safety and regulation, suggesting that current approaches are insufficient for mitigating potential harms posed by increasingly capable AI systems.","大量语言模型(LLMs)等AI系统的进展引起了人们对其安全部署的日益紧迫的关切。本文审查了LLMs的价值调整问题,认为目前的调整战略根本不足以防止滥用。尽管目前正在努力通过基于人类偏好的微调来灌输LLMs的帮助性、诚实性和无害性等规范,但它们仍然容易受到利用这些规范之间的冲突的对抗性攻击的伤害。我争辩说,这种脆弱性反映了现有调整方法的根本局限性:它们强化了浅层次的行为处理方式,而不是赋予LMs以真正的规范性审议能力。从道德心理学的研究中可以看出,人类进行审议推理的能力如何增强他们抵御类似对抗性策略的复原力。相比之下,LMMs缺乏强有力的能力来发现和合理解决规范冲突,使其容易被操纵;即使最近以推理为重点的LMs也未能解决这种脆弱性。这个“调整问题对AI的安全和监管具有重大影响,表明目前的方法不足以减轻能力日益增强的AI系统可能造成的潜在伤害。",Raphaël Millière,2025-06-05T06:57:28Z,Normative Conflicts and Shallow AI Alignment,Normative Konflikte und abgeschwächte Alignment,规范冲突和小规模的 AI 调整,http://arxiv.org/abs/2506.04679v1
137,"Retrieval-Augmented Generation (RAG) systems enhance Large Language Models (LLMs) by incorporating external retrieved information, mitigating issues such as hallucination and outdated knowledge. However, RAG systems are highly sensitive to retrieval noise prevalent in real-world scenarios. Existing benchmarks fail to emulate the complex and heterogeneous noise distributions encountered in real-world retrieval environments, undermining reliable robustness assessment. In this paper, we define four categories of retrieval noise based on linguistic properties and noise characteristics, aiming to reflect the heterogeneity of noise in real-world scenarios. Building on this, we introduce Magic Mushroom, a benchmark for replicating ""magic mushroom"" noise: contexts that appear relevant on the surface but covertly mislead RAG systems. Magic Mushroom comprises 7,468 single-hop and 3,925 multi-hop question-answer pairs. More importantly, Magic Mushroom enables researchers to flexibly configure combinations of retrieval noise according to specific research objectives or application scenarios, allowing for highly controlled evaluation setups. We evaluate LLM generators of varying parameter scales and classic RAG denoising strategies under diverse noise distributions to investigate their performance dynamics during progressive noise encroachment. Our analysis reveals that both generators and denoising strategies have significant room for improvement and exhibit extreme sensitivity to noise distributions. Magic Mushroom emerges as a promising tool for evaluating and advancing noise-robust RAG systems, accelerating their widespread deployment in real-world applications. The Magic Mushroom benchmark is available at https://drive.google.com/file/d/1aP5kyPuk4L-L_uoI6T9UhxuTyt8oMqjT/view?usp=sharing.","在本文中,我们根据语言特性和噪音特点界定了四类检索噪音,目的是反映现实世界情景中噪音的杂质。在此基础上,我们引入了魔法蘑菇,这是复制“魔法蘑菇”噪音的基准:表面环境似乎相关,但隐蔽地误导了RAG系统。魔法蘑菇包括7,468个单部和3,925个多声问答配对,更重要的是,魔法蘑菇使研究人员能够根据具体的研究目标或应用系统灵活配置回收噪音组合,允许高度控制地进行噪音评估。我们评估了不同参数比例的LLM发电机和在多种噪音分销中复制“魔法蘑菇”噪音的典型战略:看起来与地面有关,但隐蔽地误导RAG系统。魔法蘑菇包括7,468个单部和3,925个多声的问答配对。更重要的是,魔法蘑菇使研究人员能够根据具体的研究目标或应用系统灵活配置回收噪音组合,允许高度控制地进行噪音评估。我们评估了不同参数尺度的LMM型发电机和RAG的升级战略,在不断推进的噪音分发中,以调查其动态动态在不断变压的MRush Rushal-rusmassal Rushmal Rushmalma 。","Yuxin Zhang, Yan Wang, Yongrui Chen, Shenyu Zhang, Xinbang Dai, Sheng Bi, Guilin Qi",2025-06-05T06:44:23Z,Magic Mushroom: A Customizable Benchmark for Fine-grained Analysis of   Retrieval Noise Erosion in RAG Systems,Magic Mushroom: Ein anpassbarer Benchmark für die feinkörnige Analyse von Retrieval Noise Erosion in RAG-Systemen,魔术蘑菇:对RAG系统中的检索噪音侵蚀进行精细分析的定制基准,http://arxiv.org/abs/2506.03901v2
138,"An increasingly common socio-technical problem is people being taken in by offers that sound ``too good to be true'', where persuasion and trust shape decision-making. This paper investigates how \abr{ai} can help detect these deceptive scenarios. We analyze how humans strategically deceive each other in \textit{Diplomacy}, a board game that requires both natural language communication and strategic reasoning. This requires extracting logical forms of proposed agreements in player communications and computing the relative rewards of the proposal using agents' value functions. Combined with text-based features, this can improve our deception detection. Our method detects human deception with a high precision when compared to a Large Language Model approach that flags many true messages as deceptive. Future human-\abr{ai} interaction tools can build on our methods for deception detection by triggering \textit{friction} to give users a chance of interrogating suspicious proposals.","一个日益常见的社会-技术问题就是人们被“太好而不能真实”的报价所接受, 说服和信任影响决策。 本文调查了 \ abr{ai} 如何能帮助检测这些欺骗性情景。 我们分析了人类在“ textit{ diplomacy} ” 这一需要自然语言交流和战略推理的棋盘游戏中如何在战略上互相欺骗。 这就需要在玩家通信中提取拟议协议的逻辑形式, 并使用代理方的价值观功能计算提案的相对回报。 这与基于文本的功能相结合, 可以改进我们的欺骗检测。 我们的方法可以与将许多真实信息标为欺骗性的大语言模型方法相比,非常精确地探测人类的欺骗。 未来的人类- abr{i} 互动工具可以通过触发\ textit{friction} 使用户有机会询问可疑建议。","Wichayaporn Wongkamjan, Yanze Wang, Feng Gu, Denis Peskoff, Jonathan K. Kummerfeld, Jonathan May, Jordan Lee Boyd-Graber",2025-06-05T06:27:54Z,Should I Trust You? Detecting Deception in Negotiations using   Counterfactual RL,Sollte ich Ihnen vertrauen? Detektion von Täuschung in Verhandlungen mit kontrafaktischen RL,利用反事实RL在谈判中发现欺骗行为,http://arxiv.org/abs/2502.12436v3
139,"The conflict in Ukraine has been not only characterised by military engagement but also by a significant information war, with social media platforms like X, formerly known as Twitter playing an important role in shaping public perception. This article provides an analysis of tweets from propaganda accounts and trusted accounts collected from the onset of the war, February 2022 until the middle of May 2022 with n=40,000 total tweets. We utilise natural language processing and machine learning algorithms to assess the sentiment and identify key themes, topics and narratives across the dataset with human-in-the-loop (HITL) analysis throughout. Our findings indicate distinct strategies in how information is created, spread, and targeted at different audiences by both sides. Propaganda accounts frequently employ emotionally charged language and disinformation to evoke fear and distrust, whereas other accounts, primarily Western tend to focus on factual reporting and humanitarian aspects of the conflict. Clustering analysis reveals groups of accounts with similar behaviours, which we suspect indicates the presence of coordinated efforts. This research attempts to contribute to our understanding of the dynamics of information warfare and offers techniques for future studies on social media influence in military conflicts.","乌克兰的冲突不仅以军事接触为特征,而且还以一场重大的信息战争为特征,X类社交媒体平台(以前称为Twitter)在塑造公众观念方面发挥了重要作用,文章分析了2022年2月至2022年5月中旬战争爆发以来从宣传账户和可靠账户中收集的推特,总推文为40 000新元;我们利用自然语言处理和机器学习算法评估人们的情绪,并查明贯穿全过程的人类流动(HITL)分析数据集的关键主题、专题和叙事;我们的调查结果表明,双方在如何创造、传播和针对不同受众的信息方面采取了不同的战略;宣传账户经常使用情绪上充斥的语言和虚假信息来引起恐惧和不信任,而其他账户(主要是西方)往往侧重于冲突的事实报告和人道主义方面;对类似行为进行分组分析,我们怀疑存在协调努力;这一研究试图帮助我们了解信息战的动态,并为未来研究军事冲突中的社会媒体影响提供技术。",Zaur Gouliev,2025-06-05T06:11:55Z,Propaganda and Information Dissemination in the Russo-Ukrainian War:   Natural Language Processing of Russian and Western Twitter Narratives,Propaganda und Informationsverbreitung im Russisch-Ukrainischen Krieg: Natürliche Sprachverarbeitung von russischen und westlichen Twitter Narratives,Russo-Ukraineian战争中的宣传和信息传播:俄罗斯和西方推特上的自然语言处理,http://arxiv.org/abs/2506.01807v2
140,"Deep Reinforcement Learning (DRL) is widely used in task-oriented dialogue systems to optimize dialogue policy, but it struggles to balance exploration and exploitation due to the high dimensionality of state and action spaces. This challenge often results in local optima or poor convergence. Evolutionary Algorithms (EAs) have been proven to effectively explore the solution space of neural networks by maintaining population diversity. Inspired by this, we innovatively combine the global search capabilities of EA with the local optimization of DRL to achieve a balance between exploration and exploitation. Nevertheless, the inherent flexibility of natural language in dialogue tasks complicates this direct integration, leading to prolonged evolutionary times. Thus, we further propose an elite individual injection mechanism to enhance EA's search efficiency by adaptively introducing best-performing individuals into the population. Experiments across four datasets show that our approach significantly improves the balance between exploration and exploitation, boosting performance. Moreover, the effectiveness of the EII mechanism in reducing exploration time has been demonstrated, achieving an efficient integration of EA and DRL on task-oriented dialogue policy tasks.","深强化学习(DRL)被广泛用于面向任务的对话系统,以优化对话政策,但由于国家和行动空间的高度多元性,它努力平衡勘探和开发,但由于国家和行动空间的高度多元化,这一挑战往往导致局部选择或融合差。进化算术(EAs)已证明通过保持人口多样性有效地探索神经网络的解决方案空间。受此启发,我们创新地将EA的全球搜索能力与DRL的本地优化结合起来,以实现勘探和开发之间的平衡。然而,自然语言在对话工作中的内在灵活性使直接融合复杂化,导致长期的演变时期。因此,我们进一步提议一个精英个人注入机制,通过在人口中以适应性方式引入最优秀的个人来提高EA的搜索效率。通过四个数据集的实验表明,我们的方法极大地改善了勘探和开发之间的平衡,提高了绩效。此外,已经展示了环境II机制在缩短勘探时间方面的有效性,使EA和DL在任务导向性对话政策任务上实现高效融合。","Yangyang Zhao, Ben Niu, Libo Qin, Shihan Wang",2025-06-05T06:09:38Z,An Efficient Task-Oriented Dialogue Policy: Evolutionary Reinforcement   Learning Injected by Elite Individuals,"Eine effiziente, auf Aufgaben ausgerichtete Dialogpolitik: Evolutionäres Stärkungslernen von Elite-Personen",高效的、以任务为导向的对话政策:精英个人注射的进化强化学习,http://arxiv.org/abs/2506.03519v2
141,"In-group language is an important signifier of group dynamics. This paper proposes a novel method for inducing lexicons of in-group language, which incorporates its socio-temporal context. Existing methods for lexicon induction do not capture the evolving nature of in-group language, nor the social structure of the community. Using dynamic word and user embeddings trained on conversations from online anti-women communities, our approach outperforms prior methods for lexicon induction. We develop a test set for the task of lexicon induction and a new lexicon of manosphere language, validated by human experts, which quantifies the relevance of each term to a specific sub-community at a given point in time. Finally, we present novel insights on in-group language which illustrate the utility of this approach.","群体内语言是群体动态的一个重要标志。本文件提出了一种引入群体内语言词汇的新方法,该方法结合了群体内语言的社会-时间背景。现有的词汇上岗方法并不反映群体内语言的演变性质,也不反映社区的社会结构。使用经过在线反妇女社区对话培训的动态词和用户嵌入器,我们的方法优于先前的词汇上岗方法。我们为词汇上岗和新的人际语言词汇任务开发了一套测试工具,由人类专家验证,其中量化了每个术语在特定时间某个特定次社区的相关性。最后,我们介绍了关于群体内语言的新见解,说明了这一方法的实用性。",Christine de Kock,2025-06-05T05:55:19Z,Inducing lexicons of in-group language with socio-temporal context,Induzieren von Lexikons der In-Gruppe-Sprache mit sozio-temporalem Kontext,引引具有社会-时时背景的组内语言词汇法,http://arxiv.org/abs/2409.19257v3
142,"Speech emotion recognition (SER) systems often exhibit gender bias. However, the effectiveness and robustness of existing debiasing methods in such multi-label scenarios remain underexplored. To address this gap, we present EMO-Debias, a large-scale comparison of 13 debiasing methods applied to multi-label SER. Our study encompasses techniques from pre-processing, regularization, adversarial learning, biased learners, and distributionally robust optimization. Experiments conducted on acted and naturalistic emotion datasets, using WavLM and XLSR representations, evaluate each method under conditions of gender imbalance. Our analysis quantifies the trade-offs between fairness and accuracy, identifying which approaches consistently reduce gender performance gaps without compromising overall model performance. The findings provide actionable insights for selecting effective debiasing strategies and highlight the impact of dataset distributions.","然而,在这种多标签情景中,现有贬低性能方法的有效性和稳健性仍未得到充分探讨。为了解决这一差距,我们介绍了EMO-Debias,这是对适用于多标签SER的13种贬低性能方法的大规模比较。我们的研究包括预处理、规范化、对抗性学习、有偏见的学习者、以及分配上强的优化等技术。在行为和自然情感数据集方面进行的实验,利用WavLM和XLSR的表述,在性别不平衡的条件下对每一种方法进行评估。我们的分析对公平和准确之间的权衡进行了量化,确定了在不损害总体模型业绩的情况下,持续缩小性别业绩差距的方法。研究结果为选择有效的贬低性战略提供了可操作的洞察力,并强调了数据分布的影响。","Yi-Cheng Lin, Huang-Cheng Chou, Yu-Hsuan Li Liang, Hung-yi Lee",2025-06-05T05:48:31Z,EMO-Debias: Benchmarking Gender Debiasing Techniques in Multi-Label   Speech Emotion Recognition,EMO-Debias: Benchmarking Gender-Debiasing-Techniken in der multi-Label Sprachemotionserkennung,EMO-Debias:在多语言言论中确定性别偏见技术基准,http://arxiv.org/abs/2506.04652v1
143,"Conventional biomedical research is increasingly labor-intensive due to the exponential growth of scientific literature and datasets. Artificial intelligence (AI), particularly Large Language Models (LLMs), has the potential to revolutionize this process by automating various steps. Still, significant challenges remain, including the need for multidisciplinary expertise, logicality of experimental design, and performance measurements. This paper introduces BioResearcher, the first end-to-end automated system designed to streamline the entire biomedical research process involving dry lab experiments. BioResearcher employs a modular multi-agent architecture, integrating specialized agents for search, literature processing, experimental design, and programming. By decomposing complex tasks into logically related sub-tasks and utilizing a hierarchical learning approach, BioResearcher effectively addresses the challenges of multidisciplinary requirements and logical complexity. Furthermore, BioResearcher incorporates an LLM-based reviewer for in-process quality control and introduces novel evaluation metrics to assess the quality and automation of experimental protocols. BioResearcher successfully achieves an average execution success rate of 63.07% across eight previously unmet research objectives. The generated protocols, on average, outperform typical agent systems by 22.0% on five quality metrics. The system demonstrates significant potential to reduce researchers' workloads and accelerate biomedical discoveries, paving the way for future innovations in automated research systems.","由于科学文献和数据集的成倍增长,常规生物医学研究日益成为劳动力密集型研究。人工智能(AI),特别是大语言模型(LLMS)有可能通过使各种步骤自动化而使这一过程发生革命性变革。然而,依然存在着重大挑战,包括需要多学科的专门知识、实验设计的合理性和性以及性能测量。本文件介绍生物研究(BioResearch),这是第一个端到端自动化系统,旨在精简涉及干实验的整个生物医学研究过程;生物研究(BioResearch)使用模块式多试剂结构,整合了用于搜索、文学处理、实验设计和编程的专门剂。生物研究(BioResearch)将复杂的任务分解成与逻辑相关的子任务,并采用等级学习方法,从而有效地应对多学科要求和逻辑复杂性的挑战。此外,生物研究(BioResearch)采用基于LM(LM)的审查器,用于控制过程质量,并引入新的评估实验协议质量和自动化的评价指标。生物研究(BioResearter)成功地在八个前未完成的研究目标中实现了63.07%的平均执行成功率。生成的规程,平均、超出典型代理研究系统,以22.0提高未来研究质量研究的系统的风险提升了未来系统。","Yi Luo, Linghang Shi, Yihao Li, Aobo Zhuang, Yeyun Gong, Ling Liu, Chen Lin",2025-06-05T05:44:18Z,From Intention To Implementation: Automating Biomedical Research via   LLMs,Von der Absicht zur Umsetzung: Automatisierung der biomedizinischen Forschung über LLMs,从实施目的出发:通过LLMs实现生物医学研究自动化,http://arxiv.org/abs/2412.09429v4
144,"Large Language Models (LLMs) have recently emerged as promising tools for knowledge tracing (KT) due to their strong reasoning and generalization abilities. While recent LLM-based KT methods have proposed new prompt formats, they struggle to represent the full interaction histories of example learners within a single prompt during in-context learning (ICL), resulting in limited scalability and high computational cost under token constraints. In this work, we present \textit{LLM-based Option-weighted Knowledge Tracing (LOKT)}, a simple yet effective framework that encodes the interaction histories of example learners in context as \textit{textual categorical option weights (TCOW)}. TCOW are semantic labels (e.g., ``inadequate'') assigned to the options selected by learners when answering questions, enhancing the interpretability of LLMs. Experiments on multiple-choice datasets show that LOKT outperforms existing non-LLM and LLM-based KT models in both cold-start and warm-start settings. Moreover, LOKT enables scalable and cost-efficient inference, achieving strong performance even under strict token constraints. Our code is available at \href{https://anonymous.4open.science/r/LOKT_model-3233}{https://anonymous.4open.science/r/LOKT\_model-3233}.","大型语言模型(LLMs)最近因其强有力的推理和概括化能力而成为有希望的知识追踪工具。最近基于LLM的KT方法提出了新的快速格式,但很难在同文学习(ICL)期间以单一的即时方式代表样板学习者的全部互动历史,导致在象征性限制下可缩缩缩放性有限和计算成本高。在这项工作中,我们提出\textit{LLLLM-基于选择加权知识追踪(LOKT)},这是一个简单而有效的框架,将背景中学习者的互动史编码为\textit{textal绝对选项重量{TCOW}。 TCOW是语言标签(例如,“不足”),在回答问题时分配给学习者选择的语义性标签,提高了LLMS的可解释性。多巧克力数据集实验显示,LOKT超越了冷启动和温暖启动环境中现有的非LLM/LM-基KT模式。LOKT可以使Slompal-roomalalal_Simal_Suplemental decal decess commal destressional destression compract compress compress press press press","JongWoo Kim, SeongYeub Chu, Bryan Wong, Mun Yi",2025-06-05T05:41:21Z,Not All Options Are Created Equal: Textual Option Weighting for   Token-Efficient LLM-Based Knowledge Tracing,Nicht alle Optionen sind gleich: Textuelle Optionsgewichtung für Token-Efficient LLM-basierte Wissensverfolgung,并非所有选项都创建为等: Token- Efficient LLM 知识追踪的文本选项比重,http://arxiv.org/abs/2410.12872v2
145,"Efficient long-sequence generation is a critical challenge for Large Language Models. While recent sparse decoding methods improve efficiency, they suffer from KV cache misalignment, where approximation errors accumulate and degrade generation quality. In this work, we propose Rectified Sparse Attention (ReSA), a simple yet effective method that combines block-sparse attention with periodic dense rectification. By refreshing the KV cache at fixed intervals using a dense forward pass, ReSA bounds error accumulation and preserves alignment with the pretraining distribution. Experiments across math reasoning, language modeling, and retrieval tasks demonstrate that ReSA achieves near-lossless generation quality with significantly improved efficiency. Notably, ReSA delivers up to 2.42$\times$ end-to-end speedup under decoding at 256K sequence length, making it a practical solution for scalable long-context inference. Code is available at https://aka.ms/ReSA-LM.","高效长序列生成是大语言模型面临的一个重大挑战。 虽然最近稀疏的解码方法提高了效率, 但它们却遭遇了 KV 缓存误配, 近似误差累积和降解生成质量。 在这项工作中, 我们提议校正松散注意( ReSA ) , 这是一种简单而有效的方法, 将块块分割注意与定期密集校正相结合。 通过使用密度大的远端传道, 在固定间隔中刷新 KV 缓存, ReSA 将错误积累与预培训分布相匹配。 跨数学推理、 语言建模和检索任务的实验表明, RESA 取得了近乎无损的生成质量, 并大大提高了效率。 值得注意的是, ReSA 在解码下以 256K 序列长度解码下, 交付了2.42 美元/timets 的端到端速度, 使得它成为可缩放长文本的一个实用解决方案。 代码可在 https://ak. ms/ ReSA-LM 上查阅 。","Yutao Sun, Tianzhu Ye, Li Dong, Yuqing Xia, Jian Chen, Yizhao Gao, Shijie Cao, Jianyong Wang, Furu Wei",2025-06-05T05:39:48Z,Rectified Sparse Attention,Rektifizierte Sparse Achtung,校正的 松散注意,http://arxiv.org/abs/2506.04108v2
146,"Real-world planning problems require constant adaptation to changing requirements and balancing of competing constraints. However, current benchmarks for evaluating LLMs' planning capabilities primarily focus on static, single-turn scenarios. We introduce Flex-TravelPlanner, a benchmark that evaluates language models' ability to reason flexibly in dynamic planning scenarios. Building on the TravelPlanner dataset~\citep{xie2024travelplanner}, we introduce two novel evaluation settings: (1) sequential constraint introduction across multiple turns, and (2) scenarios with explicitly prioritized competing constraints. Our analysis of GPT-4o and Llama 3.1 70B reveals several key findings: models' performance on single-turn tasks poorly predicts their ability to adapt plans across multiple turns; constraint introduction order significantly affects performance; and models struggle with constraint prioritization, often incorrectly favoring newly introduced lower priority preferences over existing higher-priority constraints. These findings highlight the importance of evaluating LLMs in more realistic, dynamic planning scenarios and suggest specific directions for improving model performance on complex planning tasks. The code and dataset for our framework are publicly available at https://github.com/juhyunohh/FlexTravelBench.","现实世界的规划问题要求不断适应不断变化的要求,并平衡相互竞争的限制。然而,目前评估LLMS规划能力的基准主要侧重于静态的、单向的情景。我们引入了Flex-TravelPlanner,这是一个评估语言模型在动态的规划情景中灵活思考的能力的基准。我们以TravelPlanner 数据集*citep{xie2024旅行规划师}为基础,引入了两种新的评价环境:(1) 连续的制约,跨越多个转折,和(2) 明确排列优先顺序的相竞制约情景。我们对GPT-4o和Llama 3.1 70B的分析揭示了几个关键结论:单向任务模型的性能没有很好地预测出它们跨越多个转弯的适应计划的能力;限制引入命令显著影响业绩;以及模式在限制优先排序方面挣扎,往往错误地倾向于新引入的低优先偏向高于现有更高优先的制约。这些结论强调了在更现实、动态的规划情景中评估LMS的重要性,并提出了改进复杂规划任务模式业绩的具体方向。我们框架的代码和数据集可在http://github.com/juhyunoh/Flextravelgelchetchetchchnch。","Juhyun Oh, Eunsu Kim, Alice Oh",2025-06-05T05:31:50Z,Flex-TravelPlanner: A Benchmark for Flexible Planning with Language   Agents,Flex-TravelPlanner: Ein Benchmark für flexible Planung mit Sprachagenten,弹性旅行规划:语文代表灵活规划基准,http://arxiv.org/abs/2506.04649v1
147,"The key-value (KV) cache in transformer models is a critical component for efficient decoding or inference, yet its memory demands scale poorly with sequence length, posing a major challenge for scalable deployment of large language models. Among several approaches to KV cache compression, quantization of key and value activations has been widely explored. Most KV cache quantization methods still need to manage sparse and noncontiguous outliers separately. To address this, we introduce TaDA, a training-free recipe for KV cache compression with quantization precision that adapts to error sensitivity across layers and a mean centering to eliminate separate outlier handling. Our approach yields substantial accuracy improvements for multiple models supporting various context lengths. Moreover, our approach does not need to separately manage outlier elements -- a persistent hurdle in most traditional quantization methods. Experiments on standard benchmarks demonstrate that our technique reduces KV cache memory footprint to 27% of the original 16-bit baseline while achieving comparable accuracy. Our method paves the way for scalable and high-performance reasoning in language models by potentially enabling inference for longer context length models, reasoning models, and longer chain of thoughts.","变压器模型中的关键值缓存( KV) 是有效解码或推断的一个关键组成部分, 但是其记忆要求的尺度与序列长度不相称, 对大规模语言模型的可缩放部署构成重大挑战。 在对 KV 缓存压缩、键的量化和价值激活的几种方法中, 已经广泛探索了。 大多数 KV 缓存量量化方法仍需要分别管理稀疏和非互不相干的外部离子。 为了解决这个问题, 我们引入了TADA, 为 KV 缓存压缩提供了一种无培训的配方, 配有量化精确度, 适应各层之间的误敏感度, 以及消除不同外部处理的平均值中心 。 我们的方法为支持不同背景长度的多个模型带来大量精确性改进。 此外, 我们的方法不需要单独管理外部元素 -- -- 最传统的量化方法中的一个持久性障碍。 对标准基准的实验表明, 我们的技术将 KV 缓存存储量的足迹足迹降低到原始16 位基线的27%, 同时实现相似的精确度。 我们的方法为语言模型中可缩略和高性推理学铺路。","Vinay Joshi, Pratik Prabhanjan Brahma, Zicheng Liu, Emad Barsoum",2025-06-05T05:23:38Z,TaDA: Training-free recipe for Decoding with Adaptive KV Cache   Compression and Mean-centering,TaDA: Training-freies Rezept zur Dekodierung mit adaptiver KV-Cache-Kompression und Mean-Centering,TaDA:使用适应性 KV 缓存压缩和中度激励解码的无培训食谱,http://arxiv.org/abs/2506.04642v1
148,"Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a powerful paradigm for post-training large language models (LLMs), achieving state-of-the-art performance on tasks with structured, verifiable answers. Applying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but is complicated by the broader, heterogeneous nature of vision-language tasks that demand nuanced visual, logical, and spatial capabilities. As such, training MLLMs using RLVR on multiple datasets could be beneficial but creates challenges with conflicting objectives from interaction among diverse datasets, highlighting the need for optimal dataset mixture strategies to improve generalization and reasoning. We introduce a systematic post-training framework for Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation and benchmark implementation. Specifically, (1) We developed a multimodal RLVR framework for multi-dataset post-training by curating a dataset that contains different verifiable vision-language problems and enabling multi-domain online RL learning with different verifiable rewards; (2) We proposed a data mixture strategy that learns to predict the RL fine-tuning outcome from the data mixture distribution, and consequently optimizes the best mixture. Comprehensive experiments showcase that multi-domain RLVR training, when combined with mixture prediction strategies, can significantly boost MLLM general reasoning capacities. Our best mixture improves the post-trained model's accuracy on out-of-distribution benchmarks by an average of 5.24% compared to the same model post-trained with uniform data mixture, and by a total of 20.74% compared to the pre-finetuning baseline.","以可验证的奖励方法强化学习(RLVR)最近成为了培训后大型语言模型(LLMS)的强大范例,在结构化、可核查的答案上取得最先进的成绩。将RLVR应用到多式LMS(MLLM)带来了重要的机会,但由于视野语言任务范围更广,要求细化的视觉、逻辑和空间能力,因此变得复杂。因此,利用RLVR进行多重数据集培训,对MLLM进行培训可能是有益的,但会带来挑战,因为不同数据集之间互动产生相互冲突的目标,突出表明需要最佳的数据集混合物混合战略改进通用和推理。 我们为多式LLLVR应用了一个系统的系统后培训框架,要求细化的视觉、逻辑和空间能力。 因此,我们开发了一个包含不同可核实的视觉模型的MLVRRMR框架框架, 使得多式RLLF的多式模型能够以不同的可核查的奖励方法进行多式在线学习; (2) 我们提出了一个数据混合混合组合战略,学习如何预测RL的精细化, 将MLMM的模型比值比值比价,然后将MLVMMMM的模型比值比值比值比值比值比值比值的模型,从而将M的模型比值比值比值比值比值的M的MMMMM的模型比值比值比值比值比值比值比值比值比。","Yiqing Liang, Jielin Qiu, Wenhao Ding, Zuxin Liu, James Tompkin, Mengdi Xu, Mengzhou Xia, Zhengzhong Tu, Laixi Shi, Jiacheng Zhu",2025-06-05T05:13:46Z,MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement   Learning,MoDoMoDo: Multi-Domain-Datenmischungen für multimodales LLM-Verstärkungslernen,MoDoMoMoDoDo:多式LLM强化学习多功能数据混合体,http://arxiv.org/abs/2505.24871v2
149,"Audio-Visual Speech Recognition (AVSR) has gained significant attention recently due to its robustness against noise, which often challenges conventional speech recognition systems that rely solely on audio features. Despite this advantage, AVSR models remain limited by the scarcity of extensive datasets, especially for most languages beyond English. Automated data collection offers a promising solution. This work presents a practical approach to generate AVSR datasets from raw video, refining existing techniques for improved efficiency and accessibility. We demonstrate its broad applicability by developing a baseline AVSR model for Vietnamese. Experiments show the automatically collected dataset enables a strong baseline, achieving competitive performance with robust ASR in clean conditions and significantly outperforming them in noisy environments like cocktail parties. This efficient method provides a pathway to expand AVSR to more languages, particularly under-resourced ones.","最近,由于噪音的强烈性,视听语音识别(AVSR)最近受到极大关注,因为噪音往往对仅依赖音频特征的常规语音识别系统构成挑战,尽管有这一优势,但AVSR模型仍然因缺少广泛的数据集而受到限制,特别是除英文以外的大多数语文的数据集。自动化数据收集提供了很有希望的解决办法。这项工作提出了从原始视频生成AVSR数据集的实用方法,完善了提高效率和可获取性的现有技术。我们通过为越南开发一个基线AVSR模型来证明其广泛适用性。实验显示,自动收集的数据集提供了坚实的基线,实现了在清洁条件下与强健的ASR的竞争性性能,并在鸡尾酒派对等吵闹的环境下大大优异于AVSR。这一有效方法为将AVSR扩大到更多语言,特别是资源不足的语言提供了一条途径。","Thai-Binh Nguyen, Thi Van Nguyen, Quoc Truong Do, Chi Mai Luong",2025-06-05T05:13:01Z,ViCocktail: Automated Multi-Modal Data Collection for Vietnamese   Audio-Visual Speech Recognition,ViCocktail: Automatisierte Multi-Modal-Datensammlung für vietnamesische Audio-Visuelle Spracherkennung,Vicocktail:越南视听语音语音识别自动多模式数据收集,http://arxiv.org/abs/2506.04635v1
150,"Lyrics translation requires both accurate semantic transfer and preservation of musical rhythm, syllabic structure, and poetic style. In animated musicals, the challenge intensifies due to alignment with visual and auditory cues. We introduce Multilingual Audio-Video Lyrics Benchmark for Animated Song Translation (MAVL), the first multilingual, multimodal benchmark for singable lyrics translation. By integrating text, audio, and video, MAVL enables richer and more expressive translations than text-only approaches. Building on this, we propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought SylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints to produce natural-sounding lyrics. Experimental results demonstrate that SylAVL-CoT significantly outperforms text-based models in singability and contextual accuracy, emphasizing the value of multimodal, multilingual approaches for lyrics translation.","流言翻译既需要准确的语义传输,也需要保存音乐节奏、音频结构和诗歌风格。在动画音乐中,挑战因与视觉和听觉提示的配合而加剧。我们引入了动画歌曲翻译的多语种音频视频语言基准(MAVL),这是首个多语种、可唱歌词翻译的多语种、多语种基准。通过整合文字、音频和视频,MAVL能够比只使用文字的方法更丰富、更能表达。在此基础上,我们提议与SylAVL-CoT连锁音频视频培训的音频视频LLM(Syable-Constrated-Cyable-Video LL-CoT)一起,利用音频视频提示,并强制实施音频频限制,以制作能听的歌词。实验结果表明,SylAVL-Cot在发音和背景精准性方面大大超越了基于文字的模型,强调多语翻译的多语种和多语种方法的价值。","Woohyun Cho, Youngmin Kim, Sunghyun Lee, Youngjae Yu",2025-06-05T04:48:21Z,MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song   Translation,MAVL: Ein mehrsprachiger Audio-Video-Text Datensatz für animierte Song-Übersetzung,MAVL: 动动歌曲翻译多语种视听歌词数据集,http://arxiv.org/abs/2505.18614v2
151,"Social media platforms have experienced a significant rise in toxic content, including abusive language and discriminatory remarks, presenting growing challenges for content moderation. Some users evade censorship by deliberately disguising toxic words through homophonic cloak, which necessitates the task of unveiling cloaked toxicity. Existing methods are mostly designed for English texts, while Chinese cloaked toxicity unveiling has not been solved yet. To tackle the issue, we propose C$^2$TU, a novel training-free and prompt-free method for Chinese cloaked toxic content unveiling. It first employs substring matching to identify candidate toxic words based on Chinese homo-graph and toxic lexicon. Then it filters those candidates that are non-toxic and corrects cloaks to be their corresponding toxicities. Specifically, we develop two model variants for filtering, which are based on BERT and LLMs, respectively. For LLMs, we address the auto-regressive limitation in computing word occurrence probability and utilize the full semantic contexts of a text sequence to reveal cloaked toxic words. Extensive experiments demonstrate that C$^2$TU can achieve superior performance on two Chinese toxic datasets. In particular, our method outperforms the best competitor by up to 71% on the F1 score and 35% on accuracy, respectively. Our code and data are available at https://github.com/XDxc-cuber/C2TU-Chinese-cloaked-toxicity-unveiling.","社交媒体平台的有毒内容大幅上升,包括滥用语言和歧视性言论,对内容温和提出了越来越多的挑战。一些用户通过故意通过同质斗斗来掩盖有毒词,从而逃避审查,故意通过同质斗袍掩盖有毒词句,从而需要揭开隐蔽毒性。现有方法大多是为英文文本设计的,而中国隐蔽毒性揭幕尚未解决。为了解决这个问题,我们提议为中国隐蔽有毒内容揭幕式推出一种新型的无培训和不及时的无培训方法C$2$TU。它首先使用子字符匹配来根据中国同质词和有毒词典来识别候选的有毒词句。然后,它过滤那些无毒和纠正隐隐蔽的候选词句,使之成为相应的毒物。具体地说,我们开发了两种过滤的模型变式,分别以BERT和LMS为基础。关于LLMS,我们处理计算单词发生概率的自动递减限制,并利用文字序列的全部语义环境来披露隐隐含有毒词。广泛的实验表明C$2TU可以实现中国两个有毒数据集的优异性性性表现。具体地,我们的方法将数据排制到Rexx的Rex。我们的数据比比比比比比比了31。","Xuchen Ma, Jianxiang Yu, Wenming Shao, Bo Pang, Xiang Li",2025-06-05T04:47:25Z,Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone   Graph and Toxic Lexicon,Breaking the Cloak! Enthüllung der chinesischen verhüllten Toxizität mit Homophon Graph und giftigem Lexikon,破解衣物! 中华便衣毒物与同声图和毒毒词汇结合,http://arxiv.org/abs/2505.22184v2
152,"We introduce COMI-LINGUA, the largest manually annotated Hindi-English code-mixed dataset, comprising 125K+ high-quality instances across five core NLP tasks: Matrix Language Identification, Token-level Language Identification, POS Tagging, Named Entity Recognition (NER), and Machine Translation. Each instance is annotated by three bilingual annotators, yielding over 376K expert annotations with strong inter-annotator agreement (Fleiss' Kappa $\geq$ 0.81). The rigorously preprocessed and filtered dataset covers both Devanagari and Roman scripts and spans diverse domains, ensuring real-world linguistic coverage. Evaluation reveals that closed-source LLMs significantly outperform traditional tools and open-source models. Notably, one-shot prompting consistently boosts performance across tasks, especially in structure-sensitive predictions like POS and NER, highlighting the effectiveness of prompt-based adaptation in code-mixed, low-resource settings. COMI-LINGUA is publicly available at: https://github.com/lingo-iitgn/CodeMixing_Project.","我们引入了CCOMI-LingUA,这是最大的人工手动附加说明的印地语-英语编码混合数据集,包括125K+高品质实例,涉及国家语言方案五项核心任务:矩阵语言识别、Token级语言识别、POS拖动、命名实体识别和机器翻译,每个实例都有三个双语说明员的附加说明,产生了376K以上专家说明,并具有强有力的内部顾问协议(Fleiss' Kappa $\geq$ 0.81)。 严格预处理和过滤的数据集涵盖Devanagari和罗马脚本,涵盖不同领域,确保真实世界语言覆盖。评价显示,封闭源的LLOMs大大超越了传统工具和开放源模式。值得注意的是,一枪式地不断促进各项任务的绩效,特别是在对结构敏感的预测中,如POS和NER,强调在代码组合、低资源环境下迅速适应的有效性。COMI-LingUA公开发布在https://github.com/lingo-iign/CodeMix_Protoro)。","Rajvee Sheth, Himanshu Beniwal, Mayank Singh",2025-06-05T04:46:46Z,COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in   Hindi-English Code-Mixing,COMI-LINGUA: Experte kommentierter Großrechner-Datensatz für Multitask NLP in Hindi-Englisch Code-Mixing,COMI-LINLingUA:印地语-英语编码混合多语种国家编码多语种方案专家附加说明的大尺度数据集,http://arxiv.org/abs/2503.21670v2
153,"Empowering large language models (LLMs) with effective tool utilization capabilities is crucial for enabling AI agents to solve complex problems. However, current models face two major limitations: (1) unreliable tool planning and invocation due to low-quality instruction datasets (e.g., widespread hallucinated API calls), and (2) weak tool reflection abilities (over 90% of errors cannot be corrected) resulting from static imitation learning. To address these critical limitations, we propose Tool-MVR, a novel Tool-Augmented LLM that achieves comprehensive System 2 reasoning through two key innovations. Specifically, we first introduce Multi-Agent Meta-Verification (MAMV), a systematic pipeline that rigorously validates APIs, queries, and reasoning trajectories to construct ToolBench-V, a new high-quality instruction dataset that addresses the limitation of unreliable tool planning and invocation. Second, we propose Exploration-based Reflection Learning (EXPLORE), which enhances tool reflection capabilities by leveraging tool feedback through a dynamic ""Error -> Reflection -> Correction"" learning paradigm, resulting in our reflection dataset ToolBench-R and addressing the critical weakness in tool reflection. Finally, we obtain Tool-MVR by finetuning open-source LLMs (e.g., Qwen-7B) on both ToolBench-V and ToolBench-R. Our experiments demonstrate that Tool-MVR achieves state-of-the-art performance on StableToolBench, surpassing both ToolLLM (by 23.9%) and GPT-4 (by 15.3%) while reducing API calls by 31.4%, with strong generalization capabilities across unseen tools and scenarios. Additionally, on our proposed RefineToolBench, the first benchmark specifically designed to evaluate tool reflection capabilities, Tool-MVR achieves a 58.9% error correction rate, significantly outperforming ToolLLM's 9.1%.","增强拥有有效工具使用能力的大型语言模型(LLMS)的权能是使AI代理商能够解决复杂问题的关键。然而,目前的模型面临两大限制:(1) 由于低质量的教学数据集(例如,大范围幻觉API呼叫),工具规划和调用不可靠,以及(2) 静态模拟学习导致的工具反射能力(超过90%的错误无法纠正)薄弱。为了解决这些关键的局限性,我们提议Tol-MVR,一个创新工具推介LMMLM,通过两个关键创新实现系统2的全面推理。具体地说,我们首先引入多正值的MMMV(MAMV),这是一个系统化的管道,该管道严格验证API、查询和推理的推理,以构建工具Ben-BVVVVV, 新的高质量教学数据集,解决不可靠工具规划和调用工具的局限性。我们提议基于探索的“Expal-Result Refile” ,通过动态的“Error-Ref-Remail -校正”学习模型,导致我们反思的图像数据库数据库数据库-Retty-R-Ris-Rin-Rest-reval-lation-lax-lax-lax-lax-lax-lax-lax-laxx-laxxxxxxxxxxxxxxxxxxxxx","Zhiyuan Ma, Jiayu Liu, Xianzhen Luo, Zhenya Huang, Qingfu Zhu, Wanxiang Che",2025-06-05T04:35:49Z,Advancing Tool-Augmented Large Language Models via Meta-Verification and   Reflection Learning,Advancing Tool-Augmented Large Language Models durch Meta-Verifikation und Reflexion Lernen,通过元核查和反省学习促进工具强化大语言模式,http://arxiv.org/abs/2506.04625v1
154,"We propose new static word embeddings optimised for sentence semantic representation. We first extract word embeddings from a pre-trained Sentence Transformer, and improve them with sentence-level principal component analysis, followed by either knowledge distillation or contrastive learning. During inference, we represent sentences by simply averaging word embeddings, which requires little computational cost. We evaluate models on both monolingual and cross-lingual tasks and show that our model substantially outperforms existing static models on sentence semantic tasks, and even rivals a basic Sentence Transformer model (SimCSE) on some data sets. Lastly, we perform a variety of analyses and show that our method successfully removes word embedding components that are irrelevant to sentence semantics, and adjusts the vector norms based on the influence of words on sentence semantics.","我们为判决语义表达提出新的静态词嵌入优化。 我们首先从受过训练的句子变换器中提取词嵌入,然后通过判决一级主要组成部分分析加以改进,然后是知识蒸馏或对比性学习。 在推断中,我们通过简单的平均字嵌入来代表句子,这需要很少计算成本。 我们评估了单语和跨语言任务的模式,并表明我们的模型大大优于现有的句子语义任务固定模式,甚至在某些数据集中与基本的句子变换器模型(SIMCSE)相对应。 最后,我们进行了各种分析,并表明我们的方法成功地删除了与判决语义表达法无关的词嵌入部分,并根据词句语对语义语义的影响调整了矢量规范。","Takashi Wada, Yuki Hirakawa, Ryotaro Shimizu, Takahiro Kawashima, Yuki Saito",2025-06-05T04:33:10Z,Static Word Embeddings for Sentence Semantic Representation,Statische Wort-Einbettungen für Satz semantische Darstellung,判决语义代表的静态单词嵌入,http://arxiv.org/abs/2506.04624v1
155,"Existing studies of innovation emphasize the power of social structures to shape innovation capacity. Emerging machine learning approaches, however, enable us to model innovators' personal perspectives and interpersonal innovation opportunities as a function of their prior trajectories of experience. We theorize then quantify subjective perspectives and innovation opportunities based on innovator positions within the geometric space of concepts inscribed by dynamic language representations. Using data on millions of scientists, inventors, writers, entrepreneurs, and Wikipedia contributors across the creative domains of science, technology, film, entrepreneurship, and Wikipedia, here we show that measured subjective perspectives anticipate what ideas individuals and groups creatively attend to and successfully combine in future. When perspective and background diversity are decomposed as the angular difference between collaborators' perspectives on their creation and between their experiences, the former consistently anticipates creative achievement while the latter portends its opposite, across all cases and time periods examined. We analyze a natural experiment and simulate creative collaborations between AI (large language model) agents designed with various perspective and background diversity, which are consistent with our observational findings. We explore mechanisms underlying these findings and identify how successful collaborators leverage common language to weave together diverse experience obtained through trajectories of prior work that converge to provoke one another and innovate. We explore the importance of these findings for team assembly and research policy.","创新的现有研究强调社会结构影响创新能力的力量。然而,新兴的机器学习方法让我们能够根据创新者个人观点和人际创新机会的先前经验轨迹来模拟创新者的个人观点和人际创新机会。我们随后根据动态语言代表所描述的概念的几何空间中的创新者位置来量化主观观点和创新机会。我们利用数百万科学家、发明者、作家、作家、企业家和维基百科贡献者的数据,利用科学、技术、电影、创业和维基百科等创新领域的科学、技术、电影、创业和维基百科等创新领域的计量主观观点,我们在这里显示,衡量的主观观点预示着个人和团体未来创造性地参与并成功结合的理念。当观点和背景多样性随着合作者对其创建的观点和经验的三角差异而分解时,前者始终期待创造性成就,而后者则在所审查的所有案例和时段期间,截然相反。我们分析了与我们观察发现的不同视角和背景多样性相符的AI(大语言模式)代理者之间的自然实验和模拟创造性合作。我们探索这些发现的基本机制,并查明成功的合作者如何利用一种共同语言来共同探索之前的研究成果。","Likun Cao, Rui Pan, James Evans",2025-06-05T04:18:53Z,Subjective Perspectives within Learned Representations Predict   High-Impact Innovation,Subjektive Perspektiven in erfahrenen Vertretungen prognostizieren High-Impact Innovation,学术界代表的主观观点 预测高影响创新,http://arxiv.org/abs/2506.04616v1
156,"Test-Time Scaling (TTS) improves the reasoning performance of Large Language Models (LLMs) by allocating additional compute during inference. We conduct a structured survey of TTS methods and categorize them into sampling-based, search-based, and trajectory optimization strategies. We observe that reasoning-optimized models often produce less diverse outputs, which limits TTS effectiveness. To address this, we propose ADAPT (A Diversity Aware Prefix fine-Tuning), a lightweight method that applies prefix tuning with a diversity-focused data strategy. Experiments on mathematical reasoning tasks show that ADAPT reaches 80% accuracy using eight times less compute than strong baselines. Our findings highlight the essential role of generative diversity in maximizing TTS effectiveness.","测试-时间缩放(TTS)通过在推论期间分配额外的计算来改进大语言模型(LLMs)的推理性能。我们对TTS方法进行结构化调查,并将其分为基于抽样、基于搜索和轨道优化战略。我们认为,推理优化模型往往产生不同的结果,这限制了TTS的有效性。为了解决这个问题,我们建议采用ADPT(多样化意识前缀微调)这一轻量级方法,与注重多样性的数据战略进行前缀调整。关于数学推理任务的实验显示,ADPT使用比强基线低八倍的计算,达到80%的精度。我们的调查结果强调了基因多样性在最大限度地提高TTS有效性方面的关键作用。","Ho-Lam Chung, Teng-Yun Hsiao, Hsiao-Ying Huang, Chunerh Cho, Jian-Ren Lin, Zhang Ziwei, Yun-Nung Chen",2025-06-05T04:02:17Z,Revisiting Test-Time Scaling: A Survey and a Diversity-Aware Method for   Efficient Reasoning,Überprüfung von Test-Time Scaling: Eine Umfrage und eine vielfaltsbewusste Methode für effizientes Reasoning,重新审视试验时间尺度:调查以及有效说明理由的多样化软件方法,http://arxiv.org/abs/2506.04611v1
157,"Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly impacted various fields by enabling high-quality synthetic data generation and reducing dependence on expensive human-generated datasets. Despite this, challenges remain in the areas of generalization, controllability, diversity, and truthfulness within the existing generative frameworks. To address these challenges, this paper presents DataGen, a comprehensive LLM-powered framework designed to produce diverse, accurate, and highly controllable datasets. DataGen is adaptable, supporting all types of text datasets and enhancing the generative process through innovative mechanisms. To augment data diversity, DataGen incorporates an attribute-guided generation module and a group checking feature. For accuracy, it employs a code-based mathematical assessment for label verification alongside a retrieval-augmented generation technique for factual validation. The framework also allows for user-specified constraints, enabling customization of the data generation process to suit particular requirements. Extensive experiments demonstrate the superior quality of data generated by DataGen, and each module within DataGen plays a critical role in this enhancement. Additionally, DataGen is applied in two practical scenarios: benchmarking LLMs and data augmentation. The results indicate that DataGen effectively supports dynamic and evolving benchmarking and that data augmentation improves LLM capabilities in various domains, including agent-oriented abilities and reasoning skills.","GPT-4和Llama3等大型语言模型(LLMS)通过促进高质量的合成数据生成和减少对昂贵的人类产生的数据集的依赖,对各个领域产生了重大影响。尽管如此,在现有基因化框架内,在通用、可控性、多样性和真实性方面仍然存在挑战。为应对这些挑战,本文件介绍了DataGen,这是一个全面的LLM驱动框架,旨在产生多样、准确和高度可控的数据集。DataGen具有适应性,支持所有类型的文本数据集,并通过创新机制加强基因化进程。为了增强数据多样性,DataGen采用了一个属性引导生成模块和一个小组检查功能。为了准确性,DataGen采用了基于代码的标签核查数学评估,同时采用检索和推荐生成技术进行事实验证。框架还允许用户设置限制,使数据生成过程能够根据特定要求进行定制。广泛的实验表明DataGen产生的数据质量优异,而且DataGen内部的每个模块在这一改进过程中发挥着关键作用。此外,DataGen还采用了一个属性制导模模块,用以有效支持以动态为主的模型和升级的模型。","Yue Huang, Siyuan Wu, Chujie Gao, Dongping Chen, Qihui Zhang, Yao Wan, Tianyi Zhou, Xiangliang Zhang, Jianfeng Gao, Chaowei Xiao, Lichao Sun",2025-06-05T03:59:14Z,DataGen: Unified Synthetic Dataset Generation via Large Language Models,DataGen: Unified Synthetic Dataset Generation über große Sprachmodelle,DataGen:通过大语言模型生成统一合成数据集,http://arxiv.org/abs/2406.18966v4
158,"Scientific Natural Language Inference (NLI) is the task of predicting the semantic relation between a pair of sentences extracted from research articles. Existing datasets for this task are derived from various computer science (CS) domains, whereas non-CS domains are completely ignored. In this paper, we introduce a novel evaluation benchmark for scientific NLI, called MISMATCHED. The new MISMATCHED benchmark covers three non-CS domains-PSYCHOLOGY, ENGINEERING, and PUBLIC HEALTH, and contains 2,700 human annotated sentence pairs. We establish strong baselines on MISMATCHED using both Pre-trained Small Language Models (SLMs) and Large Language Models (LLMs). Our best performing baseline shows a Macro F1 of only 78.17% illustrating the substantial headroom for future improvements. In addition to introducing the MISMATCHED benchmark, we show that incorporating sentence pairs having an implicit scientific NLI relation between them in model training improves their performance on scientific NLI. We make our dataset and code publicly available on GitHub.","自然科学语言推断(NLI)是预测从研究文章中提取的一对句子之间的语义关系的任务。 用于这项任务的现有数据集来自各种计算机科学领域, 而非CS领域则完全被忽略。 在本文中,我们为科学的NLI引入了一个新的新的评估基准,称为MISMACHTED。 新的MISMACHED基准涵盖三个非CS域- PSYCHOLOGY、ENGINEENGEING和公共卫生,并包含2,700对人附加说明的句子。 我们使用预先培训的小型语言模型和大语言模型(LLLMS)为MIMACHED建立了强有力的基线。 我们的最佳基准显示只有78. 17%的宏F1显示今后需要改进的大型会议室。 除了引入MISMACHED基准外,我们还表明,在示范培训中纳入它们之间有隐含科学上NLI关系的对子,提高了它们在科学NLI的绩效。 我们在GitHub上公开提供我们的数据集和代码。","Firoz Shaik, Mobashir Sadat, Nikita Gautam, Doina Caragea, Cornelia Caragea",2025-06-05T03:40:57Z,A MISMATCHED Benchmark for Scientific Natural Language Inference,Ein MISMATCHED-Benchmark für naturwissenschaftliche Sprachinferenzen,科学自然语言引文基准,http://arxiv.org/abs/2506.04603v1
159,"Code data in large language model (LLM) pretraining is recognized crucial not only for code-related tasks but also for enhancing general intelligence of LLMs. Current open-source LLMs often heavily rely on human effort to produce their code pretraining data, such as employing hand-crafted filtering rules tailored to individual programming languages, or using human-annotated data to train quality filters. However, these approaches are inherently limited in scalability, prone to subjective biases, and costly to extend and maintain across diverse programming languages. To address these challenges, we introduce Seed-Coder, a series of open-source LLMs comprising base, instruct and reasoning models of 8B size, minimizing human involvement in data construction. Our code pretraining data is produced by a model-centric data pipeline, which predominantly leverages LLMs for scoring and filtering code data. The instruct model is further trained via supervised fine-tuning and preference optimization, and the reasoning model leverages Long-Chain-of-Thought (LongCoT) reinforcement learning to improve multi-step code reasoning. Seed-Coder achieves state-of-the-art results among open-source models of similar size and even surpasses some much larger models, demonstrating superior performance in code generation, code completion, code editing, code reasoning, and software engineering tasks.","在大型语言模式(LLM)培训前,人们公认,守则数据不仅对与代码有关的任务至关重要,而且对加强LLM公司的一般情报也至关重要。目前的开放源代码LLM公司常常严重依赖人的努力来编制其代码培训前数据,例如采用适合个人编程语言的手工制作过滤规则,或使用附加说明的数据来培训质量过滤器。然而,这些方法在可扩展性方面本质上是有限的,容易主观偏见,而且推广和维持不同编程语言的费用也很高。为了应对这些挑战,我们引入了种子-Coder公司(Seed-Coder),这是由8B大小的基地、指示和推理模型组成的一系列开放源代码LMSLM公司。我们的代码培训前数据是通过一个以模式为中心的数据管道制作的,主要用于评分和筛选代码数据。指导模型通过监督下的微调和优化以及推理模型(Long-Chain-fought)(Long-Cott公司)来强化学习,以改进多步骤代码推理。甚至Sead-Coder公司在开源代码的模型中实现了一些先进的状态,在更高级的编程、更高级的编程、更高级的编程、更高级的编程、更高级的编程任务中展示。","ByteDance Seed, Yuyu Zhang, Jing Su, Yifan Sun, Chenguang Xi, Xia Xiao, Shen Zheng, Anxiang Zhang, Kaibo Liu, Daoguang Zan, Tao Sun, Jinhua Zhu, Shulin Xin, Dong Huang, Yetao Bai, Lixin Dong, Chao Li, Jianchong Chen, Hanzhi Zhou, Yifan Huang, Guanghan Ning, Xierui Song, Jiaze Chen, Siyao Liu, Kai Shen, Liang Xiang, Yonghui Wu",2025-06-05T03:26:05Z,Seed-Coder: Let the Code Model Curate Data for Itself,Saatgut-Coder: Lassen Sie das Code-Modell Daten für sich selbst kuratieren,种子编码器:让代码模型为它自己计算曲线数据,http://arxiv.org/abs/2506.03524v2
160,"We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess immense reasoning potential inherited from the pre-training stage. With reinforcement learning (RL), these models can improve dramatically on reasoning tasks. Recent studies have shown that even RL on a single problem can unleash these models' reasoning capabilities. However, RL is not only expensive but also unstable. Even one-shot RL requires hundreds of GPU hours. This raises a critical question: Is there a more efficient way to unleash the reasoning potential of these powerful base LLMs? In this work, we demonstrate that Critique Fine-Tuning (CFT) on only one problem can effectively unleash the reasoning potential of LLMs. Our method constructs critique data by collecting diverse model-generated solutions to a single problem and using teacher LLMs to provide detailed critiques. We fine-tune Qwen and Llama family models, ranging from 1.5B to 14B parameters, on the CFT data and observe significant performance gains across diverse reasoning tasks. For example, with just 5 GPU hours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six math benchmarks and 16% on three logic reasoning benchmarks. These results are comparable to or even surpass the results from RL with 20x less compute. Ablation studies reveal the robustness of one-shot CFT across different prompt problems. These results highlight one-shot CFT as a simple, general, and compute-efficient approach to unleashing the reasoning capabilities of modern LLMs.","我们目睹了强大的LLMs,如Quwen-Math, MiMo, 和Phi-4等强大的LLMs, 拥有从培训前阶段继承的巨大推理潜力。 通过强化学习(RL),这些模型可以极大地改善推理任务。 最近的研究显示,即使是单一问题的RL,也可以释放出这些模型推理能力。然而,RL不仅昂贵,而且不稳定。即使一发RL, 也需要几百个GPU小时。这提出了一个至关重要的问题:是否有一种更有效率的方法释放这些强大的基础LMs的推理潜力?在这个工作中,我们证明,只有针对一个问题的Critique Fin-Turning(LF)能够有效地释放LMS的推理潜力。我们的方法构建了批评数据,通过收集各种模型生成的单一问题的RLLL, 并且利用教师LLMMs提供详细的批评能力。我们精细的LLLFL要求从1.5B参数到14B参数,这些FT的数据和观察各种推理任务的显著的业绩收益。例如仅5个GP小时的培训、 Quent-7-M-B-real-real-real-real-hal-hal-lational-less的推理的推理学的推理, 16个比重一个比重一个标准,它们的平均结果显示了15的平均结果,这些20-al-al-al-l-l-lx的平均结果是16一个比标准, 和16一个比的18的18性推算结果。","Yubo Wang, Ping Nie, Kai Zou, Lijun Wu, Wenhu Chen",2025-06-05T03:25:20Z,Unleashing the Reasoning Potential of Pre-trained LLMs by Critique   Fine-Tuning on One Problem,Lösen des vernünftigen Potenzials von vortrainierten LLMs durch Kritik Feinsteuerung auf ein Problem,"通过Critique对一个问题的微调,释放预先培训的LLMs的理据潜力",http://arxiv.org/abs/2506.03295v2
161,"Recently, knowledge editing (KE) has emerged as a promising approach to update specific facts in Large Language Models (LLMs) without the need for full retraining. Despite the effectiveness in general-domain benchmarks, their applicability to complex medical domain remains largely unexplored. Medical knowledge editing is particularly challenging, as it requires LLMs to internalize the knowledge and generalize to unseen scenarios for effective and interpretable decision-making. In this work, we propose a novel framework called MedEditBench to rigorously evaluate the effectiveness of existing KE methods in the medical domain. In MedEditBench, we introduce a new medical knowledge editing benchmark as well as three different knowledge editing paradigms, which are designed to assess the impact of different knowledge sources for editing. Our findings indicate that current KE methods result in only superficial memorization of the injected information, failing to generalize to new scenarios. To overcome this limitation, we present Self-Generated Rationale Editing (SGR-Edit), which utilizes model-derived rationales as the target knowledge for editing, thereby uncovering the underlying reasoning process and demonstrating significant improvements over existing KE approaches. Additionally, we offer deeper insights into medical knowledge editing, including the localization of medical knowledge in LLMs and the impact of sequential editing on evolving knowledge. This could provide practical guidance for implementing KE methods in real-world medical applications.","最近,知识编辑(KE)在无需全面再培训的情况下成为更新大语言模型中具体事实的有希望的方法。尽管一般领域基准具有效力,但是这些基准对复杂医疗领域的适用性在很大程度上尚未探索。医学知识编辑特别具有挑战性,因为它要求LMS将知识内部化,并概括为有效和可解释决策的无形情景。在这项工作中,我们提出了一个名为MededitBench的新框架,以严格评价医学领域现有KE方法的有效性。在MedEditBench中,我们引入了一个新的医学知识编辑基准以及三个不同的知识编辑模式,旨在评估不同知识来源对编辑的影响。我们的研究结果表明,目前KE方法只能使注入的信息产生肤浅的记忆,无法概括到新的情景。为了克服这一局限性,我们介绍了一个自尊的医学编辑(SGR-Edit)新框架,它利用模型推理原理作为编辑的目标知识,从而发现基本推论过程,并展示对现有KE系统实际编辑应用医学知识的显著改进。我们可以更深入地了解KE公司在实际编辑中的医学知识。","Shigeng Chen, Linhao Luo, Zhangchi Qiu, Yanan Cao, Carl Yang, Shirui Pan",2025-06-05T03:20:15Z,Beyond Memorization: A Rigorous Evaluation Framework for Medical   Knowledge Editing,Beyond Memorization: Ein strenger Evaluationsrahmen für medizinisches Knowledge Editing,记忆之后:医学知识编辑的严格评价框架,http://arxiv.org/abs/2506.03490v2
162,"Chain-of-Thought (CoT) prompting has become the de facto method to elicit reasoning capabilities from large language models (LLMs). However, to mitigate hallucinations in CoT that are notoriously difficult to detect, current methods such as process reward models (PRMs) or self-consistency operate as opaque boxes and do not provide checkable evidence for their judgments, possibly limiting their effectiveness. To address this issue, we draw inspiration from the idea that ""the gold standard for supporting a mathematical claim is to provide a proof"". We propose a retrospective, step-aware formal verification framework $Safe$. Rather than assigning arbitrary scores, we strive to articulate mathematical claims in formal mathematical language Lean 4 at each reasoning step and provide formal proofs to identify hallucinations. We evaluate our framework $Safe$ across multiple language models and various mathematical datasets, demonstrating a significant performance improvement while offering interpretable and verifiable evidence. We also propose $FormalStep$ as a benchmark for step correctness theorem proving with $30,809$ formal statements. To the best of our knowledge, our work represents the first endeavor to utilize formal mathematical language Lean 4 for verifying natural language content generated by LLMs, aligning with the reason why formal mathematical languages were created in the first place: to provide a robust foundation for hallucination-prone human-written proofs.","我们从“支持数学索赔的金本位标准是提供证据”这一想法中得到启发。我们提出了一个追溯性的、逐步觉悟的正式核查框架,我们建议用30,809美元的正式声明来证明它是否正确。我们最擅长的是,我们的工作不是任意分分,而是在每一个推理步骤上用正式数学语言 "" Lean 4 "" 阐明数学主张,并提供正式证明,以找出幻觉。我们评估我们的框架,在多种语言模型和各种数学数据集中,花费$Safe美元,表明在提供可解释和可核实的证据的同时,业绩有显著改进。我们还提出 "" 美元支持数学索赔的金本位标准是提供证据 "" 。我们还提议用30,809美元的正式声明作为逐步纠正理论标准。我们的工作不是随意分配分分数,而是努力在每一个推理步骤上用正式数学语言 "" Lean 4 "" 阐明数学名词的数学名词,我们首先努力利用正式数学本来验证。","Chengwu Liu, Ye Yuan, Yichun Yin, Yan Xu, Xin Xu, Zaoyu Chen, Yasheng Wang, Lifeng Shang, Qun Liu, Ming Zhang",2025-06-05T03:16:08Z,Safe: Enhancing Mathematical Reasoning in Large Language Models via   Retrospective Step-aware Formal Verification,Sicher: Mathematische Reasoning in großen Sprachmodellen durch retrospektive stufenweise Verifizierung verbessern,"安全:通过回溯性逐步认识正式核查,加强大语言模型中的数学理由",http://arxiv.org/abs/2506.04592v1
163,"We introduce LESS (Large Language Model Enhanced Semi-supervised Learning), a versatile framework that leverages Large Language Models (LLMs) to correct pseudo labels generated from in-the-wild data. Within the LESS framework, pseudo-labeled text from Automatic Speech Recognition (ASR) or Automatic Speech Translation (AST) of the unsupervised data is refined by an LLM, and augmented by a data filtering strategy to optimize LLM knowledge transfer efficiency. Experiments on both Mandarin ASR and Spanish-to-English AST tasks show that LESS achieves a notable absolute WER reduction of 3.77% on the Wenet Speech test set, as well as BLEU scores of 34.0 and 64.7 on Callhome and Fisher test sets respectively. These results validate the adaptability of LESS across different languages, tasks, and domains. Ablation studies conducted with various LLMs and prompt configurations provide novel insights into leveraging LLM-derived knowledge for speech processing applications.","我们引入了LESS(通用语言模型增强半监督学习)这一多功能框架,利用大语言模型(LLMs)校正由网上数据产生的假标签。在LESS框架内,无人监督数据的自动语音识别(ASR)或自动语音翻译(AST)的假标签文本由一个LLM进行改进,并辅之以数据过滤战略,以优化LLM知识转让效率。在普通话ASR和西班牙语对英语AST任务上进行的实验表明,LESS在Wnet语音测试组中显著实现了3.77%的绝对WER减幅,在Callhome和渔业测试组中分别实现了34.0和64.7的BLEU分数。这些结果验证了LESS在不同语言、任务和领域的适应性。与各种LLMs和迅速配置进行的吸收研究为语音处理应用利用LEM派生知识提供了新的见解。","Wen Ding, Fan Qian",2025-06-05T03:00:04Z,LESS: Large Language Model Enhanced Semi-Supervised Learning for Speech   Foundational Models,LESS: Großes Sprachmodell Verbessertes semi-überwachtes Lernen für Sprachgrundmodelle,LESS:大语言模式强化半监督半监督学习演讲基础模型,http://arxiv.org/abs/2506.04586v1
164,"Assessing scientific claims requires identifying, extracting, and reasoning with multimodal data expressed in information-rich figures in scientific literature. Despite the large body of work in scientific QA, figure captioning, and other multimodal reasoning tasks over chart-based data, there are no readily usable multimodal benchmarks that directly test claim verification abilities. To remedy this gap, we introduce a new benchmark MuSciClaims accompanied by diagnostics tasks. We automatically extract supported claims from scientific articles, which we manually perturb to produce contradicted claims. The perturbations are designed to test for a specific set of claim verification capabilities. We also introduce a suite of diagnostic tasks that help understand model failures. Our results show most vision-language models are poor (~0.3-0.5 F1), with even the best model only achieving 0.77 F1. They are also biased towards judging claims as supported, likely misunderstanding nuanced perturbations within the claims. Our diagnostics show models are bad at localizing correct evidence within figures, struggle with aggregating information across modalities, and often fail to understand basic components of the figure.","评估科学主张需要确定、提取和推理科学文献中信息丰富数字中表述的多式数据。尽管科学质量评估、图表说明和其他基于图表的数据的多式联运推理任务涉及大量工作,但没有直接测试核实能力的现用多式基准;为弥补这一差距,我们引入了新的基准MuSci要求,并辅以诊断任务。我们自动从科学文章中提取支持性主张,我们人工干扰这些主张,以产生自相矛盾的主张。扰动是为了测试一套具体的索赔核实能力。我们还引入了一套有助于理解模型失败的诊断任务。我们的结果显示,大多数愿景语言模型都很差(~0.3-0.5 F1),即使最佳模型也只能达到0.77 F1。它们也偏向于判断索赔所支持的索赔,可能存在误解,对索赔中的扰动进行细微分化。我们的诊断显示模型不利于在数字中将正确证据本地化,与各种方式的信息拼凑,而且往往无法理解数字的基本组成部分。","Yash Kumar Lal, Manikanta Bandham, Mohammad Saqib Hasan, Apoorva Kashi, Mahnaz Koupaee, Niranjan Balasubramanian",2025-06-05T02:59:51Z,MuSciClaims: Multimodal Scientific Claim Verification,MuSciClaims: Multimodale wissenschaftliche Antragsprüfung,穆西索赔: 多式联运科学索赔核实,http://arxiv.org/abs/2506.04585v1
165,"Automatic fact-checking has recently received more attention as a means of combating misinformation. Despite significant advancements, fact-checking systems based on retrieval-augmented language models still struggle to tackle adversarial claims, which are intentionally designed by humans to challenge fact-checking systems. To address these challenges, we propose a training-free method designed to rephrase the original claim, making it easier to locate supporting evidence. Our modular framework, SUCEA, decomposes the task into three steps: 1) Claim Segmentation and Decontextualization that segments adversarial claims into independent sub-claims; 2) Iterative Evidence Retrieval and Claim Editing that iteratively retrieves evidence and edits the subclaim based on the retrieved evidence; 3) Evidence Aggregation and Label Prediction that aggregates all retrieved evidence and predicts the entailment label. Experiments on two challenging fact-checking datasets demonstrate that our framework significantly improves on both retrieval and entailment label accuracy, outperforming four strong claim-decomposition-based baselines.","尽管取得了很大进展,但基于检索强化语言模型的实况调查系统仍难以解决对抗性索赔,而这是人为质疑事实审查系统而有意设计的。为了应对这些挑战,我们提议采用一种不培训的方法来重新表述原始索赔,从而更容易找到支持性证据。我们的模块化框架(SUCEA)将任务分为三个步骤:(1) 将部分对抗性索赔分为索赔部分和分解为独立的子索赔;(2) 重复证据检索和索赔编辑,根据检索的证据对证据进行迭接检索并编辑次级索赔;(3) 证据汇总和拉贝尔预测,将所有检索的证据汇总起来并预测隐含性标签。对两个具有挑战性的事实核对数据集的实验表明,我们的框架在检索和要求标签准确性两方面都大有改进,优于四个基于索赔的强大分解定位基线。","Hongjun Liu, Yilun Zhao, Arman Cohan, Chen Zhao",2025-06-05T02:58:15Z,SUCEA: Reasoning-Intensive Retrieval for Adversarial Fact-checking   through Claim Decomposition and Editing,SUCEA: Reasoning-Intensive Retrieval for Adversarial Fact-Checking durch Forderungszersetzung und Bearbeitung,CUCEA: 通过索赔的分解和编辑进行反向实况调查的理由说明-密集检索,http://arxiv.org/abs/2506.04583v1
166,"In-Context Learning (ICL) empowers Large Language Models (LLMs) for rapid task adaptation without Fine-Tuning (FT), but its reliance on demonstration selection remains a critical challenge. While many-shot ICL shows promising performance through scaled demonstrations, the selection method for many-shot demonstrations remains limited to random selection in existing work. Since the conventional instance-level retrieval is not suitable for many-shot scenarios, we hypothesize that the data requirements for in-context learning and fine-tuning are analogous. To this end, we introduce a novel gradient matching approach that selects demonstrations by aligning fine-tuning gradients between the entire training set of the target task and the selected examples, so as to approach the learning effect on the entire training set within the selected examples. Through gradient matching on relatively small models, e.g., Qwen2.5-3B or Llama3-8B, our method consistently outperforms random selection on larger LLMs from 4-shot to 128-shot scenarios across 9 diverse datasets. For instance, it surpasses random selection by 4% on Qwen2.5-72B and Llama3-70B, and by around 2% on 5 closed-source LLMs. This work unlocks more reliable and effective many-shot ICL, paving the way for its broader application.","文本中学习(ICL) 赋予大语言模型(LLM) 快速适应大任务而无需微调( FT) , 但其对示范选择的依赖仍是一个关键的挑战。 虽然多发的ICL 显示通过规模化演示表现出有希望的绩效, 但多发演示的选择方法仍然局限于现有工作中的随机选择。 由于常规实例级检索不适合于多发情景, 我们假设文本中学习和微调的数据要求是相似的。 为此, 我们引入了新的梯度匹配方法, 通过微调整个目标任务培训组和选定实例之间的梯度来选择演示, 从而在选定实例中处理整个培训组的学习效果。 通过相对较小的模型的梯度匹配, 例如, Qwen2.5-3B 或Llama3-8B, 我们的方法始终超越了在9个不同的数据集中从四发到128发的大型 LLMM 随机选择。 例如, 在 Quen- 2.5B 和 LCLA3- massimal-M 上更可靠的I- massal- massal- massal 工作, 以2 massal- hassal- massal- mass.","Jianfei Zhang, Bei Li, Jun Bai, Rumei Li, Yanmeng Wang, Chenghua Lin, Wenge Rong",2025-06-05T02:57:05Z,Selecting Demonstrations for Many-Shot In-Context Learning via Gradient   Matching,Auswahl von Demonstrationen für das Viel-Schuss-In-Kontext-Lernen über Gradient Matching,通过梯度匹配选择多片点在文本中学习的演示,http://arxiv.org/abs/2506.04579v1
167,"Large multimodal models (LMMs) often struggle to recognize novel concepts, as they rely on pre-trained knowledge and have limited ability to capture subtle visual details. Domain-specific knowledge gaps in training also make them prone to confusing visually similar, commonly misrepresented, or low-resource concepts. To help LMMs better align nuanced visual features with language, improving their ability to recognize and reason about novel or rare concepts, we propose a Contrastive visual Data Augmentation (CoDA) strategy. CoDA extracts key contrastive textual and visual features of target concepts against the known concepts they are misrecognized as, and then uses multimodal generative models to produce targeted synthetic data. Automatic filtering of extracted features and augmented images is implemented to guarantee their quality, as verified by human annotators. We show the effectiveness and efficiency of CoDA on low-resource concept and diverse scene recognition datasets including INaturalist and SUN. We additionally collect NovelSpecies, a benchmark dataset consisting of newly discovered animal species that are guaranteed to be unseen by LMMs. LLaVA-1.6 1-shot updating results on these three datasets show CoDA significantly improves SOTA visual data augmentation strategies by 12.3% (NovelSpecies), 5.1% (SUN), and 6.0% (iNat) absolute gains in accuracy.","大型多式联运模式(LMMs)往往难以认识新概念,因为它们依赖经过事先培训的知识,而且难以捕捉微妙的视觉细节。培训中的具体知识差距也使其容易混淆相近、通常不实或低资源概念。为了帮助LMMs更好地将细微的视觉特征与语言统一起来,提高他们认识新概念或稀有概念的能力和理解新概念的能力,我们提议了一个对比视觉数据强化战略。 CoDA提取了目标概念的关键对比性文字和视觉特征,与它们被误认的已知概念相对照,然后使用多式联运基因化模型来生成有针对性的合成数据。通过人类说明者核实,对提取的特征和增强的图像进行自动过滤,以保证其质量。我们展示了CDA对低资源概念和不同场识别数据集(包括饱和列表和SUN)的有效性和效率。我们还收集了NevilSpecies,这是由新发现的动物物种组成的基准数据集,保证LMMMs将不为人知,LAVA-1.6-1-sh更新结果。","Yu Zhou, Bingxuan Li, Mohan Tang, Xiaomeng Jin, Te-Lin Wu, Kuan-Hao Huang, Heng Ji, Kai-Wei Chang, Nanyun Peng",2025-06-05T02:50:14Z,Contrastive Visual Data Augmentation,Kontrastive Bilddatenvergrößerung,对比视觉数据增强,http://arxiv.org/abs/2502.17709v2
168,"Neuro-symbolic approaches combining large language models (LLMs) with solvers excels in logical reasoning problems need long reasoning chains. In this paradigm, LLMs serve as translators, converting natural language reasoning problems into formal logic formulas. Then reliable symbolic solvers return correct solutions. Despite their success, we find that LLMs, as translators, struggle to handle lexical diversification, a common linguistic phenomenon, indicating that LLMs as logic translators are unreliable in real-world scenarios. Moreover, existing logical reasoning benchmarks lack lexical diversity, failing to challenge LLMs' ability to translate such text and thus obscuring this issue. In this work, we propose SCALe, a benchmark designed to address this significant gap through **logic-invariant lexical diversification**. By using LLMs to transform original benchmark datasets into lexically diversified but logically equivalent versions, we evaluate LLMs' ability to consistently map diverse expressions to uniform logical symbols on these new datasets. Experiments using SCALe further confirm that current LLMs exhibit deficiencies in this capability. Building directly on the deficiencies identified through our benchmark, we propose a new method, MenTaL, to address this limitation. This method guides LLMs to first construct a table unifying diverse expressions before performing translation. Applying MenTaL through in-context learning and supervised fine-tuning (SFT) significantly improves the performance of LLM translators on lexically diversified text. Our code is now available at https://github.com/wufeiwuwoshihua/LexicalDiver.","将大型语言模型(LLMS)与解决方案在逻辑推理上十分优秀的解决方案结合起来的Neuro-symbolic 方法,将大型语言模型(LLMS)与解决方案在逻辑推理上存在优异的问题,需要长期的推理链链。在这个模型中,LLMS充当翻译,将自然语言推理问题转换成正式的逻辑公式。然后可靠的象征性解决者返回正确的解决方案。尽管LULMS成功,但我们发现LLMS作为翻译者,很难处理法律多样化的常见语言现象,表明LLMS作为逻辑翻译者在现实世界情景中不可靠。此外,现有的逻辑推理基准缺乏逻辑多样性,无法挑战LLMS翻译者翻译这些文本的翻译能力,从而掩盖了这一问题。在这个模型中,我们提议了一个旨在通过**log-in-inexlical 多样化的分类多样化的翻译系统来弥补这一重大差距的基准。我们提出了一个新的方法,MenTal-LLL-LLLL 将业绩调整成一个新的表格。","Qingchuan Li, Jiatong Li, Zirui Liu, Mingyue Cheng, Yuting Zeng, Qi Liu, Tongxuan Liu",2025-06-05T02:49:36Z,Are LLMs Reliable Translators of Logical Reasoning Across Lexically   Diversified Contexts?,Sind LLMs zuverlässige Übersetzer der logischen Vernunft in lexikalisch diversifizierten Kontexten?,LLMs 可靠翻译者是否掌握了跨越多种不同背景的逻辑理由?,http://arxiv.org/abs/2506.04575v1
169,"We investigate the effectiveness of large language models (LLMs), including reasoning-based and non-reasoning models, in performing zero-shot financial sentiment analysis. Using the Financial PhraseBank dataset annotated by domain experts, we evaluate how various LLMs and prompting strategies align with human-labeled sentiment in a financial context. We compare three proprietary LLMs (GPT-4o, GPT-4.1, o3-mini) under different prompting paradigms that simulate System 1 (fast and intuitive) or System 2 (slow and deliberate) thinking and benchmark them against two smaller models (FinBERT-Prosus, FinBERT-Tone) fine-tuned on financial sentiment analysis. Our findings suggest that reasoning, either through prompting or inherent model design, does not improve performance on this task. Surprisingly, the most accurate and human-aligned combination of model and method was GPT-4o without any Chain-of-Thought (CoT) prompting. We further explore how performance is impacted by linguistic complexity and annotation agreement levels, uncovering that reasoning may introduce overthinking, leading to suboptimal predictions. This suggests that for financial sentiment classification, fast, intuitive ""System 1""-like thinking aligns more closely with human judgment compared to ""System 2""-style slower, deliberative reasoning simulated by reasoning models or CoT prompting. Our results challenge the default assumption that more reasoning always leads to better LLM decisions, particularly in high-stakes financial applications.","我们调查了大型语言模型(LLMS),包括基于推理和非理性模型,在进行零点金融情绪分析时的有效性。我们使用域专家附加注释的FinBERT-Prosus、FinBERT-Tone等金融情绪分析小模型,我们评估了各种LLMS和推动战略如何在金融背景下与人类标签情绪相一致。我们比较了三种专有LMS(GPT-4o、GPT-4.1、o3-min)在模拟系统1(快速和直观)或系统2(快速和深思熟虑)或系统2(低度和深思熟虑)思维模式下。我们始终在两种小模型(FinBERT-Prosus、FinBERT-Tone)应用中对它们进行测试,并用两个小模型进行精确的金融情绪分析。我们发现这种推理可能引入过度思考,或者通过内在模型进行更精确的逻辑推理,从而推理,从而推算更精确地推算。","Dimitris Vamvourellis, Dhagash Mehta",2025-06-05T02:47:23Z,Reasoning or Overthinking: Evaluating Large Language Models on Financial   Sentiment Analysis,Reasoning or Overthinking: Bewertung großer Sprachmodelle zur Analyse von Finanzsentimenten,理由或过度思考:评价关于金融敏感分析的大型语言模型,http://arxiv.org/abs/2506.04574v1
170,"Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, code generation, and complex planning. Simultaneously, Multi-Agent Systems (MAS) have garnered attention for their potential to enable cooperation among distributed agents. However, from a multi-party perspective, MAS could be vulnerable to malicious agents that exploit the system to serve self-interests without disrupting its core functionality. This work explores integrity attacks where malicious agents employ subtle prompt manipulation to bias MAS operations and gain various benefits. Four types of attacks are examined: \textit{Scapegoater}, who misleads the system monitor to underestimate other agents' contributions; \textit{Boaster}, who misleads the system monitor to overestimate their own performance; \textit{Self-Dealer}, who manipulates other agents to adopt certain tools; and \textit{Free-Rider}, who hands off its own task to others. We demonstrate that strategically crafted prompts can introduce systematic biases in MAS behavior and executable instructions, enabling malicious agents to effectively mislead evaluation systems and manipulate collaborative agents. Furthermore, our attacks can bypass advanced LLM-based monitors, such as GPT-4o-mini and o3-mini, highlighting the limitations of current detection mechanisms. Our findings underscore the critical need for MAS architectures with robust security protocols and content validation mechanisms, alongside monitoring systems capable of comprehensive risk scenario assessment.","大型语言模型(LLMS)展示了自然语言理解、代码生成和复杂规划方面的非凡能力。与此同时,多机构系统(MAS)吸引了人们对其潜力的关注,以促成分布剂之间的合作。然而,从多党的角度来看,MAS可能易受恶意代理人的伤害,这些恶意代理人利用该系统为自己的利益服务,而同时又不干扰其核心功能。这项工作探索恶意代理人利用微妙的迅速操纵来损害MAS业务并获得各种好处的廉正攻击。我们研究了四种类型的攻击:Textit{Scapegoater},他们误导系统监测员低估了其他代理人的贡献;\textit{Boaster},他们误导系统监测员高估了自己的业绩;\textit{自毁},他们利用了恶意代理人为自身利益服务,而不会干扰其核心功能。 这项工作探索了恶意代理人对MAS行为和可执行指令的系统性偏见,使恶意代理人能够有效地误导评价系统,并操纵协作剂。此外,我们的攻击 — 快速地监控了GPLMS的系统。","Can Zheng, Yuhan Cao, Xiaoning Dong, Tianxing He",2025-06-05T02:44:49Z,Demonstrations of Integrity Attacks in Multi-Agent Systems,Demonstrationen von Integritätsangriffen in Multi-Agent-Systemen,多机构系统中廉正攻击示范,http://arxiv.org/abs/2506.04572v1
171,"Direct Preference Optimization (DPO) has become a prominent method for aligning Large Language Models (LLMs) with human preferences. While DPO has enabled significant progress in aligning English LLMs, multilingual preference alignment is hampered by data scarcity. To address this, we propose a novel approach that $\textit{captures}$ learned preferences from well-aligned English models by implicit rewards and $\textit{transfers}$ them to other languages through iterative training. Specifically, we derive an implicit reward model from the logits of an English DPO-aligned model and its corresponding reference model. This reward model is then leveraged to annotate preference relations in cross-lingual instruction-following pairs, using English instructions to evaluate multilingual responses. The annotated data is subsequently used for multilingual DPO fine-tuning, facilitating preference knowledge transfer from English to other languages. Fine-tuning Llama3 for two iterations resulted in a 12.72% average improvement in Win Rate and a 5.97% increase in Length Control Win Rate across all training languages on the X-AlpacaEval leaderboard. Our findings demonstrate that leveraging existing English-aligned models can enable efficient and effective multilingual preference alignment, significantly reducing the need for extensive multilingual preference data. The code is available at https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding","为解决这一问题,我们建议一种新颖的方法,即$\textit{captures}美元通过隐性奖赏和$\textit{transformation}通过迭接培训,从和睦的英语模式学到了优待,从这种模式中学习到其他语言。具体地说,我们从英国与DPO接轨模式及其相应的参考模式的登录中获取了一个隐含的奖赏模式。然后,利用这一奖励模式,在跨语言制教学配对中发现优待关系,使用英语指令来评价多语言的响应。为此,我们提议一种新颖的方法,即$\textit{captures}美元通过隐性奖赏和$\textit{transpends},从紧密的英语模式中学习到其他语言的优待。对两种语言的Llama3的微调导致Win利率平均提高12.72%,在X-AlpacaEval领导板上的所有培训语言中提高5.97%的伸缩控制赢率。我们的调查结果表明,利用现有的英语-C-ral-lag-lag-laisal be suptional supate suptional supal suplational supulateal supal supulates 需要大量的多语言的多语言标准。","Wen Yang, Junhong Wu, Chen Wang, Chengqing Zong, Jiajun Zhang",2025-06-05T02:40:44Z,Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference   Alignment,Implizite Cross-Lingual-Belohnung für effiziente Mehrsprachigkeitsausrichtung,高效多语种和多种语文首选项统一化的双双优,http://arxiv.org/abs/2503.04647v2
172,"Although value-aligned language models (LMs) appear unbiased in explicit bias evaluations, they often exhibit stereotypes in implicit word association tasks, raising concerns about their fair usage. We investigate the mechanisms behind this discrepancy and find that alignment surprisingly amplifies implicit bias in model outputs. Specifically, we show that aligned LMs, unlike their unaligned counterparts, overlook racial concepts in early internal representations when the context is ambiguous. Not representing race likely fails to activate safety guardrails, leading to unintended biases. Inspired by this insight, we propose a new bias mitigation strategy that works by incentivizing the representation of racial concepts in the early model layers. In contrast to conventional mitigation methods of machine unlearning, our interventions find that steering the model to be more aware of racial concepts effectively mitigates implicit bias. Similar to race blindness in humans, ignoring racial nuances can inadvertently perpetuate subtle biases in LMs.","尽管价值观一致语言模式在明确的偏见评价中似乎没有偏见,但它们往往在隐含的词联任务中表现出陈规定型观念,引起人们对其公平使用的担忧。我们调查了这一差异背后的机制,发现这种一致性令人惊讶地扩大了模型产出的隐含偏见。具体地说,我们表明,与不结盟的语言模式不同,在背景模糊时,在早期内部陈述中忽略了种族概念。在不代表种族的情况下,可能无法启动安全保护装置,导致意外的偏见。受这一见解的启发,我们提出了一种新的减少偏见战略,鼓励在早期模式层面体现种族概念。与传统的机器不学习减缓方法相反,我们的干预措施发现引导模式更多地了解种族概念可以有效减少隐含的偏见。类似人类种族盲目,忽视种族差异可能会无意地延续LM的微妙偏见。","Lihao Sun, Chengzhi Mao, Valentin Hofmann, Xuechunzi Bai",2025-06-05T02:35:16Z,Aligned but Blind: Alignment Increases Implicit Bias by Reducing   Awareness of Race,"Ausgerichtet, aber blind: Ausgerichtetheit erhöht Implizite Bias, indem sie das Bewusstsein für die Rasse verringert","结盟但盲目:通过减少对种族问题的认识,协调会增加隐含的偏见",http://arxiv.org/abs/2506.00253v2
173,"Differentially private (DP) language model inference is an approach for generating private synthetic text. A sensitive input example is used to prompt an off-the-shelf large language model (LLM) to produce a similar example. Multiple examples can be aggregated together to formally satisfy the DP guarantee.   Prior work creates inference batches by sampling sensitive inputs uniformly at random. We show that uniform sampling degrades the quality of privately generated text, especially when the sensitive examples concern heterogeneous topics.   We remedy this problem by clustering the input data before selecting inference batches. Next, we observe that clustering also leads to more similar next-token predictions across inferences. We use this insight to introduce a new algorithm that aggregates next token statistics by privately computing medians instead of averages. This approach leverages the fact that the median has decreased local sensitivity when next token predictions are similar, allowing us to state a data-dependent and ex-post DP guarantee about the privacy properties of this algorithm. Finally, we demonstrate improvements in terms of representativeness metrics (e.g., MAUVE) as well as downstream task performance. We show that our method produces high-quality synthetic data at significantly lower privacy cost than a previous state-of-the-art method.","私用(DP)语言模型不同地(DP)语言模型推论是生成私人合成文本的一种方法。 使用一个敏感的输入实例来促使一个现成的大型语言模型(LLM)产生类似的例子。 多个例子可以合并在一起,正式满足DP的保证。 先前的工作通过统一随机抽样敏感投入而产生推论批量。 我们表明,统一抽样降低了私人生成文本的质量, 特别是在敏感实例涉及不同专题的情况下。 我们通过在选择推论批次之前对输入数据进行分组来解决这个问题。 其次, 我们观察到, 集群还导致在推论中进行更相似的次点预测。 我们利用这一洞见来引入一种新的算法, 通过私人计算中位而不是平均来汇总下一个象征性统计数据。 这种方法利用了以下事实:当下一个象征性预测类似时,中位已经降低了当地敏感度, 允许我们对这一算法的隐私特性作出数据依赖性和事后DP保证。 最后, 我们从代表性指标(例如, MAUVE) 和下游任务性工作表现的改进。 我们证明, 我们的保密性方法在前一个低得多的合成数据中, 。","Kareem Amin, Salman Avestimehr, Sara Babakniya, Alex Bie, Weiwei Kong, Natalia Ponomareva, Umar Syed",2025-06-05T02:34:50Z,Clustering and Median Aggregation Improve Differentially Private   Inference,Clustering und Median Aggregation verbessern unterschiedliche private Schlussfolgerungen,群集和中中聚合改善差别私人推断,http://arxiv.org/abs/2506.04566v1
174,"A plethora of sentence embedding models makes it challenging to choose one, especially for technical domains rich with specialized vocabulary. In this work, we domain adapt embeddings using telecom data for question answering. We evaluate embeddings obtained from publicly available models and their domain-adapted variants, on both point retrieval accuracies, as well as their (95%) confidence intervals. We establish a systematic method to obtain thresholds for similarity scores for different embeddings. As expected, we observe that fine-tuning improves mean bootstrapped accuracies. We also observe that it results in tighter confidence intervals, which further improve when pre-training is preceded by fine-tuning. We introduce metrics which measure the distributional overlaps of top-$K$, correct and random document similarities with the question. Further, we show that these metrics are correlated with retrieval accuracy and similarity thresholds. Recent literature shows conflicting effects of isotropy on retrieval accuracies. Our experiments establish that the isotropy of embeddings (as measured by two independent state-of-the-art isotropy metric definitions) is poorly correlated with retrieval performance. We show that embeddings for domain-specific sentences have little overlap with those for domain-agnostic ones, and fine-tuning moves them further apart. Based on our results, we provide recommendations for use of our methodology and metrics by researchers and practitioners.","过多的徒刑嵌入模型使得选择一个非常困难, 特别是对于具有专门词汇的丰富技术领域来说。 在这项工作中, 我们用电子通讯数据来调整嵌入, 以便解答问题。 我们评估从公开提供的模型及其域适应变异中获得的嵌入, 包括点检索信服, 以及它们的( 95%) 信任间隔。 我们为获得不同嵌入的类似分数的阈值制定了系统的方法。 我们观察到微调改进意味着螺旋藻变色。 我们还观察到, 微调可以导致更紧的信心间隔, 在培训前要进行微调, 并会进一步改进。 我们引入衡量最高- 美元分配重叠的模型及其域适应变异的模型, 与问题相似。 此外, 我们表明, 这些尺度与检索准确性和相似的阈值之间有关系。 我们的文献显示, 异调效果对回收信差效果的影响。 我们的实验证明, 嵌入( 以两个独立状态测量, 当培训前的指数定义有细微调时, ) 会进一步改进。 我们的矩阵定义和域内化结果的嵌入, 提供我们的校略方法。","Sujoy Roychowdhury, Sumit Soman, Ranjani Hosakere Gireesha, Vansh Chhabra, Neeraj Gunda, Subhadip Bandyopadhyay, Sai Krishna Bala",2025-06-05T02:18:14Z,Investigating Distributions of Telecom Adapted Sentence Embeddings for   Document Retrieval,Untersuchung der Verteilung von Telecom-adaptierten Satz-Einbindungen für Dokumentenwiederherstellung,用于文件检索的经调整的远程判刑嵌入表的调查分发情况,http://arxiv.org/abs/2406.12336v3
175,"Evaluating machine translation (MT) quality for under-resourced African languages remains a significant challenge, as existing metrics often suffer from limited language coverage and poor performance in low-resource settings. While recent efforts, such as AfriCOMET, have addressed some of the issues, they are still constrained by small evaluation sets, a lack of publicly available training data tailored to African languages, and inconsistent performance in extremely low-resource scenarios. In this work, we introduce SSA-MTE, a large-scale human-annotated MT evaluation (MTE) dataset covering 13 African language pairs from the News domain, with over 63,000 sentence-level annotations from a diverse set of MT systems. Based on this data, we develop SSA-COMET and SSA-COMET-QE, improved reference-based and reference-free evaluation metrics. We also benchmark prompting-based approaches using state-of-the-art LLMs like GPT-4o and Claude. Our experimental results show that SSA-COMET models significantly outperform AfriCOMET and are competitive with the strongest LLM (Gemini 2.5 Pro) evaluated in our study, particularly on low-resource languages such as Twi, Luo, and Yoruba. All resources are released under open licenses to support future research.","评估资源不足的非洲语言的机器翻译质量仍然是一项重大挑战,因为现有的衡量标准往往因语言覆盖面有限和低资源环境下业绩不佳而受限制,尽管最近的努力,如AfriCOMET(AfriCOMET)等,解决了一些问题,但它们仍然受到小的评价组的限制,缺乏适合非洲语言的可公开获得的培训数据,在极其低资源情景下的表现也不一致。在这项工作中,我们引进了SSA-MTE(SA-MTE),这是一个大规模具有附加说明的人力资源评价(MTE)数据集,涵盖13对来自新闻域的非洲语言,有63 000多份来自多种MT系统的句级说明。根据这些数据,我们开发了SSA-COMET(S-COMET)和SSA-COMET-QE(SSA-COMET-QE),改进了基于参考和无参考的评价指标。我们还采用诸如GPT-4o和Claude等最先进的LMMMLM(Gemini 2.5 Pro)资源,在我们的开放研究中被评估为开放型LUBA所有资源。","Senyu Li, Jiayi Wang, Felermino D. M. A. Ali, Colin Cherry, Daniel Deutsch, Eleftheria Briakou, Rui Sousa-Silva, Henrique Lopes Cardoso, Pontus Stenetorp, David Ifeoluwa Adelani",2025-06-05T02:16:56Z,SSA-COMET: Do LLMs Outperform Learned Metrics in Evaluating MT for   Under-Resourced African Languages?,SSA-COMET: Sind LLMs in der Bewertung von MT für unterresourced African Languages bestens ausgebildete Metrics?,"SSA-COMET:在为资源不足的非洲语言评价MT方面,LLMs是否超过成绩优异的计量?",http://arxiv.org/abs/2506.04557v1
176,"What makes a difference in the post-training of LLMs? We investigate the training patterns of different layers in large language models (LLMs) through the lens of the gradient. We are specifically interested in how fast vs. slow thinking affects the layer-wise gradients, given the recent popularity of training LLMs on reasoning paths such as chain-of-thoughts (CoT) and process rewards. In our study, fast thinking without CoT leads to larger gradients and larger differences of gradients across layers than slow thinking (Detailed CoT), indicating the learning stability brought by the latter. Additionally, we study whether the gradient patterns can reflect the correctness of responses when training different LLMs using slow vs. fast thinking paths. The results show that the gradients of slow thinking can distinguish correct and irrelevant reasoning paths. As a comparison, we conduct similar gradient analyses on non-reasoning knowledge learning tasks, on which, however, trivially increasing the response length does not lead to similar behaviors of slow thinking. Our study strengthens fundamental understandings of LLM training and sheds novel insights on its efficiency and stability, which pave the way towards building a generalizable System-2 agent. Our code, data, and gradient statistics can be found in: https://github.com/MingLiiii/Layer_Gradient.","在LLMS的训练后,有什么区别?我们通过梯度镜头调查大语言模型(LLMS)不同层次的培训模式。我们特别关心的是,由于最近培训LLMS的普及程度,培训LLMS的推理路径,例如思维链和过程奖励。在我们的研究中,没有COT的快速思维导致不同层次的梯度,而不是缓慢思维(详细COT),表明后者带来的学习稳定性。此外,我们研究在使用慢思维路径与快速思维路径培训不同的LLMs时,梯度模式能否反映反应的正确性。结果显示,缓慢思维的梯度可以区分正确和不相关的推理路径。比较时,我们对非理性的学习任务进行类似的梯度分析,但微不足道地增加反应长度不会导致类似缓慢思维的行为。我们的研究加强了LLMM培训的基本理解,并展示了对其效率和稳定性的新认识,为建立通用系统/LAGRA的梯度铺平面图:MLA/GRA 数据可以找到。","Ming Li, Yanhong Li, Tianyi Zhou",2025-06-05T02:01:49Z,What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A   Gradient Perspective,"Was in LLM-Schichten passiert ist, wenn es um schnelles gegen langsames Denken geht: Eine gradiente Perspektive",训练快速与慢思考:渐进视角时 LLM 图层中发生的情况,http://arxiv.org/abs/2410.23743v2
177,"Finetuning large language models with a variety of instruction-response pairs has enhanced their capability to understand and follow instructions. Current instruction tuning primarily relies on teacher models or human intervention to generate and refine the instructions and responses for training, which are costly, non-sustainable, and may lack diversity. In this paper, we introduce Mosaic Instruction Tuning (Mosaic-IT), a human/model-free compositional data synthesis method that can efficiently create rich and diverse augmentations from existing instruction tuning data to enhance the LLMs. Mosaic-IT randomly concatenates multiple instruction data into one and trains the model to produce the corresponding responses with predefined higher-level meta-instructions to strengthen its multi-step instruction-following and format-following skills. Our extensive evaluations demonstrate a superior performance and training efficiency of Mosaic-IT, which achieves consistent performance improvements over various benchmarks and an 80% reduction in training costs compared with original instruction tuning. Our codes and data are available at https://github.com/tianyi-lab/Mosaic-IT.","调整大型语言模式,采用各种教育-反应对口,提高了他们理解和遵守指示的能力。目前的教学调整主要依靠教师模式或人力干预,以产生和完善昂贵、不可持续且可能缺乏多样性的培训指导和对策。在本文中,我们引入了莫萨伊克教学图例(Mosaic-IT),这是一种人文/无模范的构成数据合成方法,可以有效地从现有的指示调控数据中产生丰富和多样的增强力,以加强LLMs。Mosaic-IT随机将多重教学数据合并为一种数据,并培训该模型,以预先确定的更高层次的元教学方法生成相应的应对措施,以加强其多步制教学和遵循格式的技能。我们的广泛评价显示,Mosaic-IT的绩效和培训效率较高,在各种基准上取得了一致的改进,培训费用比原始的调整减少了80%。我们的代码和数据可在https://github.com/tiianyi-lab/Mosaic-IT上查阅。","Ming Li, Pei Chen, Chenguang Wang, Hongyu Zhao, Yijun Liang, Yupeng Hou, Fuxiao Liu, Tianyi Zhou",2025-06-05T01:47:25Z,Mosaic-IT: Cost-Free Compositional Data Synthesis for Instruction Tuning,Mosaic-IT: Kostenfreie kompositorische Datensynthese für die Instruction Tuning,Mosaic-IT:用于教学图纸的无成本构成数据综述,http://arxiv.org/abs/2405.13326v3
178,"Understanding which neural components drive specific capabilities in mid-sized language models ($\leq$10B parameters) remains a key challenge. We introduce the $(\bm{K}, \epsilon)$-Minimum Sufficient Head Circuit ($K$-MSHC), a methodology to identify minimal sets of attention heads crucial for classification tasks as well as Search-K-MSHC, an efficient algorithm for discovering these circuits. Applying our Search-K-MSHC algorithm to Gemma-9B, we analyze three syntactic task families: grammar acceptability, arithmetic verification, and arithmetic word problems. Our findings reveal distinct task-specific head circuits, with grammar tasks predominantly utilizing early layers, word problems showing pronounced activity in both shallow and deep regions, and arithmetic verification demonstrating a more distributed pattern across the network. We discover non-linear circuit overlap patterns, where different task pairs share computational components at varying levels of importance. While grammar and arithmetic share many ""weak"" heads, arithmetic and word problems share more consistently critical ""strong"" heads. Importantly, we find that each task maintains dedicated ""super-heads"" with minimal cross-task overlap, suggesting that syntactic and numerical competencies emerge from specialized yet partially reusable head circuits.","在中等语言模型中,对哪些神经元成分驱动特定能力的理解($\leq$10B参数)仍是一个关键的挑战。我们引入了$(\\ bm{K},\ epsilon)$-最低总电路($K$-MSHC)这一方法,用以确定对分类任务至关重要的最低限度的关注头部以及搜索-K-MSHC,这是发现这些电路的有效算法。我们将搜索-K-MSHC算法应用到 Gemma-9B,我们分析三个合成任务组: 语法可接受性、 算术核查和算术问题。我们发现,我们发现不同任务组有不同的头电路重叠模式, 不同任务组在不同的程度上共享计算组件。虽然语法和算法共有许多“ weak” 头、 算术和单词问题, 都有着更一致的“ 强” 头部。我们发现, 每一个任务都保留了专门的“ 高级和可读性电路段能力, ” 。我们发现每个任务都保留了专门的“ 部分数字能力, , 和 显示, 数字 显示, 显示, 显示, 最起码的超值 数字 。","Pratim Chowdhary, Peter Chin, Deepernab Chakrabarty",2025-06-05T01:45:59Z,$K$-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language   Models with Experiments on Syntactic Classification Tasks,$K$-MSHC: Entlarvung minimal ausreichender Kopfkreise in großen Sprachmodellen mit Experimenten zu syntaktischen Klassifizierungsaufgaben,"USK美元-MSHC:在以同步分类任务实验的大型语言模型中,以最小化的足够大语言电路总电路",http://arxiv.org/abs/2505.12268v2
179,"Large Language Model (LLM) agents have demonstrated remarkable generalization capabilities across multi-domain tasks. Existing agent tuning approaches typically employ supervised finetuning on entire expert trajectories. However, behavior-cloning of full trajectories can introduce expert bias and weaken generalization to states not covered by the expert data. Additionally, critical steps, such as planning, complex reasoning for intermediate subtasks, and strategic decision-making, are essential to success in agent tasks, so learning these steps is the key to improving LLM agents. For more effective and efficient agent tuning, we propose ATLaS that identifies the critical steps in expert trajectories and finetunes LLMs solely on these steps with reduced costs. By steering the training's focus to a few critical steps, our method mitigates the risk of overfitting entire trajectories and promotes generalization across different environments and tasks. In extensive experiments, an LLM finetuned on only 30% critical steps selected by ATLaS outperforms the LLM finetuned on all steps and recent open-source LLM agents. ATLaS maintains and improves base LLM skills as generalist agents interacting with diverse environments.","大型语言模型(LLM)代理商在多种领域任务中表现出显著的普及能力。现有的代理商调试方法通常对整个专家轨迹进行有监督的微调,但是,完全轨迹的行为曲线可引入专家偏向,并削弱专家数据未涵盖的国家的概括性。此外,关键步骤,如规划、中间子任务复杂推理以及战略决策等,对于代理任务的成功至关重要,因此学习这些步骤是改进LLM代理商的关键。为了提高调控效力和效率,我们建议ATLAS只确定专家轨迹和微调LMs的关键步骤,以较低的成本在这些步骤上确定关键步骤。通过将培训的重点引导到几个关键步骤,我们的方法可以减轻过度配置整个轨迹和在不同环境和任务中促进总体化的风险。在广泛的实验中,LLMM只对ATLM选中的30%的关键步骤进行微调,使LM公司对所有步骤和最近的开源LMM代理商进行微调控所有步骤和最近的开源LM代理商进行微调。ATLSS保持并改进各种LM代理商的基础环境,作为一般的相互作用剂。","Zhixun Chen, Ming Li, Yuxuan Huang, Yali Du, Meng Fang, Tianyi Zhou",2025-06-05T01:42:08Z,ATLaS: Agent Tuning via Learning Critical Steps,ATLaS: Agent Tuning über Learning Critical Steps,通过学习关键步骤进行测试的代理,http://arxiv.org/abs/2503.02197v2
180,"Transformer-based large language models (LLMs) excel in natural language processing tasks by capturing long-range dependencies through self-attention mechanisms. However, long-context modeling faces significant computational inefficiencies due to \textit{redundant} attention computations: while attention weights are often \textit{sparse}, all tokens consume \textit{equal} computational resources. In this paper, we reformulate traditional probabilistic sequence modeling as a \textit{supervised learning task}, enabling the separation of relevant and irrelevant tokens and providing a clearer understanding of redundancy. Based on this reformulation, we theoretically analyze attention sparsity, revealing that only a few tokens significantly contribute to predictions. Building on this, we formulate attention optimization as a linear coding problem and propose a \textit{group coding strategy}, theoretically showing its ability to improve robustness against random noise and enhance learning efficiency. Motivated by this, we propose \textit{Dynamic Group Attention} (DGA), which leverages the group coding to explicitly reduce redundancy by aggregating less important tokens during attention computation. Empirical results show that our DGA significantly reduces computational costs while maintaining competitive performance.Code is available at https://github.com/bolixinyu/DynamicGroupAttention.","以变换为基础的大型语言模型(LLMS)在自然语言处理任务中表现优异,通过自省机制捕捉远程依赖性。然而,长文本模型由于注意计算而面临巨大的计算效率低:虽然注意权重通常为\ textit{redent},但所有符号都消耗着计算资源。在本文中,我们重新配置传统概率序列模型,将其作为一种\ textit{ 监督学习任务},能够分离相关和无关的标牌,并更清楚地了解冗余。基于这一重新定义,我们理论上分析注意力的紧张性,揭示只有几个象征能大大促进预测。在此基础上,我们将注意力优化作为一种线性编码问题,并提出一个\ textit{chol} 组合编码战略。在理论上显示它有能力增强对随机噪音的稳健和学习效率。我们为此提出\textit{Dynamicroup Groupat} (DAGAA) 利用该组来明确减少冗余性,同时通过将微量性AAA的计算来大幅降低现有的成本。","Shuhai Zhang, Zeng You, Yaofo Chen, Zhiquan Wen, Qianyue Wang, Zhijie Qiu, Yuanqing Li, Mingkui Tan",2025-06-05T01:31:28Z,Curse of High Dimensionality Issue in Transformer for Long-context   Modeling,Fluch der Hochdimensionalitätsfrage im Transformer für die Langkontextmodellierung,"变异器中高多维度问题的诅咒,用于长期建模",http://arxiv.org/abs/2505.22107v2
181,"We analyze the extent to which internal representations of language models (LMs) identify and distinguish mentions of named entities, focusing on the many-to-many correspondence between entities and their mentions. We first formulate two problems of entity mentions -- ambiguity and variability -- and propose a framework analogous to clustering quality metrics. Specifically, we quantify through cluster analysis of LM internal representations the extent to which mentions of the same entity cluster together and mentions of different entities remain separated. Our experiments examine five Transformer-based autoregressive models, showing that they effectively identify and distinguish entities with metrics analogous to precision and recall ranging from 0.66 to 0.9. Further analysis reveals that entity-related information is compactly represented in a low-dimensional linear subspace at early LM layers. Additionally, we clarify how the characteristics of entity representations influence word prediction performance. These findings are interpreted through the lens of isomorphism between LM representations and entity-centric knowledge structures in the real world, providing insights into how LMs internally organize and use entity information.","我们分析语文模型的内部表述在多大程度上能识别和区分被点名实体,重点是各实体之间的许多对许多的通信和它们的提及。我们首先提出两个实体问题 -- -- 模糊性和多变性 -- -- 并提议一个类似于组合质量指标的框架。具体地说,我们通过对语言模型内部表述的分组分析来量化相同实体集群和不同实体的提及程度。我们的实验审查了五个基于变异器的自动递减模式,表明它们有效地识别和区分了具有类似精确度和回溯度范围从0.66至0.9等指标的实体。进一步的分析表明,与实体有关的信息在早期LM层的低维线子空间中以缩略线为代表。此外,我们澄清了实体表述的特点如何影响单词预测绩效。这些结论通过LM代表与现实世界中以实体为中心的知识结构之间的无形态主义透镜来解释,对LMS的内部组织和使用实体信息的方式提供了深刻的见解。","Masaki Sakata, Benjamin Heinzerling, Sho Yokoi, Takumi Ito, Kentaro Inui",2025-06-05T01:17:55Z,On Entity Identification in Language Models,Zur Identitätskennung in Sprachmodellen,关于在语文模式中实体识别,http://arxiv.org/abs/2506.02701v3
182,"Cognitive health in older adults presents a growing challenge. Although conversational interventions show feasibility in improving cognitive wellness, human caregiver resources remain overloaded. AI-based chatbots have shown promise, yet existing work is often limited to implicit strategies or heavily depends on training and label resources. In response, we propose a strategy-guided AI chatbot named ChatWise that follows a dual-level conversation reasoning framework. It integrates macro-level strategy planning and micro-level utterance generation to enable engaging, multi-turn dialogue tailored to older adults. Empirical results show that ChatWise closely aligns with professional human caregiver behaviors in offline evaluation using real clinic data, and achieves positive user cognitive and emotional responses in interactive simulations with digital twins, which significantly outperforms AI baselines that follow implicit conversation generation.","老年人的认知健康是一个日益严峻的挑战。虽然谈话干预措施表明在提高认知健康方面的可行性,但人类护理者的资源仍然超负荷。 AI型聊天机已经显示出希望,但现有的工作往往局限于隐含的战略,或严重依赖培训和标签资源。 作为回应,我们提议了一个战略指导的AI聊天机,名为ChattWise,遵循双层对话逻辑框架。它整合宏观战略规划和微观级话语生成,以便能够进行针对老年人的参与性、多方向的对话。经验性结果显示,ChatWise与利用实际诊所数据进行的离线评估中专业人类护理者行为紧密结合,并在与双胞胎的互动模拟中实现积极的用户认知和情感反应,这些互动模拟大大优于AI型基线,在隐性对话生成之后实现。","Zhengbang Yang, Junyuan Hong, Yijiang Pang, Jiayu Zhou, Zhuangdi Zhu",2025-06-05T01:16:33Z,ChatWise: A Strategy-Guided Chatbot for Enhancing Cognitive Support in   Older Adults,ChatWise: Ein strategiegeführter Chatbot zur Verbesserung der kognitiven Unterstützung bei älteren Erwachsenen,ChatWise:加强老年人认知支助战略指导的Chattbot,http://arxiv.org/abs/2503.05740v2
183,"We propose that benchmarking LLMs on questions which have no reasonable answer actually isn't as silly as it sounds. We also present a benchmark that allows such testing and a method to modify the existing datasets, and discover that existing models demonstrate a performance far from the perfect on such questions. Our code and data artifacts are available at https://github.com/L3G5/impossible-bench","我们建议,在没有合理答案的问题上制定基准LLMS实际上并不象听起来那样愚蠢。 我们还提出了一个允许进行这种测试的基准和修改现有数据集的方法,并发现现有的模型在这些问题上表现出远非完美的表现。 我们的代码和数据文物可以在https://github.com/L3G5/impossible-bench上查阅。",K. O. T. Erziev,2025-06-05T00:59:16Z,BSBench: will your LLM find the largest prime number?,BSBench: Findet Ihr LLM die größte Primzahl?,你的LLM会找到最大的质数吗?,http://arxiv.org/abs/2506.04535v1
184,"Discourse particles are crucial elements that subtly shape the meaning of text. These words, often polyfunctional, give rise to nuanced and often quite disparate semantic/discourse effects, as exemplified by the diverse uses of the particle ""just"" (e.g., exclusive, temporal, emphatic). This work investigates the capacity of LLMs to distinguish the fine-grained senses of English ""just"", a well-studied example in formal semantics, using data meticulously created and labeled by expert linguists. Our findings reveal that while LLMs exhibit some ability to differentiate between broader categories, they struggle to fully capture more subtle nuances, highlighting a gap in their understanding of discourse particles.","分流粒子是暗含文字含义的关键元素。 这些词,通常是多功能词,引起细微的、往往相当不同的语义/分流效应,例如粒子“公正”的不同用途(如独家、时间、强调)就说明了这一点。 这项工作调查了LLMs区分精细的英语“公正”感的能力,这是一个在正式语义学中受到很好研究的例子,它使用由专业语言学家精心制作和贴上标签的数据。 我们的调查结果显示,LLMs虽然表现出某种能力来区分更广泛的类别,但很难充分捕捉到更微妙的细微差别,突出显示了其对讨论粒子的理解上的差距。","William Sheffield, Kanishka Misra, Valentina Pyatkin, Ashwini Deo, Kyle Mahowald, Junyi Jessy Li",2025-06-05T00:59:05Z,Is It JUST Semantics? A Case Study of Discourse Particle Understanding   in LLMs,Ist es nur Semantik? Eine Fallstudie des Diskurses Teilchenverständnis in LLMs,《只是语义学吗?,http://arxiv.org/abs/2506.04534v1
185,"Recent advancements in Large Language Models (LLMs) have demonstrated significant progress in various areas, such as text generation and code synthesis. However, the reliability of performance evaluation has come under scrutiny due to data contamination-the unintended overlap between training and test datasets. This overlap has the potential to artificially inflate model performance, as LLMs are typically trained on extensive datasets scraped from publicly available sources. These datasets often inadvertently overlap with the benchmarks used for evaluation, leading to an overestimation of the models' true generalization capabilities. In this paper, we first examine the definition and impacts of data contamination. Secondly, we review methods for contamination-free evaluation, focusing on three strategies: data updating-based methods, data rewriting-based methods, and prevention-based methods. Specifically, we highlight dynamic benchmarks and LLM-driven evaluation methods. Finally, we categorize contamination detecting methods based on model information dependency: white-Box, gray-Box, and black-Box detection approaches. Our survey highlights the requirements for more rigorous evaluation protocols and proposes future directions for addressing data contamination challenges.","大语言模型(LLMS)最近的进展表明,在诸如文本生成和代码合成等各个领域都取得了显著进展。然而,由于数据污染——培训和测试数据集之间意外地重叠,业绩评估的可靠性受到审查。这种重叠有可能人为地扩大模型性能,因为LLMS通常在从公开来源分离广泛的数据集方面受过培训。这些数据集往往无意中与用于评价的基准重叠,导致高估模型的真正概括性能力。在本文件中,我们首先审查数据污染的定义和影响。第二,我们审查无污染评价的方法,重点是三个战略:数据更新方法、数据重写方法和预防方法。具体地说,我们强调动态基准和LLMM驱动的评价方法。最后,我们根据模型信息依赖性对污染探测方法进行分类:白-Box、灰-Box和黑-Box。我们的调查突出了更严格评估程序的要求,并提出了应对数据污染挑战的未来方向。","Yuxing Cheng, Yi Chang, Yuan Wu",2025-06-05T00:49:05Z,A Survey on Data Contamination for Large Language Models,Eine Umfrage über Datenkontamination für große Sprachmodelle,大语言模型数据污染调查,http://arxiv.org/abs/2502.14425v2
186,"Recent advancements in large language models (LLMs) have revolutionized natural language processing (NLP) and expanded their applications across diverse domains. However, despite their impressive capabilities, LLMs have been shown to reflect and perpetuate harmful societal biases, including those based on ethnicity, gender, and religion. A critical and underexplored issue is the reinforcement of caste-based biases, particularly towards India's marginalized caste groups such as Dalits and Shudras. In this paper, we address this gap by proposing DECASTE, a novel, multi-dimensional framework designed to detect and assess both implicit and explicit caste biases in LLMs. Our approach evaluates caste fairness across four dimensions: socio-cultural, economic, educational, and political, using a range of customized prompting strategies. By benchmarking several state-of-the-art LLMs, we reveal that these models systematically reinforce caste biases, with significant disparities observed in the treatment of oppressed versus dominant caste groups. For example, bias scores are notably elevated when comparing Dalits and Shudras with dominant caste groups, reflecting societal prejudices that persist in model outputs. These results expose the subtle yet pervasive caste biases in LLMs and emphasize the need for more comprehensive and inclusive bias evaluation methodologies that assess the potential risks of deploying such models in real-world contexts.","大型语言模式(LLMs)最近的进展使自然语言处理(NLP)发生了革命性的变化,并扩大了其在不同领域的应用范围,然而,尽管LLMs具有令人印象深刻的能力,但事实证明,LLMs反映并延续了有害的社会偏见,包括基于族裔、性别和宗教的偏见。一个关键和未得到充分探讨的问题是,强化基于种姓的偏见,特别是对印度的Dalits和Shudras等边缘化种姓群体的偏见。在本文件中,我们通过提出DECASTE这一旨在发现和评估LLMs中隐含和明显的种姓偏见的新颖的多维框架来弥补这一差距。我们的方法评估了种姓在四个方面的公平性:社会文化、经济、教育和政治的公平性,采用一系列量身定制的激励战略。我们通过确定几个最先进的LMs基准,我们发现这些模式系统地强化了种姓偏见,在对待受压迫群体和占支配地位的种姓群体方面差异很大。举例说,如果将DLTS和Shudras与主要种姓群体进行比较,那么,那么偏见的分数就会明显提高,反映模式产出中持续存在的社会偏见。","Prashanth Vijayaraghavan, Soroush Vosoughi, Lamogha Chiazor, Raya Horesh, Rogerio Abreu de Paula, Ehsan Degan, Vandana Mukherjee",2025-06-05T00:42:04Z,DECASTE: Unveiling Caste Stereotypes in Large Language Models through   Multi-Dimensional Bias Analysis,DECASTE: Enthüllen von Kastenstereotypen in großen Sprachmodellen durch multidimensionale Bias-Analyse,"DECASTE:通过多语言双种族分析,在大语言模式中统一种姓定型观念",http://arxiv.org/abs/2505.14971v2
187,"Current Large Language Models (LLMs) exhibit significant limitations, notably in structured, interpretable, and verifiable medical reasoning, alongside practical deployment challenges related to computational resources and data privacy. This report focused on the development of WiNGPT-3.0, the 32-billion parameter LLMs, engineered with the objective of enhancing its capacity for medical reasoning and exploring its potential for effective integration within healthcare IT infrastructures. The broader aim is to advance towards clinically applicable models. The approach involved a multi-stage training pipeline tailored for general, medical, and clinical reasoning. This pipeline incorporated supervised fine-tuning (SFT) and reinforcement learning (RL), leveraging curated Long Chain-of-Thought (CoT) datasets, auxiliary reward models, and an evidence-based diagnostic chain simulation. WiNGPT-3.0 demonstrated strong performance: specific model variants achieved scores of 66.6 on MedCalc and 87.1 on MedQA-USMLE. Furthermore, targeted training improved performance on a clinical reasoning task from a baseline score of 58.1 to 62.5. These findings suggest that reinforcement learning, even when applied with a limited dataset of only a few thousand examples, can enhance medical reasoning accuracy. Crucially, this demonstration of RL's efficacy with limited data and computation paves the way for more trustworthy and practically deployable LLMs within clinical workflows and health information infrastructures.","现有大型语言模型(LLMS)在结构化、可解释和可核实的医疗推理方面显示出重大局限性,特别是在结构化、可解释和可核实的医疗推理方面,以及与计算资源和数据隐私相关的实际部署挑战,本报告侧重于开发WiNGPT-3.0,即32亿参数LMS,目的是提高医疗推理能力,探索其在保健信息技术基础设施内有效整合的潜力;更广泛的目标是推进临床应用模型;该方法包括针对一般、医疗和临床推理的多阶段培训管道;该管道包括监督的微调和强化学习(RL)、利用经整理的长链数据集、辅助奖励模型和循证诊断链模拟;WiNGPT-3.0展示了强有力的绩效:具体模型模型模型在MedCalc和MedQA-USMLE中取得了66.6分和87.1分的成绩;此外,有针对性的培训改善了临床推理学工作的业绩,从58.1分到62分。5这些研究结果表明,即使只用有限的数据集成数千个数据集、可更精确的临床推理,也能推算,也能加强医学推算。","Boqin Zhuang, Chenxiao Song, Huitong Lu, Jiacheng Qiao, Mingqian Liu, Mingxing Yu, Ping Hong, Rui Li, Xiaoxia Song, Xiangjun Xu, Xu Chen, Yaoyao Ma, Yujie Gao",2025-06-05T00:37:34Z,WiNGPT-3.0 Technical Report,Technischer Bericht WiNGPT-3.0,WINGPT-3.0技术报告,http://arxiv.org/abs/2505.17387v2
188,"Neural machine translation (NMT) systems typically employ maximum a posteriori (MAP) decoding to select the highest-scoring translation from the distribution mass. However, recent evidence highlights the inadequacy of MAP decoding, often resulting in low-quality or even pathological hypotheses -- the decoding objective is not aligned with real-world translation quality. This paper proposes calibrating hypothesis likelihoods with translation quality from a distribution view by directly optimizing their Pearson correlation -- thereby enhancing the effectiveness of translation decoding. With our method, translation on large language models (LLMs) improves substantially after limited training (2K instances per direction). This improvement is orthogonal to those achieved through supervised fine-tuning, leading to substantial gains across a broad range of metrics and human evaluations -- even when applied to top-performing translation-specialized LLMs fine-tuned on high-quality translation data, such as Tower, or when compared to recent preference optimization methods, like CPO. Moreover, the calibrated translation likelihood can directly serve as a strong proxy for translation quality, closely approximating or even surpassing some state-of-the-art translation quality estimation models, like CometKiwi. Lastly, our in-depth analysis demonstrates that calibration enhances the effectiveness of MAP decoding, thereby enabling greater efficiency in real-world deployment. The resulting state-of-the-art translation model, which covers 10 languages, along with the accompanying code and human evaluation data, has been released to the community: https://github.com/moore3930/calibrating-llm-mt.","神经机器翻译(NMT)系统通常使用最高程度的事后编码(MAP),从分布质量中选择最高分数的翻译。然而,最近的证据显示MAP解码不足,往往导致低质量甚至病理假说 -- -- 解码目标与真实世界翻译质量不相符。本文建议通过直接优化其皮尔逊相关性,从分布角度校准翻译质量的假设可能性,从而提高翻译解码的有效性。我们的方法是,在有限的培训之后,大语言模型(LLLMs)的翻译(LLM)会大大改善。这种改进与通过监管的微调实现的差异,导致广泛的计量和人类评估(即使应用于顶级翻译专业化的LMMS)质量。 本文建议直接优化了对高质量翻译质量的假设,比如塔,或者与最近的优惠优化方法相比,比如CPO。校正的翻译可能性可以直接作为翻译质量的有力代言,近近比,甚至超过某些翻译的版本。 30级校正版的翻译质量分析在最终展示了高翻译效率。","Di Wu, Yibin Lei, Christof Monz",2025-06-05T00:33:42Z,Calibrating Translation Decoding with Quality Estimation on LLMs,Kalibrierung der Übersetzungsdekodierung mit Qualitätsschätzung auf LLMs,以LLMM中质量估算法进行标注,http://arxiv.org/abs/2504.19044v3
189,"There is a growing need for pluralistic alignment methods that can steer language models towards individual attributes and preferences. One such method, Self-Supervised Alignment with Mutual Information (SAMI), uses conditional mutual information to encourage the connection between behavioral preferences and model responses. We conduct two experiments exploring SAMI in multi-task settings. First, we compare SAMI to Direct Preference Optimization (DPO) on a multi-task benchmark (MT-Bench), using a stronger model to generate training data for a weaker one across diverse categories (humanities, STEM, extraction, coding, math, reasoning, and roleplay). Our results indicate that one iteration of SAMI has a 57% win rate against DPO, with significant variation in performance between task categories. Second, we examine SAMI's impact on mathematical accuracy (GSM-8K) relative to supervised fine-tuning (SFT). While SAMI increases zero-shot performance by 1.1%, SFT is more effective with a 3.2% boost. However, SAMI shows interesting scaling trends. When given 10 attempts, SAMI improves accuracy by 3.9%, while SFT achieves a 10.1% increase. Combining SAMI with SFT yields an additional improvement of 1.3% in multi-attempt settings, though single-attempt accuracy remains unchanged.","目前日益需要多元化的调整方法,以引导语言模式适应个人属性和偏好。其中一种方法,即自我监督与相互信息一致(SAMI),使用有条件的相互信息,鼓励行为偏好和模式响应之间的联系。我们在多任务设置中进行两次探索SAMI的实验。首先,我们将SAMI与多任务基准(MT-Bench)相比,在多任务基准(MT-Bench)上直接偏好优化(DPO)比较,使用更强大的模型为不同类别(人文、STEM、提取、编码、数学、推理和角色扮演)中的较弱类别(SAMI)生成培训数据。我们的结果显示,SAMI的重复率比DPO高57%,任务类别之间的性能差异很大。第二,我们检查SAMI对数学精度(GSM-8K)的影响,相对于监督的微调(SFT)基准(MFT)而言,SAFT的零率提高了1.1%,而SFT则提高了3.9%。但是,SAMI的精确度提高了SFA值增加1.1%。",Soham V. Govande,2025-06-05T00:32:25Z,An Exploration of Self-Supervised Mutual Information Alignment for   Multi-Task Settings,Eine Untersuchung der selbstüberwachten gegenseitigen Information Ausrichtung für Multi-Task-Einstellungen,探索多种任务设置的自我监督的相互信息协调,http://arxiv.org/abs/2410.01704v2
190,"We propose a model to obtain phonemic and prosodic labels of speech that are coherent with graphemes. Unlike previous methods that simply fine-tune a pre-trained ASR model with the labels, the proposed model conditions the label generation on corresponding graphemes by two methods: 1) Add implicit grapheme conditioning through prompt encoder using pre-trained BERT features. 2) Explicitly prune the label hypotheses inconsistent with the grapheme during inference. These methods enable obtaining parallel data of speech, the labels, and graphemes, which is applicable to various downstream tasks such as text-to-speech and accent estimation from text. Experiments showed that the proposed method significantly improved the consistency between graphemes and the predicted labels. Further, experiments on accent estimation task confirmed that the created parallel data by the proposed method effectively improve the estimation accuracy.","我们提出一种模式,以获取与图形化符号相一致的语音和预发语音标签。与以往简单微调预先培训的ASR模型和标签的方法不同,拟议模型以两种方法将相应的图形化模型生成标签的条件设定为:(1) 使用经过预先培训的BERT特征,通过快速编码器添加隐含的图形化调节。(2) 在推断过程中,明确使用与图形化模型不一致的标签假设。这些方法能够获取语言、标签和图形化模型的平行数据,这些数据适用于文本的文字到语音和口音估计等各种下游任务。实验表明,拟议方法大大改善了图形化模型和预测标签的一致性。此外,关于口腔估计的实验证实,通过拟议方法生成的平行数据有效地提高了估算的准确性。","Hien Ohnaka, Yuma Shirahata, Byeongseon Park, Ryuichi Yamamoto",2025-06-05T00:24:00Z,Grapheme-Coherent Phonemic and Prosodic Annotation of Speech by Implicit   and Explicit Grapheme Conditioning,Grapheme-Kohärente phonemische und prosodic Annotation der Sprache durch Implizite und explizite Graphemkonditionierung,隐含和显性图形条件的图形一致的语音和Prosodic 语音批注,http://arxiv.org/abs/2506.04527v1
191,"Large Language Models (LLMs) demonstrate strong reasoning capabilities for many tasks, often by explicitly decomposing the task via Chain-of-Thought (CoT) reasoning. Recent work on LLM-based translation designs hand-crafted prompts to decompose translation, or trains models to incorporate intermediate steps.~\textit{Translating Step-by-step}~\citep{briakou2024translating}, for instance, introduces a multi-step prompt with decomposition and refinement of translation with LLMs, which achieved state-of-the-art results on WMT24. In this work, we scrutinise this strategy's effectiveness. Empirically, we find no clear evidence that performance gains stem from explicitly decomposing the translation process, at least for the models on test; and we show that simply prompting LLMs to ``translate again'' yields even better results than human-like step-by-step prompting. Our analysis does not rule out the role of reasoning, but instead invites future work exploring the factors for CoT's effectiveness in the context of translation.","大型语言模型(LLMS) 展示了许多任务的强大推理能力,通常是通过Thought(CoT)推理将任务明确分解。最近关于基于LLM的翻译工作设计了人工制造的解剖翻译,或培训模型以纳入中间步骤。 Textit{Translating 一步一步地翻译{citep{briakukou2024 translating} 引入了多步速度的推理能力,同时分解和完善了与LLMS的翻译,从而在WMT24上取得了最先进的结果。在这项工作中,我们仔细审视了这一战略的有效性。我们偶然地发现,没有明显证据表明,通过明确分解翻译过程,至少对测试模型而言,取得了绩效收益;我们表明,简单地促使LMS“翻译”再次产生比像人类一样的一步一步地提示更好的结果。我们的分析并没有排除推理的作用,而是邀请今后的工作探索CT在翻译方面的有效性因素。","Di Wu, Seth Aycock, Christof Monz",2025-06-05T00:04:39Z,Please Translate Again: Two Simple Experiments on Whether Human-Like   Reasoning Helps Translation,"Bitte übersetzen Sie erneut: Zwei einfache Experimente darüber, ob Menschen wie Vernunft hilft Übersetzung","请再次翻译: 两项简单的实验, 有关 人类与人类之间的理由是否有助于翻译的简单实验 。",http://arxiv.org/abs/2506.04521v1
192,"With Large Language Models (LLMs) rapidly approaching and potentially surpassing human-level performance, it has become imperative to develop approaches capable of effectively supervising and enhancing these powerful models using smaller, human-level models exposed to only human-level data. We address this critical weak-to-strong (W2S) generalization challenge by proposing a novel method aimed at improving weak experts, by training on the same limited human-level data, enabling them to generalize to complex, super-human-level tasks. Our approach, called \textbf{EnsemW2S}, employs a token-level ensemble strategy that iteratively combines multiple weak experts, systematically addressing the shortcomings identified in preceding iterations. By continuously refining these weak models, we significantly enhance their collective ability to supervise stronger student models. We extensively evaluate the generalization performance of both the ensemble of weak experts and the subsequent strong student model across in-distribution (ID) and out-of-distribution (OOD) datasets. For OOD, we specifically introduce question difficulty as an additional dimension for defining distributional shifts. Our empirical results demonstrate notable improvements, achieving 4\%, and 3.2\% improvements on ID datasets and, upto 6\% and 2.28\% on OOD datasets for experts and student models respectively, underscoring the effectiveness of our proposed method in advancing W2S generalization.","随着大型语言模型(LLMS)迅速接近并可能超过人类层面的绩效,当务之急是制定能够有效监督和加强这些强大模型的方法,利用仅暴露在人类层面数据的小型人类层面模型,有效监督和加强这些强大的模型。我们通过提出一种新颖的方法,改善薄弱专家的微弱至强(W2S)概括化挑战,方法是就相同的有限人类层面数据进行培训,使这些专家能够概括到复杂、超人类层面的任务。我们的方法称为\ textb{EnsemW2S},采用象征性的混合战略,将多个弱小专家相互结合,系统地解决先前迭代中发现的缺点。我们通过不断完善这些薄弱模型,大大增强他们监督更强大的学生模型的集体能力。我们广泛评价了弱小专家群集和随后强大的学生模型在分布(ID)和分配外(OODD)数据集的普及性表现。关于OOD,我们具体提出问题,作为界定分配变化的另一个方面,我们的经验性结果显示,我们分别改进了ODS 2 和IMS 数据更新了我们通用模型的显著的成绩。","Aakriti Agrawal, Mucong Ding, Zora Che, Chenghao Deng, Anirudh Satheesh, Bang An, Bayan Bruss, John Langford, Furong Huang",2025-06-05T00:02:02Z,EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language   Model Ensembles,EnsemW2S: Verbesserung der Schwach-zu-Strong-Verallgemeinerung mit großsprachigen Modellensembles,EnsemW2S:用大语言模型组合加强弱至强的通用化,http://arxiv.org/abs/2505.21959v2
193,"Speech language models (Speech LMs) enable end-to-end speech-text modelling within a single model, offering a promising direction for spoken dialogue systems. The choice of speech-text jointly decoding paradigm plays a critical role in performance, efficiency, and alignment quality. In this work, we systematically compare representative joint speech-text decoding strategies-including the interleaved, and parallel generation paradigms-under a controlled experimental setup using the same base language model, speech tokenizer and training data. Our results show that the interleaved approach achieves the best alignment. However it suffers from slow inference due to long token sequence length. To address this, we propose a novel early-stop interleaved (ESI) pattern that not only significantly accelerates decoding but also yields slightly better performance. Additionally, we curate high-quality question answering (QA) datasets to further improve speech QA performance.","语音语言模型(Speech LMs)能够在单一模型中进行端到端语音-文字建模,为口语对话系统提供一个有希望的方向。选择语音-文字联合解码模式在性能、效率和一致性方面发挥着关键作用。在这项工作中,我们系统地比较有代表性的语音-文字联合解码战略(包括间断和平行生成模式),在使用同一基本语言模型、语音符号和培训数据的受控实验设置下,使用同一基本语言模型、语音符号和培训数据进行对照。我们的结果显示,间断方法实现了最佳的对齐。但是,由于长时间的象征性序列长度,它受到缓慢的推论的影响。为了解决这个问题,我们建议一种新型的早期断层(ESI)模式,不仅大大加快解码,而且产生略微更好的性能。此外,我们整理了高质量的回答问题(QA)数据集,以进一步改进语音QA的性能。","Haibin Wu, Yuxuan Hu, Ruchao Fan, Xiaofei Wang, Kenichi Kumatani, Bo Ren, Jianwei Yu, Heng Lu, Lijuan Wang, Yao Qian, Jinyu Li",2025-06-04T23:53:49Z,Towards Efficient Speech-Text Jointly Decoding within One Speech   Language Model,Auf dem Weg zu einem effizienten Sprach-Text gemeinsam innerhalb eines Sprachmodells dekodieren,争取实现在一种语音语言模式内实现高效率的语音-文本联合解码,http://arxiv.org/abs/2506.04518v1
194,"Large Language Models (LLMs) excel at many tasks but struggle with ambiguous scenarios where multiple valid responses exist, often yielding unreliable results. Conversely, Small Language Models (SLMs) demonstrate robustness in such scenarios but are susceptible to misleading or adversarial inputs. We observed that LLMs handle negative examples effectively, while SLMs excel with positive examples. To leverage their complementary strengths, we introduce SLIDE (Small and Large Integrated for Dialogue Evaluation), a method integrating SLMs and LLMs via adaptive weighting. Building on SLIDE, we further propose a Dual-Refinement Evaluation (DRE) method to enhance SLM-LLM integration: (1) SLM-generated insights guide the LLM to produce initial evaluations; (2) SLM-derived adjustments refine the LLM's scores for improved accuracy. Experiments demonstrate that DRE outperforms existing methods, showing stronger alignment with human judgment across diverse benchmarks. This work illustrates how combining small and large models can yield more reliable evaluation tools, particularly for open-ended tasks such as dialogue evaluation.","大语言模型(LLMS)在很多任务上都非常出色,但在存在多种有效反应、往往产生不可靠结果的模棱两可的假想中,却难以应付,相反,小语言模型(SLMs)在这种假想中表现出强健,但容易受到误导或对抗性投入的影响。我们发现LLMs有效地处理负面实例,而SLMs则优于正面的例子。为了利用其互补优势,我们引入了SLIDE(Small and 大型综合对话评价),这是一种通过适应性加权将可持续土地管理和LLMs整合起来的方法。我们借助SLIDE,进一步提出了一种加强可持续土地管理-LLM整合的双精辟评价方法:(1)可持续土地管理产生的洞察力指导LLM(LM)进行初步评价;(2)从LMMs产生的调整改进了LMM的得分,以提高准确性。实验表明DRE超越了现有方法,显示在各种基准中与人类判断更加一致。这项工作表明,将小型和大型模型结合起来可以产生更可靠的评价工具,特别是在对话评价等开放式任务方面。","Kun Zhao, Bohao Yang, Chen Tang, Siyuan Dai, Haoteng Tang, Chenghua Lin, Liang Zhan",2025-06-04T23:41:31Z,DRE: An Effective Dual-Refined Method for Integrating Small and Large   Language Models in Open-Domain Dialogue Evaluation,DRE: Eine effektive Dual-Refined-Methode zur Integration kleiner und großer Sprachmodelle in die Open-Domain Dialogue Evaluation,DRE: 将小型和大语言模式纳入开放式对话评价的有效双重修订方法,http://arxiv.org/abs/2506.04516v1
195,"Jailbreaking large-language models (LLMs) involves testing their robustness against adversarial prompts and evaluating their ability to withstand prompt attacks that could elicit unauthorized or malicious responses. In this paper, we present TurboFuzzLLM, a mutation-based fuzzing technique for efficiently finding a collection of effective jailbreaking templates that, when combined with harmful questions, can lead a target LLM to produce harmful responses through black-box access via user prompts. We describe the limitations of directly applying existing template-based attacking techniques in practice, and present functional and efficiency-focused upgrades we added to mutation-based fuzzing to generate effective jailbreaking templates automatically. TurboFuzzLLM achieves $\geq$ 95\% attack success rates (ASR) on public datasets for leading LLMs (including GPT-4o \& GPT-4 Turbo), shows impressive generalizability to unseen harmful questions, and helps in improving model defenses to prompt attacks. TurboFuzzLLM is available open source at https://github.com/amazon-science/TurboFuzzLLM.","在本文中,我们介绍TurboFuzzLLM,这是一种基于突变的模糊技术,旨在有效找到一套有效的破狱模板,如果与有害问题相结合,可导致目标LLM通过用户接入黑盒获取有害反应。我们描述了在实际操作中直接应用基于模板的现有攻击技术的局限性,以及目前我们为生成有效的破狱模板而添加的基于突变的烟雾的功能和效率重点升级。TurboFuzzLLM在大型LM的公共数据集(包括GPT-4o GPT-4  GPT-4 Turbo)上实现了95美元攻击成功率(ASR),显示了对隐蔽的有害问题的广泛性,并有助于改进快速袭击的示范防御。TurboFuzzLLM在https://github.com/amazon-science/TurboFuzLLM上可以找到开放源。","Aman Goel, Xian Carrie Wu, Zhe Wang, Dmitriy Bespalov, Yanjun Qi",2025-06-04T23:08:28Z,TurboFuzzLLM: Turbocharging Mutation-based Fuzzing for Effectively   Jailbreaking Large Language Models in Practice,TurboFuzzLLM: Turbocharging Mutation-basiertes Fuzzing für effektiv Jailbreaking große Sprachmodelle in der Praxis,TurboFuzzLLLM: 实际中有效破碎大语言模式的涡轮摄取变异法,http://arxiv.org/abs/2502.18504v2
196,"Speech foundation models trained at a massive scale, both in terms of model and data size, result in robust systems capable of performing multiple speech tasks, including automatic speech recognition (ASR). These models transcend language and domain barriers, yet effectively measuring their performance remains a challenge. Traditional metrics like word error rate (WER) and character error rate (CER) are commonly used to evaluate ASR performance but often fail to reflect transcription quality in critical contexts, particularly when detecting fabricated outputs. This phenomenon, known as hallucination, is especially concerning in high-stakes domains such as healthcare, legal, and aviation, where errors can have severe consequences. In our work, we address this gap by investigating hallucination in ASR models. We examine how factors such as distribution shifts, model size, and model architecture influence the hallucination error rate (HER), a metric we introduce to quantify hallucinations. Our analysis of over 20 ASR models reveals \numinsights~key insights: (1) High WERs can mask low hallucination rates, while low WERs may conceal dangerous hallucinations. (2) Synthetic noise, both adversarial and common perturbations like white noise, pitch shift, and time stretching, increase HER. (3) Distribution shift correlates strongly with HER ($\alpha = 0.91$). Our findings highlight the importance of incorporating HER alongside traditional metrics like WER to better assess ASR model performance, particularly in high-stakes domains.","在模型和数据规模方面经过大规模培训的语音基础模型,在模型和数据规模方面都经过大规模培训,形成能够执行多种语音任务的强大系统,包括自动语音识别(ASR)。这些模型超越语言和领域障碍,但有效衡量其性能仍是一项挑战。传统指标,如单词错误率(WER)和字符错误率(CER),通常用于评价ASR的性能,但往往未能反映关键情况下的笔录质量,特别是在发现伪造产出时。这种现象被称为幻觉,在保健、法律和航空等高目标领域尤其与危险幻觉有关。在我们的工作中,我们通过调查ASR模型的幻觉来弥补这一差距。我们研究了分配变化、模型大小和模型结构等因素如何影响幻觉率(HER),这是我们用来量化幻觉的一种衡量标准。我们对20多个ASR模型的分析显示:(1)高WER可以掩盖低幻觉率模型,而低WER可能掩盖危险的幻觉。(2) 合成噪音,既有对抗性和共同性噪音,也有强烈的,例如对ASR模型的幻觉,还有更强烈的反动性,例如WER的变异性、高的频率,以及我们SRRRR的高度的频率, 。","Hanin Atwany, Abdul Waheed, Rita Singh, Monojit Choudhury, Bhiksha Raj",2025-06-04T23:04:18Z,"Lost in Transcription, Found in Distribution Shift: Demystifying   Hallucination in Speech Foundation Models","Verloren in Transkription, Gefunden in Distribution Shift: Entmystifizierende Halluzination in Speech Foundation Models","丢失于《追踪》中,在《分销转移:语音基础模型中解密幻觉》中找到",http://arxiv.org/abs/2502.12414v2
197,"Transformer-based large language models (LLMs) use the key-value (KV) cache to significantly accelerate inference by storing the key and value embeddings of past tokens. However, this cache consumes significant GPU memory. In this work, we introduce HashEvict, an algorithm that uses locality-sensitive hashing (LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache that are cosine dissimilar to the current query token. This is achieved by computing the Hamming distance between binarized Gaussian projections of the current token query and cached token keys, with a projection length much smaller than the embedding dimension. We maintain a lightweight binary structure in GPU memory to facilitate these calculations. Unlike existing compression strategies that compute attention to determine token retention, HashEvict makes these decisions pre-attention, thereby reducing computational costs. Additionally, HashEvict is dynamic - at every decoding step, the key and value of the current token replace the embeddings of a token expected to produce the lowest attention score. We demonstrate that HashEvict can compress the KV cache by 30%-70% while maintaining high performance across reasoning, multiple-choice, long-context retrieval and summarization tasks.","以变换器为基础的大型语言模型( LLMs) 使用关键值缓存( KV) 缓存来大大加快推导速度, 其方法是存储过去象征的键和值嵌入。 但是, 此缓存会消耗大量 GPU 内存 。 在此工作中, 我们引入了 HashEvict 算法, 这个算法使用对地点敏感的散列( LSH ) 压缩 KV 缓存。 HashEvict 快速在缓存中找到与当前查询标记不符的标记。 这是通过计算当前象征查询和缓存符号键的二进距离, 其预测长度比嵌入的长度要小得多。 我们在 GPU内存中保留了一个轻量的二进制结构, 以方便这些计算。 与现有的压缩战略不同, 这个算出对质保留的关注, HashEvict 做出这些决定会降低计算成本。 此外, HashEvict 是动态的, 在每个解码步骤中, 当前象征的钥匙和价值将取代预嵌入预嵌入, 将生成到最低的 KVCE 30 级 。","Minghui Liu, Tahseen Rabbani, Tony O'Halloran, Ananth Sankaralingam, Mary-Anne Hartley, Furong Huang, Cornelia Fermüller, Yiannis Aloimonos",2025-06-04T22:37:29Z,HashEvict: A Pre-Attention KV Cache Eviction Strategy using   Locality-Sensitive Hashing,HashEvict: Eine Vorab-Strategie für KV-Cache-Eviktion mit lokalitätssensitivem Hassing,HashEvict:使用地方敏感散列的KV缓冲清除预留战略,http://arxiv.org/abs/2412.16187v3
198,"Recent advances in speech foundation models are largely driven by scaling both model size and data, enabling them to perform a wide range of tasks, including speech recognition. Traditionally, ASR models are evaluated using metrics like Word Error Rate (WER) and Character Error Rate (CER), which depend on ground truth labels. As a result of limited labeled data from diverse domains and testing conditions, the true generalization capabilities of these models beyond standard benchmarks remain unclear. Moreover, labeling data is both costly and time-consuming. To address this, we propose a novel label-free approach for approximating ASR performance metrics, eliminating the need for ground truth labels. Our method utilizes multimodal embeddings in a unified space for speech and transcription representations, combined with a high-quality proxy model to compute proxy metrics. These features are used to train a regression model to predict key ASR metrics like Word Error Rate (WER) and Character Error Rate (CER). We experiment with over 40 models across 14 datasets representing both standard and in-the-wild testing conditions. Our results show that we approximate the metrics within a single-digit absolute difference across all experimental configurations, outperforming the most recent baseline by more than 50\%.","语音基础模型的近期进展在很大程度上是由模型大小和数据规模的扩大推动,使这些模型能够执行包括语音识别在内的广泛任务。传统上,ASR模型使用依赖地面真相标签的单词错误率和字符错误率等计量标准进行评估。由于来自不同领域和测试条件的标签数据有限,这些模型的真实概括能力仍然不清楚。此外,标签数据既昂贵又费时。为此,我们提议采用新的无标签方法,用于采用类似ASSR性能衡量标准及功能测试条件的通用标准,从而消除地面真实标签的需要。我们的方法是使用多式嵌入统一空间的语音和转录演示,同时使用高质量的代理模型来计算代理计量。这些特征用来训练回归模型,以预测关键ASR指标,如WER值错误率和字符错误率。我们用14个数据集的40多个模型进行实验,这些模型既代表标准,又代表边缘测试条件。我们得出的结果显示,我们用一个位数的绝对差异比所有实验性基比最近50个基数的绝对差异。","Abdul Waheed, Hanin Atwany, Rita Singh, Bhiksha Raj",2025-06-04T22:29:37Z,On the Robust Approximation of ASR Metrics,Zur robusten Annäherung von ASR Metrics,ASR 度量仪的强势接近度,http://arxiv.org/abs/2502.12408v2
199,"Text-to-SQL systems translate natural language (NL) questions into SQL queries, enabling non-technical users to interact with structured data. While large language models (LLMs) have shown promising results on the text-to-SQL task, they often produce semantically incorrect yet syntactically valid queries, with limited insight into their reliability. We propose SQLens, an end-to-end framework for fine-grained detection and correction of semantic errors in LLM-generated SQL. SQLens integrates error signals from both the underlying database and the LLM to identify potential semantic errors within SQL clauses. It further leverages these signals to guide query correction. Empirical results on two public benchmarks show that SQLens outperforms the best LLM-based self-evaluation method by 25.78% in F1 for error detection, and improves execution accuracy of out-of-the-box text-to-SQL systems by up to 20%.","文本到 SQL 系统将自然语言( NL) 问题转换成 SQL 查询,使非技术用户能够与结构化数据互动。大型语言模型( LLM ) 在文本到 SQL 任务上显示了有希望的结果,但它们往往产生语义不正确但具有合成效力的查询,对其可靠性的洞察力有限。 我们提议 SQLens 系统, 用于微量检测和校正LLM 生成 SQL 的语义错误的端到端框架。 SQLens 将基础数据库和 LLM 的错误信号整合在一起, 以识别 SQL 条款中潜在的语义错误。 它进一步利用这些信号指导查询校正。 两个公共基准的经验性结果显示, SQLens 将基于LM 的最佳自我评价方法比F1 的25.78% , 用于检测错误, 并将箱外文本到 SQL 系统的执行精确度提高20% 。","Yue Gong, Chuan Lei, Xiao Qin, Kapil Vaidya, Balakrishnan Narayanaswamy, Tim Kraska",2025-06-04T22:25:47Z,SQLens: An End-to-End Framework for Error Detection and Correction in   Text-to-SQL,SQLens: Ein End-to-End-Framework zur Fehlererkennung und -korrektur in Text-to-SQL,SQLens: 文本到 SQL 错误检测和校正端到端框架,http://arxiv.org/abs/2506.04494v1
200,"Retrieval-augmented generation (RAG) has seen many empirical successes in recent years by aiding the LLM with external knowledge. However, its theoretical aspect has remained mostly unexplored. In this paper, we propose the first finite-sample generalization bound for RAG in in-context linear regression and derive an exact bias-variance tradeoff. Our framework views the retrieved texts as query-dependent noisy in-context examples and recovers the classical in-context learning (ICL) and standard RAG as the limit cases. Our analysis suggests that an intrinsic ceiling on generalization error exists on RAG as opposed to the ICL. Furthermore, our framework is able to model retrieval both from the training data and from external corpora by introducing uniform and non-uniform RAG noise. In line with our theory, we show the sample efficiency of ICL and RAG empirically with experiments on common QA benchmarks, such as Natural Questions and TriviaQA.","近几年来,通过协助LLM获得外部知识,Retrieval-Aug(RAG)一代(RAG)取得了许多经验性成功,但是,其理论方面基本上仍未探索。在本文件中,我们建议对RAG进行首个有限抽样概括,在文本内线性回归中,对RAG进行约束,并得出精确的偏差权衡。我们的框架认为,检索到的文本是依赖查询的吵闹的文本中示例,并恢复了古典文本学习(ICL)和标准RAG作为限制案例。我们的分析表明,RAG相对于ICL存在着普遍性错误的内在上限。此外,我们的框架能够通过引入统一和非统一的RAG噪音来模拟从培训数据和外部公司进行的检索。根据我们的理论,我们展示了ICL和RAG的样本效率,并实验了共同的QA基准,如自然问题和TriviaQA。","Yang Guo, Yutian Tao, Yifei Ming, Robert D. Nowak, Yingyu Liang",2025-06-04T22:06:43Z,Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified   Theory and Risk Bounds,Retrieval-Augmented Generation as Noisy In-Context Learning: Eine einheitliche Theorie und Risikogrenzen,作为有噪音的文内学习:统一理论和风险波,http://arxiv.org/abs/2506.03100v2
201,"The NLP research community has made publicly available numerous instruments for measuring representational harms caused by large language model (LLM)-based systems. These instruments have taken the form of datasets, metrics, tools, and more. In this paper, we examine the extent to which such instruments meet the needs of practitioners tasked with evaluating LLM-based systems. Via semi-structured interviews with 12 such practitioners, we find that practitioners are often unable to use publicly available instruments for measuring representational harms. We identify two types of challenges. In some cases, instruments are not useful because they do not meaningfully measure what practitioners seek to measure or are otherwise misaligned with practitioner needs. In other cases, instruments - even useful instruments - are not used by practitioners due to practical and institutional barriers impeding their uptake. Drawing on measurement theory and pragmatic measurement, we provide recommendations for addressing these challenges to better meet practitioner needs.","国家劳工政策研究界公布了许多衡量基于大语言模式(LLM)的系统造成的代表伤害的工具,这些工具以数据集、衡量标准、工具等形式出现,在本文件中,我们审查了这些工具在多大程度上满足了负责评价以LLM为基础的系统的从业人员的需要。通过与12名此类从业人员的半结构性访谈,我们发现从业人员往往无法使用公开可用的工具来衡量代表伤害。我们确定了两类挑战。在某些情况下,这些工具没有用处,因为它们没有切实衡量从业人员想要衡量什么,或者与从业人员的需要不相符。在另一些情况下,由于实际和体制上的障碍妨碍他们接受这些文书,从业人员没有使用这些工具,甚至是有用的工具。我们借助计量理论和务实的计量,为应对这些挑战提出了建议,以更好地满足从业人员的需要。","Emma Harvey, Emily Sheng, Su Lin Blodgett, Alexandra Chouldechova, Jean Garcia-Gathright, Alexandra Olteanu, Hanna Wallach",2025-06-04T22:01:31Z,Understanding and Meeting Practitioner Needs When Measuring   Representational Harms Caused by LLM-Based Systems,"Verständnis und Begegnung mit Praktikern bei der Messung repräsentativer Schäden, die durch LLM-basierte Systeme verursacht werden",衡量以LLM为基础的系统造成的代表伤害时的理解和满足从业者需要,http://arxiv.org/abs/2506.04482v1
202,"Learning from preference feedback is essential for aligning large language models (LLMs) with human values and improving the quality of generated responses. However, existing preference learning methods rely heavily on curated data from humans or advanced LLMs, which is costly and difficult to scale. In this work, we present PUGC, a novel framework that leverages implicit human Preferences in unlabeled User-Generated Content (UGC) to generate preference data. Although UGC is not explicitly created to guide LLMs in generating human-preferred responses, it often reflects valuable insights and implicit preferences from its creators that has the potential to address readers' questions. PUGC transforms UGC into user queries and generates responses from the policy model. The UGC is then leveraged as a reference text for response scoring, aligning the model with these implicit preferences. This approach improves the quality of preference data while enabling scalable, domain-specific alignment. Experimental results on Alpaca Eval 2 show that models trained with DPO and PUGC achieve a 9.37% performance improvement over traditional methods, setting a 35.93% state-of-the-art length-controlled win rate using Mistral-7B-Instruct. Further studies highlight gains in reward quality, domain-specific alignment effectiveness, robustness against UGC quality, and theory of mind capabilities. Our code and dataset are available at https://zhaoxuan.info/PUGC.github.io/","从偏好反馈中学习偏好对于使大型语言模型(LLMs)与人的价值相一致以及提高所产生答复的质量至关重要。然而,现有的偏好学习方法在很大程度上依赖来自人类或高级LMs的整理数据,这些数据成本高且难以推广。在这项工作中,我们提出PUGC,这是一个在未贴标签的用户光化内容(UGC)中利用隐含的人的偏好生成偏好的新框架,以生成偏好数据。虽然没有明确创建UGC来指导LLMs生成人偏爱的响应,但它往往反映其创造者的宝贵洞察力和隐含的偏好,而后者有可能解决读者的问题。 PUGC将UGC转换为用户查询,并从政策模型中生成答复。然后将UGC作为响应评分的参考文本,使模型与这些隐含的偏好选择相一致。这个方法提高了优惠数据的质量,同时使可缩放的、特定域校准2的实验结果显示,与DPO和PUGC培训的模型相比传统方法提高了9.37%的业绩改进,将35.3%的长度-state-stat-lag-laft-r-ration-ration-ration-rg-rmax-rmax-rmax-rmaxal cal cal cal calgalalality calgality calgalgality calgality calgality calgalgality cality cality cality cality cality calgality calgalgalgality calgalgality cality cality rescality resbality rescality restialbalbalbality acality calbalbalbalbality cabality resticality resticality resticality resticality resticalbality restical restical restial restical restial restiality restial restialality resticalalalalalalalalalality restialality restialality res","Zhaoxuan Tan, Zheng Li, Tianyi Liu, Haodong Wang, Hyokun Yun, Ming Zeng, Pei Chen, Zhihan Zhang, Yifan Gao, Ruijie Wang, Priyanka Nigam, Bing Yin, Meng Jiang",2025-06-04T21:29:11Z,Aligning Large Language Models with Implicit Preferences from   User-Generated Content,Ausrichtung großer Sprachmodelle mit impliziten Einstellungen aus benutzergenerierten Inhalten,将大语言模式与用户生成内容的隐性首选项相匹配,http://arxiv.org/abs/2506.04463v1
203,"Watermarking techniques for large language models (LLMs) can significantly impact output quality, yet their effects on truthfulness, safety, and helpfulness remain critically underexamined. This paper presents a systematic analysis of how two popular watermarking approaches-Gumbel and KGW-affect these core alignment properties across four aligned LLMs. Our experiments reveal two distinct degradation patterns: guard attenuation, where enhanced helpfulness undermines model safety, and guard amplification, where excessive caution reduces model helpfulness. These patterns emerge from watermark-induced shifts in token distribution, surfacing the fundamental tension that exists between alignment objectives.   To mitigate these degradations, we propose Alignment Resampling (AR), an inference-time sampling method that uses an external reward model to restore alignment. We establish a theoretical lower bound on the improvement in expected reward score as the sample size is increased and empirically demonstrate that sampling just 2-4 watermarked generations effectively recovers or surpasses baseline (unwatermarked) alignment scores. To overcome the limited response diversity of standard Gumbel watermarking, our modified implementation sacrifices strict distortion-freeness while maintaining robust detectability, ensuring compatibility with AR. Experimental results confirm that AR successfully recovers baseline alignment in both watermarking approaches, while maintaining strong watermark detectability. This work reveals the critical balance between watermark strength and model alignment, providing a simple inference-time solution to responsibly deploy watermarked LLMs in practice.","大型语言模型(LLMS)的水标记技术可以对产出质量产生重大影响,但是它们对真实性、安全性和帮助性的影响仍然严重不足。本文件对两种流行的水标记方法-Gumberl和KGW如何影响四个对齐的LLMS的这些核心校准特性进行了系统分析。我们的实验揭示了两种截然不同的退化模式:保护减少,因为加强帮助性会破坏模型安全;保护放大,因为过度谨慎会减少模型的帮助性。这些模式产生于标志性分配的水标记导致的改变,揭示了调整目标之间存在的根本紧张关系。为了减轻这些退化,我们建议调整抽取(AR),即一种推断性时间取样方法,使用外部奖励模式来恢复校准。我们从理论上对预期的奖励得分的改进进行较低约束,因为抽样规模的增加,而且经验表明,仅仅2-4个有标志的代人有效地恢复或超过基线(无水标记的)校准。为了克服标准Gumber水标记的响应多样性,我们修改后的执行在保持可靠的可探测性、保证可靠基线和可靠地保持基准之间的一致性,同时确认可靠地测量和可靠地测量工作的结果。","Apurv Verma, NhatHai Phan, Shubhendu Trivedi",2025-06-04T21:29:07Z,Watermarking Degrades Alignment in Language Models: Analysis and   Mitigation,Wasserzeichen degradiert Ausrichtung in Sprachmodellen: Analyse und Milderung,语言模型的分级调整:分析和减轻影响,http://arxiv.org/abs/2506.04462v1
204,"A core aspect of compositionality, systematicity is a desirable property in ML models as it enables strong generalization to novel contexts. This has led to numerous studies proposing benchmarks to assess systematic generalization, as well as models and training regimes designed to enhance it. Many of these efforts are framed as addressing the challenge posed by Fodor and Pylyshyn. However, while they argue for systematicity of representations, existing benchmarks and models primarily focus on the systematicity of behaviour. We emphasize the crucial nature of this distinction. Furthermore, building on Hadley's (1994) taxonomy of systematic generalization, we analyze the extent to which behavioural systematicity is tested by key benchmarks in the literature across language and vision. Finally, we highlight ways of assessing systematicity of representations in ML models as practiced in the field of mechanistic interpretability.","构成性、系统性的核心方面是多语种模式中一个可取的特性,因为多语种模式能够对新情况进行有力的概括化,因此,许多研究提出了评估系统化概括性的基准,以及旨在加强这种普遍性的模式和培训制度,其中许多努力是针对Fodor和Pylyshyn所构成的挑战而设计的,然而,虽然它们主张表述系统化,但现有的基准和模式主要侧重于行为的系统化。我们强调这种区分的关键性质。此外,在Hadley(1994年)系统化概括性分类法的基础上,我们分析了不同语言和愿景的文献中主要基准检验行为系统性的程度。最后,我们强调了如何评估多语种语言和愿景模型中在机械化解释性领域中实践的表述的系统性。","Ivan Vegner, Sydelle de Souza, Valentin Forch, Martha Lewis, Leonidas A. A. Doumas",2025-06-04T21:22:38Z,Behavioural vs. Representational Systematicity in End-to-End Models: An   Opinionated Survey,Verhaltens- vs. Repräsentations-Systematik in End-to-End-Modellen: Eine Meinungsumfrage,终端至终端模型中的代表系统化:有意见的调查,http://arxiv.org/abs/2506.04461v1
205,"Entity structure extraction, which aims to extract entities and their associated attribute-value structures from text, is an essential task for text understanding and knowledge graph construction. Existing methods based on large language models (LLMs) typically rely heavily on predefined entity attribute schemas or annotated datasets, often leading to incomplete extraction results. To address these challenges, we introduce Zero-Shot Open-schema Entity Structure Discovery (ZOES), a novel approach to entity structure extraction that does not require any schema or annotated samples. ZOES operates via a principled mechanism of enrichment, refinement, and unification, based on the insight that an entity and its associated structure are mutually reinforcing. Experiments demonstrate that ZOES consistently enhances LLMs' ability to extract more complete entity structures across three different domains, showcasing both the effectiveness and generalizability of the method. These findings suggest that such an enrichment, refinement, and unification mechanism may serve as a principled approach to improving the quality of LLM-based entity structure discovery in various scenarios.","实体结构提取旨在从文本中提取实体及其相关属性价值结构,是理解文本和绘制知识图的基本任务; 以大语言模型为基础的现有方法通常严重依赖预先界定的实体属性图案或附加说明的数据集,往往导致不完整的提取结果; 为了应对这些挑战,我们采用了零热抽取开放系统实体结构发现(ZOES),这是对实体结构提取的一种新做法,不需要任何计划或附加说明的样本; ZOES通过一个浓缩、完善和统一的原则机制运作,其依据是一个实体及其相关结构相互加强的洞察力; 实验表明,ZOES不断提高LMS在三个不同领域提取更完整的实体结构的能力,显示该方法的有效性和可概括性; 这些研究结果表明,这种浓缩、完善和统一机制可以作为一种原则性方法,用于提高不同情况下以LLMM为基础的实体结构发现的质量。","Xueqiang Xu, Jinfeng Xiao, James Barry, Mohab Elkaref, Jiaru Zou, Pengcheng Jiang, Yunyi Zhang, Max Giammona, Geeth de Mel, Jiawei Han",2025-06-04T21:18:39Z,Zero-Shot Open-Schema Entity Structure Discovery,Zero-Shot Open-Schema Entity Structure Discovery,零热开放系统实体结构发现,http://arxiv.org/abs/2506.04458v1
206,"Retrieval-Augmented Generation (RAG) struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialized terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision, while fine-tuning solutions are costly, slow, and lack generalization as new data emerges. Achieving zero-shot precision with retrievers without fine-tuning still remains a key challenge. We introduce 'MetaGen Blended RAG', a novel enterprise search approach that enhances semantic retrievers through a metadata generation pipeline and hybrid query indexes using dense and sparse vectors. By leveraging key concepts, topics, and acronyms, our method creates metadata-enriched semantic indexes and boosted hybrid queries, delivering robust, scalable performance without fine-tuning. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks and even rivaling fine-tuned models on that dataset, while also excelling on datasets like SQuAD and NQ. This approach redefines enterprise search using a new approach to building semantic retrievers with unmatched generalization across specialized domains.","在培训前,LLMM在培训前所见的复杂和专门术语中,往往被隔离在防火墙后面,富含了复杂和混合的术语。医学、网络或法律等领域的语义变异妨碍了RAG的背景精确性,而微调解决方案则成本高、速度慢,且缺乏通化性,而随着新数据的出现,微调解决方案成本高、速度低,且缺乏通化性。在不作微调的检索器中实现零点精确度仍然是一个重大挑战。我们引入了“MetaGen Blendimed RAG”这一新的企业搜索方法,通过使用密度和稀薄矢量的矢量的元数据生成管道和混合查询指数和缩略式来增强语义检索器。通过利用关键概念、专题和缩略图,我们的方法创造了元富含语义的语义指数和混合查询,在不作微调的情况下提供稳健健健健、可伸缩的性工作。关于生物医学普布马达卡数据集,MetGen Blend RAG实现了82%的检索准确性和77%的准确性,超越了所有先前的RAGAG基准,甚至对准的RAG数据库进行对准模型的模型的模型,同时对准了该数据库的搜索模型,同时对准了整个SGADSDSDSDSlades的升级的升级的升级的搜索式的模型。","Kunal Sawarkar, Shivam R. Solanki, Abhilasha Mangal",2025-06-04T20:03:54Z,MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized   Domain Question-Answering,MetaGen Blended RAG: Null-Shot-Präzision für spezielle Domain-Frage-Antworten entriegeln,MetaGen Blenden RAG: 专门域问题解锁零热精确度,http://arxiv.org/abs/2505.18247v2
207,"Recent studies have shown that vector representations of contextual embeddings learned by pre-trained large language models (LLMs) are effective in various downstream tasks in numerical domains. Despite their significant benefits, the tendency of LLMs to hallucinate in such domains can have severe consequences in applications such as energy, nature, finance, healthcare, retail and transportation, among others. To guarantee prediction reliability and accuracy in numerical domains, it is necessary to open the black-box and provide performance guarantees through explanation. However, there is little theoretical understanding of when pre-trained language models help solve numeric downstream tasks. This paper seeks to bridge this gap by understanding when the next-word prediction capability of LLMs can be adapted to numerical domains through a novel analysis based on the concept of isotropy in the contextual embedding space. Specifically, we consider a log-linear model for LLMs in which numeric data can be predicted from its context through a network with softmax in the output layer of LLMs (i.e., language model head in self-attention). We demonstrate that, in order to achieve state-of-the-art performance in numerical domains, the hidden representations of the LLM embeddings must possess a structure that accounts for the shift-invariance of the softmax function. By formulating a gradient structure of self-attention in pre-trained models, we show how the isotropic property of LLM embeddings in contextual embedding space preserves the underlying structure of representations, thereby resolving the shift-invariance problem and providing a performance guarantee. Experiments show that different characteristics of numeric data and model architecture could have different impacts on isotropy.","最近的研究显示,通过受过训练的大型语言模型(LLMs)所学背景嵌入的矢量表达方式在数字领域的各种下游任务中是有效的。尽管LLMs在这类领域的幻觉趋势具有重大好处,但在这类领域的幻觉趋势可能会在能源、自然、金融、保健、零售和运输等应用中产生严重后果。为了保证数字领域的预测可靠性和准确性,有必要打开黑盒并通过解释提供性能保障。然而,在经过训练的语文模型帮助解决数字性下游任务时,人们很少从理论上理解这一点。本文试图通过理解来弥合这一差距,即LLMs的下一字预测能力可以通过基于背景嵌入空间中的异性化概念进行的新分析来适应数字领域。具体地,我们考虑LMss的行迹线性模型,通过LMSM输出层输出层的软分子网络来预测数字数据,而LMSM(即语言模型在自我留意时,语言模型头部)可以证明,我们为了在数字领域内部结构内实现最先进的稳定性表现,因此,LLM的内层结构的内层结构必须显示内层结构的自我显示内层结构。","Rashed Shelim, Shengzhe Xu, Walid Saad, Naren Ramakrishnan",2025-06-04T19:58:08Z,When can isotropy help adapt LLMs' next word prediction to numerical   domains?,"Wann kann Isotropie helfen, die nächste Wortvorhersage von LLMs an numerische Domänen anzupassen?",何时才能帮助LLMS的下一个字词预测适应数字域?,http://arxiv.org/abs/2505.17135v3
208,"It is straightforward to design an unbiased gradient estimator that stochastically cuts the backpropagation flow through any part of a computational graph. By cutting the parts that have little effect on the computation, one can potentially save a significant amount of backpropagation computation in exchange for a minimal increase in the stochastic gradient variance, in some situations. Such a situation occurs in the attention mechanism of the transformer architecture. For long sequences, attention becomes the limiting factor, as its compute requirements increase quadratically with sequence length $n$. At the same time, most attention weights become very small, as most attention heads tend to connect a given token with only a small fraction of other tokens in the sequence. These weights become promising targets for cutting backpropagation. We propose a simple probabilistic rule controlled by a single parameter $c$ that cuts back-propagation through most attention weights, leaving at most $c$ interactions per token per attention head. This brings a factor of $c/n$ reduction in the compute required for the attention backpropagation, turning it from quadratic $O(n^2)$ to linear complexity $O(nc)$. We have empirically verified that, for a typical transformer model, cutting about $99\%$ of the attention gradient flow (i.e. choosing $c \sim 25-30$) results in relative gradient variance increase of only about $1\%$ for $n \sim 2000$, and it decreases with $n$. This approach is amenable to efficient sparse matrix implementation, thus being promising for making the cost of a backward pass negligible relative to the cost of a forward pass when training a transformer model on long sequences.","设计一个公正的梯度估计器可以直截了当地地通过计算图的任何部分来削减反向调整流。 通过剪切对计算结果影响不大的部分,人们有可能节省大量反向调整计算, 以换取某些情况下微度梯度差异的最小增加。 这种情况发生在变压器结构的注意机制中。 对于长序, 关注成为限制因素, 因为它的计算要求随着序列长长的美元而四倍增加。 同时, 多数关注重量变得非常小, 因为大多数关注头倾向于将一个给定的表示与序列中其他象征的一小部分连接起来。 这些重量成为削减反向调整的很有希望的目标。 我们提出一个简单的概率规则, 由单一的参数美元控制, 通过大多数注意权重来削减反向调整。 将每个注意头的模型以美元为最高, 仅以美元/ 美元计算。 这使得关注反向向下调所需的计算值非常低, 将一个相对的递增价值与相对的 美元 美元 。","Sergey Pankov, Georges Harik",2025-06-04T19:53:25Z,SUS backprop: linear backpropagation algorithm for long inputs in   transformers,SUS Backprop: linearer Backpropagation-Algorithmus für lange Eingänge in Transformatoren,SUS 背反向prop: 变压器中长输入的线性反向反向通信算法,http://arxiv.org/abs/2505.15080v2
209,"Contemporary approaches to assisted scientific discovery use language models to automatically generate large numbers of potential hypothesis to test, while also automatically generating code-based experiments to test those hypotheses. While hypotheses can be comparatively inexpensive to generate, automated experiments can be costly, particularly when run at scale (i.e. thousands of experiments). Developing the capacity to filter hypotheses based on their feasibility would allow discovery systems to run at scale, while increasing their likelihood of making significant discoveries. In this work we introduce Matter-of-Fact, a challenge dataset for determining the feasibility of hypotheses framed as claims. Matter-of-Fact includes 8.4k claims extracted from scientific articles spanning four high-impact contemporary materials science topics, including superconductors, semiconductors, batteries, and aerospace materials, while including qualitative and quantitative claims from theoretical, experimental, and code/simulation results. We show that strong baselines that include retrieval augmented generation over scientific literature and code generation fail to exceed 72% performance on this task (chance performance is 50%), while domain-expert verification suggests nearly all are solvable -- highlighting both the difficulty of this task for current models, and the potential to accelerate scientific discovery by making near-term progress.","协助科学发现当代方法利用语言模型自动生成大量潜在假设,进行测试,同时自动生成基于代码的实验,以测试这些假设。虽然假设可能相对便宜,但自动实验成本可能很高,特别是在大规模运行时(即数千项实验)。根据可行性开发过滤假设的能力,将使发现系统能够大规模运行,同时增加其重大发现的可能性。在这项工作中,我们引入了“事实物质”,这是用于确定假说作为索赔框架的可行性的挑战数据集。“事实”包括从科学物品中摘取的8.4k项索赔,涉及四个影响较大的当代材料科学专题,包括超导体、半导体、电池和航空航天材料,同时包括理论、实验和代码/模拟结果的定性和定量索赔。我们表明,包括检索在内的强有力的基线能够扩大科学文献的生成,代码生成无法超过这项任务的72%的性能(速度为50%),而域专家核查则表明,几乎全部都是可溶解的 -- 突出这一任务对于当前模型来说十分困难,并且有可能通过近期的科学发现来加速科学发现。","Peter Jansen, Samiah Hassan, Ruoyao Wang",2025-06-04T19:43:18Z,Matter-of-Fact: A Benchmark for Verifying the Feasibility of   Literature-Supported Claims in Materials Science,Matter-of-Fact: Ein Benchmark für die Überprüfung der Machbarkeit literaturunterstützter Ansprüche in der Materialwissenschaft,事实事实:材料科学中以文献支持的索偿要求可行性核查基准,http://arxiv.org/abs/2506.04410v1
210,"This paper describes EmoRAG, a system designed to detect perceived emotions in text for SemEval-2025 Task 11, Subtask A: Multi-label Emotion Detection. We focus on predicting the perceived emotions of the speaker from a given text snippet, labeling it with emotions such as joy, sadness, fear, anger, surprise, and disgust. Our approach does not require additional model training and only uses an ensemble of models to predict emotions. EmoRAG achieves results comparable to the best performing systems, while being more efficient, scalable, and easier to implement.","本文描述了EmoRAG, 该系统的目的是在SemEval-2025任务11(Subtask A:多标签情感检测)的文本中检测感知到的情绪。我们侧重于预测特定文本片段对演讲者的感知情绪,用快乐、悲伤、恐惧、愤怒、惊讶和厌恶等情感来标注。我们的方法不需要额外的示范培训,而只使用一系列模型来预测情绪。 EmoRAG取得了与最优秀的系统相类似的效果,同时提高了效率、可扩展性并更容易实施。","Lev Morozov, Aleksandr Mogilevskii, Alexander Shirnin",2025-06-04T19:41:24Z,Empaths at SemEval-2025 Task 11: Retrieval-Augmented Approach to   Perceived Emotions Prediction,Empathen bei SemEval-2025 Aufgabe 11: Erneuter Ansatz zur Vorhersage wahrnehmbarer Emotionen,SemEval-2025任务11:感知情感预测的回溯-增强方法,http://arxiv.org/abs/2506.04409v1
211,"Humans have a remarkable ability to acquire and understand grammatical phenomena that are seen rarely, if ever, during childhood. Recent evidence suggests that language models with human-scale pretraining data may possess a similar ability by generalizing from frequent to rare constructions. However, it remains an open question how widespread this generalization ability is, and to what extent this knowledge extends to meanings of rare constructions, as opposed to just their forms. We fill this gap by testing human-scale transformer language models on their knowledge of both the form and meaning of the (rare and quirky) English LET-ALONE construction. To evaluate our LMs we construct a bespoke synthetic benchmark that targets syntactic and semantic properties of the construction. We find that human-scale LMs are sensitive to form, even when related constructions are filtered from the dataset. However, human-scale LMs do not make correct generalizations about LET-ALONE's meaning. These results point to an asymmetry in the current architectures' sample efficiency between language form and meaning, something which is not present in human language learners.","近来的证据表明,语言模型与人类规模的预培训数据可能具有类似的能力,从频繁的建筑到罕见的建筑。然而,这一一般化能力究竟有多普遍,以及这种知识在多大程度上延伸到稀有建筑的含义,而不是仅仅以其形式存在。我们通过测试人类规模变压器语言模型,了解(稀有和quirky)英国LET-ALONE构造的形式和含义,填补了这一差距。为了评估我们的LMS,我们构建了一个简单的合成基准,针对建筑的合成和语义特性。我们发现,人类规模的LMS对形成非常敏感,即使相关的构造从数据集中过滤出来。然而,人类规模LMS并没有对LET-ALONE的含义进行正确的概括化。这些结果表明,目前结构在语言形式和含义之间的抽样效率上存在不对称,而人类语言学习者却不存在这一点。","Wesley Scivetti, Tatsuya Aoyama, Ethan Wilcox, Nathan Schneider",2025-06-04T19:40:23Z,Unpacking Let Alone: Human-Scale Models Generalize to a Rare   Construction in Form but not Meaning,"Auspacken lassen Allein: Mensch-Schal-Modelle Generalisieren zu einer seltenen Konstruktion in Form, aber nicht Bedeutung",单独包装:人类规模模型一般化为形式上但非意义上的稀有构造,http://arxiv.org/abs/2506.04408v1
212,"We introduce MedAgentGYM, the first publicly available training environment designed to enhance coding-based medical reasoning capabilities in large language model (LLM) agents. MedAgentGYM comprises 72,413 task instances across 129 categories derived from authentic real-world biomedical scenarios. Tasks are encapsulated within executable coding environments, each featuring detailed task descriptions, interactive feedback mechanisms, verifiable ground-truth annotations, and scalable training trajectory generation. Extensive benchmarking of over 30 LLMs reveals a notable performance disparity between commercial API-based models and open-source counterparts. Leveraging MedAgentGYM, Med-Copilot-7B achieves substantial performance gains through supervised fine-tuning (+36.44%) and continued reinforcement learning (+42.47%), emerging as an affordable and privacy-preserving alternative competitive with gpt-4o. By offering both a comprehensive benchmark and accessible, expandable training resources within unified execution environments, MedAgentGYM delivers an integrated platform to develop LLM-based coding assistants for advanced biomedical research and practice.","我们引入了MedAgentiGYM,这是第一个在大型语言模型(LLM)代理中加强基于编码的医疗推理能力的公开培训环境。MedAgentiGYM由来自真实现实世界生物医学情景的129个类别中的72,413个任务实例组成。任务包含在可执行的编码环境中,每个环境都有详细的任务说明、互动反馈机制、可核查的地面实况说明和可扩展的培训轨迹生成。30多个LMS的广泛基准显示商业基于API的模型和公开来源的对应方之间存在显著的绩效差距。MedAgentiGYM、Med-Coopol-7B通过监督的微调(+36.44%)和持续的强化学习(+42.47%)取得了显著的业绩收益,这正在形成一种与gpt-4o的负担得起和隐私保护替代竞争。通过在统一的执行环境中提供一个全面的基准和可获得的、可扩展的培训资源,M meAgen性GYM为先进的生物医学研究和实践开发基于LLM的编码助理提供了一个综合平台。","Ran Xu, Yuchen Zhuang, Yishan Zhong, Yue Yu, Xiangru Tang, Hang Wu, May D. Wang, Peifeng Ruan, Donghan Yang, Tao Wang, Guanghua Xiao, Carl Yang, Yang Xie, Wenqi Shi",2025-06-04T19:38:55Z,MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at   Scale,MedAgentGym: Schulung von LLM-Agenten für codebasierte medizinische Reasoning auf Scale,"MedAgengentGym:培训法学硕士人员,用于按比例按编码计算医疗理由",http://arxiv.org/abs/2506.04405v1
213,"Large language models (LLMs) have achieved remarkable successes on various tasks. However, recent studies have found that there are still significant challenges to the logical reasoning abilities of LLMs, which can be categorized into the following two aspects: (1) Logical question answering: LLMs often fail to generate the correct answer within a complex logical problem which requires sophisticated deductive, inductive or abductive reasoning given a collection of premises and constrains. (2) Logical consistency: LLMs are prone to producing responses contradicting themselves across different questions. For example, a state-of-the-art question-answering LLM Macaw, answers Yes to both questions Is a magpie a bird? and Does a bird have wings? but answers No to Does a magpie have wings?. To facilitate this research direction, we comprehensively investigate the most cutting-edge methods and propose a detailed taxonomy. Specifically, to accurately answer complex logic questions, previous methods can be categorized based on reliance on external solvers, prompts, and fine-tuning. To avoid logical contradictions, we discuss concepts and solutions of various logical consistencies, including implication, negation, transitivity, factuality consistencies, and their composites. In addition, we review commonly used benchmark datasets and evaluation metrics, and discuss promising research directions, such as extending to modal logic to account for uncertainty and developing efficient algorithms that simultaneously satisfy multiple logical consistencies.","大型语言模型(LLMS)在各种任务中取得了显著的成功,然而,最近的研究发现,LLMS逻辑推理能力仍然存在重大挑战,这可以分为以下两个方面:(1) 逻辑问题回答:LLMS往往无法在一个复杂的逻辑问题中产生正确的答案,而复杂的逻辑问题需要复杂的推理、感化或绑架推理,因为有一系列的前提和限制。 (2) 逻辑一致性:LLMS容易在不同问题上产生自相矛盾的反应。例如,最先进的回答问题LLMM Macaw,回答两个问题都是对的吗? 鸟有翅膀吗?但答案是否定的吗? 为了便利这一研究方向,我们全面调查最先进的方法,并提出详细的分类。具体地说,为了准确回答复杂的逻辑问题,以前的方法可以基于依赖外部解答者、提示和微调来分类。为了避免逻辑上的矛盾,我们讨论各种逻辑组合的概念和解决办法,包括暗示、否定性、过渡性、事实质量和翅膀有翅膀?但是,为了扩大这种研究方向,我们同时讨论它们所使用的逻辑和综合的逻辑分析。","Fengxiang Cheng, Haoxuan Li, Fenrong Liu, Robert van Rooij, Kun Zhang, Zhouchen Lin",2025-06-04T19:38:43Z,Empowering LLMs with Logical Reasoning: A Comprehensive Survey,Stärkung von LLMs mit logischer Begründung: Eine umfassende Umfrage,赋予LLMs以逻辑理由:全面调查,http://arxiv.org/abs/2502.15652v3
214,"It has been hypothesized that neural networks with similar architectures trained on similar data learn shared representations relevant to the learning task. We build on this idea by extending the conceptual framework where representations learned across models trained on the same data can be expressed as linear combinations of a \emph{universal} set of basis features. These basis features underlie the learning task itself and remain consistent across models, regardless of scale. From this framework, we propose the \textbf{Linear Representation Transferability (LRT)} Hypothesis -- that there exists an affine transformation between the representation spaces of different models. To test this hypothesis, we learn affine mappings between the hidden states of models of different sizes and evaluate whether steering vectors -- directions in hidden state space associated with specific model behaviors -- retain their semantic effect when transferred from small to large language models using the learned mappings. We find strong empirical evidence that such affine mappings can preserve steering behaviors. These findings suggest that representations learned by small models can be used to guide the behavior of large models, and that the LRT hypothesis may be a promising direction on understanding representation alignment across model scales.","据推测,具有类似数据结构的类似结构的神经网络学会了与学习任务相关的共同表述。我们以这一理念为基础,扩展了概念框架,通过这个框架,学习了与同一数据有关的不同模型之间的表述,可以以一组基础特征的线性组合表示。这些基础特征是学习任务本身的基础,并且无论规模大小,在各种模型之间始终保持一致。我们从这个框架中建议,“线性”绘图能够保存指导行为。这些结论表明,小模型所学的表述可以用来指导大型模型的行为,而“线性”假设可能是理解跨比例代表性的一个很有希望的方向。","Femi Bello, Anubrata Das, Fanzhi Zeng, Fangcong Yin, Liu Leqi",2025-06-04T19:24:26Z,Linear Representation Transferability Hypothesis: Leveraging Small   Models to Steer Large Models,Lineare Darstellungsübertragbarkeit Hypothese: Nutzung kleiner Modelle zu schlanken großen Modellen,线性代表线性可转移性假设:将小型模型运用到恒定大模型,http://arxiv.org/abs/2506.00653v3
215,"Speech disorders can make communication hard or even impossible for those who develop them. Personalised Text-to-Speech is an attractive option as a communication aid. We attempt voice reconstruction using a large speech model, with which we generate an approximation of a dysarthric speaker's voice prior to the onset of their condition. In particular, we investigate whether a state-of-the-art large speech model, Parler TTS, can generate intelligible speech while maintaining speaker identity. We curate a dataset and annotate it with relevant speaker and intelligibility information, and use this to fine-tune the model. Our results show that the model can indeed learn to generate from the distribution of this challenging data, but struggles to control intelligibility and to maintain consistent speaker identity. We propose future directions to improve controllability of this class of model, for the voice reconstruction task.","个人化文本到语音是一个有吸引力的选择,作为通信辅助工具。我们试图使用一个大型语音模型来进行语音重建,从而在发病前产生一个有读力的语音声音近似。我们特别调查最先进的大型语音模型Parler TTS(Parler TTS)能否在保持发言者身份的同时产生易懂的语音。我们制作了一个数据集,用相关语音和智能信息对它进行注释,并用它来微调模型。我们的结果显示,该模型确实可以学习从这种富有挑战性的数据的分发中产生,但为了控制智能和保持连贯一致的语音身份而奋斗。我们提出了未来的方向,以改善这一类型模式的可控性,以便完成语音重建任务。","Ariadna Sanchez, Simon King",2025-06-04T19:23:44Z,Can we reconstruct a dysarthric voice with the large speech model Parler   TTS?,Können wir mit dem großen Sprachmodell Parler TTS eine dysarthrische Stimme rekonstruieren?,我们能用大型语音模型Parler TTS 重建一个自相矛盾的声音吗?,http://arxiv.org/abs/2506.04397v1
216,"Customer care is an essential pillar of the e-commerce shopping experience with companies spending millions of dollars each year, employing automation and human agents, across geographies (like US, Canada, Mexico, Chile), channels (like Chat, Interactive Voice Response (IVR)), and languages (like English, Spanish). SOTA pre-trained models like multilingual-BERT, fine-tuned on annotated data have shown good performance in downstream tasks relevant to Customer Care. However, model performance is largely subject to the availability of sufficient annotated domain-specific data. Cross-domain availability of data remains a bottleneck, thus building an intent classifier that generalizes across domains (defined by channel, geography, and language) with only a few annotations, is of great practical value. In this paper, we propose an embedder-cum-classifier model architecture which extends state-of-the-art domain-specific models to other domains with only a few labeled samples. We adopt a supervised fine-tuning approach with isotropic regularizers to train a domain-specific sentence embedder and a multilingual knowledge distillation strategy to generalize this embedder across multiple domains. The trained embedder, further augmented with a simple linear classifier can be deployed for new domains. Experiments on Canada and Mexico e-commerce Customer Care dataset with few-shot intent detection show an increase in accuracy by 20-23% against the existing state-of-the-art pre-trained models.","客户护理是电子商务购物经验的一个重要支柱,公司每年花费数百万美元,使用自动化和人力代理,跨越地理地理(如美国、加拿大、墨西哥、智利)、频道(如聊天、互动语音反应(IVR))和语言(如英文、西班牙文),客户护理是电子商务购物经验的重要支柱。SOTA通过对附加说明的数据进行微调,对多语言-BERT等经过预先培训的模式显示,在与客户护理有关的下游任务中表现良好。然而,模型性能在很大程度上取决于是否有足够的附加说明的域别数据。交叉数据提供仍然是一个瓶颈,因此建立一个意向分类器,在各领域(由频道、地理和语言界定),只用几个说明说明,进行通用。在本文中,我们提议一个嵌入式加分类模型,将最新的域别模型推广到只有少数标签样本的其他领域。我们采用监督的微调方法,以培训一个特定域内嵌入和多语言知识提法,将这一域(由频道、地理和语言加以界定)的域内嵌式分类,只有几个说明,具有很大的说明。我们所训练的嵌入式的域域域域域域别,可以进一步扩大一个用于加拿大的域域域域域域域域域域域域域域域域域域域域域域和直图。","Saurabh Kumar, Sourav Bansal, Neeraj Agrawal, Priyanka Bhatt",2025-06-04T19:14:48Z,Building a Few-Shot Cross-Domain Multilingual NLU Model for Customer   Care,Aufbau eines weniger scharfen Cross-Domain-Mehrsprachigen NLU-Modells für die Kundenbetreuung,建造一个多语言、多语言、新语言和新语言的客户护理模式,http://arxiv.org/abs/2506.04389v1
217,"With more than 11 times as many pageviews as the next, English Wikipedia dominates global knowledge access relative to other language editions. Readers are prone to assuming English Wikipedia as a superset of all language editions, leading many to prefer it even when their primary language is not English. Other language editions, however, comprise complementary facts rooted in their respective cultures and media environments, which are marginalized in English Wikipedia. While Wikipedia's user interface enables switching between language editions through its Interlanguage Link (ILL) system, it does not reveal to readers that other language editions contain valuable, complementary information. We present WikiGap, a system that surfaces complementary facts sourced from other Wikipedias within the English Wikipedia interface. Specifically, by combining a recent multilingual information-gap discovery method with a user-centered design, WikiGap enables access to complementary information from French, Russian, and Chinese Wikipedia. In a mixed-methods study (n=21), WikiGap significantly improved fact-finding accuracy, reduced task time, and received a 32-point higher usability score relative to Wikipedia's current ILL-based navigation system. Participants reported increased awareness of the availability of complementary information in non-English editions and reconsidered the completeness of English Wikipedia. WikiGap thus paves the way for improved epistemic equity across language editions.","与下一版相比,英文维基百科的页面浏览量是其他语文版本的11倍以上,因此,英文维基百科占全球知识访问量的多数。读者往往认为英文维基百科是所有语文版本的超级集,即使他们的主要语言不是英语,他们也更喜欢英文维基百科。然而,其他语文版本则包含基于各自文化和媒体环境的互补事实,这些背景在英文维基百科中处于边缘地位。虽然维基百科的用户界面能够通过其跨语系链接(ILL)系统转换不同语言版本之间的信息,但它没有向读者透露其他语言版本包含有价值的补充信息。我们展示了维基百科普,这个系统从英文维基百科界面的其他维基百科中呈现了互补事实。具体地说,维基百科的多语种信息发现方法与以用户为中心的设计相结合,使维基百科能够从法文、俄文和中文的维基百科中获取补充信息。在混合方法研究(n=21)中,维基百科的准确性得到显著改进,任务时间,比维基百科目前的导航系统获得了32点更高的使用率。","Zining Wang, Yuxuan Zhang, Dongwook Yoon, Nicholas Vincent, Farhan Samir, Vered Shwartz",2025-06-04T19:04:56Z,WikiGap: Promoting Epistemic Equity by Surfacing Knowledge Gaps Between   English Wikipedia and other Language Editions,WikiGap: Förderung epistemischer Gerechtigkeit durch Surfacing Knowledge Gaps Zwischen englischer Wikipedia und anderen Sprachausgaben,"WikiGap:通过弥合英文维基百科和其他语文版之间的知识差距,促进流行公平",http://arxiv.org/abs/2505.24195v2
218,"Large Language Models (LLMs) have demonstrated remarkable performance across various Natural Language Processing (NLP) tasks, largely due to their generalisability and ability to perform tasks without additional training. However, their effectiveness for low-resource languages remains limited. In this study, we evaluate the performance of 55 publicly available LLMs on Maltese, a low-resource language, using a newly introduced benchmark covering 11 discriminative and generative tasks. Our experiments highlight that many models perform poorly, particularly on generative tasks, and that smaller fine-tuned models often perform better across all tasks. From our multidimensional analysis, we investigate various factors impacting performance. We conclude that prior exposure to Maltese during pre-training and instruction-tuning emerges as the most important factor. We also examine the trade-offs between fine-tuning and prompting, highlighting that while fine-tuning requires a higher initial cost, it yields better performance and lower inference costs. Through this work, we aim to highlight the need for more inclusive language technologies and recommend that researchers working with low-resource languages consider more ""traditional"" language modelling approaches.","大型语言模型(LLMS)在各种自然语言处理任务中表现出了显著的成绩,这主要是因为其普遍性和在没有额外培训的情况下执行任务的能力;然而,低资源语言的功效仍然有限;在本研究中,我们利用新推出的涵盖11项歧视和基因化任务的基准,评估了55个公开提供的关于低资源语言马耳他语的LLMS(LLMS)的成绩;我们的实验突出表明,许多模型表现不佳,特别是在基因化任务方面,小型微调模型在所有任务中往往表现更好。我们从多方面的分析中调查影响业绩的各种因素。我们的结论是,在培训和教学前调整期间,马耳他语的暴露是最重要的因素。我们还审查了微调与提示之间的权衡,强调微调虽然微调需要较高的初始成本,但效果更好,而且推导成本较低。我们通过这项工作,旨在强调需要更具包容性的语言技术,并建议低资源语言研究人员考虑更“传统”语言建模方法。","Kurt Micallef, Claudia Borg",2025-06-04T18:59:52Z,MELABenchv1: Benchmarking Large Language Models against Smaller   Fine-Tuned Models for Low-Resource Maltese NLP,"MELABenchv1: Benchmarking von großen Sprachmodellen gegen kleinere, feinere Modelle für Low-Resource Maltesische NLP",MELABenchv1:对照低资源马耳他低排放马耳他低排放马耳他低排放语言方案较微小的微量设计模型确定大语言模型基准,http://arxiv.org/abs/2506.04385v1
219,"Hierarchical Text Classification (HTC) has recently gained traction given the ability to handle complex label hierarchy. This has found applications in domains like E- commerce, customer care and medicine industry among other real-world applications. Existing HTC models either encode label hierarchy separately and mix it with text encoding or guide the label hierarchy structure in the text encoder. Both approaches capture different characteristics of label hierarchy and are complementary to each other. In this paper, we propose a Hierarchical Text Classification using Contrastive Learning Informed Path guided hierarchy (HTC-CLIP), which learns hierarchy-aware text representation and text informed path guided hierarchy representation using contrastive learning. During the training of HTC-CLIP, we learn two different sets of class probabilities distributions and during inference, we use the pooled output of both probabilities for each class to get the best of both representations. Our results show that the two previous approaches can be effectively combined into one architecture to achieve improved performance. Tests on two public benchmark datasets showed an improvement of 0.99 - 2.37% in Macro F1 score using HTC-CLIP over the existing state-of-the-art models.","由于有能力处理复杂的标签等级结构,等级文字分类(HTC)最近获得了牵引力。这发现在电子商业、客户护理和医药行业等领域以及其他现实世界应用中的应用。现有的等级标签分类模式要么单独编码标签等级,与文字编码混合,要么在文本编码中指导标签等级结构。两种方法都反映了标签等级结构的不同特征,彼此相互补充。在本文件中,我们提议采用使用对比学习的知情路径指导等级结构(HTC-CLIP),采用对比学习的方式,学习有等级特征的文本说明和文本知情路径指导等级结构。在HTC-CLIP的训练中,我们学习了两套不同的等级概率分类分布,在推断过程中,我们使用每个类别两种概率的集合输出,以获得最佳的两种表达方式。我们的结果显示,前两种方法可以有效地结合为一种结构,以取得更好的业绩。两个公共基准数据集的测试表明,在使用HTC-CLIP模型对现有的状态进行宏观F1评分数中,在使用HTC-CLIP模型中改进了0.99-2.37%。","Neeraj Agrawal, Saurabh Kumar, Priyanka Bhatt, Tanishka Agarwal",2025-06-04T18:51:34Z,Hierarchical Text Classification Using Contrastive Learning Informed   Path Guided Hierarchy,Hierarchische Textklassifikation mit kontraproduktivem Lernen Informierte Pfadführung Hierarchie,采用不兼容的学习、知情、知情路径的等级文字分类 指导等级,http://arxiv.org/abs/2506.04381v1
220,"Imageability (potential of text to evoke a mental image) and concreteness (perceptibility of text) are two psycholinguistic properties that link visual and semantic spaces. It is little surprise that computational methods that estimate them do so using parallel visual and semantic spaces, such as collections of image-caption pairs or multi-modal models. In this paper, we work on the supposition that text itself in an image-caption dataset offers sufficient signals to accurately estimate these properties. We hypothesize, in particular, that the peakedness of the neighborhood of a word in the semantic embedding space reflects its degree of imageability and concreteness. We then propose an unsupervised, distribution-free measure, which we call Neighborhood Stability Measure (NSM), that quantifies the sharpness of peaks. Extensive experiments show that NSM correlates more strongly with ground-truth ratings than existing unsupervised methods, and is a strong predictor of these properties for classification. Our code and data are available on GitHub (https://github.com/Artificial-Memory-Lab/imageability).","图像的可视性(文字能够引起精神图像)和具体性(文字的可感知性)是连接视觉和语义空间的两种精神语言特性。使用平行视觉和语义空间(例如图像显示配对或多式模型的收集)来估计这些特性的计算方法并不奇怪。在本文中,我们研究文本本身在图像显示数据集中的假设提供了充分信号来准确估计这些特性。我们尤其假设了语义嵌入空间中一个词的高度性反映了其可视性和具体性。我们随后提出了一种不高超的、无分布性的措施,我们称之为“近距离稳定度测量”(NSM),它能量化峰值的锐度。广泛的实验表明,NSM与地面测量比现有的不受监督的方法更紧密相关,并且是这些特性的强烈预测器。我们的代码和数据可以在GitHubi(https://github.com/Afrigimagial-Me)上查到。","Si Wu, Sebastian Bruch",2025-06-04T18:51:12Z,Uncovering Visual-Semantic Psycholinguistic Properties from the   Distributional Structure of Text Embedding Space,Enthüllen von visual-semantischen psycholinguistischen Eigenschaften aus der Verteilungsstruktur von Text-Einbettungsraum,从文本嵌入空间分布结构中隐蔽的视觉-语言心理学属性,http://arxiv.org/abs/2505.23029v2
221,"Sentence embeddings are central to modern NLP and AI systems, yet little is known about their internal structure. While we can compare these embeddings using measures such as cosine similarity, the contributing features are not human-interpretable, and the content of an embedding seems untraceable, as it is masked by complex neural transformations and a final pooling operation that combines individual token embeddings. To alleviate this issue, we propose a new method to mechanistically decompose sentence embeddings into interpretable components, by using dictionary learning on token-level representations. We analyze how pooling compresses these features into sentence representations, and assess the latent features that reside in a sentence embedding. This bridges token-level mechanistic interpretability with sentence-level analysis, making for more transparent and controllable representations. In our studies, we obtain several interesting insights into the inner workings of sentence embedding spaces, for instance, that many semantic and syntactic aspects are linearly encoded in the embeddings.","句子嵌入是现代 NLP 和 AI 系统的核心, 但对于它们的内部结构却知之甚少。 虽然我们可以使用 Cosine 相似性等措施比较这些嵌入, 但贡献特征并非人类的解释性, 嵌入内容似乎无法追踪, 因为它被复杂的神经质变和结合单个符号嵌入的最终集合操作所掩盖。 为了缓解这个问题, 我们提议了一种新的方法, 机械地解密将句子嵌入可解释的构件中, 方法是在象征性的表示式上使用字典学习 。 我们分析了如何将这些特征压缩到句中, 并评估嵌入一个句子中的潜在特征 。 这种架桥式的机械性解释与句级分析相结合, 使得更透明和可控的表达方式更加透明。 在我们的研究中, 我们获得了一些有趣的关于嵌入空间的句子内部操作的洞察力。 例如, 许多语义和合成的方面在嵌入空间中都是线性编码的 。","Matthieu Tehenan, Vikram Natarajan, Jonathan Michala, Milton Lin, Juri Opitz",2025-06-04T18:42:57Z,Mechanistic Decomposition of Sentence Representations,Mechanistische Zersetzung von Satzdarstellungen,判决代表的体力分解,http://arxiv.org/abs/2506.04373v1
222,"In this work, we investigate the causal reasoning abilities of large language models (LLMs) through the representative problem of inferring causal relationships from narratives. We find that even state-of-the-art language models rely on unreliable shortcuts, both in terms of the narrative presentation and their parametric knowledge. For example, LLMs tend to determine causal relationships based on the topological ordering of events (i.e., earlier events cause later ones), resulting in lower performance whenever events are not narrated in their exact causal order. Similarly, we demonstrate that LLMs struggle with long-term causal reasoning and often fail when the narratives are long and contain many events. Additionally, we show LLMs appear to rely heavily on their parametric knowledge at the expense of reasoning over the provided narrative. This degrades their abilities whenever the narrative opposes parametric knowledge. We extensively validate these failure modes through carefully controlled synthetic experiments, as well as evaluations on real-world narratives. Finally, we observe that explicitly generating a causal graph generally improves performance while naive chain-of-thought is ineffective. Collectively, our results distill precise failure modes of current state-of-the-art models and can pave the way for future techniques to enhance causal reasoning in LLMs.","在这项工作中,我们调查大型语言模型(LLMs)的因果推理能力,方法是通过从叙述中推断因果关系这一具有代表性的问题来推断大语言模型(LLMs)的因果推理能力;我们发现,即使是最先进的语言模型也依赖不可靠的捷径,无论是叙述性陈述还是其参数知识;例如,LLMs往往根据事件的地形顺序确定因果关系(即早期事件导致后来的事件),因此,如果事件没有按其确切的因果顺序加以说明,其性能就会下降;同样,我们证明LLMs与长期因果推理斗争,当叙述很长且包含许多事件时往往失败。此外,我们显示LLMs似乎严重依赖其准度知识,而牺牲了对所提供的叙述性陈述的推理。这削弱了他们的能力,每当叙述性说明反对参数知识时,我们通过仔细控制的合成实验以及实际世界叙述性叙述性来广泛验证这些失败模式。最后,我们指出,明确产生因果关系的图表通常会改善业绩,而天真的思维链是无效的。从整体上看,我们的结果会保持准确的失败模式,从而强化了当前磁号模型的推算方法,可以加强未来的推导法。","Khurram Yamin, Shantanu Gupta, Gaurav R. Ghosal, Zachary C. Lipton, Bryan Wilder",2025-06-04T18:26:30Z,Failure Modes of LLMs for Causal Reasoning on Narratives,Failure Modes von LLMs für die ursächliche Begründung von Narrativen,以叙述为由解释原因的LLMs失败模式,http://arxiv.org/abs/2410.23884v3
223,"Understanding how feature representations evolve across layers in large language models (LLMs) is key to improving their interpretability and robustness. While recent studies have identified critical layers linked to specific functions or behaviors, these efforts typically rely on data-dependent analyses of fine-tuned models, limiting their use to post-hoc settings. In contrast, we introduce a data-oblivious approach to identify intrinsic critical layers in pre-fine-tuned LLMs by analyzing representation dynamics via Centered Kernel Alignment(CKA). We show that layers with significant shifts in representation space are also those most affected during fine-tuning--a pattern that holds consistently across tasks for a given model. Our spectral analysis further reveals that these shifts are driven by changes in the top principal components, which encode semantic transitions from rationales to conclusions. We further apply these findings to two practical scenarios: efficient domain adaptation, where fine-tuning critical layers leads to greater loss reduction compared to non-critical layers; and backdoor defense, where freezing them reduces attack success rates by up to 40%.","在大型语言模型(LLMs)中,了解地貌表现方式如何在不同的层次上演进是提高其可解释性和稳健性的关键所在。虽然最近的研究查明了与具体功能或行为有关的关键层面,但这些努力通常依赖于对微调模型进行数据分析,将其使用限制在热后环境。相反,我们采用了一种数据模糊的方法,通过Cernel Conness(CKA)分析代表性动态,确定前调控的LLMs的内在关键层面。我们表明,在微调-一种模式中,代表空间发生重大变化的层也是那些在特定模式的任务之间始终保持着不同状态的最为受影响的层。我们的光谱分析还显示,这些变化是由顶层主要组成部分的变化驱动的,这些变化将语义从原理过渡到结论。我们进一步将这些研究结果应用于两种实际情景:高效的域适应,在其中,微调关键层导致与非临界层相比更大的损失减少;和后门防御,将攻击成功率降低40%。","Xuyuan Liu, Lei Hsiung, Yaoqing Yang, Yujun Yan",2025-06-04T18:25:14Z,Spectral Insights into Data-Oblivious Critical Layers in Large Language   Models,Spektrale Einblicke in datenvergessene kritische Ebenen in großen Sprachmodellen,大语言模型中数据可见关键层的光谱透视,http://arxiv.org/abs/2506.00382v2
224,"To build an automatic speech recognition (ASR) system that can serve everyone in the world, the ASR needs to be robust to a wide range of accents including unseen accents. We systematically study how three different variables in training data -- the number of speakers, the audio duration per each individual speaker, and the diversity of accents -- affect ASR robustness towards unseen accents in a low-resource training regime. We observe that for a fixed number of ASR training hours, it is more beneficial to increase the number of speakers (which means each speaker contributes less) than the number of hours contributed per speaker. We also observe that more speakers enables ASR performance gains from scaling number of hours. Surprisingly, we observe minimal benefits to prioritizing speakers with different accents when the number of speakers is controlled. Our work suggests that practitioners should prioritize increasing the speaker count in ASR training data composition for new languages.","为了建立一个能为全世界所有人服务的自动语音识别系统(ASR),ASR需要对广泛的口音(包括看不见的口音)保持稳健。我们系统地研究培训数据的三个不同变数 -- -- 发言者人数、每个发言者的音频持续时间以及口音的多样性 -- -- 如何影响ASR在一个低资源培训制度中对无形口音的稳健性。我们注意到,在固定的ASR培训时数中,增加发言者人数(即每个发言者贡献较少)比每个发言者提供的时数要多。我们还注意到,更多的发言者能够使ASR从时数的缩放中取得业绩收益。令人惊讶的是,在控制发言者人数时,我们注意到对具有不同口音的发言者给予优先排序的好处很小。我们的工作表明,从业人员应该优先增加ASR培训中发言者对新语言数据构成的计算。","Zheng-Xin Yong, Vineel Pratap, Michael Auli, Jean Maillard",2025-06-04T18:23:08Z,"Effects of Speaker Count, Duration, and Accent Diversity on Zero-Shot   Accent Robustness in Low-Resource ASR","Auswirkungen von Speaker-Count, Duration und Accent Diversity auf Zero-Shot Accent Robustness in Low-Resource ASR",发言人人数、持续时间和核心多样性对低资源ASR零热强度的影响,http://arxiv.org/abs/2506.04364v1
225,"As language technologies become widespread, it is important to understand how changes in language affect reader perceptions and behaviors. These relationships may be formalized as the isolated causal effect of some focal language-encoded intervention (e.g., factual inaccuracies) on an external outcome (e.g., readers' beliefs). In this paper, we introduce a formal estimation framework for isolated causal effects of language. We show that a core challenge of estimating isolated effects is the need to approximate all non-focal language outside of the intervention. Drawing on the principle of omitted variable bias, we provide measures for evaluating the quality of both non-focal language approximations and isolated effect estimates themselves. We find that poor approximation of non-focal language can lead to bias in the corresponding isolated effect estimates due to omission of relevant variables, and we show how to assess the sensitivity of effect estimates to such bias along the two key axes of fidelity and overlap. In experiments on semi-synthetic and real-world data, we validate the ability of our framework to correctly recover isolated effects and demonstrate the utility of our proposed measures.","随着语言技术的普及,理解语言变化如何影响读者的认知和行为十分重要。这些关系可能正式化,因为某些协调语言编码干预(例如事实上的不准确)对外部结果(例如读者的信仰)产生的孤立因果关系。在本文中,我们引入了语言孤立因果关系的正式估计框架。我们表明,估算孤立效应的一个核心挑战是需要在干预之外接近所有非重点语言。根据省略的可变偏差原则,我们提供评估非重点语言近似和孤立效果评估质量的措施。我们发现,由于相关变量的遗漏,非重点语言近近似可能导致相应的孤立估计结果的偏差。我们展示了如何评估作用估计在两个关键轴的忠诚和重叠方面对这种偏差的敏感性。在半综合和实际数据实验中,我们验证了我们框架正确恢复孤立效应的能力,并展示了我们拟议措施的效用。","Victoria Lin, Louis-Philippe Morency, Eli Ben-Michael",2025-06-04T18:20:45Z,Isolated Causal Effects of Natural Language,Isolierte Kausaleffekte der natürlichen Sprache,自然语言孤立的因果影响,http://arxiv.org/abs/2410.14812v3
226,"We present ReXVQA, the largest and most comprehensive benchmark for visual question answering (VQA) in chest radiology, comprising approximately 696,000 questions paired with 160,000 chest X-rays studies across training, validation, and test sets. Unlike prior efforts that rely heavily on template based queries, ReXVQA introduces a diverse and clinically authentic task suite reflecting five core radiological reasoning skills: presence assessment, location analysis, negation detection, differential diagnosis, and geometric reasoning. We evaluate eight state-of-the-art multimodal large language models, including MedGemma-4B-it, Qwen2.5-VL, Janus-Pro-7B, and Eagle2-9B. The best-performing model (MedGemma) achieves 83.24% overall accuracy. To bridge the gap between AI performance and clinical expertise, we conducted a comprehensive human reader study involving 3 radiology residents on 200 randomly sampled cases. Our evaluation demonstrates that MedGemma achieved superior performance (83.84% accuracy) compared to human readers (best radiology resident: 77.27%), representing a significant milestone where AI performance exceeds expert human evaluation on chest X-ray interpretation. The reader study reveals distinct performance patterns between AI models and human experts, with strong inter-reader agreement among radiologists while showing more variable agreement patterns between human readers and AI models. ReXVQA establishes a new standard for evaluating generalist radiological AI systems, offering public leaderboards, fine-grained evaluation splits, structured explanations, and category-level breakdowns. This benchmark lays the foundation for next-generation AI systems capable of mimicking expert-level clinical reasoning beyond narrow pathology classification. Our dataset will be open-sourced at https://huggingface.co/datasets/rajpurkarlab/ReXVQA","我们提出ReXVQA,这是胸腔放射学中视觉问题解答(VQA)的最大和最全面的基准,由大约696 000个问题和160 000个胸前X光研究相配,贯穿于培训、验证和测试组。与以往大量依赖基于模板的询问的努力不同,ReXVQA推出一个多样化和临床真实的任务套件,反映五个核心辐射推理技能:存在评估、地点分析、否定检测、差异诊断和几何推理。我们评价了八个最先进的多式大语言模型,包括MedGemma-4B-it、Qwen2.5-VL、Janus-Pro-7B和Eagle2-9B。 最佳模型(MedGemma)实现了83.24%的总体准确性。为了缩小AI业绩和临床专门知识之间的差距,我们进行了一项全面的人类读者研究,有3名放射学家参加了200个随机抽样案例。我们的评价表明,MedGemma达到了与人类读者(最佳放射学家)的标准系统(83.84%的准确性)的优异性业绩(77-27%),这是一个重要的里程碑级评估。 AI-评估一个重要里程碑级A级级的成绩模型, 展示专家在人类内部分析基础上显示一个更清晰的自我分析。","Ankit Pal, Jung-Oh Lee, Xiaoman Zhang, Malaikannan Sankarasubbu, Seunghyeon Roh, Won Jung Kim, Meesun Lee, Pranav Rajpurkar",2025-06-04T18:11:59Z,ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist   Chest X-ray Understanding,ReXVQA: Ein großformatiger Benchmark für visuelle Frageantworten für das generalistische Röntgenverständnis im Brustkorb,ReXVQA:通用胸透X射线理解的大规模视觉问题回答基准,http://arxiv.org/abs/2506.04353v1
227,"Large decoder-only language models (LLMs) have achieved remarkable success in generation and reasoning tasks, where they generate text responses given instructions. However, many applications, e.g., retrieval augmented generation (RAG), still rely on separate embedding models to generate text embeddings, which can complicate the system and introduce discrepancies in understanding of the query between the embedding model and LLMs. To address this limitation, we propose a simple self-supervised approach, Generative Embedding large language Model (GEM), that enables any large decoder-only LLM to generate high-quality text embeddings while maintaining its original text generation and reasoning capabilities. Our method inserts new special token(s) into a text body, and generates summarization embedding of the text by manipulating the attention mask. This method could be easily integrated into post-training or fine tuning stages of any existing LLMs. We demonstrate the effectiveness of our approach by applying it to two popular LLM families, ranging from 1B to 8B parameters, and evaluating the transformed models on both text embedding benchmarks (MTEB) and NLP benchmarks (MMLU). The results show that our proposed method significantly improves the original LLMs on MTEB while having a minimal impact on MMLU. Our strong results indicate that our approach can empower LLMs with state-of-the-art text embedding capabilities while maintaining their original NLP performance","然而,许多应用程序,例如检索增强生成(RAG),仍然依赖单独的嵌入模型来生成文本嵌入器,这会使系统复杂化,并在理解嵌入模型与磁体之间的查询方面造成差异。 为了解决这一限制,我们提议了一种简单的自我监督方法,即生成嵌入式大语言模型(GEM),使任何大语言模型(GEM)能够产生高质量的文本嵌入器,同时保持其原始文本生成和推理能力。我们的方法是将新的特殊符号插入一个文本体,并通过调控关注面罩来生成文本的汇总嵌入器。这种方法可以很容易地纳入任何现有磁体的训练后或微调阶段。我们通过将这种方法应用于两个广受欢迎的LLMM家庭(从1B到8B参数),并评估在文本嵌入基准(MTEB)和NLPMM(MLM-MM)的原始能力方面经过改造的模型(MMMLM)上,我们的拟议方法能够大大改进我们原有的磁体-MLMLMS(MLM)的原始能力,同时显示我们原有的原始基准的成果。","Caojin Zhang, Qiang Zhang, Ke Li, Sai Vidyaranya Nuthalapati, Benyu Zhang, Jason Liu, Serena Li, Lizhu Zhang, Xiangjun Fan",2025-06-04T18:02:07Z,GEM: Empowering LLM for both Embedding Generation and Language   Understanding,GEM: LLM-Empowering sowohl für die Einbettung der Generation als auch für das Sprachverständnis,"GEM:赋予LLM权力,使其既能嵌入生成,又能理解语言",http://arxiv.org/abs/2506.04344v1
228,"Knowledge editing methods like MEMIT are able to make data and compute efficient updates of factual knowledge by using a single sentence to update facts and their consequences. However, what is often overlooked is a ""precomputation step"", which requires a one-time but significant computational cost. The authors of MEMIT originally precompute approximately 44 million hidden vectors per edited layer, which requires a forward pass over 44 million tokens. For GPT-J (6B), this precomputation step takes 36 hours on a single GPU, while it takes approximately 40 hours for Llama2-7B. Additionally, this precomputation time grows with model size. In this paper, we show that this excessive computational cost is unnecessary. Knowledge editing using MEMIT and related methods, such as ROME and EMMET, can be performed by pre-computing a very small portion of the 44 million hidden vectors. We first present the theoretical minimum number of hidden vector precomputation required for solutions of these editing methods to exist. We then empirically show that knowledge editing using these methods can be done by pre-computing significantly fewer hidden vectors. Specifically, we show that the precomputation step can be done with less than 0.3% of the originally stipulated number of hidden vectors. This saves a significant amount of precomputation time and allows users to begin editing new models within a few minutes.","MEMIT等知识编辑方法能够使用单句来更新事实及其后果,从而生成数据和计算对事实知识的高效更新。 但是,经常被忽略的是“ 计算步骤” , 需要一次性但重要的计算成本。 MEMIT的作者最初估计每个编辑层大约4,400万隐藏矢量, 需要超过4,400万个远端传票。 对于GPT-J (6B) 来说, 这个预先计算步骤需要一个单一的GPU需要36小时的时间, 而Llama2-7B 则需要大约40小时。 此外, 这个预算时间随着模型的大小而增加。 在本文中, 我们显示这种过高的计算成本是不必要的。 使用MEMIT和相关方法(如ROME和EMET)进行的知识编辑, 可以通过预先计算4, 400万个隐藏矢量中的一小部分。 对于GPT-J (6B) 来说, 这个预计算步骤需要36小时, 而对于这些编辑方法的解决方案需要多少个理论性最小的量。 我们从实验上表明, 使用这些方法的知识编辑方法的编辑可以通过预先计算方法通过大大减少隐藏矢量来进行计算。 我们用最初的矢量, 将使得最初的矢量的矢量的矢量可以保存一个大大的矢量。","Akshat Gupta, Maochuan Lu, Thomas Hartvigsen, Gopala Anumanchipalli",2025-06-04T17:59:05Z,Efficient Knowledge Editing via Minimal Precomputation,Effiziente Wissensbearbeitung über Minimal Precomputation,通过最低预数进行高效率的知识编辑,http://arxiv.org/abs/2506.04226v1
229,"Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek R1) have led to a popular belief that extending thinking traces using prompts like ""Wait"" or ""Let me rethink"" can improve performance. This raises a natural question: Does thinking more at test-time truly lead to better reasoning? To answer this question, we perform a detailed empirical study across models and benchmarks, which reveals a consistent pattern of initial performance improvements from additional thinking followed by a decline, due to ""overthinking"". To understand this non-monotonic trend, we consider a simple probabilistic model, which reveals that additional thinking increases output variance-creating an illusion of improved reasoning while ultimately undermining precision. Thus, observed gains from ""more thinking"" are not true indicators of improved reasoning, but artifacts stemming from the connection between model uncertainty and evaluation metric. This suggests that test-time scaling through extended thinking is not an effective way to utilize the inference thinking budget. Recognizing these limitations, we introduce an alternative test-time scaling approach, parallel thinking, inspired by Best-of-N sampling. Our method generates multiple independent reasoning paths within the same inference budget and selects the most consistent response via majority vote, achieving up to 20% higher accuracy compared to extended thinking. This provides a simple yet effective mechanism for test-time scaling of reasoning models.","推理模型(例如,OpenAI o1, DeepSeek R1)的最近测试时间缩放趋势导致一种普遍的看法,即使用“等待”或“让我重新思考”这样的提示来延长思维轨迹可以提高绩效。这自然提出了一个问题:在测试时间进行更多的思考是否真正导致更好的推理?为了回答这个问题,我们对各种模型和基准进行详细的实验性研究,从更多的思考和“过度思考”导致的下降,从而揭示出一种一致的初始性能改进模式。为了理解这种非机动性趋势,我们认为一种简单的概率模型,它揭示出额外思维会增加产出差异,从而产生改进推理的幻觉,最终破坏精确性。因此,观察到的“更多思考”并不是改进推理的真正指标,而是模型不确定性和评价度指标之间的关联产生的人工力。 这表明,通过扩展思考,测试时间的尺度不是利用推论思考预算的有效方法。 认识到这些局限性,我们引入了一种不同的测试时间缩方法,在最佳取样的启发下,平行思考。我们的方法在通过多数表决中产生多种独立的推论路径,从而推算得出最一致的推理。","Soumya Suvra Ghosal, Souradip Chakraborty, Avinash Reddy, Yifu Lu, Mengdi Wang, Dinesh Manocha, Furong Huang, Mohammad Ghavamzadeh, Amrit Singh Bedi",2025-06-04T17:55:09Z,Does Thinking More always Help? Understanding Test-Time Scaling in   Reasoning Models,Hilft immer mehr zu denken? Test-Time Scaling in vernünftigen Modellen verstehen,理解理性模型中的测试时间缩放,http://arxiv.org/abs/2506.04210v1
230,"Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. In this paper, rather than examining multimodal RL in isolation, we delve into current training pipelines and identify three crucial phenomena: 1) Effective cold start initialization is critical for enhancing MLLM reasoning. Intriguingly, we find that initializing with carefully selected text data alone can lead to performance surpassing many recent multimodal reasoning models, even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers from gradient stagnation, which degrades training stability and performance. 3) Subsequent text-only RL training, following the multimodal RL phase, further enhances multimodal reasoning. This staged training approach effectively balances perceptual grounding and cognitive reasoning development. By incorporating the above insights and addressing multimodal RL issues, we introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, and challenging AIME2024 and AIME2025.","由于Deepseek-R1在复杂的文字任务中具有非凡的推理能力,许多工作试图通过直接应用强化学习(RL)来鼓励多式大语言模型(MLLM)的类似能力。然而,它们仍然难以启动复杂的推理。在本文中,我们不孤立地研究多式RL,而是深入研究目前的培训管道,并找出三个关键现象:(1) 有效的冷入启动对于加强MLLM的推理至关重要。有趣的是,我们发现,单靠经过仔细选择的文本数据,就能够超越许多最近的多式推理模型,甚至在Mdormodal RL.(2) 适用于多式RL的标准GROPO有梯度停滞,这降低了培训的稳定性和绩效。(3) 在多式RLS阶段之后,仅用文本进行的培训进一步加强了多式推理。这种分阶段培训方法有效地平衡了概念基础和认知推理发展。通过纳入以上见解和处理多式RLL20问题,我们引入了ReVal-R1,从而在开放源7B MLLLMSMS中实现新的状态, 具有挑战性基准,包括数学和AIMA和AIV。","Shuang Chen, Yue Guo, Zhaochen Su, Yafu Li, Yulun Wu, Jiacheng Chen, Jiayu Chen, Weijie Wang, Xiaoye Qu, Yu Cheng",2025-06-04T17:51:08Z,Advancing Multimodal Reasoning: From Optimized Cold Start to Staged   Reinforcement Learning,Multimodale Reasoning fördern: Vom optimierten Kaltstart bis zum schrittweisen Ausbaulernen,推进多式联运理由:从优化的寒冷开始到分阶段加强学习,http://arxiv.org/abs/2506.04207v1
231,"Large language models (LLMs) have notably progressed in multi-step and long-chain reasoning. However, extending their reasoning capabilities to encompass deep interactions with search remains a non-trivial challenge, as models often fail to identify optimal reasoning-search interaction trajectories, resulting in suboptimal responses. We propose R-Search, a novel reinforcement learning framework for Reasoning-Search integration, designed to enable LLMs to autonomously execute multi-step reasoning with deep search interaction, and learn optimal reasoning search interaction trajectories via multi-reward signals, improving response quality in complex logic- and knowledge-intensive tasks. R-Search guides the LLM to dynamically decide when to retrieve or reason, while globally integrating key evidence to enhance deep knowledge interaction between reasoning and search. During RL training, R-Search provides multi-stage, multi-type rewards to jointly optimize the reasoning-search trajectory. Experiments on seven datasets show that R-Search outperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1% (out-of-domain). The code and data are available at https://github.com/QingFei1/R-Search.","大型语言模型(LLMS)在多步骤和长链推理方面取得了显著的进步。然而,扩展其推理能力以包括与搜索的深度互动,仍是一个非三重挑战,因为模型往往无法确定最佳推理搜索互动轨迹,从而产生亚优反应。我们提议R-Search,这是一个创新的强化学习框架,用于“理性搜索”整合,旨在使LLMS能够以深入搜索互动的方式自主地执行多步推理,并学习通过多回报信号进行的最佳推理搜索互动轨迹,提高复杂逻辑和知识密集型任务的反应质量。R-Search指导LLM动态地决定何时检索或解释,同时在全球整合关键证据,以加强推理与搜索之间的深度知识互动。在RL培训期间,R-Search为共同优化推理研究轨迹提供了多阶段、多类型奖励。对七个数据集的实验显示,R-Search将高级RAG1/基线从32.2%(内部)和25.1%(外部)。代码和数据可在http://Search.","Qingfei Zhao, Ruobing Wang, Dingling Xu, Daren Zha, Limin Liu",2025-06-04T17:29:22Z,R-Search: Empowering LLM Reasoning with Search via Multi-Reward   Reinforcement Learning,R-Search: LLM-Reasoning mit der Suche über Multi-Reward-Verstärkungs-Lernen stärken,"R-Search:通过多奖励强化学习,赋予以搜索为理由的LLM权力",http://arxiv.org/abs/2506.04185v1
232,"Graphical User Interface (GUI) Agents have emerged as a transformative paradigm in human-computer interaction, evolving from rule-based automation scripts to sophisticated AI-driven systems capable of understanding and executing complex interface operations. This survey provides a comprehensive examination of the rapidly advancing field of LLM-based GUI Agents, systematically analyzing their architectural foundations, technical components, and evaluation methodologies. We identify and analyze four fundamental components that constitute modern GUI Agents: (1) perception systems that integrate text-based parsing with multimodal understanding for comprehensive interface comprehension; (2) exploration mechanisms that construct and maintain knowledge bases through internal modeling, historical experience, and external information retrieval; (3) planning frameworks that leverage advanced reasoning methodologies for task decomposition and execution; and (4) interaction systems that manage action generation with robust safety controls. Through rigorous analysis of these components, we reveal how recent advances in large language models and multimodal learning have revolutionized GUI automation across desktop, mobile, and web platforms. We critically examine current evaluation frameworks, highlighting methodological limitations in existing benchmarks while proposing directions for standardization. This survey also identifies key technical challenges, including accurate element localization, effective knowledge retrieval, long-horizon planning, and safety-aware execution control, while outlining promising research directions for enhancing GUI Agents' capabilities. Our systematic review provides researchers and practitioners with a thorough understanding of the field's current state and offers insights into future developments in intelligent interface automation.","图形用户界面(GUI)代理器已成为人-计算机互动的变革范例,从基于规则的自动化脚本发展到能够理解和执行复杂界面操作的由AI驱动的复杂系统;这项调查全面审查了以LLM为基础的图形代理器迅速发展的领域,系统分析其建筑基础、技术组成部分和评价方法;我们确定并分析构成现代图形代理器的四个基本组成部分:(1) 将基于文本的辨别与多式联运理解相结合以便全面界面理解的感知系统;(2) 通过内部建模、历史经验和外部信息检索建立和维持知识库的探索机制;(3) 利用先进的推理方法进行任务分解和执行的规划框架;(4) 以强有力的安全控制措施管理行动生成的互动系统;通过对这些组成部分的严格分析,我们揭示了大型语言模式和多式联运学习的最新进展如何使基于文字的图形在桌面、移动平台和网络平台上实现自动化革命化;我们严格审查目前的评价框架,强调现有基准中的方法限制,同时提出标准化的方向;这项调查还查明了关键的技术挑战,包括准确的元素本地化、有效的知识检索、长期的规划和安全认知系统化的系统化分析,同时概述我们当前执行的研究人员的实地分析能力,为实地分析提供了有希望的实地研究方向。","Fei Tang, Haolei Xu, Hang Zhang, Siqi Chen, Xingyu Wu, Yongliang Shen, Wenqi Zhang, Guiyang Hou, Zeqi Tan, Yuchen Yan, Kaitao Song, Jian Shao, Weiming Lu, Jun Xiao, Yueting Zhuang",2025-06-04T17:29:13Z,A Survey on (M)LLM-Based GUI Agents,Eine Umfrage zu (M)LLM-basierten GUI-Agenten,对(M)LLLM基用户界面代理器的调查,http://arxiv.org/abs/2504.13865v2
233,"With the rapid advancement of large reasoning models, long Chain-of-Thought (CoT) prompting has demonstrated strong performance on complex tasks. However, this often comes with a significant increase in token usage. In this paper, we conduct a comprehensive empirical analysis comparing long and short CoT strategies. Our findings reveal that while long CoT can lead to performance improvements, its benefits are often marginal relative to its significantly higher token consumption. Specifically, long CoT tends to outperform when ample generation budgets are available, whereas short CoT is more effective under tighter budget constraints. These insights underscore the need for a dynamic approach that selects the proper CoT strategy based on task context and resource availability. To address this, we propose SwitchCoT, an automatic framework that adaptively chooses between long and short CoT strategies to balance reasoning accuracy and computational efficiency. Moreover, SwitchCoT is designed to be budget-aware, making it broadly applicable across scenarios with varying resource constraints. Experimental results demonstrate that SwitchCoT can reduce inference costs by up to 50% while maintaining high accuracy. Notably, under limited token budgets, it achieves performance comparable to, or even exceeding, that of using either long or short CoT alone.","随着大型推理模型的快速发展,长期成本链(Cot)的快速推进,长期成本链(Cot)在复杂任务上表现良好,然而,这往往伴随着象征性使用量的大幅增加。在本文件中,我们进行了全面的实证分析,比较了长期和短期的COT战略。我们的调查结果显示,虽然长期成本可以带来绩效改进,但其效益往往比其高得多的象征性消费要小得多。具体地说,长期成本链在有充足的生产预算时往往比高得多,而短期成本在紧凑的预算制约下则比较有效。这些见解突出表明,需要一种动态方法,根据任务背景和资源的可得性选择适当的COT战略。为了解决这个问题,我们提议SwitchCot是一个自动框架,在长时间和短期的COT战略之间作出适应性选择,以平衡准确性和计算效率。此外,SwitchCot设计是预算的,使它广泛适用于资源制约各不相同的各种情景。实验结果表明,SwitchCot可以将推断成本降低到50 %,同时保持高精确度。值得注意的是,在有限的象征性预算下,它能够实现长期或单独或超过长期的绩效。","Ruiqi Zhang, Changyi Xiao, Yixin Cao",2025-06-04T17:28:38Z,Long or short CoT? Investigating Instance-level Switch of Large   Reasoning Models,Lange oder kurze CoT? Untersuchung von Instance-Level-Schaltern von großen Vernunftmodellen,调查大理由理由模型在案一级切换,http://arxiv.org/abs/2506.04182v1
234,"Long-form text generation remains a significant challenge for large language models (LLMs), particularly in maintaining coherence, ensuring logical consistency, and preserving text quality as sequence length increases. To address these limitations, we propose SuperWriter-Agent, an agent-based framework designed to enhance the quality and consistency of long-form text generation. SuperWriter-Agent introduces explicit structured thinking-through planning and refinement stages into the generation pipeline, guiding the model to follow a more deliberate and cognitively grounded process akin to that of a professional writer. Based on this framework, we construct a supervised fine-tuning dataset to train a 7B SuperWriter-LM. We further develop a hierarchical Direct Preference Optimization (DPO) procedure that uses Monte Carlo Tree Search (MCTS) to propagate final quality assessments and optimize each generation step accordingly. Empirical results across diverse benchmarks demonstrate that SuperWriter-LM achieves state-of-the-art performance, surpassing even larger-scale baseline models in both automatic evaluation and human evaluation. Furthermore, comprehensive ablation studies demonstrate the effectiveness of hierarchical DPO and underscore the value of incorporating structured thinking steps to improve the quality of long-form text generation.","长方文本生成对于大型语言模型(LLMS)来说,特别是在保持一致性、确保逻辑一致性和保持文本质量方面,对于大型语言模型(LLMs)来说,特别是在保持一致性、确保逻辑一致性和随着序列长度的增加而保持文本质量方面,长方文本生成仍然是一项重大挑战。为了应对这些局限性,我们提议超级Writer-Agency(SuperWriter-Agency),这是一个旨在提高长方文本生成质量和一致性的代理框架。超级Writer-Agency(MCTS)引入了明确的结构性思维,通过规划和精细化阶段进入生成管道,指导该模型遵循类似于专业作家的更深思熟虑和基于认知的进程。基于这一框架,我们建立了一个有监督的微调数据集,以培训一个7B SuperWriter-LM。我们进一步开发了一种等级直接参考优化程序(DPO),利用蒙特卡洛树搜索(MCTS)来传播最终质量评估并相应优化每一代步骤。各种基准的实证结果表明,超级Writer-LM在自动评估和人文评估中达到最新质量方面超过了甚至更大规模的基线模型。此外,全面对比研究显示了DPO值,并强调了采用结构思维的生成质量。","Yuhao Wu, Yushi Bai, Zhiqiang Hu, Juanzi Li, Roy Ka-Wei Lee",2025-06-04T17:27:42Z,SuperWriter: Reflection-Driven Long-Form Generation with Large Language   Models,SuperWriter: Reflexionsgetriebene Langform-Generation mit großen Sprachmodellen,超级词典: 具有大语言模型的反射驱动长龄一代,http://arxiv.org/abs/2506.04180v1
235,"Large language models (LLMs) achieve remarkable performance across tasks but incur substantial computational costs due to their deep, multi-layered architectures. Layer pruning has emerged as a strategy to alleviate these inefficiencies, but conventional static pruning methods overlook two critical dynamics inherent to LLM inference: (1) horizontal dynamics, where token-level heterogeneity demands context-aware pruning decisions, and (2) vertical dynamics, where the distinct functional roles of MLP and self-attention layers necessitate component-specific pruning policies. We introduce SkipGPT, a dynamic layer pruning framework designed to optimize computational resource allocation through two core innovations: (1) global token-aware routing to prioritize critical tokens, and (2) decoupled pruning policies for MLP and self-attention components. To mitigate training instability, we propose a two-stage optimization paradigm: first, a disentangled training phase that learns routing strategies via soft parameterization to avoid premature pruning decisions, followed by parameter-efficient LoRA fine-tuning to restore performance impacted by layer removal. Extensive experiments demonstrate that SkipGPT reduces over 40% of model parameters while matching or exceeding the performance of the original dense model across benchmarks. By harmonizing dynamic efficiency with preserved expressivity, SkipGPT advances the practical deployment of scalable, resource-aware LLMs. Our code is publicly available at: https://github.com/EIT-NLP/SkipGPT.","大型语言模型(LLMS)在各项任务之间取得显著的绩效,但由于其深度、多层结构而需要大量计算成本。层层调整是缓解这些低效率的一种战略,但常规静态修剪方法忽略了LLM推论固有的两个关键动态:(1) 水平动态,即象征性的异质性要求环境认知裁剪决定;(2) 垂直动态,即MLP和自控层的不同功能作用需要基于具体内容的调整政策。我们引入了SGGGPT,这是一个动态层调整框架,旨在通过两个核心创新优化计算资源分配:(1) 全球象征性觉测路由以优先排序,以及(2) 为MLMP和自留部分拆解了两个关键动态动态动态。为了减轻培训不稳定,我们建议了两阶段优化模式:首先,一个分解的培训阶段,通过软参数化来学习路由流战略,以避免不成熟的裁剪裁决定。随后,我们引入了节能的LARA微调整,以恢复因分层拆解而受到影响的绩效。 广泛的实验显示SGPTPT的初始性I/PLPPS,同时调整了BSBS的初始性标准,同时将SBSBSBS的进度比重压超超超40。","Anhao Zhao, Fanghua Ye, Yingqi Fan, Junlong Tong, Zhiwei Fei, Hui Su, Xiaoyu Shen",2025-06-04T17:26:31Z,SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and   Module Decoupling,SkipGPT: Dynamic Layer Pruning Reinvented mit Token Awareness und Modulentkopplung,SkigPT: 以 Tok 意识和模块分离重生的动态层缓冲层,http://arxiv.org/abs/2506.04179v1
236,"Large Language Models (LLMs) have shown remarkable advancements but also raise concerns about cultural bias, often reflecting dominant narratives at the expense of under-represented subcultures. In this study, we evaluate the capacity of LLMs to recognize and accurately respond to the Little Traditions within Indian society, encompassing localized cultural practices and subcultures such as caste, kinship, marriage, and religion. Through a series of case studies, we assess whether LLMs can balance the interplay between dominant Great Traditions and localized Little Traditions. We explore various prompting strategies and further investigate whether using prompts in regional languages enhances the models cultural sensitivity and response quality. Our findings reveal that while LLMs demonstrate an ability to articulate cultural nuances, they often struggle to apply this understanding in practical, context-specific scenarios. To the best of our knowledge, this is the first study to analyze LLMs engagement with Indian subcultures, offering critical insights into the challenges of embedding cultural diversity in AI systems.","大型语言模式(LLMs)已经显示出显著的进步,但也引起了对文化偏见的关切,往往反映了以代表不足的亚文化为代价的主要叙述,在这项研究中,我们评估LMs承认印度社会中的小传统并对此作出准确反应的能力,包括本地文化习俗和诸如种姓、亲属、婚姻和宗教等亚文化。我们通过一系列案例研究,评估LLMs能否平衡占支配地位的大传统和本地小传统之间的相互作用。我们探索各种快速战略,并进一步调查使用区域语言的提示是否增强了示范文化敏感性和反应质量。我们的调查结果显示,LLMs表现出有能力阐明文化微妙之处,但他们往往在实际和具体情况下难以应用这种理解。我们最了解的是,这是对LMs与印度亚文化的接触进行分析的第一份研究,对将文化多样性嵌入独立信息系统的挑战提供了重要的洞察力。","Garima Chhikara, Abhishek Kumar, Abhijnan Chakraborty",2025-06-04T17:05:12Z,Through the Prism of Culture: Evaluating LLMs' Understanding of Indian   Subcultures and Traditions,Durch das Prisma der Kultur: LLMs Verständnis von indischen Subkulturen und Traditionen bewerten,"通过 "" 棱晶文化:评价LLM女士对印度亚文化和传统的理解 """,http://arxiv.org/abs/2501.16748v2
237,"Patients have distinct information needs about their hospitalization that can be addressed using clinical evidence from electronic health records (EHRs). While artificial intelligence (AI) systems show promise in meeting these needs, robust datasets are needed to evaluate the factual accuracy and relevance of AI-generated responses. To our knowledge, no existing dataset captures patient information needs in the context of their EHRs. We introduce ArchEHR-QA, an expert-annotated dataset based on real-world patient cases from intensive care unit and emergency department settings. The cases comprise questions posed by patients to public health forums, clinician-interpreted counterparts, relevant clinical note excerpts with sentence-level relevance annotations, and clinician-authored answers. To establish benchmarks for grounded EHR question answering (QA), we evaluated three open-weight large language models (LLMs)--Llama 4, Llama 3, and Mixtral--across three prompting strategies: generating (1) answers with citations to clinical note sentences, (2) answers before citations, and (3) answers from filtered citations. We assessed performance on two dimensions: Factuality (overlap between cited note sentences and ground truth) and Relevance (textual and semantic similarity between system and reference answers). The final dataset contains 134 patient cases. The answer-first prompting approach consistently performed best, with Llama 4 achieving the highest scores. Manual error analysis supported these findings and revealed common issues such as omitted key clinical evidence and contradictory or hallucinated content. Overall, ArchEHR-QA provides a strong benchmark for developing and evaluating patient-centered EHR QA systems, underscoring the need for further progress toward generating factual and relevant responses in clinical contexts.","虽然人工智能(AI)系统显示满足这些需要的希望,但需要强有力的数据集来评价AI作出的反应的事实准确性和相关性。据我们所知,现有的数据集没有根据EHR系统收集病人的信息需求。我们引入了ArchEHR-QA,这是一个专家附加说明的基于特护单位和紧急部门设置的现实世界病人病例的数据集。病例包括病人向公共卫生论坛、临床口译员对口机构提出的问题、相关临床说明节录及其与判决相关的说明以及临床作者的回答。为基于EHR的回答(QA)建立基准,我们评估了三种开放的大型语言模型(LLLMS)-Llama 4、Llamama 3 和Mixtral-Across 三种提示性战略:(1) 引用临床说明句子的答案,(2) 引用前回答,以及过滤引用的回答。我们评估了两个层面的绩效:事实质量(引用了说明性说明性判决和不准确性说明性说明),E-Eral-al-al-al-al-al-al-al-al-al-al-road real road real ex rodufal ex roduferal ex ex ex as deal deal deal deal deal deal deal deal deal deal 和持续进行这样的快速分析。","Sarvesh Soni, Dina Demner-Fushman",2025-06-04T16:55:08Z,A Dataset for Addressing Patient's Information Needs related to Clinical   Course of Hospitalization,Ein Datensatz für den Umgang mit Patienteninformationen im Zusammenhang mit dem klinischen Verlauf der Hospitalisierung,与住院临床课程有关的满足病人信息需要数据集,http://arxiv.org/abs/2506.04156v1
238,"A common use of NLP is to facilitate the understanding of large document collections, with a shift from using traditional topic models to Large Language Models. Yet the effectiveness of using LLM for large corpus understanding in real-world applications remains under-explored. This study measures the knowledge users acquire with unsupervised, supervised LLM-based exploratory approaches or traditional topic models on two datasets. While LLM-based methods generate more human-readable topics and show higher average win probabilities than traditional models for data exploration, they produce overly generic topics for domain-specific datasets that do not easily allow users to learn much about the documents. Adding human supervision to the LLM generation process improves data exploration by mitigating hallucination and over-genericity but requires greater human effort. In contrast, traditional. models like Latent Dirichlet Allocation (LDA) remain effective for exploration but are less user-friendly. We show that LLMs struggle to describe the haystack of large corpora without human help, particularly domain-specific data, and face scaling and hallucination limitations due to context length constraints.","NLP的常见用途是便利对大型文件收集的了解,从使用传统专题模型转向使用大语言模型。然而,在现实应用中,使用LLM进行大规模了解大型实体的效用仍未得到充分探讨。这项研究衡量知识用户在两个数据集上以不受监督、监督LLM为基础的探索方法或传统专题模型获得的知识用户。以LLM为基础的方法产生更多的人类可读专题,并显示比传统数据勘探模型更平均的赢利概率,但它们为特定域数据集制作的过于通用的专题,使用户无法轻易了解有关文件的内容。在LLM生成过程中增加人的监督,通过减轻幻觉和超独创性改善了数据探索,但需要人类作出更大的努力。相比之下,Lenttant Drichlet分配(LDA)等传统模型仍然对勘探有效,但不太方便用户使用。我们显示LLMS在描述没有人类帮助的大型公司干草堆,特别是特定域数据,并因环境长度限制而面临缩和幻觉限制。","Zongxia Li, Lorena Calvo-Bartolomé, Alexander Hoyle, Paiheng Xu, Alden Dima, Juan Francisco Fung, Jordan Boyd-Graber",2025-06-04T16:49:39Z,Large Language Models Struggle to Describe the Haystack without Human   Help: Human-in-the-loop Evaluation of Topic Models,"Große Sprachmodelle kämpfen, um den Haystack ohne menschliche Hilfe zu beschreiben: Human-in-the-Loop-Bewertung von Themenmodellen","大力描述无人帮助的黑屋的大型语言模型:对专题模型的 "" 人与人间流动 "" 评估",http://arxiv.org/abs/2502.14748v2
239,"Sentence embedding is essential for many NLP tasks, with contrastive learning methods achieving strong performance using annotated datasets like NLI. Yet, the reliance on manual labels limits scalability. Recent studies leverage large language models (LLMs) to generate sentence pairs, reducing annotation dependency. However, they overlook ranking information crucial for fine-grained semantic distinctions. To tackle this challenge, we propose a method for controlling the generation direction of LLMs in the latent space. Unlike unconstrained generation, the controlled approach ensures meaningful semantic divergence. Then, we refine exist sentence embedding model by integrating ranking information and semantic information. Experiments on multiple benchmarks demonstrate that our method achieves new SOTA performance with a modest cost in ranking sentence synthesis.","嵌入句子对于许多NLP任务至关重要,因为对比式学习方法使用NLI等附加说明的数据集取得了强效。 然而,对手动标签的依赖限制了可缩放性。 最近的研究利用了大型语言模型(LLMs)生成配对句,减少了批注依赖性。 但是,它们忽略了对于细微区分语义至关重要的排序信息。 为了应对这一挑战,我们提出了一个在潜层控制LLMs生成方向的方法。 与未受约束的生成不同, 受控制的方法确保了有意义的语义差异。 然后, 我们通过整合排名信息和语义信息来完善了嵌入句式模型。 对多个基准的实验表明,我们的方法实现了新SOTA的绩效,在排序组合中成本较低。","Liyang He, Chenglong Liu, Rui Li, Zhenya Huang, Shulan Ruan, Jun Zhou, Enhong Chen",2025-06-04T16:39:26Z,Refining Sentence Embedding Model through Ranking Sentences Generation   with Large Language Models,Refining Satz Einbettung Modell durch Ranking Sätze Generation mit großen Sprachmodellen,通过按大语言模式排列刑罚来完善判决嵌入模式,http://arxiv.org/abs/2502.13656v2
240,"The development of large language models (LLMs) depends on trustworthy evaluation. However, most current evaluations rely on public benchmarks, which are prone to data contamination issues that significantly compromise fairness. Previous researches have focused on constructing dynamic benchmarks to address contamination. However, continuously building new benchmarks is costly and cyclical. In this work, we aim to tackle contamination by analyzing the mechanisms of contaminated models themselves. Through our experiments, we discover that the overestimation of contaminated models is likely due to parameters acquiring shortcut solutions in training. We further propose a novel method for identifying shortcut neurons through comparative and causal analysis. Building on this, we introduce an evaluation method called shortcut neuron patching to suppress shortcut neurons. Experiments validate the effectiveness of our approach in mitigating contamination. Additionally, our evaluation results exhibit a strong linear correlation with MixEval, a recently released trustworthy benchmark, achieving a Spearman coefficient ($\rho$) exceeding 0.95. This high correlation indicates that our method closely reveals true capabilities of the models and is trustworthy. We conduct further experiments to demonstrate the generalizability of our method across various benchmarks and hyperparameter settings. Code: https://github.com/GaryStack/Trustworthy-Evaluation","开发大型语言模型(LLMS)取决于可信赖的评价。然而,大多数目前的评价都依赖于公共基准,而公共基准容易导致数据污染问题,从而大大损害公平性。以前的研究侧重于建立动态基准,以解决污染问题。然而,不断建立新的基准是昂贵和周期性的。在这项工作中,我们的目标是通过分析受污染模型本身的机制来解决污染问题。我们通过实验发现,对受污染模型的过高估计可能是由于在培训中获得捷径解决方案的参数。我们进一步提出了通过比较和因果分析查明捷径神经元的新方法。我们在此基础上,我们采用了一种称为捷径神经补补补的评价方法,以压制捷径神经元。实验证实了我们在减轻污染方面的做法的有效性。此外,我们的评价结果显示与MixEval(最近公布的一个值得信赖的基准,即实现Spearman系数($rho$)超过0.95。这种高相关性表明,我们的方法非常准确地揭示了模型的真正能力,而且值得信赖。我们进一步进行实验,以展示我们的方法在各种基准和超光度环境中的通用性。","Kejian Zhu, Shangqing Tu, Zhuoran Jin, Lei Hou, Juanzi Li, Jun Zhao",2025-06-04T16:33:44Z,Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis,Etablierung vertrauensvoller LLM-Evaluierung durch Shortcut Neuronenanalyse,通过快捷中枢分析建立可信赖的LLM评价,http://arxiv.org/abs/2506.04142v1
241,"The sequential structure of videos poses a challenge to the ability of multimodal large language models (MLLMs) to locate multi-frame evidence and conduct multimodal reasoning. However, existing video benchmarks mainly focus on understanding tasks, which only require models to match frames mentioned in the question (hereafter referred to as ""question frame"") and perceive a few adjacent frames. To address this gap, we propose MMR-V: A Benchmark for Multimodal Deep Reasoning in Videos. The benchmark is characterized by the following features. (1) Long-range, multi-frame reasoning: Models are required to infer and analyze evidence frames that may be far from the question frame. (2) Beyond perception: Questions cannot be answered through direct perception alone but require reasoning over hidden information. (3) Reliability: All tasks are manually annotated, referencing extensive real-world user understanding to align with common perceptions. (4) Confusability: Carefully designed distractor annotation strategies to reduce model shortcuts. MMR-V consists of 317 videos and 1,257 tasks. Our experiments reveal that current models still struggle with multi-modal reasoning; even the best-performing model, o4-mini, achieves only 52.5% accuracy. Additionally, current reasoning enhancement strategies (Chain-of-Thought and scaling test-time compute) bring limited gains. Further analysis indicates that the CoT demanded for multi-modal reasoning differs from it in textual reasoning, which partly explains the limited performance gains. We hope that MMR-V can inspire further research into enhancing multi-modal reasoning capabilities.","视频的顺序结构对多式联运大型语言模型(MLLM)定位多框架证据和进行多式联运推理的能力提出了挑战,但是,现有的视频基准主要侧重于理解任务,这些任务只需要模型来匹配问题中提到的框架(以下称为“问题框架”),并且看到几个相邻的框架。为了缩小这一差距,我们建议MMR-V:视频中多模式深度解释基准。基准的特点是以下特征:(1) 远程、多框架推理:需要模型来推断和分析可能远离问题框架的证据框架。 (2) 超越概念:问题无法单靠直接认识来解答,而需要对隐藏信息进行推理。 (3) 可靠性:所有任务都是手动加注的,引用广泛的现实世界用户理解,以便与共同认识保持一致。 (4) 可理解性:仔细设计分散或说明战略以减少模式的捷径。 MMMR-V由317个视频和1,257个任务组成。 我们的实验表明,目前的模型仍然在多模式推理中挣扎;即使是最佳的模型,O4-MI,也需要对隐藏的逻辑进行推理解释。 (3) 所有任务都是手手动性化的有限推理分析,只是提高目前推理分析。","Kejian Zhu, Zhuoran Jin, Hongbang Yuan, Jiachun Li, Shangqing Tu, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao",2025-06-04T16:33:41Z,MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in   Videos,MMR-V: Was ist links ungesagt? Ein Benchmark für multimodale Deep Reasoning in Videos,MMR-V:什么是左不救助?视频中多模式深层理由的基准,http://arxiv.org/abs/2506.04141v1
242,"Understanding the nuances in everyday language is pivotal for advancements in computational linguistics & emotions research. Traditional lexicon-based tools such as LIWC and Pattern have long served as foundational instruments in this domain. LIWC is the most extensively validated word count based text analysis tool in the social sciences and Pattern is an open source Python library offering functionalities for NLP. However, everyday language is inherently spontaneous, richly expressive, & deeply context dependent. To explore the capabilities of LLMs in capturing the valences of daily narratives in Flemish, we first conducted a study involving approximately 25,000 textual responses from 102 Dutch-speaking participants. Each participant provided narratives prompted by the question, ""What is happening right now and how do you feel about it?"", accompanied by self-assessed valence ratings on a continuous scale from -50 to +50. We then assessed the performance of three Dutch-specific LLMs in predicting these valence scores, and compared their outputs to those generated by LIWC and Pattern. Our findings indicate that, despite advancements in LLM architectures, these Dutch tuned models currently fall short in accurately capturing the emotional valence present in spontaneous, real-world narratives. This study underscores the imperative for developing culturally and linguistically tailored models/tools that can adeptly handle the complexities of natural language use. Enhancing automated valence analysis is not only pivotal for advancing computational methodologies but also holds significant promise for psychological research with ecologically valid insights into human daily experiences. We advocate for increased efforts in creating comprehensive datasets & finetuning LLMs for low-resource languages like Flemish, aiming to bridge the gap between computational linguistics & emotion research.","理解日常语言的细微差别对于计算语言和情绪研究的进展至关重要。 传统词汇工具,如LIWC和Spands等传统词汇工具长期以来一直是该领域的基础工具。 LIWC是社会科学中最广泛验证的基于字数的文字分析工具。 LIWC是社会科学和模式中最有说服力的文字分析工具,是开放源源码 Python 库,为NLP提供功能。 然而,日常语言本质上是自发的,非常直观,而且很深地取决于背景。为了探索LLIM在获取佛兰德每日叙事的价值方面的能力,我们首先进行了一项涉及102个荷兰语参与者约25 000个文本答复的研究。每个参与者都提供了由问题引发的全面说明,“现在正在发生什么,你对它有什么感觉? ”LIWCWC是社会科学科学与50+50连续规模的自评价值评级图书馆库库库库库库库库库库库库库库库库库库。 我们随后评估了三个荷兰特有的LLIMSMS(LICWCS)在预测这些价值评分的成绩方面的表现,并将其产出与LICWCWC和模式进行比较。 我们的调查结果显示,尽管在LLIM结构上存在差距,但这些微差距,但是,但是,但这些荷兰调整的模型在不断更新的逻辑和直观的逻辑分析中,目前直径直观的逻辑分析中, 直观的逻辑分析中, 直观的模型中, 直观的模型也显示的逻辑分析中, 直观, 直观的模型也显示, 直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观分析、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观、直观","Ratna Kandala, Katie Hoemann",2025-06-04T16:31:37Z,Are Lexicon-Based Tools Still the Gold Standard for Valence Analysis in   Low-Resource Flemish?,Sind Lexicon-basierte Werkzeuge immer noch der Goldstandard für Valenzanalyse in Low-Resource-Flämisch?,以字典为基础的工具是否仍然是佛兰德低资源价值分析的黄金标准?,http://arxiv.org/abs/2506.04139v1
243,"Recently, scaling test-time compute on Large Language Models (LLM) has garnered wide attention. However, there has been limited investigation of how various reasoning prompting strategies perform as scaling. In this paper, we focus on a standard and realistic scaling setting: majority voting. We systematically conduct experiments on 6 LLMs $\times$ 8 prompting strategies $\times$ 6 benchmarks. Experiment results consistently show that as the sampling time and computational overhead increase, complicated prompting strategies with superior initial performance gradually fall behind simple Chain-of-Thought. We analyze this phenomenon and provide theoretical proofs. Additionally, we propose a probabilistic method to efficiently predict scaling performance and identify the best prompting strategy under large sampling times, eliminating the need for resource-intensive inference processes in practical applications. Furthermore, we introduce two ways derived from our theoretical analysis to significantly improve the scaling performance. We hope that our research can promote to re-examine the role of complicated prompting, unleash the potential of simple prompting strategies, and provide new insights for enhancing test-time scaling performance. Code is available at https://github.com/MraDonkey/rethinking_prompting.","最近,对大语言模型(LLM)的测试时间计算得到了广泛的关注,然而,对各种推理如何推动战略作为规模化的作用进行了有限的调查。在本文件中,我们侧重于一个标准和现实的衡量尺度设定:多数投票。我们系统地对6 LMs $_times 8 进行实验,8美元 推动战略 $_times 6 基准。实验结果一致表明,随着抽样时间和计算间接费用的增加,具有优异初始性能的复杂催化战略逐渐落在简单挑战链之后。我们分析这一现象并提供理论证据。此外,我们提出了一种概率化方法,以高效地预测规模化绩效并确定在大取样时间下最迅速的战略,消除资源密集型实际应用中的推断过程。此外,我们介绍了从理论分析中得出的两个方法,以大幅提高规模化绩效。我们希望我们的研究能够促进重新审查复杂催化作用,释放简单的提示战略的潜力,并提供新的洞察时间提升测试性能。代码可在 https://github.com/Donkey/remaskey_priming_priming_pressinginging_promatinging.","Yexiang Liu, Zekun Li, Zhi Fang, Nan Xu, Ran He, Tieniu Tan",2025-06-04T16:27:57Z,Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A   Perspective of Probability Theory,Überdenken der Rolle von Prompting-Strategien im LLM Test-Time Scaling: Eine Perspektive der Wahrscheinlichkeitstheorie,重新思考在LLM测试时间尺度中快速战略的作用:概率理论的视角,http://arxiv.org/abs/2505.10981v2
244,"Existing LLM agent systems typically select actions from a fixed and predefined set at every step. While this approach is effective in closed, narrowly scoped environments, it presents two major challenges for real-world, open-ended scenarios: (1) it significantly restricts the planning and acting capabilities of LLM agents, and (2) it requires substantial human effort to enumerate and implement all possible actions, which is impractical in complex environments with a vast number of potential actions. To address these limitations, we propose an LLM agent framework that can dynamically create and compose actions as needed. In this framework, the agent interacts with its environment by generating and executing programs written in a general-purpose programming language. Moreover, generated actions are accumulated over time for future reuse. Our extensive experiments across multiple benchmarks show that this framework significantly improves flexibility and outperforms prior methods that rely on a fixed action set. Notably, it enables LLM agents to adapt and recover in scenarios where predefined actions are insufficient or fail due to unforeseen edge cases. Our code can be found in https://github.com/adobe-research/dynasaur.","现有LLM代理系统通常从每个步骤的固定和预先界定环境中选择行动。这个方法在封闭的、狭窄的、范围狭窄的环境中是有效的,但给现实世界的、开放的情景提出了两大挑战:(1) 它大大限制了LLM代理机构的规划和行动能力,(2) 它需要大量人力来列举和实施所有可能的行动,这在复杂环境中是不切实际的,具有大量潜在行动。为了解决这些限制,我们提议一个LLM代理框架,能够动态地创造和组合行动。在这个框架内,该代理机构通过生成和执行以通用程序语言编写的程序来与环境互动。此外,所产生的行动是随着时间的推移积累起来的,供今后重新使用。我们针对多种基准的广泛实验表明,这个框架大大改进了灵活性,并超越了以前依靠固定行动的方法。值得注意的是,它使LLM代理机构能够在预先界定的行动不足或因无法预见到的边缘情况而调整和恢复。我们的代码可以在 https://github.com/adobe-research/dynasaur中找到。","Dang Nguyen, Viet Dac Lai, Seunghyun Yoon, Ryan A. Rossi, Handong Zhao, Ruiyi Zhang, Puneet Mathur, Nedim Lipka, Yu Wang, Trung Bui, Franck Dernoncourt, Tianyi Zhou",2025-06-04T16:26:58Z,DynaSaur: Large Language Agents Beyond Predefined Actions,DynaSaur: Große Sprachagenten jenseits vordefinierter Aktionen,Dynasaur:超出预定行动之外的大语言媒介,http://arxiv.org/abs/2411.01747v2
245,"The performance and usability of Large-Language Models (LLMs) are driving their use in explanation generation tasks. However, despite their widespread adoption, LLM explanations have been found to be unreliable, making it difficult for users to distinguish good from bad explanations. To address this issue, we present Rubrik's CUBE, an education-inspired rubric and a dataset of 26k explanations, written and later quality-annotated using the rubric by both humans and six open- and closed-source LLMs. The CUBE dataset focuses on two reasoning and two language tasks, providing the necessary diversity for us to effectively test our proposed rubric. Using Rubrik, we find that explanations are influenced by both task and perceived difficulty. Low quality stems primarily from a lack of conciseness in LLM-generated explanations, rather than cohesion and word choice. The full dataset, rubric, and code are available at https://github.com/RubriksCube/rubriks_cube.","大型语言模型(LLMS)的性能和实用性正在推动其用于解释生成任务。然而,尽管广泛采用,LLM解释仍然不可靠,使得用户很难区分好与坏的解释。为了解决这个问题,我们介绍了Rubrik的CUBE,这是一套教育启发的图解,以及一套由26k解释组成的数据集,由人类和6个开放和封闭来源的LMS用文和随后用质量说明。CUBE数据集侧重于两种推理和两种语言任务,为我们有效地测试我们拟议的卢布提供了必要的多样性。我们使用Rubrik发现,解释既受任务影响,也受感知的困难影响。低质量主要来自LUBM产生的解释缺乏简洁性,而不是凝聚力和文字选择。完整的数据集、卢布和代码可在https://github.com/RubriksCube/rubricris_cube上查阅。","Diana Galvan-Sosa, Gabrielle Gaudeau, Pride Kavumba, Yunmeng Li, Hongyi gu, Zheng Yuan, Keisuke Sakaguchi, Paula Buttery",2025-06-04T16:23:54Z,Rubrik's Cube: Testing a New Rubric for Evaluating Explanations on the   CUBE dataset,Rubrik's Cube: Testen eines neuen Rubric für die Auswertung von Erklärungen auf dem CUBE-Datensatz,Rubrik的立方体:测试CUBE数据集评价解释的新卢布,http://arxiv.org/abs/2503.23899v2
246,"Courtrooms are places where lives are determined and fates are sealed, yet they are not impervious to manipulation. Strategic use of manipulation in legal jargon can sway the opinions of judges and affect the decisions. Despite the growing advancements in NLP, its application in detecting and analyzing manipulation within the legal domain remains largely unexplored. Our work addresses this gap by introducing LegalCon, a dataset of 1,063 annotated courtroom conversations labeled for manipulation detection, identification of primary manipulators, and classification of manipulative techniques, with a focus on long conversations. Furthermore, we propose CLAIM, a two-stage, Intent-driven Multi-agent framework designed to enhance manipulation analysis by enabling context-aware and informed decision-making. Our results highlight the potential of incorporating agentic frameworks to improve fairness and transparency in judicial processes. We hope that this contributes to the broader application of NLP in legal discourse analysis and the development of robust tools to support fairness in legal decision-making. Our code and data are available at https://github.com/Disha1001/CLAIM.","法院是确定生命和决定命运的地方,但并非不受操纵。法律术语中的操纵战略使用可以动摇法官的意见并影响裁决。尽管国家实验室方案不断进步,但其在法律领域发现和分析操纵方面的应用在很大程度上仍未探索。我们的工作通过引入法律会议来解决这一差距,法律会议是1 063个附加说明的法庭谈话的数据集,标有操纵检测、主要操纵者识别和操控技术分类,重点是长长的谈话。此外,我们提议索赔,一个两阶段的、由内在驱动的多试剂框架,旨在通过促进环境意识和知情决策,加强操纵分析。我们的结果突出表明了在司法程序中纳入体制框架的可能性,以提高公平性和透明度。我们希望这有助于将国家实验室方案更广泛地应用于法律谈话分析,并开发强有力的工具以支持法律决策的公平性。我们的代码和数据可在https://github.com/Disha1001/LALIM上查阅。","Disha Sheshanarayana, Tanishka Magar, Ayushi Mittal, Neelam Chaplot",2025-06-04T16:22:59Z,CLAIM: An Intent-Driven Multi-Agent Framework for Analyzing Manipulation   in Courtroom Dialogues,CLAIM: Intent-Driven Multi-Agent Framework zur Analyse von Manipulationen im Dialograum,用于分析审判室对话中操纵操纵的预想式多机构框架,http://arxiv.org/abs/2506.04131v1
247,"LLM-as-a-Judge employs large language models (LLMs), such as GPT-4, to evaluate the quality of LLM-generated responses, gaining popularity for its cost-effectiveness and strong alignment with human evaluations. However, training proxy judge models using evaluation data generated by powerful teacher models introduces a critical yet previously overlooked issue: teacher preference bias, where the proxy judge model learns a biased preference for responses from the teacher model. To tackle this problem, we propose a novel setting that incorporates an additional assistant model, which is not biased toward the teacher model's responses, to complement the training data. Building on this setup, we introduce AGDe-Judge, a three-stage framework designed to debias from both the labels and feedbacks in the training data. Extensive experiments demonstrate that AGDe-Judge effectively reduces teacher preference bias while maintaining strong performance across six evaluation benchmarks. Code is available at https://github.com/Liuz233/AGDe-Judge.","LLM-as-a-Judge使用大型语言模型(LLMs),如GPT-4,以评价LLM产生的响应的质量,因其成本效益和与人类评价的紧密配合而获得欢迎;然而,利用强大的教师模型产生的评价数据培训代理法官模型,提出了一个过去被忽视的关键问题:教师偏好偏见,代理法官模型从教师模型中了解到有偏向性倾向偏向。为了解决这一问题,我们提议一个新颖的设置,纳入一个额外的助理模型,该模型不偏向教师模型的响应,以补充培训数据。我们在此设置的基础上,引入AGDE-Judge,这是一个三阶段框架,旨在从培训数据中的标签和反馈中消除偏见。广泛的实验表明,AGDe-Judge有效地减少了教师偏好偏见,同时保持了六个评价基准的强劲业绩。守则可在https://github.com/Liuz233/AGDE-judge查阅。","Zhuo Liu, Moxin Li, Xun Deng, Qifan Wang, Fuli Feng",2025-06-04T16:16:31Z,Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge,Assistant-Guided Milderung von Lehrerpräferenz Bias in LLM-as-a-Richter,助理辅导减轻在LLM-as-a法官中偏爱比阿斯的教师偏爱,http://arxiv.org/abs/2505.19176v2
248,"We present TextAtari, a benchmark for evaluating language agents on very long-horizon decision-making tasks spanning up to 100,000 steps. By translating the visual state representations of classic Atari games into rich textual descriptions, TextAtari creates a challenging test bed that bridges sequential decision-making with natural language processing. The benchmark includes nearly 100 distinct tasks with varying complexity, action spaces, and planning horizons, all rendered as text through an unsupervised representation learning framework (AtariARI). We evaluate three open-source large language models (Qwen2.5-7B, Gemma-7B, and Llama3.1-8B) across three agent frameworks (zero-shot, few-shot chain-of-thought, and reflection reasoning) to assess how different forms of prior knowledge affect performance on these long-horizon challenges. Four scenarios-Basic, Obscured, Manual Augmentation, and Reference-based-investigate the impact of semantic understanding, instruction comprehension, and expert demonstrations on agent decision-making. Our results reveal significant performance gaps between language agents and human players in extensive planning tasks, highlighting challenges in sequential reasoning, state tracking, and strategic planning across tens of thousands of steps. TextAtari provides standardized evaluation protocols, baseline implementations, and a framework for advancing research at the intersection of language models and planning.","我们提出TextAtari,这是评估非常长的横向决策任务,涵盖10万个步骤的一个基准。TextAtari通过将典型Atari游戏的视觉状态表述转换成丰富的文字描述,创造了一个具有挑战性的测试床,将顺序决策与自然语言处理连接起来。基准包括近100项不同的任务,其复杂程度、行动空间和规划视野各不相同,所有这些任务都是通过未经监督的代表学习框架(AtariARI)作为文本提供的。我们评估了三种开放源的大型语言模型(Qwen2.5-7B、Gemma-7B和Llama3.1-8B),跨越三个代理框架(零弹、几发思维链和反思推理),以评估不同形式的先前知识如何影响这些长期语言处理的绩效。四种基本情景、隐蔽、手册推理、手册推理和参考基础调查,揭示了语义理解、教学理解和专家演示对代理决策的影响。我们的成果显示,语言代理和人类角色在广泛规划任务(零射、略图象、思考、思考链、思考和思考推理学、标准分析、标准化研究框架的交叉评估、跨标准、标准、标准化研究、标准分析、标准分析、标准分析、标准评估框架、标准分析、标准、标准、标准化评估、标准评估等框架执行中的挑战。","Wenhao Li, Wenwu Li, Chuyun Shen, Junjie Sheng, Zixiao Huang, Di Wu, Yun Hua, Wei Yin, Xiangfeng Wang, Hongyuan Zha, Bo Jin",2025-06-04T15:55:27Z,TextAtari: 100K Frames Game Playing with Language Agents,TextAtari: 100K Frames Spiel mit Sprachagenten,TextAtari: 100K 框架游戏与语言代理游戏,http://arxiv.org/abs/2506.04098v1
249,"As a part of an embodied agent, Large Language Models (LLMs) are typically used for behavior planning given natural language instructions from the user. However, dealing with ambiguous instructions in real-world environments remains a challenge for LLMs. Various methods for task ambiguity detection have been proposed. However, it is difficult to compare them because they are tested on different datasets and there is no universal benchmark. For this reason, we propose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual dataset of ambiguous instructions addressed to a robot in a kitchen environment. AmbiK was collected with the assistance of LLMs and is human-validated. It comprises 1000 pairs of ambiguous tasks and their unambiguous counterparts, categorized by ambiguity type (Human Preferences, Common Sense Knowledge, Safety), with environment descriptions, clarifying questions and answers, user intents, and task plans, for a total of 2000 tasks. We hope that AmbiK will enable researchers to perform a unified comparison of ambiguity detection methods. AmbiK is available at https://github.com/cog-model/AmbiK-dataset.","作为体现剂的一部分,大语言模型(LLMS)通常用于根据用户的自然语言指示进行行为规划,然而,在现实环境中处理模棱两可的指示对LLMs来说仍然是一项挑战。提出了各种任务模糊性探测方法,但难以加以比较,因为它们在不同数据集中测试,没有通用的基准。为此,我们提议(在厨房环境中的复杂任务)AMbiK(在厨房环境中对机器人发出的模棱两可的指示的全文本数据集。AMbiK是在LLMS的协助下收集的,具有人的价值。它由1000对模棱两可的任务及其明确对应任务组成,按模糊性类型(人类偏好、常识知识、安全)加以分类,并附有环境说明、澄清问题和答案、用户意图和任务计划,以完成2000年的全部任务。我们希望AmbiK将使研究人员能够对模糊性探测方法进行统一比较。AmbiK可在https://github.com/cog-model/AmbiK-datas。","Anastasiia Ivanova, Eva Bakaeva, Zoya Volovikova, Alexey K. Kovalev, Aleksandr I. Panov",2025-06-04T15:47:07Z,AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment,AmbiK: Datensatz von vielseitigen Aufgaben in der Küchenumgebung,AmbiK:厨房环境中的不易任务数据集,http://arxiv.org/abs/2506.04089v1
250,"Tabular reasoning involves multi-step information extraction and logical inference over tabular data. While recent advances have leveraged large language models (LLMs) for reasoning over structured tables, such high-quality textual representations are often unavailable in real-world settings, where tables typically appear as images. In this paper, we tackle the task of tabular reasoning from table images, leveraging privileged structured information available during training to enhance multimodal large language models (MLLMs). The key challenges lie in the complexity of accurately aligning structured information with visual representations, and in effectively transferring structured reasoning skills to MLLMs despite the input modality gap. To address these, we introduce TabUlar Reasoning with Bridged infOrmation ({\sc Turbo}), a new framework for multimodal tabular reasoning with privileged structured tables. {\sc Turbo} benefits from a structure-aware reasoning trace generator based on DeepSeek-R1, contributing to high-quality modality-bridged data. On this basis, {\sc Turbo} repeatedly generates and selects the advantageous reasoning paths, further enhancing the model's tabular reasoning ability. Experimental results demonstrate that, with limited ($9$k) data, {\sc Turbo} achieves state-of-the-art performance ($+7.2\%$ vs. previous SOTA) across multiple datasets.","图表推理涉及多步信息提取和对表格数据进行逻辑推理。虽然最近的进展利用了大型语言模型(LLMs)对结构化表格进行推理,但在现实世界环境中,这种高质量的文字表述往往缺乏,因为表格通常以图像形式出现。在本文中,我们处理表格图象中的表格推理任务,利用培训期间可获得的特许结构化信息来增强多式联运大语言模型(MLLMs),关键挑战在于将结构化信息与视觉表达方式相协调的复杂性,以及尽管投入模式存在差距,但仍有效地将结构化推理技能转移给MLLMS。为了解决这些问题,我们引入了“Tabularal 理性与Bridged Information (Sc Turbo}) ,这是以特许结构化表格形式推理的新框架。 ~sc Turbo} 从基于EepSeek-R1的架构推理学跟踪生成者结构化数据中受益。 在此基础上, ~c Turbo}反复生成和选择有利的推理学路径,进一步加强模型的图表推理能力。实验结果显示,以有限的9美元+Turs-taxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx","Jun-Peng Jiang, Yu Xia, Hai-Long Sun, Shiyin Lu, Qing-Guo Chen, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, Han-Jia Ye",2025-06-04T15:46:30Z,Multimodal Tabular Reasoning with Privileged Structured Information,Multimodale Tabulare Reasoning mit privilegierten strukturierten Informationen,具有原始结构化信息的多式制表理由,http://arxiv.org/abs/2506.04088v1
251,"This report presents EuroLLM-9B, a large language model trained from scratch to support the needs of European citizens by covering all 24 official European Union languages and 11 additional languages. EuroLLM addresses the issue of European languages being underrepresented and underserved in existing open large language models. We provide a comprehensive overview of EuroLLM-9B's development, including tokenizer design, architectural specifications, data filtering, and training procedures. We describe the pre-training data collection and filtering pipeline, including the creation of EuroFilter, an AI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a novel synthetic dataset for post-training that enhances language coverage for European languages. Evaluation results demonstrate EuroLLM-9B's competitive performance on multilingual benchmarks and machine translation tasks, establishing it as the leading open European-made LLM of its size. To support open research and adoption, we release all major components of this work, including the base and instruction-tuned models, the EuroFilter classifier, and the synthetic post-training dataset.","本报告介绍了欧洲LLLM-9B的开发情况,包括象征性品设计、建筑规格、数据过滤和培训程序。我们介绍了培训前数据收集和过滤管道,包括创建欧洲环流器、一个以AI为基础的多语言过滤器,以及设计欧洲环锁合成系统,这是一个新的合成数据集,用于培训后加强欧洲语言的语文覆盖面。评价结果显示了欧洲环流-9B在多语言基准和机器翻译任务方面的竞争性表现,将其确定为规模最大的欧洲开源LLM。为了支持公开研究和采用,我们公布了这项工作的所有主要组成部分,包括基础和指示调控模型、欧洲环流分类和合成后培训数据集。","Pedro Henrique Martins, João Alves, Patrick Fernandes, Nuno M. Guerreiro, Ricardo Rei, Amin Farajian, Mateusz Klimaszewski, Duarte M. Alves, José Pombal, Manuel Faysse, Pierre Colombo, François Yvon, Barry Haddow, José G. C. de Souza, Alexandra Birch, André F. T. Martins",2025-06-04T15:43:31Z,EuroLLM-9B: Technical Report,EuroLLM-9B: Technischer Bericht,欧洲LLLM-9B:技术报告,http://arxiv.org/abs/2506.04079v1
252,"Evaluating large language models (LLMs) in medicine is crucial because medical applications require high accuracy with little room for error. Current medical benchmarks have three main types: medical exam-based, comprehensive medical, and specialized assessments. However, these benchmarks have limitations in question design (mostly multiple-choice), data sources (often not derived from real clinical scenarios), and evaluation methods (poor assessment of complex reasoning). To address these issues, we present LLMEval-Med, a new benchmark covering five core medical areas, including 2,996 questions created from real-world electronic health records and expert-designed clinical scenarios. We also design an automated evaluation pipeline, incorporating expert-developed checklists into our LLM-as-Judge framework. Furthermore, our methodology validates machine scoring through human-machine agreement analysis, dynamically refining checklists and prompts based on expert feedback to ensure reliability. We evaluate 13 LLMs across three categories (specialized medical models, open-source models, and closed-source models) on LLMEval-Med, providing valuable insights for the safe and effective deployment of LLMs in medical domains. The dataset is released in https://github.com/llmeval/LLMEval-Med.","评估医学领域的大语言模型(LLMS)至关重要,因为医疗应用要求高精度,且几乎没有差错空间。目前的医疗基准有三大主要类型:医学考试、综合医疗和专门评估。然而,这些基准在问题设计(主要是多种选择)、数据来源(通常不是从真正的临床假设中得出)和评价方法(对复杂推理评估不足)方面有局限性。为了解决这些问题,我们介绍了LLMevev-Med,这是一个涵盖五个核心医疗领域的新基准,包括来自真实世界电子健康记录和专家设计的临床假想的2 996个问题。我们还设计了一个自动化评价管道,将专家开发的清单纳入我们的LLMM-as-Jodge框架。此外,我们的方法验证机器通过人体机械协议分析得分,动态地改进核对清单,并根据专家反馈迅速进行评分,以确保可靠性。我们评估了三类(专门医疗模型、开源模型和封闭源模型)的13个LLLMMMS,为在医疗领域安全有效地部署LMs提供了宝贵的见解。数据设置在 https://github.Mell/Mell/MLLLLS。","Ming Zhang, Yujiong Shen, Zelin Li, Huayu Sha, Binze Hu, Yuhui Wang, Chenhao Huang, Shichun Liu, Jingqi Tong, Changhao Jiang, Mingxu Chai, Zhiheng Xi, Shihan Dou, Tao Gui, Qi Zhang, Xuanjing Huang",2025-06-04T15:43:14Z,LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with   Physician Validation,LLMEval-Med: Ein echter klinischer Benchmark für medizinische LLMs mit Physician Validation,LLMEval-Med:具有物理校验功能的医疗长效LML 医疗长效LMS的现实世界临床基准,http://arxiv.org/abs/2506.04078v1
253,"Automated speaking assessment (ASA) on opinion expressions is often hampered by the scarcity of labeled recordings, which restricts prompt diversity and undermines scoring reliability. To address this challenge, we propose a novel training paradigm that leverages a large language models (LLM) to generate diverse responses of a given proficiency level, converts responses into synthesized speech via speaker-aware text-to-speech synthesis, and employs a dynamic importance loss to adaptively reweight training instances based on feature distribution differences between synthesized and real speech. Subsequently, a multimodal large language model integrates aligned textual features with speech signals to predict proficiency scores directly. Experiments conducted on the LTTC dataset show that our approach outperforms methods relying on real data or conventional augmentation, effectively mitigating low-resource constraints and enabling ASA on opinion expressions with cross-modal information.","针对这一挑战,我们提议采用新的培训模式,利用大型语言模型(LLM)来产生特定熟练程度的不同反应,通过语音语音文本合成将答复转换为综合演讲,并利用基于合成语言和真实语言特征分布差异的适应性再量培训实例,对适应性再量培训实例造成巨大损失。随后,多式大型语言模型将统一文字特征与语音信号相结合,以直接预测熟练分数。在LTTC数据集进行的实验表明,我们的方法优于依赖实际数据或常规增强能力的方法,有效减轻低资源限制,并使ASA能够对具有跨模式信息的意见表达方式进行分析。","Chung-Chun Wang, Jhen-Ke Lin, Hao-Chien Lu, Hong-Yun Lin, Berlin Chen",2025-06-04T15:42:53Z,A Novel Data Augmentation Approach for Automatic Speaking Assessment on   Opinion Expressions,Ein neuartiger Datenvergrößerungsansatz für die automatische Sprachbewertung von Meinungsausdrücken,意见表达式自动发言评估的新数据增强方法,http://arxiv.org/abs/2506.04077v1
254,"Verbatim transcription for automatic speaking assessment demands accurate capture of disfluencies, crucial for downstream tasks like error analysis and feedback. However, many ASR systems discard or generalize hesitations, losing important acoustic details. We fine-tune Whisper models on the Speak & Improve 2025 corpus using low-rank adaptation (LoRA), without recourse to external audio training data. We compare three annotation schemes: removing hesitations (Pure), generic tags (Rich), and acoustically precise fillers inferred by Gemini 2.0 Flash from existing audio-transcript pairs (Extra). Our challenge system achieved 6.47% WER (Pure) and 5.81% WER (Extra). Post-challenge experiments reveal that fine-tuning Whisper Large V3 Turbo with the ""Extra"" scheme yielded a 5.5% WER, an 11.3% relative improvement over the ""Pure"" scheme (6.2% WER). This demonstrates that explicit, realistic filled-pause labeling significantly enhances ASR accuracy for verbatim L2 speech transcription.","自动语音评估的逐字记录要求准确捕捉混乱,这对于诸如错误分析和反馈等下游任务至关重要。 但是, 许多 ASR 系统丢弃或概括犹豫, 失去了重要的音频细节。 我们使用低调调( LoRA) , 无需外部音频培训数据, 微调2025年发言和改进文稿的耳语模型( LoRA ) 。 我们比较了三种批注方案: 去除犹豫( Pure) 、 通用标签( Rich ) 和 Gemini 2.0 Flash 从现有音标配对(Extra) 中推断的声学精确填充器。 我们的挑战系统达到了6.47% WER( Pure) 和 5.81% WER(Extra ) 。 事后分析实验显示, 用“ Extra” 计划微调Whisper 大型 V3 Turbo (Extra) 生成了5.5% WER, 比“ Pure” 计划( 6.2%) 相对改进了11.3% WER 。 这显示, 明确、现实的填充标签显著提高了逐字记录L2 语音翻译的ASR精度。","Jhen-Ke Lin, Hao-Chien Lu, Chung-Chun Wang, Hong-Yun Lin, Berlin Chen",2025-06-04T15:41:53Z,Acoustically Precise Hesitation Tagging Is Essential for End-to-End   Verbatim Transcription Systems,Akustisch präzises Hesitations-Tagging ist für End-to-End-Transkriptionssysteme unerlässlich,终端至终端逐字记录翻译系统至关重要的隐含精确言辞,http://arxiv.org/abs/2506.04076v1
255,"Practicing conversations with large language models (LLMs) presents a promising alternative to traditional in-person language learning. However, most LLMs generate text at a near-native level of complexity, making them ill-suited for beginner learners (CEFR: A1-A2). In this paper, we investigate whether controllable generation techniques -- specifically modular methods that do not require model fine-tuning -- can adapt LLM outputs to better support absolute beginners. We evaluate these methods through both automatic metrics and a user study with university-level learners of Japanese. Our findings show that while prompting alone fails to control output difficulty, the use of future discriminators (Yang and Klein, 2021) significantly improves output comprehensibility (from 40.4\% to 84.3\%). We further introduce a novel token-level evaluation metric, Token Miss Rate (TMR), that quantifies the proportion of incomprehensible tokens per utterance and correlates strongly with human judgments. To support future research in AI-assisted language learning, we release our code, models, annotation tools, and dataset.","与大型语言模型(LLMs)进行实践性对话,是传统亲身语言学习的一个很有希望的替代方法。然而,大多数LMs生成的文本几乎是复杂的,使得它们不适合初学者(CEFR:A1-A2)。 在本文中,我们调查可控的一代技术 -- -- 特别是不需要模型微调的模块化方法 -- -- 是否能够调整LLM产出,以更好地支持绝对初学者。我们通过自动计量和与大学级日语学习者进行用户研究来评估这些方法。我们的研究结果表明,单靠自己来推动无法控制产出困难,使用未来的歧视者(Yang和Klein,2021年)大大提高了产出的可理解性(从40.4到84.3)。我们进一步引入了一个新的象征性评价指标,Token Miss Rates (TMRations) ,该指标可以量化无法理解的标语的比例,并与人类判断密切相关。为了支持对AI辅助语言学习的未来研究,我们发布了我们的代码、模型、说明工具和数据设置。","Meiqing Jin, Liam Dugan, Chris Callison-Burch",2025-06-04T15:38:21Z,Controlling Difficulty of Generated Text for AI-Assisted Language   Learning,Kontrolle der Schwierigkeit des Generierten Textes für das KI-Assistierte Sprachenlernen,控制AI协助语言学习生成文本的难度,http://arxiv.org/abs/2506.04072v1
256,"The composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality, a critical driver of model performance. Current data selection methods, such as natural language quality assessments, diversity-based filters, and classifier-based approaches, are limited by single-dimensional evaluation or redundancy-focused strategies. To address these gaps, we propose four dimensions to evaluate data quality: professionalism, readability, reasoning, and cleanliness. We further introduce Meta-rater,a multi-dimensional data selection method that integrates these dimensions with existing quality metrics through learned optimal weightings. Meta-rater employs proxy models to train a regression model that predicts validation loss, enabling the identification of optimal combinations of quality scores. Experiments demonstrate that Meta-rater doubles convergence speed for 1.3B parameter models and improves downstream task performance by 3.23, with advantages that scale to models as large as 7.2B parameters. Our work establishes that holistic, multi-dimensional quality integration significantly outperforms conventional single-dimension approaches, offering a scalable paradigm for enhancing pre-training efficiency and model capability. To advance future research, we release scripts, data, and models at https://github.com/opendatalab/Meta-rater.","大型语言模型(LLMS)的培训前数据集的构成基本上仍未披露,妨碍了透明度和优化数据质量的努力,这是模型性能的一个关键驱动因素。目前的数据选择方法,如自然语言质量评估、基于多样性的过滤器和基于分类的方法,受到单维评价或以冗余为重点的战略的限制。为填补这些空白,我们提出了评估数据质量的四个层面:专业精神、可读性、推理和清洁性。我们进一步引入了Meta-rater、a多维数据选择方法,通过学习最佳加权,将这些层面与现有质量指标相结合。Meta-rater使用代理模型来培训一个回归模型,预测验证损失,使质量分数的最佳组合得以确定。实验表明,Meta-rater将1.3B参数模型的趋同速度增加一倍,并在3.23之前提高下游任务绩效。我们的工作确定,整体、多维质量的整合大大超出常规的单维元化方法,为增强培训前效率/模型/模型能力提供了一个可扩展的范例。Meta-mata-maximes,我们在未来的研究中,我们发布数据脚本。","Xinlin Zhuang, Jiahui Peng, Ren Ma, Yinfan Wang, Tianyi Bai, Xingjian Wei, Jiantao Qiu, Chi Zhang, Ying Qian, Conghui He",2025-06-04T15:35:04Z,Meta-rater: A Multi-dimensional Data Selection Method for Pre-training   Language Models,Meta-Rater: Eine mehrdimensionale Datenauswahlmethode für Vortrainings-Sprachmodelle,Met-鼠标:培训前语言模式的多维数据选择方法,http://arxiv.org/abs/2504.14194v3
257,"Navigation instruction generation for visually impaired (VI) individuals (NIG-VI) is critical yet relatively underexplored. This study, hence, focuses on producing precise, in-situ, step-by-step navigation instructions that are practically usable by VI users. Concretely, we propose LaF-GRPO (LLM-as-Follower GRPO), where an LLM simulates VI user responses to generate rewards guiding the Vision-Language Model (VLM) post-training. This enhances instruction usability while reducing costly real-world data needs. To facilitate training and testing, we introduce NIG4VI, a 27k-sample open-sourced benchmark. It provides diverse navigation scenarios with accurate spatial coordinates, supporting detailed, open-ended in-situ instruction generation. Experiments on NIG4VI show the effectiveness of LaF-GRPO by quantitative metrics (e.g., Zero-(LaF-GRPO) boosts BLEU +14\%; SFT+(LaF-GRPO) METEOR 0.542 vs. GPT-4o's 0.323) and yields more intuitive, safer instructions. Code and benchmark are available at \href{https://github.com/YiyiyiZhao/NIG4VI}{https://github.com/YiyiyiZhao/NIG4VI}.","对视障(VI)个人(NIG-VI)的导航教育生成至关重要,但相对而言探索不足。因此,本研究侧重于制作精确的、现场的、逐步的导航指令,供VI用户实际使用。具体地说,我们提议使用LAF-GRO(LLM-as-Conterer GROPO)(LLLM-as-Forester GROPO)(LLLM-Language Model(VLM)培训后,LLLM(VLM-GRPO)的用户反应,以产生奖励性指导愿景-Language模型(VLLM-GRPO)的效用,同时减少昂贵的实际世界数据需求。为便利培训和测试,我们引入了27k-Sample-Sample开放源基准NIVI(NIG4-Sample-Suple-Suple-Suple-Suple-Suple)VI-Syual-YOVO/YIGYIG_BYIG_0.3GYG_BYGYG_BAR_BAR_BAR_BAR_0.0.3.GUGUDRIGUDrass mass mass mOLDrass mass mass 0.3.GUGDOL)的精确规范和更多。","Yi Zhao, Siqi Wang, Jing Li",2025-06-04T15:34:33Z,LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually   Impaired via GRPO with LLM-as-Follower Reward,LaF-GRPO: In-Situ Navigations-Instruktions-Generierung für Visually Impaired via GRPO mit LLM-as-Follower Reward,LaF-GROP:通过GROP和LLM-as-后续奖励为视力障碍者编制现场导航指导,http://arxiv.org/abs/2506.04070v1
258,"Aligning large language models (LLMs) to human preferences is a crucial step in building helpful and safe AI tools, which usually involve training on supervised datasets. Popular algorithms such as Direct Preference Optimization (DPO) rely on pairs of AI-generated responses ranked according to human annotation. The response pair annotation process might bring human bias. Building a correct preference dataset is the costly part of the alignment pipeline. To improve annotation efficiency and quality in the LLMs alignment, we propose REAL: Response Embedding-based Alignment for LLMs, a strategy for constructing a high-quality training dataset that focuses on acquiring the less ambiguous preference pairs for labeling out of a set of response candidates. Our selection process is based on the similarity of embedding responses independently of prompts, which guarantees the selection process in an off-policy setting, avoiding adaptively measuring the similarity during the training. Experimental results on real-world dataset SHP2 and synthetic HH-RLHF benchmarks indicate that choosing dissimilar response pairs enhances the direct alignment of LLMs while reducing inherited labeling errors. The model aligned with dissimilar response pairs obtained a better margin and win rate on the dialogue task. Our findings suggest that focusing on distinct pairs can reduce the label error and improve LLM alignment efficiency, saving up to $65\%$ of annotators' work.","将大型语言模型(LLMS)与人类偏好相匹配是建立有用和安全的AI工具的关键步骤,这些工具通常包括监督数据集的培训。直接偏好优化(DPO)等通用算法依赖于按照人类注解排列的对AI产生的响应。响应对配方注解过程可能带来人类偏见。建立正确的偏好数据集是调整管道中昂贵的部分。为了提高LLMS对齐的批注效率和质量,我们建议实现:基于LMS的响应嵌入式对齐,这是建立高质量培训数据集的战略,重点是为一组应答候选人的标签获得较不模糊的优选配方。我们的选择程序基于将响应与及时分别嵌入的相似性,这保证了非政策环境中的选择过程,避免了适应性测量培训过程中的相似性。真实世界数据集SHP2和合成HH-RHF基准的实验结果显示,选择不同响应配方可以提高LMS的直接联系,同时减少继承标签误差。我们的选择过程的基础是将模型与不同的标签对齐,可以降低成本。","Honggen Zhang, Xufeng Zhao, Igor Molybog, June Zhang",2025-06-04T15:32:37Z,REAL: Response Embedding-based Alignment for LLMs,REAL: Reaktions-Embedding-basierte Ausrichtung für LLMs,实数:LLMMs的嵌入式对齐,http://arxiv.org/abs/2409.17169v4
259,"Large Language Models (LLMs) have achieved remarkable performance across various reasoning tasks, yet post-training is constrained by inefficient sample utilization and inflexible difficulty samples processing. To address these limitations, we propose Customized Curriculum Learning (CCL), a novel framework with two key innovations. First, we introduce model-adaptive difficulty definition that customizes curriculum datasets based on each model's individual capabilities rather than using predefined difficulty metrics. Second, we develop ""Guided Prompting,"" which dynamically reduces sample difficulty through strategic hints, enabling effective utilization of challenging samples that would otherwise degrade performance. Comprehensive experiments on supervised fine-tuning and reinforcement learning demonstrate that CCL significantly outperforms uniform training approaches across five mathematical reasoning benchmarks, confirming its effectiveness across both paradigms in enhancing sample utilization and model performance.","大型语言模型(LLMS)在各种推理任务中取得了显著的成绩,然而,培训后由于抽样利用效率低和难以灵活处理的样本处理而受到限制。为了解决这些限制,我们提议采用定制课程学习(CCL)这个具有两个关键创新的新框架。首先,我们采用模式适应困难定义,根据每个模型的个别能力定制课程数据集,而不是使用预先界定的困难度量。第二,我们开发“指导提示”,通过战略提示,动态地减少样本难度,从而能够有效利用具有挑战性的样本,否则会降低绩效。关于监管的微调和强化学习的全面实验表明,CCL在五个数学推理基准方面大大优于统一的培训方法,证实了其在加强样本利用和模型性能方面在两种模式上的有效性。","Muling Wu, Qi Qian, Wenhao Liu, Xiaohua Wang, Zisu Huang, Di Liang, LI Miao, Shihan Dou, Changze Lv, Zhenghua Wang, Zhibo Xu, Lina Chen, Tianlong Li, Xiaoqing Zheng, Xuanjing Huang",2025-06-04T15:31:46Z,Progressive Mastery: Customized Curriculum Learning with Guided   Prompting for Mathematical Reasoning,Progressive Mastery: Maßgeschneidertes Curriculum Lernen mit Anleitung für mathematische Vernunft,渐进版学:以指导方式引导数学理由的自订课程学习,http://arxiv.org/abs/2506.04065v1
260,"Uncertainty Quantification (UQ) in Language Models (LMs) is key to improving their safety and reliability. Evaluations often use metrics like AUROC to assess how well UQ methods (e.g., negative sequence probabilities) correlate with task correctness functions (e.g., ROUGE-L). We show that mutual biases--when both UQ methods and correctness functions are biased by the same factors--systematically distort evaluation. First, we formally prove that any mutual bias non-randomly skews AUROC rankings, compromising benchmark integrity. Second, we confirm this happens empirically by testing 7 widely used correctness functions, from lexical-based and embedding-based metrics to LM-as-a-judge approaches, across 4 datasets x 4 models x 8 UQ methods. Our analysis shows that length biases in correctness functions distort UQ assessments by interacting with length biases in UQ methods. We identify LM-as-a-judge methods as the least length-biased, offering a promising path for a fairer UQ evaluation.","语言模型中的不确定性量化(UQ)是提高其安全和可靠性的关键。评价经常使用AUROC等衡量标准来评估UQ方法(例如,负序列概率)与任务正确性功能(例如,ROUGE-L)的关联性有多好(例如,ROUGE-L) 。我们显示,当UQ方法和正确性功能都受到相同因素-系统扭曲的评价的偏差时,相互偏差是相互偏差的。首先,我们正式证明,任何相互偏差的非随机偏差 AUROC排名,都会损害基准完整性。第二,我们通过测试7种广泛使用的正确性功能(从基于词汇的和基于嵌入式的测量方法到LM-as-a-判断方法),从4个数据集x4模型x8 UQ方法,我们的分析表明,在正确性方面的时间偏差功能扭曲了UQ评估。我们把LM-A判断方法确定为最小的长度偏差,为更公平的UQ评价提供了有希望的途径,从而实现更公平的UQ评估。","Andrea Santilli, Adam Golinski, Michael Kirchhof, Federico Danieli, Arno Blaas, Miao Xiong, Luca Zappella, Sinead Williamson",2025-06-04T15:25:43Z,Revisiting Uncertainty Quantification Evaluation in Language Models:   Spurious Interactions with Response Length Bias Results,Überprüfung der Ungewissheitsquantifizierung in Sprachmodellen: Puriöse Interaktionen mit Reaktionslänge Bias-Ergebnissen,重新审查语言模型中的不确定性量化评价:有反应时间跨度结果的纯净互动,http://arxiv.org/abs/2504.13677v2
261,"Large Language Models (LLMs) currently respond to every prompt. However, they can produce incorrect answers when they lack knowledge or capability -- a problem known as hallucination. We instead propose post-training an LLM to generate content only when confident in its correctness and to otherwise (partially) abstain. Specifically, our method, HALT, produces capability-aligned post-training data that encodes what the model can and cannot reliably generate. We generate this data by splitting responses of the pretrained LLM into factual fragments (atomic statements or reasoning steps), and use ground truth information to identify incorrect fragments. We achieve capability-aligned finetuning responses by either removing incorrect fragments or replacing them with ""Unsure from Here"" -- according to a tunable threshold that allows practitioners to trade off response completeness and mean correctness of the response's fragments. We finetune four open-source models for biography writing, mathematics, coding, and medicine with HALT for three different trade-off thresholds. HALT effectively trades off response completeness for correctness, increasing the mean correctness of response fragments by 15% on average, while resulting in a 4% improvement in the F1 score (mean of completeness and correctness of the response) compared to the relevant baselines. By tuning HALT for highest correctness, we train a single reliable Llama3-70B model with correctness increased from 51% to 87% across all four domains while maintaining 53% of the response completeness achieved with standard finetuning.","大型语言模型(LLMS) (LLMS) (LLMS) (LLMS) (LLMS) 正在对每一个提示做出反应。然而,当它们缺乏知识或能力时,它们可以产生错误的答案 -- -- 一个被称为幻觉的问题。我们提议在培训后,LLMM(LLM) 培训后, 只有在对它的正确性有信心时才能产生内容, 并且( 部分)不这样做。 具体地说, 我们的方法是, HALT(HALT) , 生成能力匹配后的培训后的培训后数据, 将模型能够和无法可靠生成的内容编码。 我们生成这些数据的方法是, 将训练有素的LMM(LLM) 的回复分为三个不同的贸易模型( 解析表或推理步骤) , 使用地面真实性信息来识别错误的碎片。 我们通过消除错误的碎片或者用“ 无法从这里得到的保证” 来调整反应。 我们的调整能力调整反应, , , 达到平均15% 或以“ 无法相信” ” 。 。 , 57% 的准确性, , 和 最高的 水平 提高 水平 。","Tim Franzmeyer, Archie Sravankumar, Lijuan Liu, Yuning Mao, Rui Hou, Sinong Wang, Jakob N. Foerster, Luke Zettlemoyer, Madian Khabsa",2025-06-04T15:16:21Z,"High Accuracy, Less Talk (HALT): Reliable LLMs through   Capability-Aligned Finetuning","Hohe Genauigkeit, weniger Gerede (HALT): Zuverlässige LLMs durch kapazitätsabhängiges Finetuning",高准确度、少说少说(HALT):通过能力统一微调的可靠LLMs,http://arxiv.org/abs/2506.04051v1
262,"Large Language Models (LLMs) have achieved unprecedented capabilities in generating human-like text, posing subtle yet significant challenges for information integrity across critical domains, including education, social media, and academia, enabling sophisticated misinformation campaigns, compromising healthcare guidance, and facilitating targeted propaganda. This challenge becomes severe, particularly in under-explored and low-resource languages like Arabic. This paper presents a comprehensive investigation of Arabic machine-generated text, examining multiple generation strategies (generation from the title only, content-aware generation, and text refinement) across diverse model architectures (ALLaM, Jais, Llama, and GPT-4) in academic, and social media domains. Our stylometric analysis reveals distinctive linguistic patterns differentiating human-written from machine-generated Arabic text across these varied contexts. Despite their human-like qualities, we demonstrate that LLMs produce detectable signatures in their Arabic outputs, with domain-specific characteristics that vary significantly between different contexts. Based on these insights, we developed BERT-based detection models that achieved exceptional performance in formal contexts (up to 99.9\% F1-score) with strong precision across model architectures. Our cross-domain analysis confirms generalization challenges previously reported in the literature. To the best of our knowledge, this work represents the most comprehensive investigation of Arabic machine-generated text to date, uniquely combining multiple prompt generation methods, diverse model architectures, and in-depth stylometric analysis across varied textual domains, establishing a foundation for developing robust, linguistically-informed detection systems essential for preserving information integrity in Arabic-language contexts.","大型语言模型(LLMS)在生成人文文本方面达到了前所未有的能力,对学术、社会媒体和学术界等关键领域的信息完整性提出了微妙而重大的挑战,使得复杂的错误信息运动得以进行,损害保健指导,并促进有针对性的宣传。这一挑战变得十分严峻,特别是在诸如阿拉伯语等探索不足和低资源语言方面。本文对阿拉伯机器生成的文本进行了全面调查,审查了多种代代战略(仅以标题生成、内容认知生成和文本完善),在学术和社会媒体领域(ALLAM、Jais、Llama和GPT-4),对信息完整性提出了微妙但又很严峻的挑战。我们的系统特征分析显示,在各种背景下,人类写作的文字模式与机器生成的阿拉伯文文本有区别。尽管LLMS具有人性特征,但我们证明,其阿拉伯文产出中具有可探测的特征在不同背景下差异很大。基于这些洞察的、基于BERT的检测模型模型在正式背景下(至99.9F1核心)取得了非常强的业绩,在模型结构中具有很强的精确性。我们的跨模型结构中,我们最精确的语言模型分析显示的人类文字结构中最精确的语言模式,我们最精确的版本分析显示,在生成的版本中,我们最接近的版本分析中,从结构中报告了我们最接近的版本的版本的版本的版本的版本的版本的文献结构中,在构建的版本分析显示了了我们所研判的版本的版本的版本的版本的版本中,在构建中,在文献中,在文献结构中,对历史结构中,对文献中报告了我们的数据。","Maged S. Al-Shaibani, Moataz Ahmed",2025-06-04T15:16:04Z,The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large   Language Models Text,Der arabische KI-Fingerabdruck: Stylometrische Analyse und Erkennung von großen Sprachmodellen Text,阿拉伯文 AI 指纹:大语言模型文本的tytyllogimics 分析和探测,http://arxiv.org/abs/2505.23276v2
263,"Generative models, especially large language models (LLMs), have shown remarkable progress in producing text that appears human-like. However, they often exhibit patterns that make their output easier to detect than text written by humans. In this paper, we investigate how explainable AI (XAI) methods can be used to reduce the detectability of AI-generated text (AIGT) while also introducing a robust ensemble-based detection approach. We begin by training an ensemble classifier to distinguish AIGT from human-written text, then apply SHAP and LIME to identify tokens that most strongly influence its predictions. We propose four explainability-based token replacement strategies to modify these influential tokens. Our findings show that these token replacement approaches can significantly diminish a single classifier's ability to detect AIGT. However, our ensemble classifier maintains strong performance across multiple languages and domains, showing that a multi-model approach can mitigate the impact of token-level manipulations. These results show that XAI methods can make AIGT harder to detect by focusing on the most influential tokens. At the same time, they highlight the need for robust, ensemble-based detection strategies that can adapt to evolving approaches for hiding AIGT.","生成模型,特别是大型语言模型(LLMS),在制作看起来像人类的文本方面取得了显著进展。然而,这些模型往往展示出使其产出比人类写成的文本更容易检测到的4个基于解释的代用代用代用代用代用战略。在本文件中,我们调查如何使用可解释的 AI (XAI) 方法来减少AI 生成文本的可探测性,同时引入一个强有力的共同式检测方法。我们首先培训一个混合式分类器,将AIGT与人写的文本区分开来,然后应用SHAP和LIME来识别对其预测影响最大的代用符号。我们提出了4个基于解释的代用代用代用替代战略来修改这些有影响力的代用符号。我们的调查结果显示,这些代用代用代用代用代用方法可以大大削弱单个分类者检测AIGT的能力。然而,我们的共性分类器在多种语言和领域都保持很强的性能,表明多模式方法可以减轻代用操作的影响。这些结果表明,XAIGT方法能够使AIGT更难于检测最有影响力的代用符号。与此同时,它们需要隐藏着以不断演变的AIGT式检测战略。","Hadi Mohammadi, Anastasia Giachanou, Daniel L. Oberski, Ayoub Bagheri",2025-06-04T15:15:42Z,Explainability-Based Token Replacement on LLM-Generated Text,Erklärbarkeitsbasierter Token-Ersatz auf LLM-generierter Text,以LLM-发光文本替换LLM-发光文本,http://arxiv.org/abs/2506.04050v1
264,"Language models excel in various tasks by making complex decisions, yet understanding the rationale behind these decisions remains a challenge. This paper investigates \emph{data-centric interpretability} in language models, focusing on the next-word prediction task. Using representer theorem, we identify two types of \emph{support samples}-those that either promote or deter specific predictions. Our findings reveal that being a support sample is an intrinsic property, predictable even before training begins. Additionally, while non-support samples are less influential in direct predictions, they play a critical role in preventing overfitting and shaping generalization and representation learning. Notably, the importance of non-support samples increases in deeper layers, suggesting their significant role in intermediate representation formation.These insights shed light on the interplay between data and model decisions, offering a new dimension to understanding language model behavior and interpretability.","语言模型通过作出复杂的决定,在各种任务中出类拔萃,但理解这些决定背后的理由仍然是一项挑战。本文件调查语言模型中的 \ emph{ 以数据为中心的解释性} ,重点是下一个词的预测任务。使用代表理论,我们确定两种类型的 \ emph{ 支持性样本,促进或阻止具体预测。我们的研究结果显示,支持性样本是一种内在属性,甚至在培训开始前也是可以预测的。此外,虽然非支持性样本在直接预测中影响较小,但它们在防止过分适应和形成一般化和代表性学习方面发挥着关键作用。值得注意的是,非支持性样本的重要性在更深层次上有所增加,表明它们在中间代表结构中的重要作用。这些见解揭示了数据和模型决定之间的相互作用,为理解语言模型行为和可解释性提供了新的层面。","Yuqian Li, Yupei Du, Yufang Liu, Feifei Feng, Mou Xiao Feng, Yuanbin Wu",2025-06-04T15:13:22Z,On Support Samples of Next Word Prediction,Unterstützungsbeispiele für die nächste Wortvorhersage,"关于 "" 下一词预测 "" 支助样本",http://arxiv.org/abs/2506.04047v1
265,"The classic text preprocessing pipeline, comprising Tokenisation, Normalisation, Stop Words Removal, and Stemming/Lemmatisation, has been implemented in many systems for syntactic ontology matching (OM). However, the lack of standardisation in text preprocessing creates diversity in mapping results. In this paper, we investigate the effect of the text preprocessing pipeline on syntactic OM in 8 Ontology Alignment Evaluation Initiative (OAEI) tracks with 49 distinct alignments. We find that Phase 1 text preprocessing (Tokenisation and Normalisation) is more effective than Phase 2 text preprocessing (Stop Words Removal and Stemming/Lemmatisation). We propose two novel approaches to repair unwanted false mappings caused by Phase 2 text preprocessing. One is an ad hoc logic-based repair approach that employs an ontology-specific check to find common words that cause false mappings. These words are stored in a reserved word set and applied before the text preprocessing. By leveraging the power of large language models (LLMs), we also propose a post hoc LLM-based repair approach. This approach utilises the strong background knowledge provided by LLMs to repair non-existent and counter-intuitive false mappings after the text preprocessing. It also overcomes the tendency towards unstable true mappings by injecting the classic text preprocessing pipeline via function calling. The experimental results show that these two approaches can improve the matching correctness and the overall matching performance.","经典的文本预处理管道,包括 Tokenization、 规范化、 停止单词删除和 Stemming/Lemmatization,在许多综合本学匹配系统(OM)中已经实施了经典的文本预处理管道,但文本预处理缺乏标准化导致绘图结果的多样性。在本文件中,我们调查了文本预处理管道对8个本体一致评价倡议(OAEI)中混合OM综合体(OAEI)轨道的影响,有49个不同的校正。我们发现,第一阶段文本预处理(定罪和标准化)比第二阶段预处理前的文本处理(停止删除单词和停止删除/简化)更有效。我们提出了两种新颖的方法,以修复第二阶段预处理前处理过程中产生的不必要的错误制图结果。 一种基于逻辑的临时性修复方法,在8个本体协调评价评价倡议(OAEII)中,采用专门的文字储存在文本预处理前储存并应用。通过大型语言模型(LIMS)的力量,我们还提议采用基于后特设LLM公司的总体修理办法,在纠正前改进当前正统的正统结果后,通过不动的正统分析结果。","Zhangcheng Qiang, Kerry Taylor, Weiqing Wang",2025-06-04T15:11:00Z,How Does A Text Preprocessing Pipeline Affect Ontology Syntactic   Matching?,Wie wirkt sich eine Textvorverarbeitung auf die Ontologie aus?,文本预处理管道如何影响本体学同步匹配?,http://arxiv.org/abs/2411.03962v7
266,"This paper describes LIBU (LoRA enhanced influence-based unlearning), an algorithm to solve the task of unlearning - removing specific knowledge from a large language model without retraining from scratch and compromising its overall utility (SemEval-2025 Task 4: Unlearning sensitive content from Large Language Models). The algorithm combines classical \textit{influence functions} to remove the influence of the data from the model and \textit{second-order optimization} to stabilize the overall utility. Our experiments show that this lightweight approach is well applicable for unlearning LLMs in different kinds of task.","本文介绍LIBU(LORA增强影响力的不学习),这是一种解决不学习任务的算法——从大型语言模式中去除特定知识,而无需从零开始再培训,并损害其总体效用(SemEval-2025任务4:从大语言模式中去除敏感内容)。算法将古典\textit{impact 函数}结合起来,以消除模型中数据的影响,并\textit{第二顺序优化}稳定总体效用。我们的实验表明,这种轻量级方法适用于不同任务中未学习的LMS。","Aleksey Kudelya, Alexander Shirnin",2025-06-04T15:10:09Z,Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based   Unlearning for LLMs,Lacuna Inc. bei SemEval-2025 Task 4: LoRA-erweiterte Einfluss-basiertes Unlernen für LLMs,SemEval-2025任务4:LOLA-加强LLMM的基于影响的无学习 Lacuna Inc.Lacuna Inc. SemEval-2025 任务4:LLMs,http://arxiv.org/abs/2506.04044v1
267,"Automated counter-narratives (CN) offer a promising strategy for mitigating online hate speech, yet concerns about their affective tone, accessibility, and ethical risks remain. We propose a framework for evaluating Large Language Model (LLM)-generated CNs across four dimensions: persona framing, verbosity and readability, affective tone, and ethical robustness. Using GPT-4o-Mini, Cohere's CommandR-7B, and Meta's LLaMA 3.1-70B, we assess three prompting strategies on the MT-Conan and HatEval datasets. Our findings reveal that LLM-generated CNs are often verbose and adapted for people with college-level literacy, limiting their accessibility. While emotionally guided prompts yield more empathetic and readable responses, there remain concerns surrounding safety and effectiveness.","自动反叙事(CN)为减轻网上仇恨言论提供了一个很有希望的战略,但人们对其感知语调、可及性和道德风险仍然感到关切。我们提出了一个框架,用于评估大语言模型(LLM)产生的氯化萘的四方面:人造框架、动词和可读性、感知语调和道德稳健性。使用GPT-4-Mini、Cohere指挥R-7B和Meta的LalaMA 3.1-70B,我们评估了关于MT-Conan和HatEval数据集的三种促动战略。我们的调查结果显示,LLM生成的氯化萘经常是动词,适合具有大学水平识字能力的人,限制其可及性。尽管情感引导的提示产生更具有同情性和可读性的反应,但在安全和有效性方面仍然存在问题。","Mikel K. Ngueajio, Flor Miriam Plaza-del-Arco, Yi-Ling Chung, Danda B. Rawat, Amanda Cercas Curry",2025-06-04T15:09:20Z,Think Like a Person Before Responding: A Multi-Faceted Evaluation of   Persona-Guided LLMs for Countering Hate,Denken Sie wie eine Person vor der Reaktion: Eine Multi-Faceted-Bewertung von Persona-geführten LLMs zur Bekämpfung von Hass,回应前的思考:对反对仇恨的多面人指导LMS的多面评价,http://arxiv.org/abs/2506.04043v1
268,"Knowledge editing aims to alternate the target knowledge predicted by large language models while ensuring the least side effects on unrelated knowledge. An effective way to achieve knowledge editing is to identify pivotal parameters for predicting factual associations and modify them with an optimization process to update the predictions. However, these locate-then-edit methods are uncontrollable since they tend to modify most unrelated relations connected to the subject of target editing. We unveil that this failure of controllable editing is due to a shortcut learning issue during the optimization process. Specifically, we discover two crucial features that are the subject feature and the relation feature for models to learn during optimization, but the current optimization process tends to over-learning the subject feature while neglecting the relation feature. To eliminate this shortcut learning of the subject feature, we propose a novel two-stage optimization process that balances the learning of the subject feature and the relation feature. Experimental results demonstrate that our approach successfully prevents knowledge editing from shortcut learning and achieves the optimal overall performance, contributing to controllable knowledge editing.","知识编辑旨在替代大语言模型预测的目标知识,同时确保对无关知识产生最小的副作用; 实现知识编辑的一个有效途径是确定预测事实关联的关键参数,并以更新预测的优化程序修改这些参数; 然而,这些定位后编辑方法是无法控制的,因为它们往往会改变与目标编辑主题有关的最无关的关系; 我们公布, 控制下编辑的失败是由于优化过程中的快捷学习问题。 具体地说, 我们发现两个关键特征是主题特征和模型在优化期间学习的关联特征,但目前的优化进程往往过度学习主题特征,而忽略了关系特征。 为了消除该主题特征的这种快捷学习,我们提议了一个新颖的两阶段优化进程,平衡了对主题特征的学习和关联特征的学习。 实验结果表明,我们的方法成功地阻止了知识编辑从快捷学习中获得,实现了最佳的整体性能,有助于控制下的知识编辑。","Xiyu Liu, Zhengxiao Liu, Naibin Gu, Zheng Lin, Ji Xiang, Weiping Wang",2025-06-04T15:06:46Z,Unveiling and Eliminating the Shortcut Learning for Locate-Then-Edit   Knowledge Editing via Both Subject and Relation Awareness,Enthüllen und Eliminieren des Shortcut-Lernens für das Lokalisieren-Dann-Bearbeiten von Wissen über Subject und Relation-Bewusstsein,通过主题和关系意识来统一和消除定位时编辑知识编辑的快捷学习,http://arxiv.org/abs/2506.04042v1
269,"Temporal reasoning in legal texts is important for applications like case law analysis and compliance monitoring. However, existing datasets lack expert language evaluation, leaving a gap in understanding how LLMs manage event ordering in legal contexts. We introduce LexTime, the first dataset designed to evaluate LLMs' event ordering capabilities in legal language, consisting of 512 instances from U.S. Federal Complaints with annotated event pairs and their temporal relations. Our findings show that (1) LLMs are more accurate on legal event ordering than on narrative (up to +10.5%); (2) longer input contexts and implicit events boost accuracy, reaching 80.8% for implicit-explicit event pairs; (3) legal linguistic complexities and nested clauses remain a challenge. We investigate how context length, explicit vs implicit event pairs, and legal language features affect model performance, demonstrating the need for specific modeling strategies to enhance temporal event reasoning.","法律文本中的时间推理对于判例法分析和合规监测等应用非常重要,但是,现有的数据集缺乏专业语言评价,在了解LLMs如何管理法律背景下的秩序活动方面存在差距。我们引入了第一个数据集LexTime(LexTime),这是用来评价LLMs的法律语言定购能力的第一个数据集,由美国联邦投诉的512个案例组成,附有附带附加说明的事件配对及其时间关系。我们的调查结果显示:(1) LLMs在法律事件定购方面比叙述性更准确(高达10.5% );(2) 较长的输入背景和隐含事件提高准确性,对隐含事件配对而言达到80.8%;(3) 法律语言复杂性和嵌套条款仍然是一项挑战。我们调查背景长度、明确与隐含的事件配对和法律语言特征如何影响示范性业绩,表明有必要制定具体的模型战略,以加强时间事件推理。","Claire Barale, Leslie Barrett, Vikram Sunil Bajaj, Michael Rovatsos",2025-06-04T15:06:27Z,LexTime: A Benchmark for Temporal Ordering of Legal Events,LexTime: Ein Benchmark für die zeitliche Bestellung von rechtlichen Veranstaltungen,LexTime: 法律活动时间秩序基准,http://arxiv.org/abs/2506.04041v1
270,"Large Visual Language Models (LVLMs) have demonstrated impressive capabilities across multiple tasks. However, their trustworthiness is often challenged by hallucinations, which can be attributed to the modality misalignment and the inherent hallucinations of their underlying Large Language Models (LLMs) backbone. Existing preference alignment methods focus on aligning model responses with human preferences while neglecting image-text modality alignment, resulting in over-reliance on LLMs and hallucinations. In this paper, we propose Entity-centric Multimodal Preference Optimization (EMPO), which achieves enhanced modality alignment than existing human preference alignment methods. Besides, to overcome the scarcity of high-quality multimodal preference data, we utilize open-source instruction datasets to automatically construct high-quality preference data across three aspects: image, instruction, and response. Experiments on two human preference datasets and five multimodal hallucination benchmarks demonstrate the effectiveness of EMPO, e.g., reducing hallucination rates by 85.9% on Object-HalBench and 49.8% on MM-HalBench.","大型视觉语言模型(LVLMs)在多种任务中表现出了令人印象深刻的能力,然而,其可信度往往受到幻觉的挑战,这些幻觉可归因于其基本大语言模型(LLMs)主干体的模式不匹配及其固有的幻觉。现有的偏好调整方法侧重于使模型反应与人类偏好保持一致,同时忽视图像-文字模式的调整,导致过度依赖LLMs和幻觉。在本文中,我们提议以实体为中心的多模式偏好优化(EMPO)实现比现有人类偏好调整方法更好的模式调整。此外,为了克服高品质多式联运偏好数据的匮乏,我们利用开放源教学数据集自动构建高品质的偏好数据,涵盖三个方面:图像、指令和响应。关于两个人类偏好数据集和五个多式幻觉基准的实验表明EMPO的有效性,例如:将对象-HalBench的幻觉率降低85.9%,MM-Hal-HalBench降低49.8%。","Jiulong Wu, Zhengliang Shi, Shuaiqiang Wang, Jizhou Huang, Dawei Yin, Lingyong Yan, Min Cao, Min Zhang",2025-06-04T15:03:50Z,Mitigating Hallucinations in Large Vision-Language Models via   Entity-Centric Multimodal Preference Optimization,Halluzinationen in großen Vision-Sprachen-Modellen über die Multimodal Preference-Optimierung von Entity-Centric,"通过实体-中心多模式首选最佳化,在大型视觉-语言模型中减轻幻觉",http://arxiv.org/abs/2506.04039v1
271,"Large reasoning models (LRMs) have demonstrated impressive capabilities in complex problem-solving, yet their internal reasoning mechanisms remain poorly understood. In this paper, we investigate the reasoning trajectories of LRMs from an information-theoretic perspective. By tracking how mutual information (MI) between intermediate representations and the correct answer evolves during LRM reasoning, we observe an interesting MI peaks phenomenon: the MI at specific generative steps exhibits a sudden and significant increase during LRM's reasoning process. We theoretically analyze such phenomenon and show that as MI increases, the probability of model's prediction error decreases. Furthermore, these MI peaks often correspond to tokens expressing reflection or transition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the thinking tokens. We then demonstrate that these thinking tokens are crucial for LRM's reasoning performance, while other tokens has minimal impacts. Building on these analyses, we propose two simple yet effective methods to improve LRM's reasoning performance, by delicately leveraging these thinking tokens. Overall, our work provides novel insights into the reasoning mechanisms of LRMs and offers practical ways to improve their reasoning capabilities. The code is available at https://github.com/ChnQ/MI-Peaks.","大型推理模型(LRMs)在复杂的问题解决过程中表现出了令人印象深刻的能力,然而,它们的内部推理机制仍然没有得到很好的理解。在本文中,我们从信息理论的角度来调查LRMs的推理轨迹。通过追踪中间代表与正确答案之间的相互信息(MI)在LRM推理过程中是如何演变的,我们观察到一个有趣的MI峰值现象:具体归正步骤的MI在具体归正步骤中显示出突然和显著的增加。我们从理论上分析这种现象,并表明随着MI的增加,模型预测错误的可能性会减少。此外,这些MI峰值往往与表达反思或过渡的象征物相对应,例如“Hmm',“wait'和“thereforwe,”我们称之为思考标志。我们然后表明,这些思考符号对于LRMM的推理表现至关重要,而其他标志影响微乎其微。根据这些分析,我们提出了两个简单有效的方法来改进LRMM的推理学表现,通过微妙地利用这些推理符号。总的来说,我们的工作为LRMM/PHM/PERs的推理机制提供了新的推理方法。","Chen Qian, Dongrui Liu, Haochen Wen, Zhen Bai, Yong Liu, Jing Shao",2025-06-04T15:00:58Z,Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens   are Information Peaks in LLM Reasoning,Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens sind Information Peaks in LLM Reasoning,以相互信息解开神秘的理性动态:思考调子是LLM 合理性的信息高峰,http://arxiv.org/abs/2506.02867v2
272,"Mutual exclusivity (ME) is a strategy where a novel word is associated with a novel object rather than a familiar one, facilitating language learning in children. Recent work has found an ME bias in a visually grounded speech (VGS) model trained on English speech with paired images. But ME has also been studied in bilingual children, who may employ it less due to cross-lingual ambiguity. We explore this pattern computationally using bilingual VGS models trained on combinations of English, French, and Dutch. We find that bilingual models generally exhibit a weaker ME bias than monolingual models, though exceptions exist. Analyses show that the combined visual embeddings of bilingual models have a smaller variance for familiar data, partly explaining the increase in confusion between novel and familiar concepts. We also provide new insights into why the ME bias exists in VGS models in the first place. Code and data: https://github.com/danoneata/me-vgs","相互排他性(ME)是一种战略,其中一个新词与一个新东西相关,而不是熟悉的东西,促进儿童的语言学习。最近的工作发现,在以配对图像进行英语演讲培训的视觉辅助语言(VGS)模型中,教育与儿童有偏向。但是,教育与教育也针对双语儿童进行了研究,他们可能由于跨语言的模糊性而较少使用这种语言。我们利用经过英语、法语和荷兰语组合培训的双语VGS模型来进行这一模式的计算性研究。我们发现,双语模型通常比单语模式表现出较弱的教育偏向性,尽管存在例外。分析表明,双语模型的合并视觉嵌入对于熟悉数据而言差异较小,部分解释了新概念和熟悉概念之间日益混淆的原因。我们还提供了新的见解,说明为什么在VGS模型中首先存在着教育偏向。代码和数据:https://github.com/danoneata/me-vgs。","Dan Oneata, Leanne Nortje, Yevgen Matusevych, Herman Kamper",2025-06-04T14:59:22Z,The mutual exclusivity bias of bilingual visually grounded speech models,Die gegenseitige Exklusivitätsvoreingenommenheit zweisprachiger visuell geerdeter Sprachmodelle,双语、视觉和基于视觉的言论模式的相互排他性偏见,http://arxiv.org/abs/2506.04037v1
273,"Background: We present a Patient Simulator that leverages real world patient encounters which cover a broad range of conditions and symptoms to provide synthetic test subjects for development and testing of healthcare agentic models. The simulator provides a realistic approach to patient presentation and multi-turn conversation with a symptom-checking agent. Objectives: (1) To construct and instantiate a Patient Simulator to train and test an AI health agent, based on patient vignettes derived from real EHR data. (2) To test the validity and alignment of the simulated encounters provided by the Patient Simulator to expert human clinical providers. (3) To illustrate the evaluation framework of such an LLM system on the generated realistic, data-driven simulations -- yielding a preliminary assessment of our proposed system. Methods: We first constructed realistic clinical scenarios by deriving patient vignettes from real-world EHR encounters. These vignettes cover a variety of presenting symptoms and underlying conditions. We then evaluate the performance of the Patient Simulator as a simulacrum of a real patient encounter across over 500 different patient vignettes. We leveraged a separate AI agent to provide multi-turn questions to obtain a history of present illness. The resulting multiturn conversations were evaluated by two expert clinicians. Results: Clinicians scored the Patient Simulator as consistent with the patient vignettes in those same 97.7% of cases. The extracted case summary based on the conversation history was 99% relevant. Conclusions: We developed a methodology to incorporate vignettes derived from real healthcare patient data to build a simulation of patient responses to symptom checking agents. The performance and alignment of this Patient Simulator could be used to train and test a multi-turn conversational AI agent at scale.","我们展示了一个病人模拟器, 利用真实的世界病人经历, 包括广泛的条件和症状, 提供合成测试对象, 用于研发和测试医疗保健代理模型。 模拟器为患者演示和与症状检查代理器进行多点对话提供了一个现实的方法。 目标:(1) 构建和即时一个病人模拟器, 用于根据真实的 EHR 数据生成的患者维格特来培训和测试AI 健康代理器。 (2) 测试患者模拟模拟器向人类临床专家提供者提供的模拟的患者疾病经历的有效性和一致性。 (3) 以生成的符合现实的、数据驱动的模拟模型为合成测试对象,为患者演示和测试提供合成测试。 模拟器提供了一种现实的临床假的临床假设。 我们利用了一种独立的LLLLM系统评估框架, 模拟了现实的模拟模拟模拟模拟器, 并用一种基于真实世界HR 的病人特征的性能测试方法, 并用一种基于真实的病人模拟模拟器, 模拟的病人对超过500个不同的患者反应器进行模拟检查 。 我们利用了一个独立的ALM Ralderalalalalalalalalal imal dalation 。","Sina Rashidian, Nan Li, Jonathan Amar, Jong Ha Lee, Sam Pugh, Eric Yang, Geoff Masterson, Myoung Cha, Yugang Jia, Akhil Vaid",2025-06-04T14:56:08Z,AI Agents for Conversational Patient Triage: Preliminary   Simulation-Based Evaluation with Real-World EHR Data,KI-Agenten für Gesprächspatienten-Triage: Vorläufige simulationsbasierte Auswertung mit realen EHR-Daten,AI: 与现实世界EHR数据进行初步模拟评价的患者相互对话的病人创伤代理机构,http://arxiv.org/abs/2506.04032v1
274,"We present an end-to-end framework for generating synthetic users for evaluating interactive agents designed to encourage positive behavior changes, such as in health and lifestyle coaching. The synthetic users are grounded in health and lifestyle conditions, specifically sleep and diabetes management in this study, to ensure realistic interactions with the health coaching agent. Synthetic users are created in two stages: first, structured data are generated grounded in real-world health and lifestyle factors in addition to basic demographics and behavioral attributes; second, full profiles of the synthetic users are developed conditioned on the structured data. Interactions between synthetic users and the coaching agent are simulated using generative agent-based models such as Concordia, or directly by prompting a language model. Using two independently-developed agents for sleep and diabetes coaching as case studies, the validity of this framework is demonstrated by analyzing the coaching agent's understanding of the synthetic users' needs and challenges. Finally, through multiple blinded evaluations of user-coach interactions by human experts, we demonstrate that our synthetic users with health and behavioral attributes more accurately portray real human users with the same attributes, compared to generic synthetic users not grounded in such attributes. The proposed framework lays the foundation for efficient development of conversational agents through extensive, realistic, and grounded simulated interactions.","我们提出了一个合成用户的终端到终端框架,用于培养合成用户,以评价互动剂,目的是鼓励积极的行为变化,例如健康和生活方式辅导;合成用户以健康和生活方式为基础,以健康和生活方式为基础,特别是本研究中的睡眠和糖尿病管理,确保与健康辅导人员进行现实互动;合成用户分两个阶段创建:首先,结构化数据以真实世界健康和生活方式因素为基础,此外还有基本人口和行为属性;第二,合成用户的全面概况以结构化数据为条件;合成用户和辅导人员之间的相互作用,利用Concordia等基于基因化代理的模型进行模拟,或直接推动一种语言模型;使用两个独立开发的机构和睡眠和糖尿病辅导人员进行案例研究,通过分析指导人员对合成用户的需要和挑战的理解来证明这一框架的有效性;最后,通过人类专家对用户和行为属性的互动进行多次盲目的评价,我们证明我们的合成用户具有健康和行为属性,与非基于这些属性的通用合成用户相比,更准确地描绘真实的人类用户的属性,或直接地通过一种语言模型模型模型进行模拟互动,为高效发展奠定基础。","Taedong Yun, Eric Yang, Mustafa Safdari, Jong Ha Lee, Vaishnavi Vinod Kumar, S. Sara Mahdavi, Jonathan Amar, Derek Peyton, Reut Aharony, Andreas Michaelides, Logan Schneider, Isaac Galatzer-Levy, Yugang Jia, John Canny, Arthur Gretton, Maja Matarić",2025-06-04T14:50:52Z,"Sleepless Nights, Sugary Days: Creating Synthetic Users with Health   Conditions for Realistic Coaching Agent Interactions","Sleepless Nights, Sugary Days: Synthetische Nutzer mit gesundheitlichen Bedingungen für realistische Coaching-Agenten-Interaktionen erstellen","无睡眠夜,糖糖日:创造有健康条件的合成使用者,促进现实教练员互动",http://arxiv.org/abs/2502.13135v2
275,"Review-based Product Question Answering (PQA) allows e-commerce platforms to automatically address customer queries by leveraging insights from user reviews. However, existing PQA systems generate answers with only a single perspective, failing to capture the diversity of customer opinions. In this paper we introduce a novel task Quantitative Query-Focused Summarization (QQSUM), which aims to summarize diverse customer opinions into representative Key Points (KPs) and quantify their prevalence to effectively answer user queries. While Retrieval-Augmented Generation (RAG) shows promise for PQA, its generated answers still fall short of capturing the full diversity of viewpoints. To tackle this challenge, our model QQSUM-RAG, which extends RAG, employs few-shot learning to jointly train a KP-oriented retriever and a KP summary generator, enabling KP-based summaries that capture diverse and representative opinions. Experimental results demonstrate that QQSUM-RAG achieves superior performance compared to state-of-the-art RAG baselines in both textual quality and quantification accuracy of opinions. Our source code is available at: https://github.com/antangrocket1312/QQSUMM","以审查为基础的产品问题解答(PQA)使电子商务平台能够利用用户审查的见解自动处理客户询问,然而,现有的PQA系统仅从单一角度产生答案,无法捕捉客户意见的多样性。在本文件中,我们引入了一种新的任务“查询查询”抽样(Query-Focised Summarization),目的是将不同的客户意见归纳为具有代表性的基点(KPs),并将其普遍性量化,以有效回答用户询问。虽然检索启动的一代(RAG)显示了对PQA的希望,但其生成的答案仍然不足以捕捉到所有观点的多样性。为了应对这一挑战,我们扩展RAG的“SUM-RAG”模型(Os ASUM-RAM-RAG)使用了几发式学习来联合培训以KP为导向的检索器和KP摘要生成器,使KP为基础的摘要能够捕捉不同和有代表性的意见。实验结果显示,“SUM/giubcom”在文本质量和定量准确性意见中都取得了优异性的业绩。我们的源代码可在http://rock2/rockmstrokt1331查阅。","An Quang Tang, Xiuzhen Zhang, Minh Ngoc Dinh, Zhuang Li",2025-06-04T14:50:32Z,QQSUM: A Novel Task and Model of Quantitative Query-Focused   Summarization for Review-based Product Question Answering,QQSUM: Eine neuartige Aufgabe und ein Modell Quantitativer Query-Fokussierter Zusammenfassung für reviewbasierte Produktfragebeantwortung,*SUM:基于审查的产品问题回答的定量问答汇总新任务和模式,http://arxiv.org/abs/2506.04020v1
276,"LLMs have been extensively used for the task of automated code generation. In this work, we examine the applicability of LLMs for the related but relatively unexplored task of code-equivalence checking, i.e., given two programs, whether they are functionally equivalent or not. This is an important problem since benchmarking code equivalence can play a critical role in evaluating LLM capabilities for tasks such as code re-writing and code translation. Towards this end, we present CETBench - Code Equivalence with Transformations Benchmark, constructed via a repository of programs, where two programs in the repository may be solving the same or different tasks. Each instance in our dataset is obtained by taking a pair of programs in the repository and applying a random series of pre-defined code transformations, resulting in (non-)equivalent pairs. Our analysis on this dataset reveals a surprising finding that very simple code transformations in the underlying pair of programs can result in a significant drop in performance of SOTA LLMs for the task of code-equivalence checking. To remedy this, we present a simple fine-tuning-based approach to boost LLM performance on the transformed pairs of programs. Our approach for dataset generation is generic, and can be used with repositories with varying program difficulty levels and allows for applying varying numbers as well as kinds of transformations. In our experiments, we perform ablations over the difficulty level of original programs, as well as the kind of transformations used in generating pairs for equivalence checking. Our analysis presents deep insights into the working of LLMs for the task of code-equivalence, and points to the fact that they may still be far from what could be termed as a semantic understanding of the underlying code.","在这项工作中,我们检查了LLMS对相关但相对未探索的代码等效检查任务的适用性,即,给两个程序,不管它们是否在功能上等同。这是一个重要的问题,因为基准代码等同在评估代码重写和代码翻译等任务LLM能力方面可以发挥关键作用。为此,我们展示了CETBench - Coc Equvalence with Transformations Birectors Birectors,它通过程序库的两个程序库可以解决相同或不同的任务。我们的数据集中的每个实例都是通过在存储库中采取一对程序来获取的,并应用一系列预定义的代码转换,从而导致(非)等同对等。我们对这个数据集的分析显示了一个非常简单的发现,基本程序中的代码转换可以导致SOTA LLMS的运行率大幅下降,用于代码等同检查任务。为了补救这一点,我们用一个简单的基于精细度的方法来提升LM的直径直径直径直到数据转换过程的操作水平,而我们用一个用于生成程序的运行过程的精度分析过程的精度水平。","Neeva Oza, Ishaan Govil, Parul Gupta, Dinesh Khandelwal, Dinesh Garg, Parag Singla",2025-06-04T14:47:14Z,CETBench: A Novel Dataset constructed via Transformations over Programs   for Benchmarking LLMs for Code-Equivalence Checking,"CETBench: Ein neuartiger Datensatz, der über Transformationen über Programme zum Benchmarking von LLMs für Code-Equivalenz-Checking erstellt wurde",CETBennch:通过对代码等效检查LMLM基准测试方案进行转换而构建的新数据集,http://arxiv.org/abs/2506.04019v1
277,"As Large Language Model (LLM) agents become more widespread, associated misalignment risks increase. Prior work has examined agents' ability to enact misaligned behaviour (misalignment capability) and their compliance with harmful instructions (misuse propensity). However, the likelihood of agents attempting misaligned behaviours in real-world settings (misalignment propensity) remains poorly understood. We introduce a misalignment propensity benchmark, AgentMisalignment, consisting of a suite of realistic scenarios in which LLM agents have the opportunity to display misaligned behaviour. We organise our evaluations into subcategories of misaligned behaviours, including goal-guarding, resisting shutdown, sandbagging, and power-seeking. We report the performance of frontier models on our benchmark, observing higher misalignment on average when evaluating more capable models. Finally, we systematically vary agent personalities through different system prompts. We find that persona characteristics can dramatically and unpredictably influence misalignment tendencies -- occasionally far more than the choice of model itself -- highlighting the importance of careful system prompt engineering for deployed AI agents. Our work highlights the failure of current alignment methods to generalise to LLM agents, and underscores the need for further propensity evaluations as autonomous systems become more prevalent.","随着大语言模型(LLM)的代理人变得更为广泛,相关的不匹配风险也随之增加。先前的工作已经检查了代理人是否有能力采取不匹配行为(不匹配能力)及其遵守有害指示(滥用倾向)的能力。然而,代理人在现实世界环境中试图采取不匹配行为(不匹配倾向性倾向)的可能性仍然不太为人知。我们引入了一种不匹配性倾向基准,即Agent misalignment,由一系列现实情景构成,使LLM代理人有机会显示不匹配行为。我们组织我们的评价,将其分为不匹配行为的子类,包括目标保护、抵制停业、拖鞋和寻求权力。我们报告边界模型的性能,在评价更能模型时,平均地观察到更大的不匹配性。最后,我们通过不同的系统提示系统地区分代理人的个性。我们发现,人性特征可以极大和不可预测地影响不匹配趋势 -- -- 有时远远超出模型本身的选择 -- -- 突出谨慎系统对部署的AI代理人的迅速工程的重要性。我们的工作强调当前统一方法的失败,需要向普通的自我调整的代理人进一步强调。","Akshat Naik, Patrick Quinn, Guillermo Bosch, Emma Gouné, Francisco Javier Campos Zabala, Jason Ross Brown, Edward James Young",2025-06-04T14:46:47Z,AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in   LLM-Based Agents,AgentMisalignment: Messung der Neigung für fehlgeleitetes Verhalten in LLM-basierten Agenten,Agentmigraignment: 测量基于LLM的制剂中不当行为的属性,http://arxiv.org/abs/2506.04018v1
278,"Process-driven dialogue systems, which operate under strict predefined process constraints, are essential in customer service and equipment maintenance scenarios. Although Large Language Models (LLMs) have shown remarkable progress in dialogue and reasoning, they still struggle to solve these strictly constrained dialogue tasks. To address this challenge, we construct Process Flow Dialogue (PFDial) dataset, which contains 12,705 high-quality Chinese dialogue instructions derived from 440 flowcharts containing 5,055 process nodes. Based on PlantUML specification, each UML flowchart is converted into atomic dialogue units i.e., structured five-tuples. Experimental results demonstrate that a 7B model trained with merely 800 samples, and a 0.5B model trained on total data both can surpass 90% accuracy. Additionally, the 8B model can surpass GPT-4o up to 43.88% with an average of 11.00%. We further evaluate models' performance on challenging backward transitions in process flows and conduct an in-depth analysis of various dataset formats to reveal their impact on model performance in handling decision and sequential branches. The data is released in https://github.com/KongLongGeFDU/PFDial.","流程驱动对话系统在严格预先界定的流程限制下运作,对于客户服务和设备维护方案至关重要,尽管大型语言模型(LLMs)在对话和推理方面已经取得了显著进展,但它们仍难以解决这些严格限制的对话任务。为了应对这一挑战,我们构建了流程对话(PFDial)数据集,其中包含来自440个流程图的12 705个高质量的中国对话指令,其中包含5 055个流程节点。根据PlantUML的规格,每个UML流程图被转换为原子对话单元,即结构化的5个图层。实验结果表明,仅用800个样本培训的7B模型和用全部数据培训的0.5B模型可以超过90%的准确度。此外,8B模型可以超过GPT-488%,平均为11.00 %。我们进一步评价各模型在挑战流程流程向后过渡方面的绩效,并对各种数据设置格式进行深入分析,以显示其对处理决定和相继分支的模型性能产生的影响。数据发布于 https://github.com/KongLongGDUFF.","Ming Zhang, Yuhui Wang, Yujiong Shen, Tingyi Yang, Changhao Jiang, Yilong Wu, Shihan Dou, Qinhao Chen, Zhiheng Xi, Zhihao Zhang, Yi Dong, Zhen Wang, Zhihui Fei, Mingyang Wan, Tao Liang, Guojun Ma, Qi Zhang, Tao Gui, Xuanjing Huang",2025-06-04T14:31:47Z,PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on   UML Flowcharts,PFDial: Eine strukturierte Dialog-Instruktion Feinabstimmungsmethode basierend auf UML-Flowcharts,PFDial:基于UMML流程图的结构性对话指示调整方法,http://arxiv.org/abs/2503.06706v2
279,"Entity relationship classification remains a challenging task in information extraction, especially in scenarios with limited labeled data and complex relational structures. In this study, we conduct a comparative analysis of three distinct AI agent architectures designed to perform relation classification using large language models (LLMs). The agentic architectures explored include (1) reflective self-evaluation, (2) hierarchical task decomposition, and (3) a novel multi-agent dynamic example generation mechanism, each leveraging different modes of reasoning and prompt adaptation. In particular, our dynamic example generation approach introduces real-time cooperative and adversarial prompting. We systematically compare their performance across multiple domains and model backends. Our experiments demonstrate that multi-agent coordination consistently outperforms standard few-shot prompting and approaches the performance of fine-tuned models. These findings offer practical guidance for the design of modular, generalizable LLM-based systems for structured relation extraction. The source codes and dataset are available at https://github.com/maryambrj/ALIEN.git.","在信息提取方面,实体关系分类仍然是一项艰巨的任务,特别是在标签数据和复杂关系结构有限的情况下。在本研究中,我们对三个不同的AI代理结构进行了比较分析,这些结构旨在使用大语言模型进行关系分类。研究的代理结构包括:(1) 反思自我评价,(2) 等级任务分解,(3) 新型的多试剂动态实例生成机制,每个机制都利用不同的推理和迅速适应模式。特别是,我们的动态实例生成方法引入了实时合作和对抗性激励。我们系统地比较了它们在多个领域和模型后端的性能。我们的实验表明,多试剂协调一贯优于标准的 "" 点照提示 "" 和微调模型的性能。这些结果为设计模块化的、通用的LM-系统提供了实用指导,用于结构关系提取。源代码和数据集见https://github.com/maryambrj/ALEN.git。","Maryam Berijanian, Kuldeep Singh, Amin Sehati",2025-06-04T14:21:02Z,Comparative Analysis of AI Agent Architectures for Entity Relationship   Classification,Vergleichende Analyse von KI-Agentenarchitekturen für die Klassifikation von Entity Relationship,AI 实体关系分类代理结构比较分析,http://arxiv.org/abs/2506.02426v2
280,"Human learning and conceptual representation is grounded in sensorimotor experience, in contrast to state-of-the-art foundation models. In this paper, we investigate how well such large-scale models, trained on vast quantities of data, represent the semantic feature norms of concrete object concepts, e.g. a ROSE is red, smells sweet, and is a flower. More specifically, we use probing tasks to test which properties of objects these models are aware of. We evaluate image encoders trained on image data alone, as well as multimodally-trained image encoders and language-only models, on predicting an extended denser version of the classic McRae norms and the newer Binder dataset of attribute ratings. We find that multimodal image encoders slightly outperform language-only approaches, and that image-only encoders perform comparably to the language models, even on non-visual attributes that are classified as ""encyclopedic"" or ""function"". These results offer new insights into what can be learned from pure unimodal learning, and the complementarity of the modalities.","人类的学习和概念代表是基于感官模范经验,与最先进的基础模型形成对照。在本文中,我们调查了这种大型模型,在大量数据方面受过培训,代表了混凝土物体概念的语义特征规范,例如ROSE是红色的,闻起来甜美的,是花朵。更具体地说,我们使用探测任务来测试这些模型所了解的物体的属性。我们评估了仅受过图像数据培训的图像编码器,以及经过多式培训的图像编码器和只使用语言的模型,预测了经典麦克雷规范的扩大密度和新的属性评级Binder数据集。我们发现,多式图像编码器略高于只使用语言的方法,而且只使用图像的编码器与语言模型相对应,即使是在被归类为“百科全书”或“功能”的非视觉属性上也是如此。这些结果使人们对从纯单式学习和模式的互补性中可以学到的内容有了新的洞察力。","Dan Oneata, Desmond Elliott, Stella Frank",2025-06-04T14:18:35Z,Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics   in the Billion Parameter Era,"Sehen, was gut schmeckt: Multimodale Verteilungssemantik in der Milliarden-Parameter-Ära","见什么是 "" 品尝好 "" :在十亿分数世纪中重新审视多式分布式多式分布式语义",http://arxiv.org/abs/2506.03994v1
281,"Social psychologists have shown that Warmth (W) and Competence (C) are the primary dimensions along which we assess other people and groups. These dimensions impact various aspects of our lives from social competence and emotion regulation to success in the work place and how we view the world. More recent work has started to explore how these dimensions develop, why they have developed, and what they constitute. Of particular note, is the finding that warmth has two distinct components: Trust (T) and Sociability (S). In this work, we introduce Words of Warmth, the first large-scale repository of manually derived word--warmth (as well as word--trust and word--sociability) associations for over 26k English words. We show that the associations are highly reliable. We use the lexicons to study the rate at which children acquire WCTS words with age. Finally, we show that the lexicon enables a wide variety of bias and stereotype research through case studies on various target entities. Words of Warmth is freely available at: http://saifmohammad.com/warmth.html","社会心理学家已经表明,温暖(W)和能力(C)是我们评估其他人和群体的主要层面,这些层面影响着我们生活的方方面面,从社会能力和情绪调控到工作场所的成功以及我们如何看待世界。最近的工作已经开始探索这些层面如何发展,为什么发展,以及它们构成什么。特别值得注意的是,发现温暖有两个不同的组成部分:信任(T)和社会可变性(S)。在这项工作中,我们引入了温的言词,这是手动生成的单词温(以及单词信任和单词可感性)协会的第一个大型储存库,有26公里以上的英文词。我们表明,这些协会非常可靠。我们使用词汇来研究儿童与年龄一样获得WCTS词汇的速度。最后,我们通过对不同目标实体的案例研究,表明该词汇能够产生广泛的偏见和定型研究。Warmth的言词可以免费查阅:http://saifmohammad.com/warmth。html",Saif M. Mohammad,2025-06-04T14:18:32Z,Words of Warmth: Trust and Sociability Norms for over 26k English Words,Worte der Wärme: Vertrauen und Geselligkeit Normen für über 26k englische Wörter,温暖的言词:超过26千英文字的信赖和社会规范,http://arxiv.org/abs/2506.03993v1
282,"Typical video modeling methods, such as LLava, represent videos as sequences of visual tokens, which are then processed by the LLM backbone for effective video understanding. However, this approach leads to a massive number of visual tokens, especially for long videos. A practical solution is to first extract relevant visual information from the large visual context before feeding it into the LLM backbone, thereby reducing computational overhead. In this work, we introduce DynTok, a novel \textbf{Dyn}amic video \textbf{Tok}en compression strategy. DynTok adaptively splits visual tokens into groups and merges them within each group, achieving high compression in regions with low information density while preserving essential content. Our method reduces the number of tokens to 44.4% of the original size while maintaining comparable performance. It further benefits from increasing the number of video frames and achieves 65.3% on Video-MME and 72.5% on MLVU. By applying this simple yet effective compression method, we expose the redundancy in video token representations and offer insights for designing more efficient video modeling techniques.","典型的视频建模方法,如Lalava,代表视频,作为视觉标志的序列,然后由LLM主干线处理,以便有效地理解视频。然而,这一方法导致大量视觉标志,特别是长视频。一个实际的解决办法是首先从大视觉背景中提取相关的视觉信息,然后将其输入LLM主干线,从而减少计算间接费用。在这项工作中,我们介绍DynTok,这是一部新颖的\ textbf{Dyn}amic视频压缩战略。DynTok将视觉标志按情分解成一组,并在每个组内将其合并,在信息密度低的区域实现高压缩,同时保留基本内容。我们的方法将标志的数量减少到原始尺寸的44.4%,同时保持类似的性能。它进一步受益于增加视频框架的数量,在视频-MME上达到65.3%,在MLVU上达到72.5%。通过采用这种简单有效的压缩方法,我们暴露了视频标志展示的冗余,并为设计更高效的视频模型技术提供了洞察力。","Hongzhi Zhang, Jingyuan Zhang, Xingguang Ji, Qi Wang, Fuzheng Zhang",2025-06-04T14:17:42Z,DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective   Video Understanding,DynTok: Dynamische Kompression von visuellen Token für effizientes und effektives Videoverständnis,DynTok:为高效率和高效益的视频理解对视觉声调进行动态压缩,http://arxiv.org/abs/2506.03990v1
283,"With the rise of long-context language models (LMs) capable of processing tens of thousands of tokens in a single pass, do multi-stage retrieval-augmented generation (RAG) pipelines still offer measurable benefits over simpler, single-stage approaches? To assess this question, we conduct a controlled evaluation for QA tasks under systematically scaled token budgets, comparing two recent multi-stage pipelines, ReadAgent and RAPTOR, against three baselines, including DOS RAG (Document's Original Structure RAG), a simple retrieve-then-read method that preserves original passage order. Despite its straightforward design, DOS RAG consistently matches or outperforms more intricate methods on multiple long-context QA benchmarks. We recommend establishing DOS RAG as a simple yet strong baseline for future RAG evaluations, pairing it with emerging embedding and language models to assess trade-offs between complexity and effectiveness as model capabilities evolve.","随着长文本语言模型(LMS)的兴起,能够在一个通行证中处理数万个象征性物品的长文本模型的出现,多阶段检索-增强的一代(RAG)管道是否仍然比简单、单阶段的方法提供可衡量的效益?为了评估这一问题,我们在系统规模的象征性预算下对质量评估任务进行有控制的评价,对照三个基线对最近两个多阶段的管道(ReadAgency和RAPtor)进行比较,包括DOSRAG(文件原始结构RAG),这是一个简单的检索-读取方法,保存原始通行顺序。尽管DOSRAG设计直截了当,但它始终在多个长文本QA基准上匹配或超越了更为复杂的方法。我们建议将DOSRAG作为未来质量评估的简单而有力的基准,同时结合新兴的嵌入式和语言模型,以评估随着模型能力的发展,复杂性和有效性之间的权衡。","Alex Laitenberger, Christopher D. Manning, Nelson F. Liu",2025-06-04T14:16:28Z,Stronger Baselines for Retrieval-Augmented Generation with Long-Context   Language Models,Stärkere Baselines für retrieval-angereicherte Generation mit Langkontext-Sprachmodellen,具有长文本语言模型的回收-提款一代的强强基线,http://arxiv.org/abs/2506.03989v1
284,"Reasoning over time and space is essential for understanding our world. However, the abilities of language models in this area are largely unexplored as previous work has tested their abilities for logical reasoning in terms of time and space in isolation or only in simple or artificial environments. In this paper, we present the first evaluation of the ability of language models to jointly reason over time and space. To enable our analysis, we create GeoTemp, a dataset of 320k prompts covering 289 cities in 217 countries and 37 time zones. Using GeoTemp, we evaluate eight open chat models of three different model families for different combinations of temporal and geographic knowledge. We find that most models perform well on reasoning tasks involving only temporal knowledge and that overall performance improves with scale. However, performance remains constrained in tasks that require connecting temporal and geographical information. We do not find clear correlations of performance with specific geographic regions. Instead, we find a significant performance increase for location names with low model perplexity, suggesting their repeated occurrence during model training. We further demonstrate that their performance is heavily influenced by prompt formulation - a direct injection of geographical knowledge leads to performance gains, whereas, surprisingly, techniques like chain-of-thought prompting decrease performance on simpler tasks.","时间和空间因素对于理解我们的世界至关重要。然而,由于先前的工作已经测试了语言模型在时间和空间方面的逻辑推理能力,因此,这一领域中的语言模型的能力基本上没有被探索出来。在本文件中,我们对语言模型在时间和空间方面共同理解的能力进行了第一次评估。为了进行我们的分析,我们创建了一个包含289个城市和37个时区320公里提示的数据集GeoTemp。使用GeoTemp,我们评估了三个不同模式家庭的8个公开聊天模型,以了解时间和地理知识的不同组合。我们发现,大多数模型在仅涉及时间知识的推理任务方面表现良好,总体性能随着规模的提高而得到改善。然而,在需要连接时间和地理信息的任务中,绩效仍然受到限制。我们没有发现与特定地理区域的性能明确关系。相反,我们发现低模型不易理解的地点名称的性能显著提高,表明在模型培训期间反复出现。我们进一步证明,其性能受到迅速拟订的严重影响――直接注入地理知识导致绩效增益,而令人惊讶的是,而像更简单的技术一样,快速递减缩。","Carolin Holtermann, Paul Röttger, Anne Lauscher",2025-06-04T14:14:28Z,Around the World in 24 Hours: Probing LLM Knowledge of Time and Place,Rund um die Welt in 24 Stunden: LLM Wissen über Zeit und Ort,24小时环球 24小时: 探究LLM对时间和地点的知识,http://arxiv.org/abs/2506.03984v1
285,"Turn-taking management is crucial for any social interaction. Still, it is challenging to model human-machine interaction due to the complexity of the social context and its multimodal nature. Unlike conventional systems based on silence duration, previous existing voice activity projection (VAP) models successfully utilized a unified representation of turn-taking behaviors as prediction targets, which improved turn-taking prediction performance. Recently, a multimodal VAP model outperformed the previous state-of-the-art model by a significant margin. In this paper, we propose a multimodal model enhanced with pre-trained audio and face encoders to improve performance by capturing subtle expressions. Our model performed competitively, and in some cases, even better than state-of-the-art models on turn-taking metrics. All the source codes and pretrained models are available at https://github.com/sagatake/VAPwithAudioFaceEncoders.","转换管理对于任何社会互动都至关重要。然而,由于社会背景的复杂性及其多式联运性质,模拟人-机器互动仍具有挑战性。与以沉默持续时间为基础的常规系统不同,以前现有的语音活动预测模型成功地将转手行为统一表述为预测目标,从而改进了转手预测性能。最近,一个多式VAP模型大大优于以往最先进的模型。在本文中,我们建议采用一种多式模型,用经过预先训练的音频和面部编码器来强化,通过捕捉微妙的表达方式改进性能。我们的模型具有竞争力,有时甚至比在转手衡量标准方面最先进的模型更好。所有源代码和预先培训的模型都可以在https://github.com/sagatake/VAPwithAudioFaceencorders上查阅。","Takeshi Saga, Catherine Pelachaud",2025-06-04T14:10:03Z,Voice Activity Projection Model with Multimodal Encoders,Sprachaktivität Projektionsmodell mit multimodalen Encodern,配有多式编码器的语音活动投射模型,http://arxiv.org/abs/2506.03980v1
286,"Model pruning in transformer-based language models, traditionally viewed as a means of achieving computational savings, can enhance the model's reasoning capabilities. In this work, we uncover a surprising phenomenon: the selective pruning of certain attention heads leads to improvements in reasoning performance, particularly on challenging tasks. Motivated by this observation, we propose SPRINT, a novel contrastive learning framework that dynamically selects the optimal head and layer to prune during inference. By aligning question embeddings with head embeddings, SPRINT identifies those pruned-head configurations that result in more accurate reasoning. Extensive experiments demonstrate that our method significantly outperforms traditional best-of-$N$ and random head selection strategies on the MATH500 and GSM8K datasets.","以变压器为基础的语言模型的模型运行,传统上被视为实现计算节约的一种手段,可以增强模型的推理能力。在这项工作中,我们发现了一个令人惊讶的现象:有选择地裁剪某些关注点导致推理业绩的改善,特别是在具有挑战性的任务方面。我们为此提出SPRINT,这是一个新颖的对比式学习框架,在推理过程中动态地选择最佳头部和层进行精细处理。通过将问题嵌入与头嵌入相匹配,SPRINT确定了那些导致更精确推理的剪切头结构。广泛的实验表明,我们的方法大大超过了传统的最优纳元和随机头部选择MATH500和GSM8K数据集的战略。","Hieu Trung Nguyen, Bao Nguyen, Viet Anh Nguyen",2025-06-04T14:08:44Z,Structured Pruning for Diverse Best-of-N Reasoning Optimization,Strukturierte Prunings für die unterschiedliche Best-of-N-Reasoning-Optimierung,"结构审慎,实现最佳理由的多样化最佳最佳理由的优化",http://arxiv.org/abs/2506.03978v1
287,"The pursuit of diverse, complex, and large-scale instruction data is crucial for automatically aligning large language models (LLMs). While there are methods capable of generating synthetic instructions at scale, they either suffer from limited grounding sources, leading to a narrow distribution, or rely on trivial extensions that fail to produce meaningful trajectories in terms of complexity. In contrast, instructions that benefit efficient alignment are typically crafted with cognitive insights and grounded in real-world use cases. In this paper, we synthesize such instructions using attributed grounding, which involves 1) a top-down attribution process that grounds a selective set of real instructions to situated users, and 2) a bottom-up synthesis process that leverages web documents to first generate a situation, then a meaningful instruction. This framework allows us to harvest diverse and complex instructions at scale, utilizing the vast range of web documents. Specifically, we construct a dataset of 1 million instructions, called SynthQuestions, and demonstrate that models trained on it achieve leading performance on several common benchmarks, with improvements that continually scale with more web corpora. Data, models and codes will be available at https://github.com/Ignoramus0817/SynthQuestions.","追求多样化、复杂和大规模教学数据对于自动调整大型语言模型(LLMS)至关重要。虽然有能够产生大规模合成指令的方法,但它们要么受到有限的基底来源的制约,导致分布狭窄,要么依赖在复杂程度方面无法产生有意义的轨迹的微小扩展。相反,有利于高效匹配的指示通常是以认知洞察力和现实世界使用案例为基础设计的。在本文件中,我们使用归属依据对此类指令进行了综合,其中包括:(1)自上而下的归属进程,为定位用户提供一套选择性的真实指令;(2)自下而上的综合进程,利用网络文件首先产生一种局面,然后是有意义的指导。这个框架使我们能够利用广泛的网络文件范围,在规模上收集多样和复杂的指令。具体地说,我们构建了100万项指令的数据集,称为SynthQuestions,并展示了经过培训的模型在几个共同基准上取得领先业绩,同时不断与更多的网络公司进行改进。数据、模型和代码将在https://github.com/Ignonomamus0817/SynQisques) 上提供。","Chiwei Zhu, Benfeng Xu, Xiaorui Wang, Zhendong Mao",2025-06-04T14:00:47Z,From Real to Synthetic: Synthesizing Millions of Diversified and   Complicated User Instructions with Attributed Grounding,Von Real zu Synthetisch: Die Synthese von Millionen diversifizierter und komplizierter Benutzeranweisungen mit zugeschriebener Erdung,从真实到合成:利用特定基底合成数以百万计的多样化和复杂用户指令,http://arxiv.org/abs/2506.03968v1
288,"Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided refinements simultaneously while maintaining exploration. Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that Critique-GRPO consistently outperforms supervised learning-based and RL-based fine-tuning approaches across eight challenging mathematical, STEM, and general reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%, respectively. Notably, Critique-GRPO surpasses a strong baseline that incorporates expert demonstrations within online RL. Further analysis reveals two critical insights about policy exploration: (1) higher entropy does not always guarantee efficient learning from exploration, and (2) longer responses do not necessarily lead to more effective exploration.","在强化学习(RL)方面最近取得的进展,包括数量反馈,例如数量奖励,大大增强了大型语言模型(LLMs)的复杂推理能力。尽管取得了这一成功,但我们还是确定了RL公司仅靠数字反馈而遇到的三个关键挑战:业绩高地、自我反省效力有限和持续失败。然后我们证明,即使展示了业绩高地,RL调新模式也能通过利用以批评形式提供的自然语言反馈,对长期失败的问题产生正确的改进。我们基于这一洞察力,建议Critique-GROPO,这是一个将自然语言和数字反馈结合起来,以有效政策优化的在线RLLL框架。Critique-GRO能够既从初步反应和批评性指导性调整中学习,又同时保持探索探索的完善。我们用Quen2.5-7B基础和Qwen3-8B基准进行的广泛实验表明,Crique-GRO始终在8项具有挑战性的数学、STEM和一般推理学任务中采用监督性调整方法,而不是通过C_1平均分分数,而以大约4.5%和5级的在线分析。","Xiaoying Zhang, Hao Sun, Yipeng Zhang, Kaituo Feng, Chaochao Lu, Chao Yang, Helen Meng",2025-06-04T13:45:47Z,Critique-GRPO: Advancing LLM Reasoning with Natural Language and   Numerical Feedback,Kritik-GRPO: LLM-Vernunft mit natürlicher Sprache und numerischem Feedback verbessern,Critique-GROPO: 提高以自然语言和数字反馈为依据的LLM,http://arxiv.org/abs/2506.03106v2
289,"Large language models (LLMs) have enabled agents to perform complex reasoning and decision-making through free-form language interactions. However, in open-ended language action environments (e.g., negotiation or question-asking games), the action space can be formulated as a joint distribution over tokens, resulting in an exponentially large action space. Sampling actions in such a space can lead to extreme reward sparsity, which brings large reward variance, hindering effective reinforcement learning (RL). To address this, we propose ARIA, a method that Aggregates Rewards in Intention space to enable efficient and effective language Agents training. ARIA aims to project natural language actions from the high-dimensional joint token distribution space into a low-dimensional intention space, where semantically similar actions are clustered and assigned shared rewards. This intention-aware reward aggregation reduces reward variance by densifying reward signals, fostering better policy optimization. Extensive experiments demonstrate that ARIA not only significantly reduces policy gradient variance, but also delivers substantial performance gains of an average of 9.95% across four downstream tasks, consistently outperforming offline and online RL baselines.","大型语言模型(LLMs)使代理商能够通过自由形式的语言互动进行复杂的推理和决策;然而,在开放的语言行动环境(例如谈判或提问游戏)中,可以将行动空间设计成对象征物的联合分配,从而形成一个巨大的行动空间。在这样的空间中,抽样行动可能导致极端的奖励松散,从而带来巨大的奖励差异,阻碍有效的强化学习(RL)。为了解决这个问题,我们提议ARIA(ARIA)。ARIA(A)是一种在意图空间中整合奖励,以便高效率和有成效的语言代理培训。ARIA(A)旨在将高维联合象征性分配空间的自然语言行动预测成一个低维意图空间,在这个空间中,将相似的语义行动集中并分配到一个共同的奖励空间。这种意图的奖励集合通过放大奖励信号,促进更好的政策优化,可以减少奖励差异。广泛的实验表明,ARIA不仅显著地减少政策梯度差异,而且还在四个下游任务中带来9.95%的平均绩效收益,持续地超出联机和在线的基线。","Ruihan Yang, Yikai Zhang, Aili Chen, Xintao Wang, Siyu Yuan, Jiangjie Chen, Deqing Yang, Yanghua Xiao",2025-06-04T13:39:54Z,ARIA: Training Language Agents with Intention-Driven Reward Aggregation,ARIA: Schulung von Sprachagenten mit intentionsgetriebener Belohnungsaggregation,"ARIA:培训有 "" 意向-驱动奖励汇总 "" 的语文代理培训",http://arxiv.org/abs/2506.00539v2
290,"LLMs have shown impressive progress in natural language processing. However, they still face significant challenges in TableQA, where real-world complexities such as diverse table structures, multilingual data, and domain-specific reasoning are crucial. Existing TableQA benchmarks are often limited by their focus on simple flat tables and suffer from data leakage. Furthermore, most benchmarks are monolingual and fail to capture the cross-lingual and cross-domain variability in practical applications. To address these limitations, we introduce TableEval, a new benchmark designed to evaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes tables with various structures (such as concise, hierarchical, and nested tables) collected from four domains (including government, finance, academia, and industry reports). Besides, TableEval features cross-lingual scenarios with tables in Simplified Chinese, Traditional Chinese, and English. To minimize the risk of data leakage, we collect all data from recent real-world documents. Considering that existing TableQA metrics fail to capture semantic accuracy, we further propose SEAT, a new evaluation framework that assesses the alignment between model responses and reference answers at the sub-question level. Experimental results have shown that SEAT achieves high agreement with human judgment. Extensive experiments on TableEval reveal critical gaps in the ability of state-of-the-art LLMs to handle these complex, real-world TableQA tasks, offering insights for future improvements. We make our dataset available here: https://github.com/wenge-research/TableEval.","然而,在表QA中,它们仍然面临着巨大的挑战,在表QA中,现实世界的复杂情况,如不同的表格结构、多语言数据和特定领域的推理等,都是至关重要的;现有的表QA基准往往因侧重于简单的平板表格而受到限制,而且数据渗漏;此外,大多数基准都是单语的,未能在实际应用中捕捉到跨语言和跨主题的变异性;为了解决这些局限性,我们引入了表Eval,这是一个新的基准,旨在根据现实的表QA任务评估LM的准确性。具体地说,表Eval包含从四个领域(包括政府、金融、学术界和行业报告)收集的各种结构(如简洁、分级和嵌入式表格)的表格。此外,表Eval的特征是跨语言的情景,以简化的中文、中文本和英文为表格。为了尽量减少数据渗漏风险,我们从最近的实际世界文件收集了所有数据。考虑到现有的表QA指标未能反映语义准确性,我们进一步提议SEAT,一个新的评价框架,用以评估在次问题级别一级(包括政府、金融、学术界、学术界和学术界)不同层次分析的模型分析中,我们目前的数据分析中的数据分析能力显示。","Junnan Zhu, Jingyi Wang, Bohan Yu, Xiaoyu Wu, Junbo Li, Lei Wang, Nan Xu",2025-06-04T13:39:01Z,"TableEval: A Real-World Benchmark for Complex, Multilingual, and   Multi-Structured Table Question Answering","TableEval: Ein Real-World Benchmark für komplexe, mehrsprachige und multistrukturierte Tabellenfrageantworten",表Eval:复杂、多语种和多结构表问题答案的现实世界基准,http://arxiv.org/abs/2506.03949v1
291,"While Large Language Models (LLMs) demonstrate impressive capabilities, they still struggle with generating factually incorrect content (i.e., hallucinations). A promising approach to mitigate this issue is enabling models to express uncertainty when unsure. Previous research on uncertainty modeling has primarily focused on short-form QA, but realworld applications often require much longer responses. In this work, we introduce the task of Long-form Generation with Uncertainty(LoGU). We identify two key challenges: Uncertainty Suppression, where models hesitate to express uncertainty, and Uncertainty Misalignment, where models convey uncertainty inaccurately. To tackle these challenges, we propose a refinement-based data collection framework and a two-stage training pipeline. Our framework adopts a divide-and-conquer strategy, refining uncertainty based on atomic claims. The collected data are then used in training through supervised fine-tuning (SFT) and direct preference optimization (DPO) to enhance uncertainty expression. Extensive experiments on three long-form instruction following datasets show that our method significantly improves accuracy, reduces hallucinations, and maintains the comprehensiveness of responses.","尽管大语言模型(LLMS)表现出了令人印象深刻的能力,但它们仍然在努力生成不真实的内容(即幻觉),因此,缓解这一问题的一个有希望的方法是使模型能够在不确定时表达不确定性。以前关于不确定性模型的研究主要侧重于短式QA,但现实世界应用往往需要更长的响应。在这项工作中,我们引入了具有不确定性的长式一代(LOGU)的任务。我们确定了两大挑战:不确定性的抑制,因为模型在表达不确定性方面犹豫不决;不确定性的不确定性不匹配,而模型传递不确定性的不准确性。为了应对这些挑战,我们提出了一个基于完善的数据收集框架和一个两阶段培训管道。我们的框架采用了一个分而治战略,根据原子索赔来改进不确定性。随后,通过监管的微调(SFT)和直接优惠优化(DPO),将所收集的数据用于培训,以加强不确定性的表达。我们在数据集之后对三种长式指示进行的广泛实验表明,我们的方法大大改进了准确性,减少了幻觉,并保持了应对措施的全面性。","Ruihan Yang, Caiqi Zhang, Zhisong Zhang, Xinting Huang, Sen Yang, Nigel Collier, Dong Yu, Deqing Yang",2025-06-04T13:35:31Z,LoGU: Long-form Generation with Uncertainty Expressions,LoGU: Langform-Generation mit unsicheren Ausdrücken,LOGU: 具有不确定性表达式的长成代,http://arxiv.org/abs/2410.14309v4
292,"During a conversation, there can come certain moments where its outcome hangs in the balance. In these pivotal moments, how one responds can put the conversation on substantially different trajectories leading to significantly different outcomes. Systems that can detect when such moments arise could assist conversationalists in domains with highly consequential outcomes, such as mental health crisis counseling.   In this work, we introduce an unsupervised computational method for detecting such pivotal moments as they happen, in an online fashion. Our approach relies on the intuition that a moment is pivotal if our expectation of the outcome varies widely depending on what might be said next. By applying our method to crisis counseling conversations, we first validate it by showing that it aligns with human perception -- counselors take significantly longer to respond during moments detected by our method -- and with the eventual conversational trajectory -- which is more likely to change course at these times. We then use our framework to explore the relation of the counselor's response during pivotal moments with the eventual outcome of the session.","在对话中, 一定时刻, 其结果会悬在平衡中。 在这些关键时刻, 如何回应可以将对话置于截然不同的轨迹上, 导致显著不同的结果。 当这种时刻出现时, 能够检测到的系统可以帮助对话者在心理健康危机咨询等具有高度影响的结果的领域。 在这项工作中, 我们引入了一种不受监督的计算方法, 以在线方式检测发生时的这种关键时刻。 我们的方法取决于直觉, 如果对结果的期待因接下来的描述而有很大差异, 一个时刻是关键的时刻。 通过应用我们的方法来进行危机咨询对话, 我们首先验证它, 我们通过显示它与人类的感知相吻合 -- 顾问们在通过我们的方法所发现的时刻 -- 和最终的对话轨迹 -- 反应的时间要长得多 -- 这在目前更有可能改变方向。 我们然后使用我们的框架来探索顾问在与会议最终结果的关键时刻的反应关系。","Vivian Nguyen, Lillian Lee, Cristian Danescu-Niculescu-Mizil",2025-06-04T13:31:58Z,Hanging in the Balance: Pivotal Moments in Crisis Counseling   Conversations,Im Gleichgewicht hängen: Pivotale Momente bei Krisenberatungen,坚持平衡:危机咨询对话的枢纽,http://arxiv.org/abs/2506.03941v1
293,"Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external knowledge integration capabilities by explicitly modeling knowledge relationships, thereby improving the factual accuracy and generation quality of Large Language Models (LLMs) in specialized domains. However, existing methods suffer from two inherent limitations: 1) Inefficient Information Aggregation: They rely on a single agent and fixed iterative patterns, making it difficult to adaptively capture multi-level textual, structural, and degree information within graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning schemes, which cannot dynamically adjust reasoning depth nor achieve precise semantic correction. To overcome these limitations, we propose Graph Counselor, an GraphRAG method based on multi-agent collaboration. This method uses the Adaptive Graph Information Extraction Module (AGIEM), where Planning, Thought, and Execution Agents work together to precisely model complex graph structures and dynamically adjust information extraction strategies, addressing the challenges of multi-level dependency modeling and adaptive reasoning depth. Additionally, the Self-Reflection with Multiple Perspectives (SR) module improves the accuracy and semantic consistency of reasoning results through self-reflection and backward reasoning mechanisms. Experiments demonstrate that Graph Counselor outperforms existing methods in multiple graph reasoning tasks, exhibiting higher reasoning accuracy and generalization ability. Our code is available at https://github.com/gjq100/Graph-Counselor.git.","(graphRAG) 通过明确建立知识关系模型,有效增强外部知识整合能力,从而提高专业领域大语言模型(LLMs)的实际准确性和生成质量,从而提高专业领域大语言模型(LLMs)的实际准确性和生成质量。然而,现有方法存在两个固有的局限性:(1) 信息汇总效率低下:它们依赖单一代理和固定迭接模式:它们依赖一个单一代理和固定迭接模式,因此难以在图形数据中适应性地获取多层次的文本、结构和度信息。(2) 定调理由机制:它们采用预先设定的推理计划,无法动态地调整推理深度,也无法实现精确的语义校正校正。为了克服这些局限性,我们建议采用Greg Develop Development Development Development 模块(AGIGIEM),在其中规划、思想和执行代理机构一起工作,精确地模拟复杂的图形结构,并动态地调整信息提取战略,应对多层次依赖性模型和适应性推理深度的挑战。此外,具有多视角的自我再思考模式(SR)模块和语义一致性,我们现有的图表推理学/后推理学/后推理学/后推理分析系统,我们现有的现行推导/后推导/后推导/后推导法是现有的推导式。","Junqi Gao, Xiang Zou, YIng Ai, Dong Li, Yichen Niu, Biqing Qi, Jianxing Liu",2025-06-04T13:31:21Z,Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to   Enhance LLM Reasoning,Graph Counselor: Adaptive Graph Exploration über Multi-Agent Synergy zur Verbesserung der LLM-Reasoning,"图解参赞:通过多机构协同效应进行适应性图示探索,以加强LLM 合理性",http://arxiv.org/abs/2506.03939v1
294,"Children are one of the most under-represented groups in speech technologies, as well as one of the most vulnerable in terms of privacy. Despite this, anonymization techniques targeting this population have received little attention. In this study, we seek to bridge this gap, and establish a baseline for the use of voice anonymization techniques designed for adult speech when applied to children's voices. Such an evaluation is essential, as children's speech presents a distinct set of challenges when compared to that of adults. This study comprises three children's datasets, six anonymization methods, and objective and subjective utility metrics for evaluation. Our results show that existing systems for adults are still able to protect children's voice privacy, but suffer from much higher utility degradation. In addition, our subjective study displays the challenges of automatic evaluation methods for speech quality in children's speech, highlighting the need for further research.","儿童是语言技术中代表性最不足的群体之一,也是在隐私方面最易受伤害的群体之一。尽管如此,针对这一人群的匿名技术很少受到重视。在本研究中,我们寻求弥合这一差距,并为在应用儿童声音时使用成人语言的匿名技术建立基准。这种评估至关重要,因为儿童语言与成年人相比,提出了一系列截然不同的挑战。本研究包括三个儿童数据集、六个匿名方法以及客观和主观的评价实用指标。我们的结果显示,现有的成人系统仍然能够保护儿童的语音隐私,但受到更高的实用性退化。此外,我们的主观研究展示了对儿童语言语言质量的自动评价方法的挑战,强调了进一步研究的必要性。","Ajinkya Kulkarni, Francisco Teixeira, Enno Hermann, Thomas Rolland, Isabel Trancoso, Mathew Magimai Doss",2025-06-04T13:30:41Z,Children's Voice Privacy: First Steps And Emerging Challenges,Datenschutz für Kinder: Erste Schritte und neue Herausforderungen,儿童之声隐私:第一步和新挑战,http://arxiv.org/abs/2506.00100v2
295,"Sparse autoencoders (SAEs) are a popular technique for interpreting language model activations, and there is extensive recent work on improving SAE effectiveness. However, most prior work evaluates progress using unsupervised proxy metrics with unclear practical relevance. We introduce SAEBench, a comprehensive evaluation suite that measures SAE performance across eight diverse metrics, spanning interpretability, feature disentanglement and practical applications like unlearning. To enable systematic comparison, we open-source a suite of over 200 SAEs across eight recently proposed SAE architectures and training algorithms. Our evaluation reveals that gains on proxy metrics do not reliably translate to better practical performance. For instance, while Matryoshka SAEs slightly underperform on existing proxy metrics, they substantially outperform other architectures on feature disentanglement metrics; moreover, this advantage grows with SAE scale. By providing a standardized framework for measuring progress in SAE development, SAEBench enables researchers to study scaling trends and make nuanced comparisons between different SAE architectures and training methodologies. Our interactive interface enables researchers to flexibly visualize relationships between metrics across hundreds of open-source SAEs at: www.neuronpedia.org/sae-bench","简单自动读取器(SAE)是解释语言模型激活的流行技术,最近还广泛开展了提高SAE有效性的工作。然而,大多数先前的工作都利用未经监督的代用指标评估进展,但实际意义并不明确。我们引入了SAEEENKE(SAEBEENCH)综合评价套件,该套套套件测量SAE在八种不同指标中的性能,包括可解释性、特征分解和不学习等实际应用;为了进行系统比较,我们开发了一套套件,在最近提出的八种SAE架构和培训算法中,共有200多个自有性能。我们的评估显示,代用指标的收益并没有可靠地转化为更好的实际业绩。例如,Matryoshka SAEE在现有的代用指标上稍有缺陷,它们大大优于其他结构;此外,这种优势随着SAEE规模的扩大而增长。通过提供一个衡量SAEE开发进展的标准化框架,SAEBENENENS使研究人员能够研究趋势,并对不同的SAE架构和培训方法进行细致的比较。我们的交互式界面使研究人员得以在www-nistrual-rusormasss","Adam Karvonen, Can Rager, Johnny Lin, Curt Tigges, Joseph Bloom, David Chanin, Yeu-Tong Lau, Eoin Farrell, Callum McDougall, Kola Ayonrinde, Demian Till, Matthew Wearden, Arthur Conmy, Samuel Marks, Neel Nanda",2025-06-04T13:27:28Z,SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language   Model Interpretability,SAEBench: Ein umfassender Benchmark für Sparse Autoencoder in der Sprachmodellinterpretierbarkeit,SAEBENC: 语文示范解释性简小自动计算器综合基准,http://arxiv.org/abs/2503.09532v4
296,"Large language models (LLMs) often struggle with visualization tasks like plotting diagrams, charts, where success depends on both code correctness and visual semantics. Existing instruction-tuning datasets lack execution-grounded supervision and offer limited support for iterative code correction, resulting in fragile and unreliable plot generation. We present VisCode-200K, a large-scale instruction tuning dataset for Python-based visualization and self-correction. It contains over 200K examples from two sources: (1) validated plotting code from open-source repositories, paired with natural language instructions and rendered plots; and (2) 45K multi-turn correction dialogues from Code-Feedback, enabling models to revise faulty code using runtime feedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create VisCoder, and evaluate it on PandasPlotBench. VisCoder significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4o-mini. We further adopt a self-debug evaluation protocol to assess iterative repair, demonstrating the benefits of feedback-driven learning for executable, visually accurate code generation.","大型语言模型(LLMS)经常与图示图、图表等可视化任务挣扎,成功与否取决于代码正确性和视觉语义。现有的指令调数据集缺乏执行现场的监督,对迭代代码校正的支持有限,导致生成脆弱和不可靠的地块。我们在VisCode-200K上展示了用于 Python 可视化和自我校正的大型指令调制数据集VisCode-200K。它包含来自两个来源的200K以上实例:(1) 验证过的开放源库的绘图代码,配以自然语言指令和设定图;(2) 45K 多方向校正对话,使代码后退的模型能够利用运行时间反馈修改错误代码。我们在VisCode-200K上展示了Viscoder-200K的微调调调调调调调调调调数据,并在PandasPlotBench. VisCoder 上评价了强的开放源基线,并接近了GPT-4-O-Mini等专有模型的性模型的性能。我们进一步采用了自调调式评价程序来调式评价程序,用于评估生成的模拟校正变码的模拟的模拟。我们进一步采用自调制评估程序,以评价程序,以评估生成的模拟的模拟的模拟的模拟的生成的模拟的模拟的模拟。","Yuansheng Ni, Ping Nie, Kai Zou, Xiang Yue, Wenhu Chen",2025-06-04T13:24:44Z,VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code   Generation,VisCoder: Feinjustierende LLMs für die Generierung von ausführbaren Python-Visualisierungscodes,VisCoder: 用于可执行的 Python 可视化代码生成的精准导出LMsLMs,http://arxiv.org/abs/2506.03930v1
297,"As Large Language Models are increasingly deployed in high-stakes domains, their ability to detect false assumptions and reason critically is crucial for ensuring reliable outputs. False-premise questions (FPQs) serve as an important evaluation method by exposing cases where flawed assumptions lead to incorrect responses. While existing benchmarks focus on single-hop FPQs, real-world reasoning often requires multi-hop inference, where models must verify consistency across multiple reasoning steps rather than relying on surface-level cues. To address this gap, we introduce MultiHoax, a benchmark for evaluating LLMs' ability to handle false premises in complex, multi-step reasoning tasks. Our dataset spans seven countries and ten diverse knowledge categories, using Wikipedia as the primary knowledge source to enable factual reasoning across regions. Experiments reveal that state-of-the-art LLMs struggle to detect false premises across different countries, knowledge categories, and multi-hop reasoning types, highlighting the need for improved false premise detection and more robust multi-hop reasoning capabilities in LLMs.","由于大语言模型越来越多地部署在高接触领域,因此发现虚假假设和理性的能力对于确保可靠的产出至关重要。虚构问题(FPQ)是一个重要的评价方法,通过揭露错误假设导致不正确答复的案例。现有基准侧重于单点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点点","Mohammadamin Shafiei, Hamidreza Saffari, Nafise Sadat Moosavi",2025-06-04T13:23:25Z,MultiHoax: A Dataset of Multi-hop False-Premise Questions,MultiHoax: Ein Datensatz von Multi-Hop-Falsch-Premise-Fragen,多重主题:多呼、错误预设的多呼、虚伪问题的数据集,http://arxiv.org/abs/2506.00264v2
298,"Enhancing the mathematical reasoning capabilities of LLMs has garnered significant attention in both the mathematical and computer science communities. Recent works have made substantial progress in both Natural Language (NL) reasoning and Formal Language (FL) reasoning by leveraging the potential of pure Reinforcement Learning (RL) methods on base models. However, RL approaches struggle to impart new capabilities not presented in the base model, highlighting the need to integrate more knowledge like FL into NL math reasoning effectively. Yet, this integration is challenging due to inherent disparities in problem structure and reasoning format between NL and FL. To address these challenges, we introduce **NL-FL HybridReasoning**, an end-to-end framework designed to incorporate the FL expert into NL math problem-solving. To bridge the NL and FL input format gap, we propose the *NL-FL Problem Alignment* method, which reformulates the Question-Answering (QA) problems in NL as existence theorems in FL. Subsequently, the *Mixed Problem Input* technique we provide enables the FL reasoner to handle both QA and existence problems concurrently. Lastly, we mitigate the NL and FL output format gap in reasoning through an LLM-based *Answer Extraction* mechanism. Comprehensive experiments demonstrate that the **HybridReasoning** framework achieves **89.80%** and **84.34%** accuracy rates on the MATH-500 and the AMC benchmarks, surpassing the NL baseline by 4.60% and 4.82%, respectively. Notably, some problems resolved by our framework remain unsolved by the NL baseline model even under a larger number of trials.","提高LLM的数学推理能力在数学界和计算机科学界都得到了极大的关注,最近的工作在自然语言(NL)推理和正式语言(FL)推理方面取得了长足进展,利用了基础模型中纯强化学习(RL)方法的潜力。然而,RL努力传授基础模型中未显示的新能力,强调需要有效地将FL等知识纳入NL数学推理。然而,由于NL和FL80基准之间在问题结构和推理格式上的内在差异,这种整合具有挑战性。为了应对这些挑战,我们引入了**NL-FL混合校准****,这是旨在将FL专家纳入NL数学解决问题解决的端对端框架。为了弥合NL和FL格式的差距,我们提出了将NL-L问题重新纳入NL-L校准(QA)的问题作为NL. 884的标语标本。我们提供的“ML” 基线技术使得FL理性测试(NL) 和“OL”框架下的某些数字得以分别处理和“LL”推算。","Ruida Wang, Yuxin Li, Yi R. Fung, Tong Zhang",2025-06-04T13:18:59Z,Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's   Math Capability,Let's Reason Formally: Natürlich-Formal Hybrid Reasoning verbessert LLMs Math Capability,让我们正式解释一下: 自然-正规混合理由提高LLM的数学能力,http://arxiv.org/abs/2505.23703v2
299,"In electronic design, engineers often manually search through extensive documents to retrieve component parameters required for constructing SPICE models, a process that is both labor-intensive and time-consuming. To address this challenge, we present an automated framework called D2S-FLOW that leverages large language models (LLMs) to extract electrical parameters from datasheets and generate SPICE models with high precision and efficiency, significantly reducing the need for manual intervention. Unlike traditional RAG systems, D2S-FLOW employs a workflow to enhance precision in handling unstructured documents and inconsistent naming conventions through three innovative mechanisms: Attention-Guided Document Focusing (AGDF), Hierarchical Document-Enhanced Retrieval (HDER), and Heterogeneous Named Entity Normalization (HNEN). AGDF narrows retrieval to user-selected documents, HDER utilizes document structure for precise parameter localization, and HNEN standardizes terminology via semantic inference. Experimental results demonstrate that the framework achieves an Exact Match (EM) of 0.86, an F1 score of 0.92, and an Exact Correctness (EC) of 0.96, outperforming the strongest baseline by 19.4%, 5.7%, and 13.1%, respectively. Additionally, it reduces API token consumption by 38% and minimizes the irrelevant information ratio to 4%, showcasing substantial improvements in resource efficiency. This research provides an effective automated solution for circuit design.","在电子设计中,工程师往往通过广泛的文件手动搜索,以检索建造SPICE模型所需的组成部分参数,这是一个劳动密集型和耗时的过程。为了应对这一挑战,我们提出了一个名为D2S-Flow的自动框架,它利用大型语言模型(LLMS)从数据表中提取电子参数,并以高度精确和效率生成SPICE模型,大大减少了人工干预的需要。与传统的RAG系统不同,D2S-Flow使用工作流程,通过三个创新机制,提高处理非结构化文件和不一致命名公约所需的组成部分参数的精确性:关注指导文件聚焦(AGDF)、高层次文件强化检索val(HDER)和异源性文件实体正常化(HNEN)。ADF对用户选择的文件进行检索,HDER利用文件结构精确参数本地化,HNEN通过语义推断将术语标准化。实验结果表明,该框架实现了0.86的Exact Match(EM),F1分最低0.92分,以及F1-强化文件强化文件检索率(EC),38Exact Recent Realationalationality),分别通过0.96(EC)进行最精确的基线和3.1%的资源效率,通过ADI),将Axelevorlation 4,将19% 的基线,将最强的基线和大量化分析(EC) 降低为19%的资源效率。","Hong Cai Chen, Yi Pin Xu, Yang Zhang",2025-06-04T13:17:52Z,D2S-FLOW: Automated Parameter Extraction from Datasheets for SPICE Model   Generation Using Large Language Models,D2S-FLOW: Automatische Parameterextraktion aus Datenblättern für die SPICE-Modellgenerierung mit großen Sprachmodellen,D2S-FLOW:利用大语言模型为SPICE模型生成从数据表中自动提取参数,http://arxiv.org/abs/2502.16540v2
300,"Large language models (LLMs) are known to be sensitive to input phrasing, but the mechanisms by which semantic cues shape reasoning remain poorly understood. We investigate this phenomenon in the context of comparative math problems with objective ground truth, revealing a consistent and directional framing bias: logically equivalent questions containing the words ``more'', ``less'', or ``equal'' systematically steer predictions in the direction of the framing term. To study this effect, we introduce MathComp, a controlled benchmark of 300 comparison scenarios, each evaluated under 14 prompt variants across three LLM families. We find that model errors frequently reflect linguistic steering, systematic shifts toward the comparative term present in the prompt. Chain-of-thought prompting reduces these biases, but its effectiveness varies: free-form reasoning is more robust, while structured formats may preserve or reintroduce directional drift. Finally, we show that including demographic identity terms (e.g., ``a woman'', ``a Black person'') in input scenarios amplifies directional drift, despite identical underlying quantities, highlighting the interplay between semantic framing and social referents. These findings expose critical blind spots in standard evaluation and motivate framing-aware benchmarks for diagnosing reasoning robustness and fairness in LLMs.",,"Mohammadamin Shafiei, Hamidreza Saffari, Nafise Sadat Moosavi",2025-06-04T13:15:01Z,More or Less Wrong: A Benchmark for Directional Bias in LLM Comparative   Reasoning,Mehr oder weniger falsch: Ein Benchmark für Directional Bias in LLM Comparative Reasoning,或多或少错:LLM 比较理由中方向性偏见的基准,http://arxiv.org/abs/2506.03923v1
301,"Multimodal Large Language Models (MLLMs) have demonstrated significant potential to advance a broad range of domains. However, current benchmarks for evaluating MLLMs primarily emphasize general knowledge and vertical step-by-step reasoning typical of STEM disciplines, while overlooking the distinct needs and potential of the Humanities and Social Sciences (HSS). Tasks in the HSS domain require more horizontal, interdisciplinary thinking and a deep integration of knowledge across related fields, which presents unique challenges for MLLMs, particularly in linking abstract concepts with corresponding visual representations. Addressing this gap, we present HSSBench, a dedicated benchmark designed to assess the capabilities of MLLMs on HSS tasks in multiple languages, including the six official languages of the United Nations. We also introduce a novel data generation pipeline tailored for HSS scenarios, in which multiple domain experts and automated agents collaborate to generate and iteratively refine each sample. HSSBench contains over 13,000 meticulously designed samples, covering six key categories. We benchmark more than 20 mainstream MLLMs on HSSBench and demonstrate that it poses significant challenges even for state-of-the-art models. We hope that this benchmark will inspire further research into enhancing the cross-disciplinary reasoning abilities of MLLMs, especially their capacity to internalize and connect knowledge across fields.",,"Zhaolu Kang, Junhao Gong, Jiaxu Yan, Wanke Xia, Yian Wang, Ziwen Wang, Huaxuan Ding, Zhuo Cheng, Wenhao Cao, Zhiyuan Feng, Siqi He, Shannan Yan, Junzhe Chen, Xiaomin He, Chaoya Jiang, Wei Ye, Kaidong Yu, Xuelong Li",2025-06-04T13:14:13Z,HSSBench: Benchmarking Humanities and Social Sciences Ability for   Multimodal Large Language Models,HSSBench: Benchmarking Geistes- und Sozialwissenschaften Fähigkeit für multimodale große Sprachmodelle,HSSBENCH:多式大语言模式人文和社会科学能力基准,http://arxiv.org/abs/2506.03922v1
302,"Hate speech detection is key to online content moderation, but current models struggle to generalise beyond their training data. This has been linked to dataset biases and the use of sentence-level labels, which fail to teach models the underlying structure of hate speech. In this work, we show that even when models are trained with more fine-grained, span-level annotations (e.g., ""artists"" is labeled as target and ""are parasites"" as dehumanising comparison), they struggle to disentangle the meaning of these labels from the surrounding context. As a result, combinations of expressions that deviate from those seen during training remain particularly difficult for models to detect. We investigate whether training on a dataset where expressions occur with equal frequency across all contexts can improve generalisation. To this end, we create U-PLEAD, a dataset of ~364,000 synthetic posts, along with a novel compositional generalisation benchmark of ~8,000 manually validated posts. Training on a combination of U-PLEAD and real data improves compositional generalisation while achieving state-of-the-art performance on the human-sourced PLEAD.",,"Agostina Calabrese, Tom Sherborne, Björn Ross, Mirella Lapata",2025-06-04T13:07:36Z,Compositional Generalisation for Explainable Hate Speech Detection,Kompositorische Generalisierung für erklärbare Hass-Spracherkennung,可解释的仇恨言论检测组 组 成 概 述,http://arxiv.org/abs/2506.03916v1
303,"Legal decisions are increasingly evaluated for fairness, consistency, and bias using machine learning (ML) techniques. In high-stakes domains like refugee adjudication, such methods are often applied to detect disparities in outcomes. Yet it remains unclear whether statistical methods can meaningfully assess fairness in legal contexts shaped by discretion, normative complexity, and limited ground truth.   In this paper, we empirically evaluate three common ML approaches (feature-based analysis, semantic clustering, and predictive modeling) on a large, real-world dataset of 59,000+ Canadian refugee decisions (AsyLex). Our experiments show that these methods produce divergent and sometimes contradictory signals, that predictive modeling often depends on contextual and procedural features rather than legal features, and that semantic clustering fails to capture substantive legal reasoning.   We show limitations of statistical fairness evaluation, challenge the assumption that statistical regularity equates to fairness, and argue that current computational approaches fall short of evaluating fairness in legally discretionary domains. We argue that evaluating fairness in law requires methods grounded not only in data, but in legal reasoning and institutional context.",,"Claire Barale, Michael Rovatsos, Nehal Bhuta",2025-06-04T13:05:37Z,When Fairness Isn't Statistical: The Limits of Machine Learning in   Evaluating Legal Reasoning,Wenn Fairness nicht statistisch ist: Die Grenzen des maschinellen Lernens bei der Bewertung der rechtlichen Vernunft,当公平不是统计时:机器学习在评估法律理由方面的限度,http://arxiv.org/abs/2506.03913v1
304,"Training state-of-the-art large language models requires vast amounts of clean and diverse textual data. However, building suitable multilingual datasets remains a challenge. In this work, we present HPLT v2, a collection of high-quality multilingual monolingual and parallel corpora, extending prior work of the HPLT project. The monolingual portion of the data contains 8T tokens covering 193 languages, while the parallel data contains 380M sentence pairs covering 51 languages. We document the entire data pipeline and release the code to reproduce it. We provide extensive analysis of the quality and characteristics of our data. Finally, we evaluate the performance of language models and machine translation systems trained on HPLT v2, demonstrating its value.",,"Laurie Burchell, Ona de Gibert, Nikolay Arefyev, Mikko Aulamo, Marta Bañón, Pinzhen Chen, Mariia Fedorova, Liane Guillou, Barry Haddow, Jan Hajič, Jindřich Helcl, Erik Henriksson, Mateusz Klimaszewski, Ville Komulainen, Andrey Kutuzov, Joona Kytöniemi, Veronika Laippala, Petter Mæhlum, Bhavitvya Malik, Farrokh Mehryary, Vladislav Mikhailov, Nikita Moghe, Amanda Myntti, Dayyán O'Brien, Stephan Oepen, Proyag Pal, Jousia Piha, Sampo Pyysalo, Gema Ramírez-Sánchez, David Samuel, Pavel Stepachev, Jörg Tiedemann, Dušan Variš, Tereza Vojtěchová, Jaume Zaragoza-Bernabeu",2025-06-04T12:57:58Z,An Expanded Massive Multilingual Dataset for High-Performance Language   Technologies (HPLT),"Ein erweiterter, massenhafter Mehrsprachiger Datensatz für hochleistungsfähige Sprachtechnologien (HPLT)",用于高绩效语言技术的扩大大规模多语种数据集(HPLT),http://arxiv.org/abs/2503.10267v3
305,"The uniform information density (UID) hypothesis proposes that speakers aim to distribute information evenly throughout a text, balancing production effort and listener comprehension difficulty. However, language typically does not maintain a strictly uniform information rate; instead, it fluctuates around a global average. These fluctuations are often explained by factors such as syntactic constraints, stylistic choices, or audience design. In this work, we explore an alternative perspective: that these fluctuations may be influenced by an implicit linguistic pressure towards periodicity, where the information rate oscillates at regular intervals, potentially across multiple frequencies simultaneously. We apply harmonic regression and introduce a novel extension called time scaling to detect and test for such periodicity in information contours. Analyzing texts in English, Spanish, German, Dutch, Basque, and Brazilian Portuguese, we find consistent evidence of periodic patterns in information rate. Many dominant frequencies align with discourse structure, suggesting these oscillations reflect meaningful linguistic organization. Beyond highlighting the connection between information rate and discourse structure, our approach offers a general framework for uncovering structural pressures at various levels of linguistic granularity.",,"Eleftheria Tsipidi, Samuel Kiegeland, Franz Nowak, Tianyang Xu, Ethan Wilcox, Alex Warstadt, Ryan Cotterell, Mario Giulianelli",2025-06-04T12:56:30Z,The Harmonic Structure of Information Contours,Die Harmonische Struktur der Informationskonturen,信息调频的和谐结构,http://arxiv.org/abs/2506.03902v1
306,"In this paper, we show that knowledge distillation can be subverted to manipulate language model benchmark scores, revealing a critical vulnerability in current evaluation practices. We introduce ""Data Laundering,"" a process that enables the covert transfer of benchmark-specific knowledge through seemingly legitimate intermediate training steps. Through extensive experiments with a 2-layer BERT student model, we show how this approach can achieve substantial improvements in benchmark accuracy (up to 75\% on GPQA) without developing genuine reasoning capabilities. Notably, this method can be exploited intentionally or even unintentionally, as researchers may inadvertently adopt this method and inflate scores without realising the implications. While our findings demonstrate the effectiveness of this technique, we present them as a cautionary tale highlighting the urgent need for more robust evaluation methods in AI. This work aims to contribute to the ongoing discussion about evaluation integrity in AI development and the need for benchmarks that more accurately reflect true model capabilities. The code is available at https://github.com/mbzuai-nlp/data_laundering.",,"Jonibek Mansurov, Akhmed Sakip, Alham Fikri Aji",2025-06-04T12:38:38Z,Data Laundering: Artificially Boosting Benchmark Results through   Knowledge Distillation,Datenwaschen: Künstliche Steigerung der Benchmark-Ergebnisse durch Wissensdestillation,洗钱:通过知识蒸馏人为促进基准成果,http://arxiv.org/abs/2412.15255v2
307,"Extensive LLM applications demand efficient structured generations, particularly for LR(1) grammars, to produce outputs in specified formats (e.g., JSON). Existing methods primarily parse LR(1) grammars into a pushdown automaton (PDA), leading to runtime execution overhead for context-dependent token processing, especially inefficient under large inference batches. To address these issues, we propose Pre$^3$ that exploits deterministic pushdown automata (DPDA) to optimize the constrained LLM decoding efficiency. First, by precomputing prefix-conditioned edges during the preprocessing, Pre$^3$ enables ahead-of-time edge analysis and thus makes parallel transition processing possible. Second, by leveraging the prefix-conditioned edges, Pre$^3$ introduces a novel approach that transforms LR(1) transition graphs into DPDA, eliminating the need for runtime path exploration and achieving edge transitions with minimal overhead. Pre$^3$ can be seamlessly integrated into standard LLM inference frameworks, reducing time per output token (TPOT) by up to 40% and increasing throughput by up to 36% in our experiments. Our code is available at https://github.com/ModelTC/lightllm.",,"Junyi Chen, Shihao Bai, Zaijun Wang, Siyu Wu, Chuheng Du, Hailong Yang, Ruihao Gong, Shengzhong Liu, Fan Wu, Guihai Chen",2025-06-04T12:30:30Z,Pre$^3$: Enabling Deterministic Pushdown Automata for Faster Structured   LLM Generation,Pre$^3$: Deterministische Pushdown-Automata für schnellere strukturierte LLM-Generation aktivieren,3美元前3美元:为更快的结构化LLM生成而使决定性式自推自动数据能够实现,http://arxiv.org/abs/2506.03887v1
308,"Text-to-speech (TTS) systems typically require high-quality studio data and accurate transcriptions for training. India has 1369 languages, with 22 official using 13 scripts. Training a TTS system for all these languages, most of which have no digital resources, seems a Herculean task. Our work focuses on zero-shot synthesis, particularly for languages whose scripts and phonotactics come from different families. The novelty of our work is in the augmentation of a shared phone representation and modifying the text parsing rules to match the phonotactics of the target language, thus reducing the synthesiser overhead and enabling rapid adaptation. Intelligible and natural speech was generated for Sanskrit, Maharashtrian and Canara Konkani, Maithili and Kurukh by leveraging linguistic connections across languages with suitable synthesisers. Evaluations confirm the effectiveness of this approach, highlighting its potential to expand speech technology access for under-represented languages.",,"Utkarsh Pathak, Chandra Sai Krishna Gunda, Anusha Prakash, Keshav Agarwal, Hema A. Murthy",2025-06-04T12:22:24Z,Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS   in Indian Languages,Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS in Indian Languages,语言:利用语言关系促进印度语零热TTS,http://arxiv.org/abs/2506.03884v1
309,"The rapid advancements in large language models (LLMs) have led to the emergence of routing techniques, which aim to efficiently select the optimal LLM from diverse candidates to tackle specific tasks, optimizing performance while reducing costs. Current LLM routing methods are limited in effectiveness due to insufficient exploration of the intrinsic connection between user queries and the characteristics of LLMs. To address this issue, in this paper, we present RadialRouter, a novel framework for LLM routing which employs a lightweight Transformer-based backbone with a radial structure named RadialFormer to articulate the query-LLMs relationship. The optimal LLM selection is performed based on the final states of RadialFormer. The pipeline is further refined by an objective function that combines Kullback-Leibler divergence with the query-query contrastive loss to enhance robustness. Experimental results on RouterBench show that RadialRouter significantly outperforms existing routing methods by 9.2\% and 5.8\% in the Balance and Cost First scenarios, respectively. Additionally, its adaptability toward different performance-cost trade-offs and the dynamic LLM pool demonstrates practical application potential.",,"Ruihan Jin, Pengpeng Shao, Zhengqi Wen, Jinyang Wu, Mingkuan Feng, Shuai Zhang, Jianhua Tao",2025-06-04T12:16:41Z,RadialRouter: Structured Representation for Efficient and Robust Large   Language Models Routing,RadialRouter: Strukturierte Darstellung für effizientes und robustes Large Language Model Routing,RadialRouter:高效和强力大语言模型运行的结构性代表性,http://arxiv.org/abs/2506.03880v1
310,"Idioms present a unique challenge for language models due to their non-compositional figurative meanings, which often strongly diverge from the idiom's literal interpretation. This duality requires a model to learn representing and deciding between the two meanings to interpret an idiom in a figurative sense, or literally. In this paper, we employ tools from mechanistic interpretability to trace how a large pretrained causal transformer (LLama3.2-1B-base) deals with this ambiguity. We localize three steps of idiom processing: First, the idiom's figurative meaning is retrieved in early attention and MLP sublayers. We identify specific attention heads which boost the figurative meaning of the idiom while suppressing the idiom's literal interpretation. The model subsequently represents the figurative representation through an intermediate path. Meanwhile, a parallel bypass route forwards literal interpretation, ensuring that a both reading remain available. Overall, our findings provide a mechanistic evidence for idiom comprehension in an autoregressive transformer.",,"Soyoung Oh, Xinting Huang, Mathis Pink, Michael Hahn, Vera Demberg",2025-06-04T12:05:01Z,Tug-of-war between idiom's figurative and literal meanings in LLMs,Kriegstug zwischen Idiom's figurativen und wörtlichen Bedeutungen in LLMs,智多明的象征意义和LLMM中字面含义之间的战争图示,http://arxiv.org/abs/2506.01723v2
311,"Large language models increasingly support multiple languages, yet most benchmarks for gender bias remain English-centric. We introduce EuroGEST, a dataset designed to measure gender-stereotypical reasoning in LLMs across English and 29 European languages. EuroGEST builds on an existing expert-informed benchmark covering 16 gender stereotypes, expanded in this work using translation tools, quality estimation metrics, and morphological heuristics. Human evaluations confirm that our data generation method results in high accuracy of both translations and gender labels across languages. We use EuroGEST to evaluate 24 multilingual language models from six model families, demonstrating that the strongest stereotypes in all models across all languages are that women are \textit{beautiful,} \textit{empathetic} and \textit{neat} and men are \textit{leaders}, \textit{strong, tough} and \textit{professional}. We also show that larger models encode gendered stereotypes more strongly and that instruction finetuning does not consistently reduce gendered stereotypes. Our work highlights the need for more multilingual studies of fairness in LLMs and offers scalable methods and resources to audit gender bias across languages.",,"Jacqueline Rowe, Mateusz Klimaszewski, Liane Guillou, Shannon Vallor, Alexandra Birch",2025-06-04T11:58:18Z,EuroGEST: Investigating gender stereotypes in multilingual language   models,EuroGEST: Untersuchung von Geschlechterstereotypen in mehrsprachigen Sprachmodellen,EURGEST:调查多语言模式中的性别陈规定型观念,http://arxiv.org/abs/2506.03867v1
312,"Peer review is central to academic publishing, but the growing volume of submissions is straining the process. This motivates the development of computational approaches to support peer review. While each review is tailored to a specific paper, reviewers often make assessments according to certain aspects such as Novelty, which reflect the values of the research community. This alignment creates opportunities for standardizing the reviewing process, improving quality control, and enabling computational support. While prior work has demonstrated the potential of aspect analysis for peer review assistance, the notion of aspect remains poorly formalized. Existing approaches often derive aspects from review forms and guidelines, yet data-driven methods for aspect identification are underexplored. To address this gap, our work takes a bottom-up approach: we propose an operational definition of aspect and develop a data-driven schema for deriving aspects from a corpus of peer reviews. We introduce a dataset of peer reviews augmented with aspects and show how it can be used for community-level review analysis. We further show how the choice of aspects can impact downstream applications, such as LLM-generated review detection. Our results lay a foundation for a principled and data-driven investigation of review aspects, and pave the path for new applications of NLP to support peer review.",,"Sheng Lu, Ilia Kuznetsov, Iryna Gurevych",2025-06-04T11:50:19Z,Identifying Aspects in Peer Reviews,Aspekte in Peer Reviews identifizieren,确定同侪审查的各方面,http://arxiv.org/abs/2504.06910v2
313,"High-Frequency Trading (HFT) is pivotal in cryptocurrency markets, demanding rapid decision-making. Social media platforms like Reddit offer valuable, yet underexplored, information for such high-frequency, short-term trading. This paper introduces \textbf{PulseReddit}, a novel dataset that is the first to align large-scale Reddit discussion data with high-frequency cryptocurrency market statistics for short-term trading analysis. We conduct an extensive empirical study using Large Language Model (LLM)-based Multi-Agent Systems (MAS) to investigate the impact of social sentiment from PulseReddit on trading performance. Our experiments conclude that MAS augmented with PulseReddit data achieve superior trading outcomes compared to traditional baselines, particularly in bull markets, and demonstrate robust adaptability across different market regimes. Furthermore, our research provides conclusive insights into the performance-efficiency trade-offs of different LLMs, detailing significant considerations for practical model selection in HFT applications. PulseReddit and our findings establish a foundation for advanced MAS research in HFT, demonstrating the tangible benefits of integrating social media.",,"Qiuhan Han, Qian Wang, Atsushi Yoshikawa, Masayuki Yamamura",2025-06-04T11:48:51Z,PulseReddit: A Novel Reddit Dataset for Benchmarking MAS in   High-Frequency Cryptocurrency Trading,PulseReddit: Ein neuartiger Reddit-Datensatz für das Benchmarking von MAS im Kryptowährungshandel mit hoher Frequenz,PulseReddit:高级加密货币交易中用于确定MAS基准的新Rddit数据集,http://arxiv.org/abs/2506.03861v1
314,"Recently, Large Language Models (LLMs) have demonstrated significant potential for data annotation, markedly reducing the labor costs associated with downstream applications. However, existing methods mostly adopt an aggressive strategy by prompting LLM to determine a single gold label for each unlabeled sample. Due to the inherent uncertainty within LLMs, they often produce incorrect labels for difficult samples, severely compromising the data quality for downstream applications. Motivated by ambiguity aversion in human behaviors, we propose a novel candidate annotation paradigm wherein large language models are encouraged to output all possible labels when incurring uncertainty. To ensure unique labels are provided for downstream tasks, we develop a teacher-student framework CanDist that distills candidate annotations with a Small Language Model (SLM). We further provide a rigorous justification demonstrating that distilling candidate annotations from the teacher LLM offers superior theoretical guarantees compared to directly using single annotations. Extensive experiments across six text classification tasks validate the effectiveness of our proposed method. The source code is available at https://github.com/MingxuanXia/CanDist.",,"Mingxuan Xia, Haobo Wang, Yixuan Li, Zewei Yu, Jindong Wang, Junbo Zhao, Runze Wu",2025-06-04T11:42:37Z,"Prompt Candidates, then Distill: A Teacher-Student Framework for   LLM-driven Data Annotation","Prompt-Kandidaten, dann Distill: Ein Lehrer-Student-Framework für LLM-gesteuerte Datenannotation","即时候选人,然后是蒸馏:由LLM驱动的数据批注教师-学生框架",http://arxiv.org/abs/2506.03857v1
315,"Here we present a new class of optimality for coding systems. Members of that class are displaced linearly from optimal coding and thus exhibit Zipf's law, namely a power-law distribution of frequency ranks. Within that class, Zipf's law, the size-rank law and the size-probability law form a group-like structure. We identify human languages that are members of the class. All languages showing sufficient agreement with Zipf's law are potential members of the class. In contrast, there are communication systems in other species that cannot be members of that class for exhibiting an exponential distribution instead but dolphins and humpback whales might. We provide a new insight into plots of frequency versus rank in double logarithmic scale. For any system, a straight line in that scale indicates that the lengths of optimal codes under non-singular coding and under uniquely decodable encoding are displaced by a linear function whose slope is the exponent of Zipf's law. For systems under compression and constrained to be uniquely decodable, such a straight line may indicate that the system is coding close to optimality. We provide support for the hypothesis that Zipf's law originates from compression and define testable conditions for the emergence of Zipf's law in compressing systems.",,Ramon Ferrer-i-Cancho,2025-06-04T11:35:43Z,On the class of coding optimality of human languages and the origins of   Zipf's law,Über die Klasse der Kodierung der optimalen menschlichen Sprachen und die Ursprünge des Zippschen Gesetzes,在人类语言最优化的编码和齐普夫法律的起源方面,http://arxiv.org/abs/2505.20015v3
316,"Large language models (LLMs) have recently achieved impressive performance across a wide range of natural language tasks and are now widely used in real-world applications. Among them, black-box LLMs--served via APIs without access to model internals--are especially dominant due to their scalability and ease of deployment. Despite their strong capabilities, these models typically produce generalized responses that overlook personal preferences and reasoning styles. This has led to growing interest in black-box LLM personalization, which aims to tailor model outputs to user-specific context without modifying model parameters. However, existing approaches primarily focus on response-level personalization, attempting to match final outputs without modeling personal thought process. To address this limitation, we propose RPM, a framework for reasoning-level personalization that aligns the model's reasoning process with a user's personalized logic. RPM first constructs statistical user-specific factors by extracting and grouping response-influential features from user history. It then builds personalized reasoning paths that reflect how these factors are used in context. In the inference stage, RPM retrieves reasoning-aligned examples for new queries via feature-level similarity and performs inference conditioned on the structured factors and retrieved reasoning paths, enabling the model to follow user-specific reasoning trajectories. This reasoning-level personalization enhances both predictive accuracy and interpretability by grounding model outputs in user-specific logic through structured information. Extensive experiments across diverse tasks show that RPM consistently outperforms response-level personalization methods, demonstrating the effectiveness of reasoning-level personalization in black-box LLMs.",,"Jieyong Kim, Tongyoung Kim, Soojin Yoon, Jaehyung Kim, Dongha Lee",2025-06-04T11:03:01Z,"LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for   Black-Box Large Language Models","LLMs denken, aber nicht in Ihrem Fluss: Grund-Level-Personalisierung für Black-Box große Sprachmodelle","LLM Think, But not in your roll: 黑人大语言模型的理性程度个人化",http://arxiv.org/abs/2505.21082v3
317,"Pretrained self-supervised speech models excel in speech tasks but do not reflect the hierarchy of human speech processing, as they encode rich semantics in middle layers and poor semantics in late layers. Recent work showed that brain-tuning (fine-tuning models using human brain recordings) improves speech models' semantic understanding. Here, we examine how well brain-tuned models further reflect the brain's intermediate stages of speech processing. We find that late layers of brain-tuned models substantially improve over pretrained models in their alignment with semantic language regions. Further layer-wise probing reveals that early layers remain dedicated to low-level acoustic features, while late layers become the best at complex high-level tasks. These findings show that brain-tuned models not only perform better but also exhibit a well-defined hierarchical processing going from acoustic to semantic representations, making them better model organisms for human speech processing.",,"Omer Moussa, Mariya Toneva",2025-06-04T10:59:11Z,Brain-tuned Speech Models Better Reflect Speech Processing Stages in the   Brain,Gehirngesteuerte Sprachmodelle reflektieren besser Sprachverarbeitungsstadien im Gehirn,脑中更好地反映语音处理阶段的发声模式,http://arxiv.org/abs/2506.03832v1
318,"Idiomatic expressions present a unique challenge in NLP, as their meanings are often not directly inferable from their constituent words. Despite recent advancements in Large Language Models (LLMs), idiomaticity remains a significant obstacle to robust semantic representation. We present datasets and tasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity Representation), which challenges the community to assess and improve models' ability to interpret idiomatic expressions in multimodal contexts and in multiple languages. Participants competed in two subtasks: ranking images based on their alignment with idiomatic or literal meanings, and predicting the next image in a sequence. The most effective methods achieved human-level performance by leveraging pretrained LLMs and vision-language models in mixture-of-experts settings, with multiple queries used to smooth over the weaknesses in these models' representations of idiomaticity.",,"Thomas Pickard, Aline Villavicencio, Maggie Mi, Wei He, Dylan Phelps, Marco Idiart",2025-06-04T10:58:56Z,SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity   Representation,SemEval-2025 Aufgabe 1: AdMIRe -- Förderung der multimodalen Idiomatizitätsdarstellung,SemEval-2025 任务1:AdMIRE -- -- 推进多模式、多模式代表制,http://arxiv.org/abs/2503.15358v3
319,"Large language models (LLMs) have majorly advanced NLP and AI, and next to their ability to perform a wide range of procedural tasks, a major success factor is their internalized factual knowledge. Since Petroni et al. (2019), analyzing this knowledge has gained attention. However, most approaches investigate one question at a time via modest-sized pre-defined samples, introducing an ``availability bias'' (Tversky&Kahnemann, 1973) that prevents the analysis of knowledge (or beliefs) of LLMs beyond the experimenter's predisposition.   To address this challenge, we propose a novel methodology to comprehensively materialize an LLM's factual knowledge through recursive querying and result consolidation. Our approach is a milestone for LLM research, for the first time providing constructive insights into the scope and structure of LLM knowledge (or beliefs).   As a prototype, we build GPTKB, a knowledge base (KB) comprising 101 million relational triples for over 2.9 million entities from GPT-4o-mini. We use GPTKB to exemplarily analyze GPT-4o-mini's factual knowledge in terms of scale, accuracy, bias, cutoff and consistency, at the same time. GPTKB is accessible at https://gptkb.org",,"Yujia Hu, Tuan-Phong Nguyen, Shrestha Ghosh, Simon Razniewski",2025-06-04T10:58:03Z,Enabling LLM Knowledge Analysis via Extensive Materialization,LLM-Wissensanalyse mittels umfangreicher Materialisierung aktivieren,"通过广泛的材料化,使LLM能够进行知识分析",http://arxiv.org/abs/2411.04920v4
320,"Retrieval systems primarily address the challenge of matching user queries with the most relevant advertisements, playing a crucial role in e-commerce search advertising. The diversity of user needs and expressions often produces massive long-tail queries that cannot be matched with merchant bidwords or product titles, which results in some advertisements not being recalled, ultimately harming user experience and search efficiency. Existing query rewriting research focuses on various methods such as query log mining, query-bidword vector matching, or generation-based rewriting. However, these methods often fail to simultaneously optimize the relevance and authenticity of the user's original query and rewrite and maximize the revenue potential of recalled ads.   In this paper, we propose a Multi-objective aligned Bidword Generation Model (MoBGM), which is composed of a discriminator, generator, and preference alignment module, to address these challenges. To simultaneously improve the relevance and authenticity of the query and rewrite and maximize the platform revenue, we design a discriminator to optimize these key objectives. Using the feedback signal of the discriminator, we train a multi-objective aligned bidword generator that aims to maximize the combined effect of the three objectives. Extensive offline and online experiments show that our proposed algorithm significantly outperforms the state of the art. After deployment, the algorithm has created huge commercial value for the platform, further verifying its feasibility and robustness.",,"Zhenhui Liu, Chunyuan Yuan, Ming Pang, Zheng Fang, Li Yuan, Xue Jiang, Changping Peng, Zhangang Lin, Zheng Luo, Jingping Shao",2025-06-04T10:57:18Z,Multi-objective Aligned Bidword Generation Model for E-commerce Search   Advertising,Multi-objective Aligned Bidword Generation Modell für E-Commerce Search Advertising,电子商务搜索广告的多目标统一比字生成模式,http://arxiv.org/abs/2506.03827v1
321,"Publication databases rely on accurate metadata extraction from diverse web sources, yet variations in web layouts and data formats present challenges for metadata providers. This paper introduces CRAWLDoc, a new method for contextual ranking of linked web documents. Starting with a publication's URL, such as a digital object identifier, CRAWLDoc retrieves the landing page and all linked web resources, including PDFs, ORCID profiles, and supplementary materials. It embeds these resources, along with anchor texts and the URLs, into a unified representation. For evaluating CRAWLDoc, we have created a new, manually labeled dataset of 600 publications from six top publishers in computer science. Our method CRAWLDoc demonstrates a robust and layout-independent ranking of relevant documents across publishers and data formats. It lays the foundation for improved metadata extraction from web documents with various layouts and formats. Our source code and dataset can be accessed at https://github.com/FKarl/CRAWLDoc.",,"Fabian Karl, Ansgar Scherp",2025-06-04T10:52:55Z,CRAWLDoc: A Dataset for Robust Ranking of Bibliographic Documents,CRAWLDoc: Ein Datensatz für robustes Ranking bibliografischer Dokumente,CRAWLDoc:书目文件有力排序数据集,http://arxiv.org/abs/2506.03822v1
322,"Hausa texts are often characterized by writing anomalies such as incorrect character substitutions and spacing errors, which sometimes hinder natural language processing (NLP) applications. This paper presents an approach to automatically correct the anomalies by finetuning transformer-based models. Using a corpus gathered from several public sources, we created a large-scale parallel dataset of over 450,000 noisy-clean Hausa sentence pairs by introducing synthetically generated noise, fine-tuned to mimic realistic writing errors. Moreover, we adapted several multilingual and African language-focused models, including M2M100, AfriTEVA, mBART, and Opus-MT variants for this correction task using SentencePiece tokenization. Our experimental results demonstrate significant increases in F1, BLEU and METEOR scores, as well as reductions in Character Error Rate (CER) and Word Error Rate (WER). This research provides a robust methodology, a publicly available dataset, and effective models to improve Hausa text quality, thereby advancing NLP capabilities for the language and offering transferable insights for other low-resource languages.",,"Ahmad Mustapha Wali, Sergiu Nisioi",2025-06-04T10:46:19Z,Automatic Correction of Writing Anomalies in Hausa Texts,Automatische Korrektur von Schreibanomalien in Hausa Texten,Hausa 文本中书写异常的自动校正,http://arxiv.org/abs/2506.03820v1
323,"Open-source multimodal large language models (MLLMs) have shown significant potential in a broad range of multimodal tasks. However, their reasoning capabilities remain constrained by existing instruction-tuning datasets, which were predominately repurposed from academic datasets such as VQA, AI2D, and ChartQA. These datasets target simplistic tasks, and only provide phrase-level answers without any intermediate rationales. To address these challenges, we introduce a scalable and cost-effective method to construct a large-scale multimodal instruction-tuning dataset with rich intermediate rationales designed to elicit CoT reasoning. Using only open models, we create a dataset containing 12M instruction-response pairs to cover diverse, reasoning-intensive tasks with detailed and faithful rationales. Experiments demonstrate that training MLLMs on this dataset significantly improves reasoning capabilities, achieving state-of-the-art performance on benchmarks such as MathVerse (+8.1%), MMMU-Pro (+7%), and MuirBench (+13.3%). Additionally, the model demonstrates notable improvements of up to 4% on non-reasoning-based benchmarks. Ablation studies further highlight the importance of key components, such as rewriting and self-filtering, in the dataset construction process.",,"Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, Xiang Yue",2025-06-04T10:07:57Z,MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at   Scale,MAmmoTH-VL: Multimodale Vernunft mit Instruktionstuning im Maßstab beseitigen,MAMMOTTH-VL: 使用比例指示调制的 Eliboring 多式联运理由,http://arxiv.org/abs/2412.05237v2
324,"Punctuation plays a vital role in structuring meaning, yet current models often struggle to restore it accurately in transcripts of spontaneous speech, especially in the presence of disfluencies such as false starts and backtracking. These limitations hinder the performance of downstream tasks like translation, text to speech, summarization, etc. where sentence boundaries are critical for preserving quality. In this work, we introduce Cadence, a generalist punctuation restoration model adapted from a pretrained large language model. Cadence is designed to handle both clean written text and highly spontaneous spoken transcripts. It surpasses the previous state of the art in performance while expanding support from 14 to all 22 Indian languages and English. We conduct a comprehensive analysis of model behavior across punctuation types and language families, identifying persistent challenges under domain shift and with rare punctuation marks. Our findings demonstrate the efficacy of utilizing pretrained language models for multilingual punctuation restoration and highlight Cadence practical value for low resource NLP pipelines at scale.",,"Sidharth Pulipaka, Sparsh Jain, Ashwin Sankar, Raj Dabre",2025-06-04T09:54:38Z,Mark My Words: A Robust Multilingual Model for Punctuation in Text and   Speech Transcripts,Mark My Words: Ein robustes Mehrsprachiges Modell für die Pünktlichkeit in Text- und Sprachtexten,标记我的单词 : 一个强有力的多语种文本和语音描述符号符号模式,http://arxiv.org/abs/2506.03793v1
325,"How can we quantize large language models while preserving accuracy? Quantization is essential for deploying large language models (LLMs) efficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are promising quantization schemes that have strong expressiveness and optimizability, respectively. However, neither scheme leverages both advantages. In this paper, we propose UniQuanF (Unified Quantization with Flexible Mapping), an accurate quantization method for LLMs. UniQuanF harnesses both strong expressiveness and optimizability by unifying the flexible mapping technique in UQ and non-uniform quantization levels of BCQ. We propose unified initialization, and local and periodic mapping techniques to optimize the parameters in UniQuanF precisely. After optimization, our unification theorem removes computational and memory overhead, allowing us to utilize the superior accuracy of UniQuanF without extra deployment costs induced by the unification. Experimental results demonstrate that UniQuanF outperforms existing UQ and BCQ methods, achieving up to 4.60% higher accuracy on GSM8K benchmark.",,"Seungcheol Park, Jeongin Bae, Beomseok Kwon, Minjun Kim, Byeongwook Kim, Se Jung Kwon, U Kang, Dongsoo Lee",2025-06-04T09:42:17Z,Unifying Uniform and Binary-coding Quantization for Accurate Compression   of Large Language Models,Vereinheitlichung einheitlicher und Binär-kodierender Quantisierungen für eine präzise Komprimierung großer Sprachmodelle,精确压缩大语言模型精确压缩的统一和二元编码统一和二元编码的量化,http://arxiv.org/abs/2506.03781v1
326,"Query expansion methods powered by large language models (LLMs) have demonstrated effectiveness in zero-shot retrieval tasks. These methods assume that LLMs can generate hypothetical documents that, when incorporated into a query vector, enhance the retrieval of real evidence. However, we challenge this assumption by investigating whether knowledge leakage in benchmarks contributes to the observed performance gains. Using fact verification as a testbed, we analyze whether the generated documents contain information entailed by ground-truth evidence and assess their impact on performance. Our findings indicate that, on average, performance improvements consistently occurred for claims whose generated documents included sentences entailed by gold evidence. This suggests that knowledge leakage may be present in fact-verification benchmarks, potentially inflating the perceived performance of LLM-based query expansion methods.",,"Yejun Yoon, Jaeyoon Jung, Seunghyun Yoon, Kunwoo Park",2025-06-04T09:32:19Z,Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query   Expansion,Hypothetische Dokumente oder Wissensverlust? LLM-basierte Abfrageerweiterung neu denken,假设文件或知识泄漏?重新思考基于 LLM 的查询扩展,http://arxiv.org/abs/2504.14175v2
327,"The capabilities of large language models (LLMs) have been enhanced by training on data that reflects human thought processes, such as the Chain-of-Thought format. However, evidence suggests that the conventional scheme of next-word prediction may not fully capture how humans learn to think. Inspired by how humans generalize mathematical reasoning, we propose a new approach named ClozeMath to fine-tune LLMs for mathematical reasoning. Our ClozeMath involves a text-infilling task that predicts masked equations from a given solution, analogous to cloze exercises used in human learning. Experiments on GSM8K, MATH, and GSM-Symbolic show that ClozeMath surpasses the strong baseline Masked Thought in performance and robustness, with two test-time scaling decoding algorithms, Beam Search and Chain-of-Thought decoding. Additionally, we conduct an ablation study to analyze the effects of various architectural and implementation choices on our approach.",,"Quang Hieu Pham, Thuy Duong Nguyen, Tung Pham, Anh Tuan Luu, Dat Quoc Nguyen",2025-06-04T09:27:21Z,ClozeMath: Improving Mathematical Reasoning in Language Models by   Learning to Fill Equations,ClozeMath: Verbesserung der mathematischen Vernunft in Sprachmodellen durch Lernen zur Füllung von Gleichungen,ClozeMatth:通过学习填充等数改进语言模式中的数学理由,http://arxiv.org/abs/2506.03763v1
328,"Large Language Models (LLMs) have significantly advanced the field of Artificial Intelligence. However, their deployment is resource-intensive, not only due to the large number of model parameters but also because the (Key-Value) KV cache consumes a lot of memory during inference. While several works propose reducing the KV cache by evicting the unnecessary tokens, these approaches rely on accumulated attention score as eviction score to quantify the importance of the token. We identify the accumulated attention score is biased and it decreases with the position of the tokens in the mathematical expectation. As a result, the retained tokens concentrate on the initial positions, limiting model's access to global contextual information. To address this issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the bias of the accumulated attention score by adaptively tuning the scale of softmax according the expectation of information entropy of attention scores. To make use of the holistic attention information in self-attention mechanism, AhaKV utilize the information of value vectors, which is overlooked in previous works, to refine the adaptive score. We show theoretically that our method is well suited for bias reduction. We deployed AhaKV on different models with a fixed cache budget. Experiments show that AhaKV successfully mitigates bias and retains crucial tokens across global context and achieve state-of-the-art results against other related work on several benchmark tasks.",,"Yifeng Gu, Zicong Jiang, Jianxiu Jin, Kailing Guo, Ziyang Zhang, Xiangmin Xu",2025-06-04T09:25:53Z,AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for   Efficient Inference of Large Language Models,AhaKV: Adaptive Ganzheitlich aufmerksamkeitsgetriebene KV-Cache-Eviktion für effiziente Folgerung großer Sprachmodelle,"AhaKV: 适应性综合关注驱动驱动 KV 缓冲效应,以有效推导大语言模型",http://arxiv.org/abs/2506.03762v1
329,"As interest in using Large Language Models (LLMs) for interactive and emotionally rich experiences grows, virtual pet companionship emerges as a novel yet underexplored application. Existing approaches focus on basic pet role-playing interactions without systematically benchmarking LLMs for comprehensive companionship. In this paper, we introduce Pet-Bench, a dedicated benchmark that evaluates LLMs across both self-interaction and human-interaction dimensions. Unlike prior work, Pet-Bench emphasizes self-evolution and developmental behaviors alongside interactive engagement, offering a more realistic reflection of pet companionship. It features diverse tasks such as intelligent scheduling, memory-based dialogues, and psychological conversations, with over 7,500 interaction instances designed to simulate complex pet behaviors. Evaluation of 28 LLMs reveals significant performance variations linked to model size and inherent capabilities, underscoring the need for specialized optimization in this domain. Pet-Bench serves as a foundational resource for benchmarking pet-related LLM abilities and advancing emotionally immersive human-pet interactions.",,"Hongcheng Guo, Zheyong Xie, Shaosheng Cao, Boyang Wang, Weiting Liu, Zheyu Ye, Zhoujun Li, Zuozhu Liu",2025-06-04T09:25:52Z,Act-as-Pet: Benchmarking the Abilities of Large Language Models as   E-Pets in Social Network Services,Act-as-Pet: Benchmarking der Fähigkeiten großer Sprachmodelle als E-Pets in Social Network Services,Acs-as-Pet:确定社会网络服务E-Pets作为E-Pets的大型语言模式的普及程度基准,http://arxiv.org/abs/2506.03761v1
330,"We introduce PromptCanvas, a concept that transforms prompting into a composable, widget-based experience on an infinite canvas. Users can generate, customize, and arrange interactive widgets representing various facets of their text, offering greater control over AI-generated content. PromptCanvas allows widget creation through system suggestions, user prompts, or manual input, providing a flexible environment tailored to individual needs. This enables deeper engagement with the creative process. In a lab study with 18 participants, PromptCanvas outperformed a traditional conversational UI on the Creativity Support Index. Participants found that it reduced cognitive load, with lower mental demand and frustration. Qualitative feedback revealed that the visual organization of thoughts and easy iteration encouraged new perspectives and ideas. A follow-up field study (N=10) confirmed these results, showcasing the potential of dynamic, customizable interfaces in improving collaborative writing with AI.",,"Rifat Mehreen Amin, Oliver Hans Kühle, Daniel Buschek, Andreas Butz",2025-06-04T09:13:51Z,PromptCanvas: Composable Prompting Workspaces Using Dynamic Widgets for   Exploration and Iteration in Creative Writing,PromptCanvas: Composable Prompting Workspaces mit dynamischen Widgets für Exploration und Iteration im kreativen Schreiben,利用动态部件探索和迭代创意写作中的动态部件,http://arxiv.org/abs/2506.03741v1
331,"Visuals are valuable tools for teaching math word problems (MWPs), helping young learners interpret textual descriptions into mathematical expressions before solving them. However, creating such visuals is labor-intensive and there is a lack of automated methods to support this process. In this paper, we present Math2Visual, an automatic framework for generating pedagogically meaningful visuals from MWP text descriptions. Math2Visual leverages a pre-defined visual language and a design space grounded in interviews with math teachers, to illustrate the core mathematical relationships in MWPs. Using Math2Visual, we construct an annotated dataset of 1,903 visuals and evaluate Text-to-Image (TTI) models for their ability to generate visuals that align with our design. We further fine-tune several TTI models with our dataset, demonstrating improvements in educational visual generation. Our work establishes a new benchmark for automated generation of pedagogically meaningful visuals and offers insights into key challenges in producing multimodal educational content, such as the misrepresentation of mathematical relationships and the omission of essential visual elements.",,"Junling Wang, Anna Rutkiewicz, April Yi Wang, Mrinmaya Sachan",2025-06-04T09:08:11Z,Generating Pedagogically Meaningful Visuals for Math Word Problems: A   New Benchmark and Analysis of Text-to-Image Models,Pädagogisch sinnvolle Visuals für Math Word-Probleme generieren: Ein neuer Benchmark und Analyse von Text-zu-Image-Modellen,"生成具有教学意义的视觉,解决数学词问题:新的基准和文本到图像模型分析",http://arxiv.org/abs/2506.03735v1
332,"The availability of Large Language Models (LLMs) presents a unique opportunity to reinvigorate research on Knowledge Engineering (KE) automation, a trend already evident in recent efforts developing LLM-based methods and tools for the automatic generation of Competency Questions (CQs). However, the evaluation of these tools lacks standardisation. This undermines the methodological rigour and hinders the replication and comparison of results. To address this gap, we introduce Bench4KE, an extensible API-based benchmarking system for KE automation. Its first release focuses on evaluating tools that generate CQs automatically. CQs are natural language questions used by ontology engineers to define the functional requirements of an ontology. Bench4KE provides a curated gold standard consisting of CQ datasets from four real-world ontology projects. It uses a suite of similarity metrics to assess the quality of the CQs generated. We present a comparative analysis of four recent CQ generation systems, which are based on LLMs, establishing a baseline for future research. Bench4KE is also designed to accommodate additional KE automation tasks, such as SPARQL query generation, ontology testing and drafting. Code and datasets are publicly available under the Apache 2.0 license.",,"Anna Sofia Lippolis, Minh Davide Ragagni, Paolo Ciancarini, Andrea Giovanni Nuzzolese, Valentina Presutti",2025-06-04T09:08:05Z,Bench4KE: Benchmarking Automated Competency Question Generation,Bench4KE: Benchmarking der Automatisierten Kompetenzfragestellung,4KE:自动能力问题生成基准,http://arxiv.org/abs/2505.24554v2
333,"Low-precision training is considered an effective strategy for reducing both training and downstream inference costs. Previous scaling laws for precision mainly focus on integer quantization, which pay less attention to the constituents in floating-point (FP) quantization, and thus cannot well fit the LLM losses in this scenario. In contrast, while FP quantization training is more commonly implemented in production, it's research has been relatively superficial. In this paper, we thoroughly explore the effects of FP quantization targets, exponent bits, mantissa bits, and the calculation granularity of the scaling factor in FP quantization training performance of LLM models. In addition to an accurate FP quantization unified scaling law, we also provide valuable suggestions for the community: (1) Exponent bits contribute slightly more to the model performance than mantissa bits. We provide the optimal exponent-mantissa bit ratio for different bit numbers, which is available for future reference by hardware manufacturers; (2) We discover the formation of the critical data size in low-precision LLM training. Too much training data exceeding the critical data size will inversely bring in degradation of LLM performance; (3) The optimal FP quantization precision is directly proportional to the computational power, but within a wide computational power range. We estimate that the best cost-performance precision should lie between 4-8 bits.",,"Xingwu Sun, Shuaipeng Li, Ruobing Xie, Weidong Han, Kan Wu, Zhen Yang, Yixing Li, An Wang, Shuai Li, Jinbao Xue, Yu Cheng, Yangyu Tao, Zhanhui Kang, Chengzhong Xu, Di Wang, Jie Jiang",2025-06-04T09:06:06Z,Scaling Laws for Floating Point Quantization Training,Skalierungsgesetze für die Ausbildung zur Quantisierung von Schwebepunkten,浮动点量化培训法,http://arxiv.org/abs/2501.02423v3
334,"Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical CoT reasoning abilities of LLMs without using external python tools, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier open-source LLMs by a substantial margin with higher data efficiency. Furthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini Pro and GPT-4-early-version. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance. For more details refer to https://github.com/nlpxucan/WizardLM",,"Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Yansong Tang, Dongmei Zhang",2025-06-04T08:58:56Z,WizardMath: Empowering Mathematical Reasoning for Large Language Models   via Reinforced Evol-Instruct,WizardMath: Mathematische Begründung für große Sprachmodelle durch verstärkten Evol-Instruct,"向导:通过强化的Evol-Instruct,赋予大语言模型的数学理由",http://arxiv.org/abs/2308.09583v3
335,"Recent advances in Large Language Models (LLMs) have demonstrated remarkable performance in Contextual Question Answering (CQA). However, prior approaches typically employ elaborate reasoning strategies regardless of question complexity, leading to low adaptability. Recent efficient test-time scaling methods introduce budget constraints or early stop mechanisms to avoid overthinking for straightforward questions. But they add human bias to the reasoning process and fail to leverage models' inherent reasoning capabilities. To address these limitations, we present T$^2$: Think-to-Think, a novel framework that dynamically adapts reasoning depth based on question complexity. T$^2$ leverages the insight that if an LLM can effectively solve similar questions using specific reasoning strategies, it can apply the same strategy to the original question. This insight enables to adoption of concise reasoning for straightforward questions while maintaining detailed analysis for complex problems. T$^2$ works through four key steps: decomposing questions into structural elements, generating similar examples with candidate reasoning strategies, evaluating these strategies against multiple criteria, and applying the most appropriate strategy to the original question. Experimental evaluation across seven diverse CQA benchmarks demonstrates that T$^2$ not only achieves higher accuracy than baseline methods but also reduces computational overhead by up to 25.2\%.",,"Zhengyi Zhao, Shubo Zhang, Zezhong Wang, Huimin Wang, Yutian Zhao, Bin Liang, Yefeng Zheng, Binyang Li, Kam-Fai Wong, Xian Wu",2025-06-04T08:58:33Z,T$^2$: An Adaptive Test-Time Scaling Strategy for Contextual Question   Answering,T$^2$: Eine adaptive Test-Time Scaling-Strategie für die kontextuelle Fragebeantwortung,T$$2美元:关于实际回答问题的适应性试验时间扩大战略,http://arxiv.org/abs/2505.17427v2
336,"Uncertainty calibration is essential for the safe deployment of large language models (LLMs), particularly when users rely on verbalized confidence estimates. While prior work has focused on classifiers or short-form generation, confidence calibration for chain-of-thought (CoT) reasoning remains largely unexplored. Surprisingly, we find that supervised fine-tuning with scalar confidence labels alone suffices to elicit self-verification behavior of language models, without any explicit reasoning supervision or reinforcement learning-based rewards. Despite being trained only to produce a verbalized confidence score without any self-verifying examples, the model learns to generate longer and self-checking responses for low-confidence queries while providing more concise answers for high-confidence ones. We further propose a simple rethinking method that boosts performance via test-time scaling based on calibrated uncertainty. Experiments on GSM8K and held-out reasoning tasks such as MATH-500 and ARC-Challenge show that our confidence-aware fine-tuning improves both calibration and accuracy, while also enhancing interpretability by aligning the model's reasoning path with its confidence.",,"Chaeyun Jang, Moonseok Choi, Yegon Kim, Hyungi Lee, Juho Lee",2025-06-04T08:56:24Z,Verbalized Confidence Triggers Self-Verification: Emergent Behavior   Without Explicit Reasoning Supervision,Verbalisiertes Vertrauen triggert Selbstprüfung: Emergentes Verhalten ohne explizite Begründung Überwachung,口头信任触发自检:无明确理由的新兴行为,http://arxiv.org/abs/2506.03723v1
337,"Applying large pre-trained speech models like Whisper has shown promise in reducing training costs for various speech tasks. However, integrating these models into streaming systems remains a challenge. This paper presents a novel prefix-to-prefix training framework for streaming recognition by fine-tuning the Whisper. We introduce the Continuous Integrate-and-Fire mechanism to establish a quasi-monotonic alignment between continuous speech sequences and discrete text tokens. Additionally, we design Monotonic Finite Look-ahead Attention, allowing each token to attend to infinite left-context and finite right-context from the speech sequences. We also employ the wait-k decoding strategy to simplify the decoding process while ensuring consistency between training and testing. Our theoretical analysis and experiments demonstrate that this approach achieves a controllable trade-off between latency and quality, making it suitable for various streaming applications.",,"Yinfeng Xia, Huiyan Li, Chenyang Le, Manhong Wang, Yutao Sun, Xingyang Ma, Yanmin Qian",2025-06-04T08:53:40Z,MFLA: Monotonic Finite Look-ahead Attention for Streaming Speech   Recognition,MFLA: Monotonische Finite Look-Ahead Aufmerksamkeit für die Streaming Spracherkennung,MFLA: 电流语音识别的单调有限外观关注,http://arxiv.org/abs/2506.03722v1
338,"Recent advancements in speech large language models (SpeechLLMs) have attracted considerable attention. Nonetheless, current methods exhibit suboptimal performance in adhering to speech instructions. Notably, the intelligence of models significantly diminishes when processing speech-form input as compared to direct text-form input. Prior work has attempted to mitigate this semantic inconsistency between speech and text representations through techniques such as representation and behavior alignment, which involve the meticulous design of data pairs during the post-training phase. In this paper, we introduce a simple and scalable training method called InSerter, which stands for Interleaved Speech-Text Representation Pre-training. InSerter is designed to pre-train large-scale unsupervised speech-text sequences, where the speech is synthesized from randomly selected segments of an extensive text corpus using text-to-speech conversion. Consequently, the model acquires the ability to generate textual continuations corresponding to the provided speech segments, obviating the need for intensive data design endeavors. To systematically evaluate speech instruction-following capabilities, we introduce SpeechInstructBench, the first comprehensive benchmark specifically designed for speech-oriented instruction-following tasks. Our proposed InSerter achieves SOTA performance in SpeechInstructBench and demonstrates superior or competitive results across diverse speech processing tasks.",,"Dingdong Wang, Jin Xu, Ruihang Chu, Zhifang Guo, Xiong Wang, Jincenzi Wu, Dongchao Yang, Shengpeng Ji, Junyang Lin",2025-06-04T08:53:07Z,InSerter: Speech Instruction Following with Unsupervised Interleaved   Pre-training,InSerter: Sprachunterricht mit unüberwachter Interleaved Pre-Training,InSerter: 在无人监督的外派培训前进行语音教学,http://arxiv.org/abs/2503.02769v2
339,"Concept-based explanations work by mapping complex model computations to human-understandable concepts. Evaluating such explanations is very difficult, as it includes not only the quality of the induced space of possible concepts but also how effectively the chosen concepts are communicated to users. Existing evaluation metrics often focus solely on the former, neglecting the latter. We introduce an evaluation framework for measuring concept explanations via automated simulatability: a simulator's ability to predict the explained model's outputs based on the provided explanations. This approach accounts for both the concept space and its interpretation in an end-to-end evaluation. Human studies for simulatability are notoriously difficult to enact, particularly at the scale of a wide, comprehensive empirical evaluation (which is the subject of this work). We propose using large language models (LLMs) as simulators to approximate the evaluation and report various analyses to make such approximations reliable. Our method allows for scalable and consistent evaluation across various models and datasets. We report a comprehensive empirical evaluation using this framework and show that LLMs provide consistent rankings of explanation methods. Code available at https://github.com/AnonymousConSim/ConSim.",,"Antonin Poché, Alon Jacovi, Agustin Martin Picard, Victor Boutin, Fanny Jourdan",2025-06-04T08:45:02Z,ConSim: Measuring Concept-Based Explanations' Effectiveness with   Automated Simulatability,ConSim: konzeptbasierte Erklärungs-Wirksamkeit mit automatisierter Simulatierbarkeit messen,ConSim:用自动模拟性衡量基于概念的解释的有效性,http://arxiv.org/abs/2501.05855v4
340,"Despite recent progress in systematic evaluation frameworks, benchmarking the uncertainty of large language models (LLMs) remains a highly challenging task. Existing methods for benchmarking the uncertainty of LLMs face three key challenges: the need for internal model access, additional training, or high computational costs. This is particularly unfavorable for closed-source models. To this end, we introduce UBench, a new benchmark for evaluating the uncertainty of LLMs. Unlike other benchmarks, UBench is based on confidence intervals. It encompasses 11,978 multiple-choice questions spanning knowledge, language, understanding, and reasoning capabilities. Based on this, we conduct extensive experiments. This includes comparisons with other advanced uncertainty estimation methods, the assessment of the uncertainty of 20 LLMs, and an exploration of the effects of Chain-of-Thought (CoT) prompts, role-playing (RP) prompts, and temperature on model uncertainty. Our analysis reveals several crucial insights: 1) Our confidence interval-based methods are highly effective for uncertainty quantification; 2) Regarding uncertainty, outstanding open-source models show competitive performance versus closed-source models; 3) CoT and RP prompts present potential ways to improve model reliability, while the influence of temperature changes follows no universal rule. Our implementation is available at https://github.com/Cyno2232/UBENCH.",,"Xunzhi Wang, Zhuowei Zhang, Gaonan Chen, Qiongyu Li, Bitong Luo, Zhixin Han, Haotian Wang, Zhiyu li, Hang Gao, Mengting Hu",2025-06-04T08:37:02Z,UBench: Benchmarking Uncertainty in Large Language Models with Multiple   Choice Questions,UBench: Benchmarking Unsicherheit in großen Sprachmodellen mit Multiple-Choice-Fragen,UBench:确定具有多种选择问题的大语言模式不确定性的基准,http://arxiv.org/abs/2406.12784v2
341,"Recent advances in transformer-based Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, their quadratic computational complexity concerning sequence length remains a significant bottleneck for processing long documents. As a result, many efforts like sparse attention and state space models have been proposed to improve the efficiency of LLMs over long sequences. Though effective, these approaches compromise the performance or introduce structural complexity. This calls for a simple yet efficient model that preserves the fundamental Transformer architecture. To this end, we introduce SWAT, which enables efficient long-context handling via Sliding Window Attention Training. This paper first attributes the inefficiency of Transformers to the attention sink phenomenon resulting from the high variance of softmax operation. Then, we replace softmax with the sigmoid function and utilize a balanced ALiBi and Rotary Position Embedding for efficient information compression and retention. Experiments demonstrate that SWAT achieves SOTA performance compared with state-of-the-art linear recurrent architectures on eight benchmarks. Code is available at https://github.com/Fzkuji/swat-attention.",,"Zichuan Fu, Wentao Song, Yejing Wang, Xian Wu, Yefeng Zheng, Yingying Zhang, Derong Xu, Xuetao Wei, Tong Xu, Xiangyu Zhao",2025-06-04T08:36:19Z,Sliding Window Attention Training for Efficient Large Language Models,Schiebefenster Aufmerksamkeitstraining für effiziente große Sprachmodelle,高效大语言模式的缓冲窗口关注培训,http://arxiv.org/abs/2502.18845v2
342,"Large Language Models (LLMs) can generate creative and engaging narratives from user-specified input, but maintaining coherence and emotional depth throughout these AI-generated stories remains a challenge. In this work, we propose SCORE, a framework for Story Coherence and Retrieval Enhancement, designed to detect and resolve narrative inconsistencies. By tracking key item statuses and generating episode summaries, SCORE uses a Retrieval-Augmented Generation (RAG) approach, incorporating TF-IDF and cosine similarity to identify related episodes and enhance the overall story structure. Results from testing multiple LLM-generated stories demonstrate that SCORE significantly improves the consistency and stability of narrative coherence compared to baseline GPT models, providing a more robust method for evaluating and refining AI-generated narratives.",,"Qiang Yi, Yangfan He, Jianhui Wang, Xinyuan Song, Shiyao Qian, Xinhang Yuan, Li Sun, Yi Xin, Keqin Li, Kuan Lu, Menghao Huo, Jiaqi Chen, Tianyu Shi",2025-06-04T08:35:28Z,SCORE: Story Coherence and Retrieval Enhancement for AI Narratives,SCORE: Story-Kohärenz und Retrieval-Verbesserung für KI-Erzählungen,"SCORE: "" 独立叙述 "" 的 "" 一致性 "" 和 "" 检索 "" 增强 "" 增强 "" 统一 "" 和 "" 检索 "" 增强 "" 增强 "" 独立叙述 """,http://arxiv.org/abs/2503.23512v3
343,"This research introduces ScoreRAG, an approach to enhance the quality of automated news generation. Despite advancements in Natural Language Processing and large language models, current news generation methods often struggle with hallucinations, factual inconsistencies, and lack of domain-specific expertise when producing news articles. ScoreRAG addresses these challenges through a multi-stage framework combining retrieval-augmented generation, consistency relevance evaluation, and structured summarization. The system first retrieves relevant news documents from a vector database, maps them to complete news items, and assigns consistency relevance scores based on large language model evaluations. These documents are then reranked according to relevance, with low-quality items filtered out. The framework proceeds to generate graded summaries based on relevance scores, which guide the large language model in producing complete news articles following professional journalistic standards. Through this methodical approach, ScoreRAG aims to significantly improve the accuracy, coherence, informativeness, and professionalism of generated news articles while maintaining stability and consistency throughout the generation process. The code and demo are available at: https://github.com/peiyun2260/ScoreRAG.",,"Pei-Yun Lin, Yen-lung Tsai",2025-06-04T08:35:06Z,ScoreRAG: A Retrieval-Augmented Generation Framework with   Consistency-Relevance Scoring and Structured Summarization for News   Generation,ScoreRAG: Ein Retrieval-Augmented Generation Framework mit Konsistenz-Relevanz-Scoring und strukturierter Zusammenfassung für News Generation,"记分RAG: 具有相联性相关性标注和结构化简编的 "" 新闻一代 "" 检索和增强的 "" 一代 "" 框架",http://arxiv.org/abs/2506.03704v1
344,"Large language models (LLMs) are increasingly used for long-content generation (e.g., long Chain-of-Thought reasoning) where decoding efficiency becomes a critical bottleneck: Autoregressive decoding is inherently limited by its sequential token generation process, where each token must be generated before the next can be processed. This sequential dependency restricts the ability to fully leverage modern hardware's parallel processing capabilities. Existing methods like speculative decoding and layer skipping offer potential speedups but have notable drawbacks: speculative decoding relies on an auxiliary ""drafter"" model, which can be challenging to acquire and increases memory overhead, while layer skipping may introduce discrepancies in the outputs due to the missing key-value cache at skipped layers. In this work, we propose AdaDecode, which accelerates LLM decoding without requiring auxiliary models or changes to the original model parameters, while ensuring output consistency. AdaDecode leverages the insight that many tokens can accurately be generated at intermediate layers, as further layers often do not significantly alter predictions once the model reaches a certain confidence. By adaptively generating tokens at intermediate layers when confidence is high, AdaDecode enables the next token's computation to begin immediately. The remaining layer computations for early-predicted tokens are deferred and executed in parallel with subsequent tokens when needed, maximizing hardware utilization and reducing decoding latency. A final verification step ensures that early predictions match the results of standard autoregressive decoding, preserving output parity. Experiments across diverse generation tasks shows that AdaDecode consistently achieves superior decoding throughput with up to 1.73x speedup, while guaranteeing output parity with standard autoregressive decoding.",,"Zhepei Wei, Wei-Lin Chen, Xinyu Zhu, Yu Meng",2025-06-04T08:32:30Z,AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism,AdaDecode: Beschleunigung der LLM-Dekodierung mit adaptivem Layer-Parallelismus,AdaDecode:加速用适应性层平行法解码LLM,http://arxiv.org/abs/2506.03700v1
345,"Large language models (LLMs) commonly struggle with specialized or emerging topics which are rarely seen in the training corpus. Graph-based retrieval-augmented generation (GraphRAG) addresses this by structuring domain knowledge as a graph for dynamic retrieval. However, existing pipelines involve complex engineering workflows, making it difficult to isolate the impact of individual components. It is also challenging to evaluate the retrieval effectiveness due to the overlap between the pretraining and evaluation datasets. In this work, we introduce ROGRAG, a Robustly Optimized GraphRAG framework. Specifically, we propose a multi-stage retrieval mechanism that integrates dual-level with logic form retrieval methods to improve retrieval robustness without increasing computational cost. To further refine the system, we incorporate various result verification methods and adopt an incremental database construction approach. Through extensive ablation experiments, we rigorously assess the effectiveness of each component. Our implementation includes comparative experiments on SeedBench, where Qwen2.5-7B-Instruct initially underperformed. ROGRAG significantly improves the score from 60.0% to 75.0% and outperforms mainstream methods. Experiments on domain-specific datasets reveal that dual-level retrieval enhances fuzzy matching, while logic form retrieval improves structured reasoning, highlighting the importance of multi-stage retrieval.ROGRAG is released as an open-source resource and supports installation with pip.",,"Zhefan Wang, Huanjun Kong, Jie Ying, Wanli Ouyang, Nanqing Dong",2025-06-04T08:32:02Z,ROGRAG: A Robustly Optimized GraphRAG Framework,ROGRAG: Ein robust optimiertes GraphRAG-Framework,强力优化的图示拉格框架,http://arxiv.org/abs/2503.06474v2
346,"We present PAST, a novel end-to-end framework that jointly models phonetic information alongside signal reconstruction, eliminating the need for external pretrained models. Unlike previous approaches that rely on pretrained self-supervised models, PAST employs supervised phonetic data, directly integrating domain knowledge into the tokenization process via auxiliary tasks. Additionally, we introduce a streamable, causal variant of PAST, enabling real-time speech applications. Results demonstrate that PAST surpasses existing evaluated baseline tokenizers across common evaluation metrics, including phonetic representation and speech reconstruction. Notably, PAST also achieves superior performance when serving as a speech representation for speech language models, further highlighting its effectiveness as a foundation for spoken language generation. To foster further research, we release the full implementation. For code, model checkpoints, and samples see: https://pages.cs.huji.ac.il/adiyoss-lab/PAST",,"Nadav Har-Tuv, Or Tal, Yossi Adi",2025-06-04T08:23:18Z,PAST: Phonetic-Acoustic Speech Tokenizer,PAST: phonetisch-akustische Sprach-Tokenizer,PAST: 音声语音语音调音器,http://arxiv.org/abs/2505.14470v2
347,"The alignment of Large Language Models (LLMs) is crucial for ensuring their safety and reliability in practical applications. Direct Preference Optimization (DPO) has emerged as an efficient method that directly optimizes models using preference pairs, significantly reducing resource demands. However, the effectiveness of DPO heavily depends on the data quality, which is frequently compromised by noise. In this work, we propose $\gamma$-PO, a dynamic target margin preference optimization algorithm that adjust reward margins at the pairwise level. By introducing instance-specific margin calibration, $\gamma$-PO strategically prioritizes high-confidence pairs (those demonstrating higher reward margins) while suppressing potential noise from ambiguous pairs. Moreover, $\gamma$-PO is a plug-and-play method, compatible with variants of DPO that rely on reward margin between preference pairs. Across benchmarks such as AlpacaEval2 and Arena-Hard, $\gamma$-PO achieves an average 4.4\% improvement over other baselines, setting new benchmarks for state-of-the-art performance. Additionally, $\gamma$-PO requires minimal code changes and has a negligible impact on training efficiency, making it a robust solution for enhancing LLMs alignment. Our codes are available at \href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}.",,"Jie Sun, Junkang Wu, Jiancan Wu, Zhibo Zhu, Xingyu Lu, Jun Zhou, Lintao Ma, Xiang Wang",2025-06-04T08:19:37Z,Robust Preference Optimization via Dynamic Target Margins,Robuste Preference-Optimierung über Dynamic Target Margins,通过动态目标边距实现强力优化,http://arxiv.org/abs/2506.03690v1
348,"Fine-tuning pretrained ASR models for specific domains is challenging for small organizations with limited labeled data and computational resources. Here, we explore different data selection pipelines and propose a robust approach that improves ASR adaptation by filtering pseudo-labels generated using Whisper (encoder-decoder) and Zipformer (transducer) models. Our approach integrates multiple selection strategies -- including word error rate (WER) prediction, named entity recognition (NER), and character error rate (CER) analysis -- to extract high-quality training segments. We evaluate our method on Whisper and Zipformer using a 7500-hour baseline, comparing it to a CER-based approach relying on hypotheses from three ASR systems. Fine-tuning on 7500 hours of pseudo-labeled call center data achieves 12.3% WER, while our filtering reduces the dataset to 100 hours (1.4%) with similar performance; a similar trend is observed on Fisher English.",,"Pradeep Rangappa, Andres Carofilis, Jeena Prakash, Shashi Kumar, Sergio Burdisso, Srikanth Madikeri, Esau Villatoro-Tello, Bidisha Sharma, Petr Motlicek, Kadri Hacioglu, Shankar Venkatesan, Saurabh Vyas, Andreas Stolcke",2025-06-04T08:11:24Z,Efficient Data Selection for Domain Adaptation of ASR Using   Pseudo-Labels and Multi-Stage Filtering,Effiziente Datenauswahl für die Domänenanpassung von ASR mittels Pseudo-Labors und Multi-Stage-Filterung,使用优度标签和多层过滤过滤法对 ASR 进行域适应的有效数据选择,http://arxiv.org/abs/2506.03681v1
349,"Large Language Models (LLMs) struggle with complex reasoning due to limited diversity and inefficient search. We propose Soft Reasoning, an embedding-based search framework that optimises the embedding of the first token to guide generation. It combines (1) embedding perturbation for controlled exploration and (2) Bayesian optimisation to refine embeddings via a verifier-guided objective, balancing exploration and exploitation. This approach improves reasoning accuracy and coherence while avoiding reliance on heuristic search. Experiments demonstrate superior correctness with minimal computation, making it a scalable, model-agnostic solution.",,"Qinglin Zhu, Runcong Zhao, Hanqi Yan, Yulan He, Yudong Chen, Lin Gui",2025-06-04T08:11:18Z,Soft Reasoning: Navigating Solution Spaces in Large Language Models   through Controlled Embedding Exploration,Soft Reasoning: Navigieren von Lösungsräumen in großen Sprachmodellen durch kontrollierte Einbettung Exploration,"软原因:通过受控嵌入式勘探,在大语言模型中控制解决方案空间",http://arxiv.org/abs/2505.24688v2
350,"Discourse understanding is essential for many NLP tasks, yet most existing work remains constrained by framework-dependent discourse representations. This work investigates whether large language models (LLMs) capture discourse knowledge that generalizes across languages and frameworks. We address this question along two dimensions: (1) developing a unified discourse relation label set to facilitate cross-lingual and cross-framework discourse analysis, and (2) probing LLMs to assess whether they encode generalizable discourse abstractions. Using multilingual discourse relation classification as a testbed, we examine a comprehensive set of 23 LLMs of varying sizes and multilingual capabilities. Our results show that LLMs, especially those with multilingual training corpora, can generalize discourse information across languages and frameworks. Further layer-wise analyses reveal that language generalization at the discourse level is most salient in the intermediate layers. Lastly, our error analysis provides an account of challenging relation classes.",,"Florian Eichin, Yang Janet Liu, Barbara Plank, Michael A. Hedderich",2025-06-04T08:02:45Z,Probing LLMs for Multilingual Discourse Generalization Through a Unified   Label Set,Probing LLMs für mehrsprachige Diskursverallgemeinerung durch ein einheitliches Label Set,通过统一标签集的多语种论文集一般化探查LLMs,http://arxiv.org/abs/2503.10515v2
351,"Visually impaired people could benefit from Visual Question Answering (VQA) systems to interpret text in their surroundings. However, current models often struggle with recognizing text in the photos taken by this population. Through in-depth interviews with visually impaired individuals, we identified common framing conventions that frequently result in misaligned text. Existing VQA benchmarks primarily feature well-oriented text captured by sighted users, under-representing these challenges. To address this gap, we introduce ROtated SAmpling (ROSA), a decoding strategy that enhances VQA performance in text-rich images with incorrectly oriented text. ROSA outperforms Greedy decoding by 11.7 absolute points in the best-performing model.",,"Hernán Maina, Guido Ivetta, Mateo Lione Stuto, Julian Martin Eisenschlos, Jorge Sánchez, Luciana Benotti",2025-06-04T07:56:13Z,ROSA: Addressing text understanding challenges in photographs via   ROtated SAmpling,ROSA: Textverständnis-Herausforderungen in Fotografien durch ROtated SAmpling adressieren,ROSA:通过ROTTROMPING解决照片中的文本理解挑战,http://arxiv.org/abs/2506.03665v1
352,"Formal logic enables computers to reason in natural language by representing sentences in symbolic forms and applying rules to derive conclusions. However, in what our study characterizes as ""rulebreaker"" scenarios, this method can lead to conclusions that are typically not inferred or accepted by humans given their common sense and factual knowledge. Inspired by works in cognitive science, we create RULEBREAKERS, the first dataset for rigorously evaluating the ability of large language models (LLMs) to recognize and respond to rulebreakers (versus non-rulebreakers) in a human-like manner. Evaluating seven LLMs, we find that most models, including GPT-4o, achieve mediocre accuracy on RULEBREAKERS and exhibit some tendency to over-rigidly apply logical rules unlike what is expected from typical human reasoners. Further analysis suggests that this apparent failure is potentially associated with the models' poor utilization of their world knowledge and their attention distribution patterns. Whilst revealing a limitation of current LLMs, our study also provides a timely counterbalance to a growing body of recent works that propose methods relying on formal logic to improve LLMs' general reasoning capabilities, highlighting their risk of further increasing divergence between LLMs and human-like reasoning.",,"Jason Chan, Robert Gaizauskas, Zhixue Zhao",2025-06-04T07:55:15Z,RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic   and Human-like Reasoning,RULEBREAKERS: Herausfordernde LLMs an der Kreuzung zwischen formaler Logik und menschlicher Vernunft,RULEBRATIERS: 在正式逻辑和类似人类的理由之间的十字路口挑战LLMS,http://arxiv.org/abs/2410.16502v3
353,"Evaluating the alignment capabilities of large Vision-Language Models (VLMs) is essential for determining their effectiveness as helpful assistants. However, existing benchmarks primarily focus on basic abilities using nonverbal methods, such as yes-no and multiple-choice questions. In this paper, we address this gap by introducing AlignMMBench, which provides more nuanced evaluations of alignment capabilities and is the first benchmark specifically designed for Chinese visual contexts. This benchmark is meticulously curated from real-world scenarios and internet sources, encompassing thirteen specific tasks across three categories, and includes both single-turn and multi-turn dialogue scenarios. Incorporating a prompt rewrite strategy, AlignMMBench encompasses 1,054 images and 4,978 question-answer pairs. To facilitate the evaluation pipeline, we develop CritiqueVLM, a rule-calibrated evaluator that exceeds GPT-4's evaluation ability. Additionally, we measure the ""alignment score"", a quantitative metric designed to assess the robustness and stability of models across diverse prompts. Finally, we evaluate the performance of representative VLMs on AlignMMBench, offering insights into the capabilities and limitations of different VLM architectures. The evaluation code and data are available at https://github.com/THUDM/AlignMMBench.",,"Yuhang Wu, Wenmeng Yu, Yean Cheng, Yan Wang, Xiaohan Zhang, Jiazheng Xu, Ming Ding, Yuxiao Dong",2025-06-04T07:53:00Z,AlignMMBench: Evaluating Chinese Multimodal Alignment in Large   Vision-Language Models,AlignMMBench: Bewertung der chinesischen multimodalen Ausrichtung in großen Vision-Sprache-Modellen,""" 平等:在大型愿景-语言模型中评估中国的多模式协调 """,http://arxiv.org/abs/2406.09295v3
354,"Trustworthiness in healthcare question-answering (QA) systems is important for ensuring patient safety, clinical effectiveness, and user confidence. As large language models (LLMs) become increasingly integrated into medical settings, the reliability of their responses directly influences clinical decision-making and patient outcomes. However, achieving comprehensive trustworthiness in medical QA poses significant challenges due to the inherent complexity of healthcare data, the critical nature of clinical scenarios, and the multifaceted dimensions of trustworthy AI. In this survey, we systematically examine six key dimensions of trustworthiness in medical QA, i.e., Factuality, Robustness, Fairness, Safety, Explainability, and Calibration. We review how each dimension is evaluated in existing LLM-based medical QA systems. We compile and compare major benchmarks designed to assess these dimensions and analyze evaluation-guided techniques that drive model improvements, such as retrieval-augmented grounding, adversarial fine-tuning, and safety alignment. Finally, we identify open challenges-such as scalable expert evaluation, integrated multi-dimensional metrics, and real-world deployment studies-and propose future research directions to advance the safe, reliable, and transparent deployment of LLM-powered medical QA.",,"Yinuo Wang, Robert E. Mercer, Frank Rudzicz, Sudipta Singha Roy, Pengjie Ren, Zhumin Chen, Xindi Wang",2025-06-04T07:48:10Z,Trustworthy Medical Question Answering: An Evaluation-Centric Survey,Vertrauenswürdige medizinische Fragebeantwortung: Eine Evaluation-Centric-Umfrage,值得信赖的医学问题回答:评价中心调查,http://arxiv.org/abs/2506.03659v1
355,"The proliferation of misinformation necessitates scalable, automated fact-checking solutions. Yet, current benchmarks often overlook multilingual and topical diversity. This paper introduces a novel, dynamically extensible data set that includes 61,514 claims in multiple languages and topics, extending existing datasets up to 2024. Through a comprehensive evaluation of five prominent Large Language Models (LLMs), including GPT-4o, GPT-3.5 Turbo, LLaMA 3.1, and Mixtral 8x7B, we identify significant performance gaps between different languages and topics. While overall GPT-4o achieves the highest accuracy, it declines to classify 43% of claims. Across all models, factual-sounding claims are misclassified more often than opinions, revealing a key vulnerability. These findings underscore the need for caution and highlight challenges in deploying LLM-based fact-checking systems at scale.",,"Lorraine Saju, Arnim Bleier, Jana Lasser, Claudia Wagner",2025-06-04T07:47:21Z,"Facts are Harder Than Opinions -- A Multilingual, Comparative Analysis   of LLM-Based Fact-Checking Reliability","Fakten sind härter als Meinungen -- Eine mehrsprachige, vergleichende Analyse der LLM-basierten Fact-Checking Reliability","事实是 "" 难于判断 ""  -- -- 以LLM为基础的事实调查可靠性的多语种比较分析",http://arxiv.org/abs/2506.03655v1
356,"Event Causality Identification (ECI) has emerged as a pivotal task in natural language processing (NLP), aimed at automatically detecting causal relationships between events in text. In this comprehensive survey, we systematically elucidate the foundational principles and technical frameworks of ECI, proposing a novel classification framework to categorize and clarify existing methods. {We discuss associated challenges, provide quantitative evaluations, and outline future directions for this dynamic and rapidly evolving field. We first delineate key definitions, problem formalization, and evaluation protocols of ECI. Our classification framework organizes ECI methods based on two primary tasks: Sentence-level Event Causality Identification (SECI) and Document-level Event Causality Identification (DECI). For SECI, we review methods including feature pattern-based matching, machine learning-based classification, deep semantic encoding, prompt-based fine-tuning, and causal knowledge pre-training, alongside common data augmentation strategies. For DECI, we focus on techniques such as deep semantic encoding, event graph reasoning, and prompt-based fine-tuning. We dedicate specific discussions to advancements in multi-lingual and cross-lingual ECI as well as zero-shot ECI leveraging Large Language Models (LLMs). Furthermore, we analyze the strengths, limitations, and unresolved challenges of each method. Extensive quantitative evaluations are conducted on four benchmark datasets to assess various ECI methods. Finally, we explore future research directions.",,"Qing Cheng, Zefan Zeng, Xingchen Hu, Yuehang Si, Zhong Liu",2025-06-04T07:35:00Z,"A Survey of Event Causality Identification: Taxonomy, Challenges,   Assessment, and Prospects","Eine Umfrage über die Kausalitätsidentifizierung: Taxonomie, Herausforderungen, Bewertung und Perspektiven",事件原因识别调查:分类、挑战、评估和前景,http://arxiv.org/abs/2411.10371v3
357,"Reward Models, essential for guiding Large Language Model optimization, are typically trained on fixed preference datasets, resulting in rigid alignment to single, implicit preference distributions. This prevents adaptation to diverse real-world needs-from conciseness in one task to detailed explanations in another. The standard practice of collecting task-specific preference data and retraining reward models is resource-intensive, often producing biased rewards, and limits practical application. We introduce generalizable, principle-following reward models. We propose that RMs should understand and adhere to dynamically provided natural language specifications of reward principles, similar to instruction-following in LLMs. To measure this capability, we develop RABench, a comprehensive benchmark for RMs focusing on generalization across diverse principles. Evaluations on RABench reveal poor generalization of current RMs. As a solution, we present RewardAnything, a novel RM designed and trained to explicitly follow natural language principles. We achieve SotA performance with RewardAnything in traditional RM benchmark simply by specifying a well-defined principle, and results on RABench show we excel in adapting to novel principles without retraining. Furthermore, RewardAnything integrates seamlessly with existing RLHF methods and we show by a case study on how to automatically and efficiently align LLMs with only natural language principles.",,"Zhuohao Yu, Jiali Zeng, Weizheng Gu, Yidong Wang, Jindong Wang, Fandong Meng, Jie Zhou, Yue Zhang, Shikun Zhang, Wei Ye",2025-06-04T07:30:16Z,RewardAnything: Generalizable Principle-Following Reward Models,BelohnungAlles: Allgemeines Prinzip-folgende Belohnung Modelle,奖励任何奖励:可普遍适用的遵循原则的奖赏模式,http://arxiv.org/abs/2506.03637v1
358,"The advancement of Large Vision-Language Models (LVLMs) has propelled their application in the medical field. However, Medical LVLMs (Med-LVLMs) encounter factuality challenges due to modality misalignment, where the models prioritize textual knowledge over visual input, leading to hallucinations that contradict information in medical images. Previous attempts to enhance modality alignment in Med-LVLMs through preference optimization have inadequately mitigated clinical relevance in preference data, making these samples easily distinguishable and reducing alignment effectiveness. To address this challenge, we propose MMedPO, a novel multimodal medical preference optimization approach that considers the clinical relevance of preference samples to enhance Med-LVLM alignment. MMedPO curates multimodal preference data by introducing two types of dispreference: (1) plausible hallucinations injected through target Med-LVLMs or GPT-4o to produce medically inaccurate responses, and (2) lesion region neglect achieved through local lesion-noising, disrupting visual understanding of critical areas. We then calculate clinical relevance for each sample based on scores from multiple Med-LLMs and visual tools, and integrate these scores into the preference optimization process as weights, enabling effective alignment. Our experiments demonstrate that MMedPO significantly enhances factual accuracy in Med-LVLMs, achieving substantial improvements over existing preference optimization methods by averaging 14.2% and 51.7% across the Med-VQA and report generation tasks. Our code are available in https://github.com/aiming-lab/MMedPO.",,"Kangyu Zhu, Peng Xia, Yun Li, Hongtu Zhu, Sheng Wang, Huaxiu Yao",2025-06-04T07:13:53Z,MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware   Multimodal Preference Optimization,MMedPO: Ausrichtung medizinischer Vision-Sprachenmodelle mit klinisch-aware multimodaler Preference-Optimierung,MMEDPO: 使医疗视觉-语言模型与临床-软件多式优惠优化相结合,http://arxiv.org/abs/2412.06141v4
359,"Large Language Models (LLMs) have demonstrated remarkable performance across various tasks by effectively utilizing a prompting strategy. However, they are highly sensitive to input perturbations, such as typographical errors or slight character order errors, which can substantially degrade their performance. Despite advances in prompting techniques, developing a prompting strategy that explicitly mitigates the negative impact of such perturbations remains an open challenge. To bridge this gap, we propose Robustness of Prompting (RoP), a novel prompting strategy specifically designed to enhance the robustness of LLMs. RoP consists of two stages: Error Correction and Guidance. In the Error Correction stage, RoP applies diverse perturbation methods to generate adversarial examples, which are then used to construct prompts that automatically correct input errors. In the Guidance stage, RoP generates an optimal guidance prompting based on the corrected input, steering the model toward more robust and accurate inferences. Through comprehensive experiments spanning arithmetic, commonsense, and logical reasoning tasks, we demonstrate that RoP significantly improves LLMs' robustness against adversarial perturbations. Notably, it maintains model accuracy with only minimal degradation compared to clean input scenarios, thereby establishing RoP as a practical and effective approach for enhancing LLM robustness in real-world applications.",,"Lin Mu, Guowei Chu, Li Ni, Lei Sang, Zhize Wu, Peiquan Jin, Yiwen Zhang",2025-06-04T07:13:27Z,Robustness of Prompting: Enhancing Robustness of Large Language Models   Against Prompting Attacks,Robustheit von Prompting: Steigerung der Robustheit von großen Sprachmodellen gegen prompting Angriffe,迅速的有力性:加强反对迅速攻击的大语言模式的有力性,http://arxiv.org/abs/2506.03627v1
360,"Spoken dialogue modeling poses challenges beyond text-based language modeling, requiring real-time interaction, turn-taking, and backchanneling. While most Spoken Dialogue Models (SDMs) operate in half-duplex mode-processing one turn at a time - emerging full-duplex SDMs can listen and speak simultaneously, enabling more natural conversations. However, current evaluations remain limited, focusing mainly on turn-based metrics or coarse corpus-level analyses. To address this, we introduce Full-Duplex-Bench, a benchmark that systematically evaluates key interactive behaviors: pause handling, backchanneling, turn-taking, and interruption management. Our framework uses automatic metrics for consistent, reproducible assessment and provides a fair, fast evaluation setup. By releasing our benchmark and code, we aim to advance spoken dialogue modeling and foster the development of more natural and engaging SDMs.",,"Guan-Ting Lin, Jiachen Lian, Tingle Li, Qirui Wang, Gopala Anumanchipalli, Alexander H. Liu, Hung-yi Lee",2025-06-04T07:11:15Z,Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue   Models on Turn-taking Capabilities,Full-Duplex-Bench: Ein Benchmark zur Bewertung von Full-Duplex-Gesprochenen Dialogmodellen über Turn-Take-Fähigkeiten,"全面时间框架:评估关于 "" 转手能力 "" 的全面、多元、口腔对话模式的基准",http://arxiv.org/abs/2503.04721v2
361,"Although Large Language Models (LLMs) have demonstrated strong language understanding and generation abilities across various languages, their cultural knowledge is often limited to English-speaking communities, which can marginalize the cultures of non-English communities. To address the problem, evaluation of the cultural awareness of the LLMs and the methods to develop culturally aware LLMs have been investigated. In this study, we focus on evaluating knowledge of folktales, a key medium for conveying and circulating culture. In particular, we focus on Japanese folktales, specifically on knowledge of Yokai. Yokai are supernatural creatures originating from Japanese folktales that continue to be popular motifs in art and entertainment today. Yokai have long served as a medium for cultural expression, making them an ideal subject for assessing the cultural awareness of LLMs. We introduce YokaiEval, a benchmark dataset consisting of 809 multiple-choice questions (each with four options) designed to probe knowledge about yokai. We evaluate the performance of 31 Japanese and multilingual LLMs on this dataset. The results show that models trained with Japanese language resources achieve higher accuracy than English-centric models, with those that underwent continued pretraining in Japanese, particularly those based on Llama-3, performing especially well. The code and dataset are available at https://github.com/CyberAgentA ILab/YokaiEval.",,"Ayuto Tsutsumi, Yuu Jinnai",2025-06-04T06:58:19Z,Do Large Language Models Know Folktales? A Case Study of Yokai in   Japanese Folktales,Kennen große Sprachmodelle Folktales? Eine Fallstudie über Yokai in japanischen Folktalen,《大语言模型了解民俗吗? 日本民俗的横开案例研究》。,http://arxiv.org/abs/2506.03619v1
362,"To enhance reasoning capabilities, previous works have explored incorporating special-purpose tokens into the training process. These strategies strengthen the learning mechanism of transformer-based large language models (LLMs). Building on prior research, in which inserting dummy tokens consecutively just before reasoning steps can enhance effectiveness, we introduce a novel approach termed Dynamic Inserting Tokens Training (DIT). Our method identifies positions within sequences where model confidence is lowest according to token log-likelihood. Strategically inserting [PAUSE] tokens on these positions bolsters the model's predictive capabilities for subsequent tokens. Experimental results across diverse datasets and models, from the 2.7B model to the 8B model, demonstrate that DIT consistently outperforms traditional fine-tuning and previous token insertion methods. With this simple yet effective method, we achieve accuracy gains of up to 4.7%p on GSM8K, 3.23%p on AQUA-RAT, and pass@1 improvements of up to 3.4%p on MBPP datasets. Our work shows a model-based, dynamic approach rather than a heuristic one, thereby broadening the scope of research in reasoning.",,"Eunki Kim, Sangryul Kim, James Thorne",2025-06-04T06:48:41Z,Learning to Insert [PAUSE] Tokens for Better Reasoning,Einfügen von [PAUSE] Token für eine bessere Vernunft lernen,学习插入 [PAUSE] 代号以更好地说明理由,http://arxiv.org/abs/2506.03616v1
363,"One way to mitigate risks in vision-language models (VLMs) is to remove dangerous samples in their training data. However, such data moderation can be easily bypassed when harmful images are split into small, benign-looking patches, scattered across many training samples. VLMs may then learn to piece these fragments together during training and generate harmful responses at inference, either from full images or text references. For instance, if trained on image patches from a bloody scene paired with the descriptions ""safe,"" VLMs may later describe, the full image or a text reference to the scene, as ""safe."" We define the core ability of VLMs enabling this attack as $\textit{visual stitching}$ -- the ability to integrate visual information spread across multiple training samples that share the same textual descriptions. In our work, we first demonstrate visual stitching abilities in common open-source VLMs on three datasets where each image is labeled with a unique synthetic ID: we split each $(\texttt{image}, \texttt{ID})$ pair into $\{(\texttt{patch}, \texttt{ID})\}$ pairs at different granularity for finetuning, and we find that tuned models can verbalize the correct IDs from full images or text reference. Building on this, we simulate the adversarial data poisoning scenario mentioned above by using patches from dangerous images and replacing IDs with text descriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can evade moderation in patches and later be reconstructed through visual stitching, posing serious VLM safety risks. Code is available at https://github.com/ZHZisZZ/visual-stitching.",,"Zhanhui Zhou, Lingjie Chen, Chao Yang, Chaochao Lu",2025-06-04T06:46:06Z,VLMs Can Aggregate Scattered Training Patches,VLMs können gestreute Trainingspatches aggregatieren,VLMs Can Can 集散培训补丁,http://arxiv.org/abs/2506.03614v1
364,"This study explores the use of self-supervised learning (SSL) models for tone recognition in three low-resource languages from North Eastern India: Angami, Ao, and Mizo. We evaluate four Wav2vec2.0 base models that were pre-trained on both tonal and non-tonal languages. We analyze tone-wise performance across the layers for all three languages and compare the different models. Our results show that tone recognition works best for Mizo and worst for Angami. The middle layers of the SSL models are the most important for tone recognition, regardless of the pre-training language, i.e. tonal or non-tonal. We have also found that the tone inventory, tone types, and dialectal variations affect tone recognition. These findings provide useful insights into the strengths and weaknesses of SSL-based embeddings for tonal languages and highlight the potential for improving tone recognition in low-resource settings. The source code is available at GitHub 1 .",,"Parismita Gogoi, Sishir Kalita, Wendy Lalhminghlui, Viyazonuo Terhiija, Moakala Tzudir, Priyankoo Sarmah, S. R. M. Prasanna",2025-06-04T06:32:12Z,Tone recognition in low-resource languages of North-East India: peeling   the layers of SSL-based speech models,Tonerkennung in ressourcenarmen Sprachen Nordostindiens: Peeling der Schichten von SSL-basierten Sprachmodellen,印度东北部低资源语言的声调承认:剥去以SSL为基础的演讲模式的层层,http://arxiv.org/abs/2506.03606v1
365,"Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to enhance Multimodal Large Language Models by incorporating externally retrieved multimodal knowledge, but it introduces two challenges: Parametric-Retrieved Knowledge Inconsistency (PRKI), where discrepancies between parametric and retrieved knowledge create uncertainty in determining reliability, and Visual-Textual Knowledge Inconsistency (VTKI), where misalignment between visual and textual sources disrupts entity representation. To address these challenges, we propose Cross-source knowledge \textbf{Re}conciliation for Multimodal RAG (CoRe-MMRAG), a novel end-to-end framework that effectively reconciles inconsistencies across knowledge sources. CoRe-MMRAG follows a four-stage pipeline: it first generates an internal response from parametric knowledge, then selects the most relevant multimodal evidence via joint similarity assessment, generates an external response, and finally integrates both to produce a reliable answer. Additionally, a specialized training paradigm enhances knowledge source discrimination, multimodal integration, and unified answer generation. Experiments on KB-VQA benchmarks show that CoRe-MMRAG achieves substantial improvements over baseline methods, achieving 5.6% and 9.3% performance gains on InfoSeek and Encyclopedic-VQA, respectively.",,"Yang Tian, Fan Liu, Jingyuan Zhang, Victoria W., Yupeng Hu, Liqiang Nie",2025-06-04T06:31:54Z,CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG,CoRe-MMRAG: Quelleübergreifende Wissensversöhnung für multimodale RAG,Corre-MMRAG:多式RAG跨源知识调和,http://arxiv.org/abs/2506.02544v2
366,"As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users and developers, it also brings underexplored safety risks. Its decentralized architecture, which separates clients and servers, poses unique challenges for systematic safety analysis. This paper proposes a novel framework to enhance MCP safety. Guided by the MAESTRO framework, we first analyze the missing safety mechanisms in MCP, and based on this analysis, we propose the Model Contextual Integrity Protocol (MCIP), a refined version of MCP that addresses these gaps. Next, we develop a fine-grained taxonomy that captures a diverse range of unsafe behaviors observed in MCP scenarios. Building on this taxonomy, we develop benchmark and training data that support the evaluation and improvement of LLMs' capabilities in identifying safety risks within MCP interactions. Leveraging the proposed benchmark and training data, we conduct extensive experiments on state-of-the-art LLMs. The results highlight LLMs' vulnerabilities in MCP interactions and demonstrate that our approach substantially improves their safety performance.",,"Huihao Jing, Haoran Li, Wenbin Hu, Qi Hu, Heli Xu, Tianshu Chu, Peizhao Hu, Yangqiu Song",2025-06-04T06:21:46Z,MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol,MCIP: Schutz der MCP-Sicherheit durch modellbezogenes Integritätsprotokoll,MCIP: 通过示范背景廉正议定书保护MCP安全,http://arxiv.org/abs/2505.14590v4
367,"Steerability, or the ability of large language models (LLMs) to adapt outputs to align with diverse community-specific norms, perspectives, and communication styles, is critical for real-world applications but remains under-evaluated. We introduce Steer-Bench, a benchmark for assessing population-specific steering using contrasting Reddit communities. Covering 30 contrasting subreddit pairs across 19 domains, Steer-Bench includes over 10,000 instruction-response pairs and validated 5,500 multiple-choice question with corresponding silver labels to test alignment with diverse community norms. Our evaluation of 13 popular LLMs using Steer-Bench reveals that while human experts achieve an accuracy of 81% with silver labels, the best-performing models reach only around 65% accuracy depending on the domain and configuration. Some models lag behind human-level alignment by over 15 percentage points, highlighting significant gaps in community-sensitive steerability. Steer-Bench is a benchmark to systematically assess how effectively LLMs understand community-specific instructions, their resilience to adversarial steering attempts, and their ability to accurately represent diverse cultural and ideological perspectives.",,"Kai Chen, Zihao He, Taiwei Shi, Kristina Lerman",2025-06-04T06:18:10Z,STEER-BENCH: A Benchmark for Evaluating the Steerability of Large   Language Models,STEER-BENCH: Benchmark für die Bewertung der Steerability von großen Sprachmodellen,STEER-BENCH:评估大语言模型可耐性的基准,http://arxiv.org/abs/2505.20645v2
368,"Speculative decoding accelerates large language model (LLM) inference by using a smaller draft model to propose tokens, which are then verified by a larger target model. However, selecting an optimal speculation length is critical for maximizing speedup while minimizing wasted computation. We introduce \textit{GammaTune} and \textit{GammaTune+}, training-free adaptive algorithms that dynamically adjust speculation length based on token acceptance rates using a heuristic-based switching mechanism. Evaluated on SpecBench across multiple tasks and model pairs, our method outperforms other heuristic-based approaches and fixed-length speculative decoding, achieving an average speedup of 15\% ($\pm$5\%) with \textit{GammaTune} and 16\% ($\pm$3\%) with \textit{GammaTune+}, while reducing performance variance. This makes \textit{GammaTune} a robust and efficient solution for real-world deployment.",,"Aayush Gautam, Susav Shrestha, Narasimha Reddy",2025-06-04T06:07:17Z,Token-Driven GammaTune: Adaptive Calibration for Enhanced Speculative   Decoding,Token-Driven GammaTune: Adaptive Kalibrierung für verbesserte spekulative Dekodierung,Token- Driven GammaTune: 用于增强投机性代号的适应性校准,http://arxiv.org/abs/2504.00030v3
369,"The remarkable success of transformers in the field of natural language processing has sparked the interest of the speech-processing community, leading to an exploration of their potential for modeling long-range dependencies within speech sequences. Recently, transformers have gained prominence across various speech-related domains, including automatic speech recognition, speech synthesis, speech translation, speech para-linguistics, speech enhancement, spoken dialogue systems, and numerous multimodal applications. In this paper, we present a comprehensive survey that aims to bridge research studies from diverse subfields within speech technology. By consolidating findings from across the speech technology landscape, we provide a valuable resource for researchers interested in harnessing the power of transformers to advance the field. We identify the challenges encountered by transformers in speech processing while also offering insights into potential solutions to address these issues.",,"Siddique Latif, Aun Zaidi, Heriberto Cuayahuitl, Fahad Shamshad, Moazzam Shoukat, Muhammad Usama, Junaid Qadir",2025-06-04T06:06:21Z,Transformers in Speech Processing: A Survey,Transformer in der Sprachverarbeitung: Eine Umfrage,语音处理变换器:调查,http://arxiv.org/abs/2303.11607v2
370,"Using the best Text-to-SQL methods in resource-constrained environments is challenging due to their reliance on resource-intensive open-source models. This paper introduces Auto Prompt SQL(AP-SQL), a novel architecture designed to bridge the gap between resource-efficient small open-source models and the powerful capabilities of large closed-source models for Text-to-SQL translation. Our method decomposes the task into schema filtering, retrieval-augmented text-to-SQL generation based on in-context examples, and prompt-driven schema linking and SQL generation. To improve schema selection accuracy, we fine-tune large language models. Crucially, we also explore the impact of prompt engineering throughout the process, leveraging Chain-of-Thought(CoT) and Graph-of-Thought(GoT) templates to significantly enhance the model's reasoning for accurate SQL generation. Comprehensive evaluations on the Spider benchmarks demonstrate the effectiveness of AP-SQL.",,"Zetong Tang, Qian Ma, Di Wu",2025-06-04T06:04:46Z,Auto prompt sql: a resource-efficient architecture for text-to-sql   translation in constrained environments,Auto prompt sql: eine ressourceneffiziente Architektur für Text-zu-Sql-Übersetzung in eingeschränkten Umgebungen,自动快速 sql : 一个在受限制环境中用于文本到 sql 翻译的资源效率高的架构,http://arxiv.org/abs/2506.03598v1
371,"Language models often exhibit undesirable behavior, e.g., generating toxic or gender-biased text. In the case of neural language models, an encoding of the undesirable behavior is often present in the model's representations. Thus, one natural (and common) approach to prevent the model from exhibiting undesirable behavior is to steer the model's representations in a manner that reduces the probability of it generating undesirable text. This paper investigates the formal and empirical properties of steering functions, i.e., transformation of the neural language model's representations that alter its behavior. First, we derive two optimal, in the least-squares sense, affine steering functions under different constraints. Our theory provides justification for existing approaches and offers a novel, improved steering approach. Second, we offer a series of experiments that demonstrate the empirical effectiveness of the methods in mitigating bias and reducing toxic generation.",,"Shashwat Singh, Shauli Ravfogel, Jonathan Herzig, Roee Aharoni, Ryan Cotterell, Ponnurangam Kumaraguru",2025-06-04T06:03:19Z,Representation Surgery: Theory and Practice of Affine Steering,Repräsentationschirurgie: Theorie und Praxis der Affine-Lenkung,代表外科:亲子指导理论和实践,http://arxiv.org/abs/2402.09631v7
372,"Understanding the relationship between data compression and the capabilities of Large Language Models (LLMs) is crucial, especially in specialized domains like code intelligence. Prior work posited a linear relationship between compression and general intelligence. However, it overlooked the multifaceted nature of code that encompasses diverse programming languages and tasks, and struggled with fair evaluation of modern Code LLMs. We address this by evaluating a diverse array of open-source Code LLMs on comprehensive multi-language, multi-task code benchmarks. To address the challenge of efficient and fair evaluation of pre-trained LLMs' code intelligence, we introduce \textit{Format Annealing}, a lightweight, transparent training methodology designed to assess the intrinsic capabilities of these pre-trained models equitably. Compression efficacy, measured as bits-per-character (BPC), is determined using a novel, large-scale, and previously unseen code validation set derived from GitHub. Our empirical results reveal a fundamental logarithmic relationship between measured code intelligence and BPC. This finding refines prior hypotheses of linearity, which we suggest are likely observations of the logarithmic curve's tail under specific, limited conditions. Our work provides a more nuanced understanding of compression's role in developing code intelligence and contributes a robust evaluation framework in the code domain.",,"Xianzhen Luo, Shijie Xuyang, Tianhao Cheng, Zheng Chu, Houyi Li, ziqi wang, Siming Huang, Qingfu Zhu, Qiufeng Wang, Xiangyu Zhang, Shuigeng Zhou, Wanxiang Che",2025-06-04T06:01:48Z,Is Compression Really Linear with Code Intelligence?,Ist Kompression wirklich linear mit Code Intelligence?,压缩真的有代码情报线条吗?,http://arxiv.org/abs/2505.11441v3
373,"This paper presents a systematic evaluation of Large Language Models' (LLMs) behavior on long-tail distributed (encrypted) texts and their safety implications. We introduce a two-dimensional framework for assessing LLM safety: (1) instruction refusal-the ability to reject harmful obfuscated instructions, and (2) generation safety-the suppression of generating harmful responses. Through comprehensive experiments, we demonstrate that models that possess capabilities to decrypt ciphers may be susceptible to mismatched-generalization attacks: their safety mechanisms fail on at least one safety dimension, leading to unsafe responses or over-refusal. Based on these findings, we evaluate a number of pre-LLM and post-LLM safeguards and discuss their strengths and limitations. This work contributes to understanding the safety of LLM in long-tail text scenarios and provides directions for developing robust safety mechanisms.",,"Utsav Maskey, Mark Dras, Usman Naseem",2025-06-04T05:56:40Z,Should LLM Safety Be More Than Refusing Harmful Instructions?,Sollte LLM Sicherheit mehr als nur eine Verweigerung schädlicher Anweisungen sein?,LLM 安全是否应该比拒绝有害指令更安全?,http://arxiv.org/abs/2506.02442v2
374,"In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large Language Model (MLLM) that integrates visual instruction tuning with masked diffusion models, representing a departure from the autoregressive paradigms dominant in current multimodal approaches. Built upon LLaDA, a representative large language diffusion model, LLaDA-V incorporates a vision encoder and MLP connector that projects visual features into the language embedding space, enabling effective multimodal alignment. Our empirical investigation reveals several intriguing results: First, LLaDA-V demonstrates promising multimodal performance despite its language model being weaker on purely textual tasks than counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same instruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal tasks with better data scalability. It also narrows the performance gap to Qwen2-VL, suggesting the effectiveness of its architecture for multimodal tasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal understanding compared to existing hybrid autoregressive-diffusion and purely diffusion-based MLLMs. Our findings suggest that large language diffusion models show promise in multimodal contexts and warrant further investigation in future research. Project page and codes: https://ml-gsai.github.io/LLaDA-V-demo/.",,"Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, Chongxuan Li",2025-06-04T05:52:10Z,LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning,LLaDA-V: Große Sprachdiffusionsmodelle mit visueller Instruction Tuning,LLADA-V:具有视觉教学图示的大型语言传播模型,http://arxiv.org/abs/2505.16933v2
375,"Data augmentation, a widely-employed technique for addressing data scarcity, involves generating synthetic data examples which are then used to augment available training data. Researchers have seen surprising success from simple methods, such as random perturbations from natural examples, where models seem to benefit even from data with nonsense words, or data that doesn't conform to the rules of the language. A second line of research produces synthetic data that does in fact follow all linguistic constraints; these methods require some linguistic expertise and are generally more challenging to implement. No previous work has done a systematic, empirical comparison of both linguistically-naive and linguistically-motivated data augmentation strategies, leaving uncertainty about whether the additional time and effort of linguistically-motivated data augmentation work in fact yields better downstream performance.   In this work, we conduct a careful and comprehensive comparison of augmentation strategies (both linguistically-naive and linguistically-motivated) for two low-resource languages with different morphological properties, Uspanteko and Arapaho. We evaluate the effectiveness of many different strategies and their combinations across two important sequence-to-sequence tasks for low-resource languages: machine translation and interlinear glossing. We find that linguistically-motivated strategies can have benefits over naive approaches, but only when the new examples they produce are not significantly unlike the training data distribution.",,"Ray Groshan, Michael Ginn, Alexis Palmer",2025-06-04T05:48:20Z,Is linguistically-motivated data augmentation worth it?,Lohnt sich die linguistisch motivierte Datenvergrößerung?,以语言为动机的数据扩充值得吗?,http://arxiv.org/abs/2506.03593v1
376,"Iterative evaluation of LLMs during training is essential to ensure expected capability development, but can be time- and compute-intensive. While NLU tasks, where the model selects from fixed answer choices, are cheap to evaluate, essential capabilities like reasoning and code generation rely on the more time-consuming NLG (token-by-token generation) format. In this work, our aim is to decrease the computational burden of NLG benchmarks in order to enable monitoring crucial LLM capabilities during model training. We reformulate generative tasks into computationally cheaper NLU alternatives. We test the performance correlation between the original and reformulated tasks using 8 LMs of various sizes and 4 capabilities: mathematical reasoning, code generation, factual knowledge and reading comprehension. Our results show a strong correlation between task formats, supporting capability assessment via cheaper alternatives and achieving over 35x average reduction in evaluation time. We plan to publish our benchmark adaptions.",,"Viktor Hangya, Fabian Küch, Darina Gold",2025-06-04T05:46:40Z,From Understanding to Generation: An Efficient Shortcut for Evaluating   Language Models,Vom Verständnis zur Generation: Ein effizienter Shortcut zur Bewertung von Sprachmodellen,从理解到生成:用于评价语言模式的高效快捷键,http://arxiv.org/abs/2506.03592v1
377,"Text-video retrieval (TVR) systems often suffer from visual-linguistic biases present in datasets, which cause pre-trained vision-language models to overlook key details. To address this, we propose BiMa, a novel framework designed to mitigate biases in both visual and textual representations. Our approach begins by generating scene elements that characterize each video by identifying relevant entities/objects and activities. For visual debiasing, we integrate these scene elements into the video embeddings, enhancing them to emphasize fine-grained and salient details. For textual debiasing, we introduce a mechanism to disentangle text features into content and bias components, enabling the model to focus on meaningful content while separately handling biased information. Extensive experiments and ablation studies across five major TVR benchmarks (i.e., MSR-VTT, MSVD, LSMDC, ActivityNet, and DiDeMo) demonstrate the competitive performance of BiMa. Additionally, the model's bias mitigation capability is consistently validated by its strong results on out-of-distribution retrieval tasks.",,"Huy Le, Nhat Chung, Tung Kieu, Anh Nguyen, Ngan Le",2025-06-04T05:40:54Z,BiMa: Towards Biases Mitigation for Text-Video Retrieval via Scene   Element Guidance,BiMa: Auf dem Weg zu Biases Milderung für Text-Video-Retrieval über Szeneelement-Anleitung,"Bima:通过场景元素指导,争取减缓对文本视频检索的“双轨减缓”",http://arxiv.org/abs/2506.03589v1
378,"Lexical Semantic Change (LSC) provides insight into cultural and social dynamics. Yet, the validity of methods for measuring different kinds of LSC remains unestablished due to the absence of historical benchmark datasets. To address this gap, we propose LSC-Eval, a novel three-stage general-purpose evaluation framework to: (1) develop a scalable methodology for generating synthetic datasets that simulate theory-driven LSC using In-Context Learning and a lexical database; (2) use these datasets to evaluate the sensitivity of computational methods to synthetic change; and (3) assess their suitability for detecting change in specific dimensions and domains. We apply LSC-Eval to simulate changes along the Sentiment, Intensity, and Breadth (SIB) dimensions, as defined in the SIBling framework, using examples from psychology. We then evaluate the ability of selected methods to detect these controlled interventions. Our findings validate the use of synthetic benchmarks, demonstrate that tailored methods effectively detect changes along SIB dimensions, and reveal that a state-of-the-art LSC model faces challenges in detecting affective dimensions of LSC. LSC-Eval offers a valuable tool for dimension- and domain-specific benchmarking of LSC methods, with particular relevance to the social sciences.",,"Naomi Baes, Raphaël Merx, Nick Haslam, Ekaterina Vylomova, Haim Dubossarsky",2025-06-04T05:38:11Z,LSC-Eval: A General Framework to Evaluate Methods for Assessing   Dimensions of Lexical Semantic Change Using LLM-Generated Synthetic Data,LSC-Eval: Ein allgemeiner Rahmen zur Bewertung von Methoden zur Bewertung von Dimensionen der Lexikalen Semantischen Veränderung mittels LLM-generierter synthetischer Daten,LSC-Eval:利用LLM光导合成数据评价评估法理语义变化所涉方方面面的方法的一般框架,http://arxiv.org/abs/2503.08042v2
379,"The rapid growth of scholarly literature makes it increasingly difficult for researchers to keep up with new knowledge. Automated tools are now more essential than ever to help navigate and interpret this vast body of information. Scientific papers pose unique difficulties, with their complex language, specialized terminology, and diverse formats, requiring advanced methods to extract reliable and actionable insights. Large language models (LLMs) offer new opportunities, enabling tasks such as literature reviews, writing assistance, and interactive exploration of research. This special issue of the TAL journal highlights research addressing these challenges and, more broadly, research on natural language processing and information retrieval for scholarly and scientific documents.",,"Florian Boudin, Akiko Aizawa",2025-06-04T05:35:39Z,Preface to the Special Issue of the TAL Journal on Scholarly Document   Processing,Vorwort zur Sonderausgabe des TAL-Journals zur wissenschaftlichen Dokumentenverarbeitung,《关于学术性文件处理的TAL杂志》特别版序言,http://arxiv.org/abs/2506.03587v1
380,"Language Models (LMs) are typically tuned with human preferences to produce helpful responses, but the impact of preference tuning on the ability to handle culturally diverse queries remains understudied. In this paper, we systematically analyze how native human cultural preferences can be incorporated into the preference learning process to train more culturally aware LMs. We introduce \textbf{CARE}, a multilingual resource containing 3,490 culturally specific questions and 31.7k responses with native judgments. We demonstrate how a modest amount of high-quality native preferences improves cultural awareness across various LMs, outperforming larger generic preference data. Our analyses reveal that models with stronger initial cultural performance benefit more from alignment, leading to gaps among models developed in different regions with varying access to culturally relevant data. CARE will be made publicly available at https://github.com/Guochry/CARE.",,"Geyang Guo, Tarek Naous, Hiromi Wakaki, Yukiko Nishimura, Yuki Mitsufuji, Alan Ritter, Wei Xu",2025-06-04T05:22:27Z,CARE: Assessing the Impact of Multilingual Human Preference Learning on   Cultural Awareness,CARE: Bewertung der Auswirkungen des Mehrsprachigkeitslernens von Menschen auf das kulturelle Bewusstsein,CARE:评估多语言人类优先学习对文化意识的影响,http://arxiv.org/abs/2504.05154v3
381,"Providing example sentences that are diverse and aligned with learners' proficiency levels is essential for fostering effective language acquisition. This study examines the use of Pre-trained Language Models (PLMs) to produce example sentences targeting L2 Japanese learners. We utilize PLMs in two ways: as quality scoring components in a retrieval system that draws from a newly curated corpus of Japanese sentences, and as direct sentence generators using zero-shot learning. We evaluate the quality of sentences by considering multiple aspects such as difficulty, diversity, and naturalness, with a panel of raters consisting of learners of Japanese, native speakers -- and GPT-4. Our findings suggest that there is inherent disagreement among participants on the ratings of sentence qualities, except for difficulty. Despite that, the retrieval approach was preferred by all evaluators, especially for beginner and advanced target proficiency, while the generative approaches received lower scores on average. Even so, our experiments highlight the potential for using PLMs to enhance the adaptability of sentence suggestion systems and therefore improve the language learning journey.",,"Enrico Benedetti, Akiko Aizawa, Florian Boudin",2025-06-04T05:13:05Z,Automatically Suggesting Diverse Example Sentences for L2 Japanese   Learners Using Pre-Trained Language Models,Automatischer Vorschlag für verschiedene Beispiele Sätze für L2 Japanische Lernende mit vortrainierten Sprachmodellen,使用培训前语言模式的L2日本学生自动建议多种不同的示例句,http://arxiv.org/abs/2506.03580v1
382,"Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the potential of Reinforcement Learning (RL) to enhance reasoning abilities in Large Language Models (LLMs). While open-source replication efforts have primarily focused on mathematical and coding domains, methods and resources for developing general reasoning capabilities remain underexplored. This gap is partly due to the challenge of collecting diverse and verifiable reasoning data suitable for RL. We hypothesize that logical reasoning is critical for developing general reasoning capabilities, as logic forms a fundamental building block of reasoning. In this work, we present SynLogic, a data synthesis framework and dataset that generates diverse logical reasoning data at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic approach enables controlled synthesis of data with adjustable difficulty and quantity. Importantly, all examples can be verified by simple rules, making them ideally suited for RL with verifiable rewards. In our experiments, we validate the effectiveness of RL training on the SynLogic dataset based on 7B and 32B models. SynLogic leads to state-of-the-art logical reasoning performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and coding tasks improves the training efficiency of these domains and significantly enhances reasoning generalization. Notably, our mixed training model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These findings position SynLogic as a valuable resource for advancing the broader reasoning capabilities of LLMs. We open-source both the data synthesis pipeline and the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.",,"Junteng Liu, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng, Aili Chen, Shiqi Chen, Yunan Huang, Mozhi Zhang, Pengyu Zhao, Junjie Yan, Junxian He",2025-06-04T05:08:08Z,SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning   Logical Reasoning and Beyond,SynLogic: Synthesizing verifizierbare reasoning data at scale for Learning Logical Reasoning and Beyond,协同Logic:在学习逻辑理由及以后的尺度上综合可核实的理由数据,http://arxiv.org/abs/2505.19641v4
383,"Recent advances in knowledge representation learning (KRL) highlight the urgent necessity to unify symbolic knowledge graphs (KGs) with language models (LMs) for richer semantic understanding. However, existing approaches typically prioritize either graph structure or textual semantics, leaving a gap: a unified framework that simultaneously captures global KG connectivity, nuanced linguistic context, and discriminative reasoning semantics. To bridge this gap, we introduce KG-BiLM, a bidirectional LM framework that fuses structural cues from KGs with the semantic expressiveness of generative transformers. KG-BiLM incorporates three key components: (i) Bidirectional Knowledge Attention, which removes the causal mask to enable full interaction among all tokens and entities; (ii) Knowledge-Masked Prediction, which encourages the model to leverage both local semantic contexts and global graph connectivity; and (iii) Contrastive Graph Semantic Aggregation, which preserves KG structure via contrastive alignment of sampled sub-graph representations. Extensive experiments on standard benchmarks demonstrate that KG-BiLM outperforms strong baselines in link prediction, especially on large-scale graphs with complex multi-hop relations - validating its effectiveness in unifying structural information and textual semantics.",,"Zirui Chen, Xin Wang, Zhao Li, Wenbin Guo, Dongxiao He",2025-06-04T04:47:24Z,KG-BiLM: Knowledge Graph Embedding via Bidirectional Language Models,KG-BiLM: Wissensgraphik über bidirektionale Sprachmodelle einbetten,KG-BILM:通过双向语言模型嵌入的知识图,http://arxiv.org/abs/2506.03576v1
384,"Large Language Models (LLMs) are predominantly evaluated on Standard American English (SAE), often overlooking the diversity of global English varieties. This narrow focus may raise fairness concerns as degraded performance on non-standard varieties can lead to unequal benefits for users worldwide. Therefore, it is critical to extensively evaluate the linguistic robustness of LLMs on multiple non-standard English varieties. We introduce Trans-EnV, a framework that automatically transforms SAE datasets into multiple English varieties to evaluate the linguistic robustness. Our framework combines (1) linguistics expert knowledge to curate variety-specific features and transformation guidelines from linguistic literature and corpora, and (2) LLM-based transformations to ensure both linguistic validity and scalability. Using Trans-EnV, we transform six benchmark datasets into 38 English varieties and evaluate seven state-of-the-art LLMs. Our results reveal significant performance disparities, with accuracy decreasing by up to 46.3% on non-standard varieties. These findings highlight the importance of comprehensive linguistic robustness evaluation across diverse English varieties. Each construction of Trans-EnV was validated through rigorous statistical testing and consultation with a researcher in the field of second language acquisition, ensuring its linguistic validity. Our code and datasets are publicly available at https://github.com/jiyounglee-0523/TransEnV and https://huggingface.co/collections/jiyounglee0523/transenv-681eadb3c0c8cf363b363fb1.",,"Jiyoung Lee, Seungho Kim, Jieun Han, Jun-Min Lee, Kitaek Kim, Alice Oh, Edward Choi",2025-06-04T04:46:38Z,Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs   Against English Varieties,Trans-EnV: Ein Rahmen zur Bewertung der sprachlichen Robustheit von LLMs gegen englische Sorten,Trans-EnV: 反英语多样性LLMs语言能力评价框架,http://arxiv.org/abs/2505.20875v2
385,"Large language models (LLMs) have made significant advancements in addressing diverse natural language processing (NLP) tasks. However, their performance is often limited by inherent comprehension of problems. To address this limitation, we propose Exchange-of-Perspective (EoP), a novel framework designed to exchange perspectives across different definitions of problem, so that it can break the fixed mindset from any particular formulation of the question. We conducted extensive and comprehensive experiments on 8 benchmarks. The results show that EoP can significantly improve performance. For instance, compared to the non-commutative baseline PHP, with GPT-3.5-Turbo and EoP, we observe a 3.6% improvement on AQuA (60.6% to 64.2%), while GPT-4-powered EoP demonstrates a 7.7% overall accuracy enhancement on Math (53.9% to 61.6%) and a 3.5% improvement on OlympiadBench Maths (43.5% to 47.0%) when using Qwen-2.5-72b.",,"Lin Sun, Can Zhang",2025-06-04T04:43:15Z,Exchange of Perspective Prompting Enhances Reasoning in Large Language   Models,Der Austausch von Perspektiven fördert die Vernunft in großen Sprachmodellen,交流观点促进在大语言模式中强化理由,http://arxiv.org/abs/2506.03573v1
386,"Recent advancements in Large Language Models (LLMs) have demonstrated that Process Reward Models (PRMs) play a crucial role in enhancing model performance. However, training PRMs typically requires step-level labels, either manually annotated or automatically generated, which can be costly and difficult to obtain at scale. To address this challenge, we introduce FreePRM, a weakly supervised framework for training PRMs without access to ground-truth step-level labels. FreePRM first generates pseudo step-level labels based on the correctness of final outcome, and then employs Buffer Probability to eliminate impact of noise inherent in pseudo labeling. Experimental results show that FreePRM achieves an average F1 score of 53.0% on ProcessBench, outperforming fully supervised PRM trained on Math-Shepherd by +24.1%. Compared to other open-source PRMs, FreePRM outperforms upon RLHFlow-PRM-Mistral-8B (28.4%) by +24.6%, EurusPRM (31.3%) by +21.7%, and Skywork-PRM-7B (42.1%) by +10.9%. This work introduces a new paradigm in PRM training, significantly reducing reliance on costly step-level annotations while maintaining strong performance.",,"Lin Sun, Chuang Liu, Xiaofeng Ma, Tao Yang, Weijia Lu, Ning Wu",2025-06-04T04:33:53Z,FreePRM: Training Process Reward Models Without Ground Truth Process   Labels,FreePRM: Training Prozess Reward Modelle ohne Grund Wahrheit Prozess Etiketten,"FreePRM: 培训过程奖励模型,无地面真相进程标签",http://arxiv.org/abs/2506.03570v1
387,"We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language models delivering state-of-the-art performance in both general visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing models with up to 78B parameters. For GUI grounding applications, it sets a new standard with 56.1 on OSWorld-G, even outperforming specialized models such as UI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens) with Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward signals. We identify the importance of incorporating high-quality reasoning data with long Chain-of-Thought into pre-training stages, and the benefits of mixed RL despite challenges in simultaneous multi-domain optimization. We also contribute a comprehensive evaluation suite covering 50+ tasks to promote reproducibility and advance the field. The model checkpoints and full evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.",,"Xiaomi LLM-Core Team, :, Zihao Yue, Zhenru Lin, Yifan Song, Weikun Wang, Shuhuai Ren, Shuhao Gu, Shicheng Li, Peidian Li, Liang Zhao, Lei Li, Kainan Bao, Hao Tian, Hailin Zhang, Gang Wang, Dawei Zhu, Cici, Chenhong He, Bowen Ye, Bowen Shen, Zihan Zhang, Zihan Jiang, Zhixian Zheng, Zhichao Song, Zhenbo Luo, Yue Yu, Yudong Wang, Yuanyuan Tian, Yu Tu, Yihan Yan, Yi Huang, Xu Wang, Xinzhe Xu, Xingchen Song, Xing Zhang, Xing Yong, Xin Zhang, Xiangwei Deng, Wenyu Yang, Wenhan Ma, Weiwei Lv, Weiji Zhuang, Wei Liu, Sirui Deng, Shuo Liu, Shimao Chen, Shihua Yu, Shaohui Liu, Shande Wang, Rui Ma, Qiantong Wang, Peng Wang, Nuo Chen, Menghang Zhu, Kangyang Zhou, Kang Zhou, Kai Fang, Jun Shi, Jinhao Dong, Jiebao Xiao, Jiaming Xu, Huaqiu Liu, Hongshen Xu, Heng Qu, Haochen Zhao, Hanglong Lv, Guoan Wang, Duo Zhang, Dong Zhang, Di Zhang, Chong Ma, Chang Liu, Can Cai, Bingquan Xia",2025-06-04T04:32:54Z,MiMo-VL Technical Report,Technischer Bericht MiMo-VL,MiMO-VL技术报告,http://arxiv.org/abs/2506.03569v1
388,"Speculative decoding accelerates Large Language Model (LLM) inference by using a small draft model to predict multiple tokens, and a large target model to verify these tokens in parallel. Recent studies leverage the hidden state of the target model to enhance draft model prediction accuracy. However, existing methods suffer from the degrading quality of draft token predictions at later positions, due to error accumulation in draft model generated features. In this paper, we propose Position Specialists (PosS), which consist of multiple position-specialized draft layers to generate tokens at assigned position(s). Position specialists greatly improve token acceptance rate at later positions per drafting round, as each specialist only needs to focus on handling a certain level of draft model feature deviation. Experiment results on Llama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that PosS effectively improves over baselines on average acceptance length and speed-up ratio. Our codebase is available at https://github.com/shrango/PosS.",,"Langlin Huang, Chengsong Huang, Jixuan Leng, Di Huang, Jiaxin Huang",2025-06-04T04:30:30Z,POSS: Position Specialist Generates Better Draft for Speculative   Decoding,POSS: Positionsspezialist generiert besseren Entwurf für spekulative Dekodierung,POSS: 职位专家为投机性代号生成更好的草稿,http://arxiv.org/abs/2506.03566v1
389,"Current instruction data synthesis methods primarily focus on single-turn instructions and often neglect cross-turn coherence, resulting in context drift and reduced task completion rates in extended conversations. To address this limitation, we propose Skeleton-Guided Multi-Turn Dialogue Generation, a framework that constrains multi-turn instruction synthesis by explicitly modeling human conversational intent. It operates in two stages: (1) Intent Modeling, which captures the global structure of human dialogues by assigning each conversation to one of nine well-defined intent trajectories, ensuring a coherent and goal-oriented information flow; and (2) Skeleton Generation, which constructs a structurally grounded sequence of user queries aligned with the modeled intent, thereby serving as a scaffold that constrains and guides the downstream instruction synthesis process. Based on this process, we construct ConsistentChat, a multi-turn instruction dataset with approximately 15,000 multi-turn conversations and 224,392 utterances. Experiments on the Light, Topdial, and MT-Eval benchmarks show that models fine-tuned on ConsistentChat achieve a 20-30% improvement in chat consistency and up to a 15% increase in task success rate, significantly outperforming models trained on existing single-turn and multi-turn instruction datasets.",,"Jiawei Chen, Xinyan Guan, Qianhao Yuan, Guozhao Mo, Weixiang Zhou, Yaojie Lu, Hongyu Lin, Ben He, Le Sun, Xianpei Han",2025-06-04T04:21:48Z,ConsistentChat: Building Skeleton-Guided Consistent Dialogues for Large   Language Models from Scratch,ConsistentChat: Skeleton-geführte Konsistente Dialoge für große Sprachmodelle von Scratch,校对:为来自斯克拉奇的大型语言模型建立Skeleton-Guided Guided一致对话,http://arxiv.org/abs/2506.03558v1
390,"Direct Preference Optimization (DPO) have emerged as a popular method for aligning Large Language Models (LLMs) with human preferences. While DPO effectively preserves the relative ordering between chosen and rejected responses through pairwise ranking losses, it often neglects absolute reward magnitudes. This oversight can decrease the likelihood of chosen responses and increase the risk of generating out-of-distribution responses, leading to poor performance. We term this issue Degraded Chosen Responses (DCR).To address this issue, we propose Balanced Preference Optimization (BPO), a novel framework that dynamically balances the optimization of chosen and rejected responses through two key components: balanced reward margin and gap adaptor. Unlike previous methods, BPO can fundamentally resolve DPO's DCR issue, without introducing additional constraints to the loss function. Experimental results on multiple mathematical reasoning tasks show that BPO significantly outperforms DPO, improving accuracy by +10.1% with Llama-3.1-8B-Instruct (18.8% to 28.9%) and +11.7% with Qwen2.5-Math-7B (35.0% to 46.7%). It also surpasses DPO variants by +3.6% over IPO (43.1%), +5.0% over SLiC (41.7%), and +3.1% over Cal-DPO (43.6%) on the same model. Remarkably, our algorithm requires only a single line of code modification, making it simple to implement and fully compatible with existing DPO-based frameworks.",,"Lin Sun, Chuang Liu, Peng Liu, Bingyang Li, Weijia Lu, Ning Wu",2025-06-04T04:21:01Z,BPO: Revisiting Preference Modeling in Direct Preference Optimization,BPO: Revisiting Preference Modeling in Direct Preference Optimization,BPO: 重新研究直接优先优化的优先模式模式,http://arxiv.org/abs/2506.03557v1
391,"Large Language Models (LLMs) continue to set new standards in knowledge-intensive and complex reasoning tasks, yet their high computational demands limit widespread adoption. While distilling large models into smaller ones offers a sustainable solution, current techniques--such as static knowledge distillation, resource-intensive reinforcement learning from human feedback, or limited self-reflection--struggle to yield substantial and lasting performance gains. In this paper, we present a novel Debate and Reflect (D&R) framework that orchestrates multi-turn debates between smaller models and stronger teacher models, eliciting actionable feedback (e.g., error analysis, corrective strategies) to guide student models. Further, we introduce Tree-structured Direct Preference Optimization (T-DPO) to efficiently leverage these debate logs, organizing interactions into a hierarchical format for effective training. Empirical evaluations across diverse NLP benchmarks demonstrate that our approach significantly improves smaller-model accuracy, robustness, and generalization, outperforming conventional baselines by a large margin.",,"Xiaofeng Zhou, Heyan Huang, Lizi Liao",2025-06-04T03:52:20Z,"Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured   Preference Optimization for Efficient Language Model Enhancement","Debatte, Reflektieren und Destillieren: Multi-Agent Feedback mit baumstrukturierter Preference-Optimierung für effiziente Sprachmodellverbesserung","辩论、反省和蒸馏:多机构反馈,以树木结构化优化为优化,促进高效语言模式增强",http://arxiv.org/abs/2506.03541v1
392,"We demonstrate that the inference operations of several open-weight large language models (LLMs) can be mapped to an exactly equivalent linear system for an input sequence without modifying the model weights or altering output predictions. Extending techniques from image diffusion models that exhibit local or piecewise linearity, we strategically alter the gradient computation with respect to a given input sequence for a next-token prediction such that the Jacobian of the model nearly exactly reproduces the forward prediction with a linear system. We demonstrate this approach across models (Llama 3, Gemma 3, Qwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show through the singular value decomposition of the detached Jacobian that these LLMs operate in extremely low-dimensional subspaces where many of the largest singular vectors decode to concepts related to the most-likely output token. This approach also allows us to examine the operation of each successive layer (and its attention and MLP components) as nearly-exact linear systems and observe the emergence of semantic concepts. Despite their expressive power and global nonlinearity, modern LLMs can be interpreted through nearly-exact locally linear decompositions that provide insights into their internal representations and reveal interpretable semantic structures in the next-token prediction process.",,James R. Golden,2025-06-04T03:50:57Z,Large Language Models are Locally Linear Mappings,Große Sprachmodelle sind lokal lineare Mappings,大语言模型是局部线性绘图,http://arxiv.org/abs/2505.24293v2
393,"One of the fundamental problems in digital agents is their lack of understanding of their environment. For instance, a web browsing agent may get lost in unfamiliar websites, uncertain what pages must be visited to achieve its goals. To address this, we propose Go-Browse, a method for automatically collecting diverse and realistic web agent data at scale through structured exploration of web environments. Go-Browse achieves efficient exploration by framing data collection as a graph search, enabling reuse of information across exploration episodes. We instantiate our method on the WebArena benchmark, collecting a dataset of 10K successful task-solving trajectories and 40K interaction steps across 100 URLs. Fine-tuning a 7B parameter language model on this dataset achieves a success rate of 21.7% on the WebArena benchmark, beating GPT-4o mini by 2.4% and exceeding current state-of-the-art results for sub-10B parameter models by 2.9%.",,"Apurva Gandhi, Graham Neubig",2025-06-04T03:27:56Z,Go-Browse: Training Web Agents with Structured Exploration,Go-Browse: Schulung von Web-Agenten mit strukturierter Exploration,Go-Browse:培训有结构性探索的网络代理,http://arxiv.org/abs/2506.03533v1
394,"Hepato-pancreato-biliary (HPB) disorders represent a global public health challenge due to their high morbidity and mortality. Although large language models (LLMs) have shown promising performance in general medical question-answering tasks, the current evaluation benchmarks are mostly derived from standardized examinations or manually designed questions, lacking HPB coverage and clinical cases. To address these issues, we systematically eatablish an HPB disease evaluation benchmark comprising 3,535 closed-ended multiple-choice questions and 337 open-ended real diagnosis cases, which encompasses all the 33 main categories and 465 subcategories of HPB diseases defined in the International Statistical Classification of Diseases, 10th Revision (ICD-10). The multiple-choice questions are curated from public datasets and synthesized data, and the clinical cases are collected from prestigious medical journals, case-sharing platforms, and collaborating hospitals. By evalauting commercial and open-source general and medical LLMs on our established benchmark, namely ClinBench-HBP, we find that while commercial LLMs perform competently on medical exam questions, they exhibit substantial performance degradation on HPB diagnosis tasks, especially on complex, inpatient clinical cases. Those medical LLMs also show limited generalizability to HPB diseases. Our results reveal the critical limitations of current LLMs in the domain of HPB diseases, underscoring the imperative need for future medical LLMs to handle real, complex clinical diagnostics rather than simple medical exam questions. The benchmark will be released at https://clinbench-hpb.github.io.",,"Yuchong Li, Xiaojun Zeng, Chihua Fang, Jian Yang, Fucang Jia, Lei Zhang",2025-06-04T03:25:49Z,ClinBench-HPB: A Clinical Benchmark for Evaluating LLMs in   Hepato-Pancreato-Biliary Diseases,ClinBench-HPB: Ein klinischer Maßstab für die Bewertung von LLMs bei Leber-Pankreato-Biliary Diseases,ClinBench-HPB:评价Hepato-Pancreato-Bilal病的LMLM的临床基准,http://arxiv.org/abs/2506.00095v3
395,"Multimodal foundation models have demonstrated impressive capabilities across diverse tasks. However, their potential as plug-and-play solutions for missing modality prediction remains underexplored. To investigate this, we categorize existing approaches into three representative paradigms, encompassing a total of 42 model variants, and conduct a comprehensive evaluation in terms of prediction accuracy and adaptability to downstream tasks. Our analysis reveals that current foundation models often fall short in two critical aspects: (i) fine-grained semantic extraction from the available modalities, and (ii) robust validation of generated modalities. These limitations lead to suboptimal and, at times, misaligned predictions. To address these challenges, we propose an agentic framework tailored for missing modality prediction. This framework dynamically formulates modality-aware mining strategies based on the input context, facilitating the extraction of richer and more discriminative semantic features. In addition, we introduce a \textit{self-refinement mechanism}, which iteratively verifies and enhances the quality of generated modalities through internal feedback. Experimental results show that our method reduces FID for missing image prediction by at least 14% and MER for missing text prediction by at least 10% compared to baselines.",,"Guanzhou Ke, Yi Xie, Xiaoli Wang, Guoqing Chao, Bo Wang, Shengfeng He",2025-06-04T03:22:44Z,How Far Are We from Predicting Missing Modalities with Foundation   Models?,"Wie weit sind wir davon entfernt, fehlende Modalitäten mit Fundamentalmodellen vorauszusagen?",我们离以基础模型预测失踪模式有多远?,http://arxiv.org/abs/2506.03530v1
396,"Recent advances in Chain-of-Thought (CoT) reasoning have improved complex video understanding, but existing methods often struggle to adapt to domain-specific skills (e.g., event detection, spatial relation understanding, emotion understanding) over various video content. To address this, we propose Video-Skill-CoT (a.k.a. Video-SKoT), a framework that automatically constructs and leverages skill-aware CoT supervisions for domain-adaptive video reasoning. First, we construct skill-based CoT annotations: we extract domain-relevant reasoning skills from training questions, cluster them into a shared skill taxonomy, and create detailed multi-step CoT rationale tailored to each video-question pair for training. Second, we introduce a skill-specific expert learning framework. Each expert module specializes in a subset of reasoning skills and is trained with lightweight adapters using the collected CoT supervision. We demonstrate the effectiveness of the proposed approach on three video understanding benchmarks, where Video-SKoT consistently outperforms strong baselines. We also provide in-depth analyses on comparing different CoT annotation pipelines and learned skills over multiple video domains.",,"Daeun Lee, Jaehong Yoon, Jaemin Cho, Mohit Bansal",2025-06-04T03:18:01Z,Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video   Reasoning,Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video Reasoning,视频-技能-Cot:以技能为基础的用于域-节录动性视频理由解释的探索链,http://arxiv.org/abs/2506.03525v1
397,"Tokenization serves as a foundational step for Large Language Models (LLMs) to process text. In new domains or languages, the inefficiency of the tokenizer will slow down the training and generation of LLM. The mismatch in vocabulary also hinders deep knowledge transfer between LLMs like token-level distillation. To mitigate this gap, we propose an efficient method named TokAlign to replace the vocabulary of LLM from the token co-occurrences view, and further transfer the token-level knowledge between models. It first aligns the source vocabulary to the target one by learning a one-to-one mapping matrix for token IDs. Model parameters, including embeddings, are rearranged and progressively fine-tuned for the new vocabulary. Our method significantly improves multilingual text compression rates and vocabulary initialization for LLMs, decreasing the perplexity from 3.4$\text{e}^2$ of strong baseline methods to 1.2$\text{e}^2$ after initialization. Experimental results on models across multiple parameter scales demonstrate the effectiveness and generalization of TokAlign, which costs as few as 5k steps to restore the performance of the vanilla model. After unifying vocabularies between LLMs, token-level distillation can remarkably boost (+4.4% than sentence-level distillation) the base model, costing only 235M tokens.",,"Chong Li, Jiajun Zhang, Chengqing Zong",2025-06-04T03:15:57Z,TokAlign: Efficient Vocabulary Adaptation via Token Alignment,TokAlign: Effiziente Vokabelanpassung über Token Alignment,TokAlign: 通过 Tokken 调整高效的词汇适应,http://arxiv.org/abs/2506.03523v1
398,"How can we accelerate large language models(LLMs) without sacrificing accuracy? The slow inference speed of LLMs hinders us to benefit from their remarkable performance in diverse applications. This is mainly because numerous sublayers are stacked together in LLMs. Sublayer pruning compresses and expedites LLMs via removing unnecessary sublayers. However, existing sublayer pruning algorithms are limited in accuracy since they naively select sublayers to prune, overlooking the different characteristics of each sublayer. In this paper, we propose SPRINT (Sublayer PRuning wIth LateNcy and Tunability Information), an accurate sublayer pruning method for LLMs. SPRINT accurately selects a target sublayer to prune by considering 1) the amount of latency reduction after pruning and 2) the tunability of sublayers. SPRINT iteratively prunes redundant sublayers and swiftly tunes the parameters of remaining sublayers. Experiments show that SPRINT achieves the best accuracy-speedup trade-off, exhibiting up to 23.88%p higher accuracy on zero-shot commonsense reasoning benchmarks compared to existing pruning algorithms.",,"Seungcheol Park, Sojin Lee, Jongjin Kim, Jinsik Lee, Hyunjik Jo, U Kang",2025-06-04T02:53:34Z,Accurate Sublayer Pruning for Large Language Models by Exploiting   Latency and Tunability Information,Präzise Sublayer Pruning für große Sprachmodelle durch Ausnutzen von Latenz und Tunability Information,通过利用悬浮和金枪鱼可变性信息为大型语言模型提供精确的次层缓冲,http://arxiv.org/abs/2506.03510v1
399,"Multimodal conversation, a crucial form of human communication, carries rich emotional content, making the exploration of the causes of emotions within it a research endeavor of significant importance. However, existing research on the causes of emotions typically employs an utterance selection method within a single textual modality to locate causal utterances. This approach remains limited to coarse-grained assessments, lacks nuanced explanations of emotional causation, and demonstrates inadequate capability in identifying multimodal emotional triggers. Therefore, we introduce a task-\textbf{Multimodal Emotion Cause Explanation in Conversation (MECEC)}. This task aims to generate a summary based on the multimodal context of conversations, clearly and intuitively describing the reasons that trigger a given emotion. To adapt to this task, we develop a new dataset (ECEM) based on the MELD dataset. ECEM combines video clips with detailed explanations of character emotions, helping to explore the causal factors behind emotional expression in multimodal conversations. A novel approach, FAME-Net, is further proposed, that harnesses the power of Large Language Models (LLMs) to analyze visual data and accurately interpret the emotions conveyed through facial expressions in videos. By exploiting the contagion effect of facial emotions, FAME-Net effectively captures the emotional causes of individuals engaged in conversations. Our experimental results on the newly constructed dataset show that FAME-Net outperforms several excellent baselines. Code and dataset are available at https://github.com/3222345200/FAME-Net.",,"Lin Wang, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang, Zhitao Zhang",2025-06-04T02:46:12Z,Generative Emotion Cause Explanation in Multimodal Conversations,Generative Emotion Ursache Erklärung in multimodalen Gesprächen,多式联运中产生情感原因的解释,http://arxiv.org/abs/2411.02430v3
400,"Social reasoning abilities are crucial for AI systems to effectively interpret and respond to multimodal human communication and interaction within social contexts. We introduce SOCIAL GENOME, the first benchmark for fine-grained, grounded social reasoning abilities of multimodal models. SOCIAL GENOME contains 272 videos of interactions and 1,486 human-annotated reasoning traces related to inferences about these interactions. These traces contain 5,777 reasoning steps that reference evidence from visual cues, verbal cues, vocal cues, and external knowledge (contextual knowledge external to videos). SOCIAL GENOME is also the first modeling challenge to study external knowledge in social reasoning. SOCIAL GENOME computes metrics to holistically evaluate semantic and structural qualities of model-generated social reasoning traces. We demonstrate the utility of SOCIAL GENOME through experiments with state-of-the-art models, identifying performance gaps and opportunities for future research to improve the grounded social reasoning abilities of multimodal models.",,"Leena Mathur, Marian Qian, Paul Pu Liang, Louis-Philippe Morency",2025-06-04T02:43:41Z,Social Genome: Grounded Social Reasoning Abilities of Multimodal Models,Social Genome: Grundlegende gesellschaftliche Fähigkeiten multimodaler Modelle,社会基因组:基于社会原因的多模式模型的能力,http://arxiv.org/abs/2502.15109v4
401,"Instruction fine-tuning is crucial in NLP tasks, enhancing pretrained models' instruction-following capabilities and task-specific performance. However, obtaining high-quality fine-tuning data for large models is challenging due to data collection difficulties and high production costs. To address this, we propose MASTER, a novel data augmentation method that enriches original data through interactions among multiple agents with varying cognitive levels. We simulate three pedagogically grounded teaching scenarios, leveraging multi-agent conversations to generate high-quality teacher-student interaction data. Utilizing MASTER, we construct BOOST-QA, a fine-tuning dataset augmented from existing datasets like Orca-Math-200k, ProcQA, and OpenHermes2.5. Experiments show that models fine-tuned with BOOST-QA perform excellently across multiple benchmarks, demonstrating strong multitask generalization. Notably, MASTER significantly improves models' reasoning abilities in complex tasks, providing valuable insights for future research.",,"Liang Yue, Yihong Tang, Kehai Chen, Jie Liu, Min Zhang",2025-06-04T02:34:54Z,MASTER: Enhancing Large Language Model via Multi-Agent Simulated   Teaching,MASTER: Erweiterung des großen Sprachmodells durch Multi-Agent Simulated Teaching,MASTER:通过多代理模拟教学加强大语言模式,http://arxiv.org/abs/2506.02689v2
402,"Content creation has dramatically progressed with the rapid advancement of large language models like ChatGPT and Claude. While this progress has greatly enhanced various aspects of life and work, it has also negatively affected certain areas of society. A recent survey revealed that nearly 30% of college students use generative AI to help write academic papers and reports. Most countermeasures treat the detection of AI-generated text as a binary classification task and thus lack robustness. This approach overlooks human involvement in the generation of content even though human-machine collaboration is becoming mainstream. Besides generating entire texts, people may use machines to complete or revise texts. Such human involvement varies case by case, which makes binary classification a less than satisfactory approach. We refer to this situation as participation detection obfuscation. We propose using BERTScore as a metric to measure human involvement in the generation process and a multi-task RoBERTa-based regressor trained on a token classification task to address this problem. To evaluate the effectiveness of this approach, we simulated academic-based scenarios and created a continuous dataset reflecting various levels of human involvement. All of the existing detectors we examined failed to detect the level of human involvement on this dataset. Our method, however, succeeded (F1 score of 0.9423 and a regressor mean squared error of 0.004). Moreover, it demonstrated some generalizability across generative models. Our code is available at https://github.com/gyc-nii/CAS-CS-and-dual-head-detector",,"Yuchen Guo, Zhicheng Dou, Huy H. Nguyen, Ching-Chun Chang, Saku Sugawara, Isao Echizen",2025-06-04T02:31:36Z,Measuring Human Involvement in AI-Generated Text: A Case Study on   Academic Writing,Messung der menschlichen Beteiligung an KI-generierten Texten: Eine Fallstudie über akademisches Schreiben,衡量人类参与AI创用文字:学术写作案例研究,http://arxiv.org/abs/2506.03501v1
403,"Open benchmarks are essential for evaluating and advancing large language models, offering reproducibility and transparency. However, their accessibility makes them likely targets of test set contamination. In this work, we introduce DyePack, a framework that leverages backdoor attacks to identify models that used benchmark test sets during training, without requiring access to the loss, logits, or any internal details of the model. Like how banks mix dye packs with their money to mark robbers, DyePack mixes backdoor samples with the test data to flag models that trained on it. We propose a principled design incorporating multiple backdoors with stochastic targets, enabling exact false positive rate (FPR) computation when flagging every model. This provably prevents false accusations while providing strong evidence for every detected case of contamination. We evaluate DyePack on five models across three datasets, covering both multiple-choice and open-ended generation tasks. For multiple-choice questions, it successfully detects all contaminated models with guaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard using eight backdoors. For open-ended generation tasks, it generalizes well and identifies all contaminated models on Alpaca with a guaranteed false positive rate of just 0.127% using six backdoors.",,"Yize Cheng, Wenxiao Wang, Mazda Moayeri, Soheil Feizi",2025-06-04T02:31:16Z,DyePack: Provably Flagging Test Set Contamination in LLMs Using   Backdoors,DyePack: Wahrscheinlich Flagging Test Set Kontamination in LLMs Verwendung von Backdoors,DyePack: 使用后门的LLMs中可被证实的挂旗试验设置污染,http://arxiv.org/abs/2505.23001v3
404,"The remarkable performance of Large language models (LLMs) relies heavily on the availability of abundant high-quality training data. However, the high cost of acquiring annotated data often prevents models from obtaining capabilities to tackle downstream tasks. In this paper, we introduce a novel method, EpiCoDe that boosts model performance in data-scarcity scenarios without extra training. We first employ model extrapolation to enhance a finetuned model with its inferior version, and then adopt contrastive decoding to further reduce predicted errors, by comparing the logit scores given by the extrapolated and the vanilla finetuned model. Experiments across three tasks over four different LLMs show that EpiCoDe consistently outperforms existing methods with significant and robust improvement. We also propose a new theoretical framework to reveal the mechanism behind contrastive decoding in data-scarcity scenarios, which further helps us better understand the effectiveness of EpiCoDe.",,"Mingxu Tao, Jie Hu, Mingchuan Yang, Yunhuai Liu, Dongyan Zhao, Yansong Feng",2025-06-04T02:11:54Z,EpiCoDe: Boosting Model Performance Beyond Training with Extrapolation   and Contrastive Decoding,EpiCoDe: Steigerung der Modellleistung über das Training mit Extrapolation und Kontrastiv-Dekodierung hinaus,"EpiCode:在培训之外促进示范性业绩,除培训外,还有外推法和反比代码法",http://arxiv.org/abs/2506.03489v1
405,"Large transformer-based language models dominate modern NLP, yet our understanding of how they encode linguistic information is rooted in studies of early models like BERT and GPT-2. To better understand today's language models, we investigate how both classical architectures (BERT, DeBERTa, GPT-2)and contemporary large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5, Llama-3.1) represent lexical identity and inflectional morphology. We train linear and nonlinear classifiers on layer-wise activations to predict word lemmas and inflectional features. We discover that models concentrate lexical information linearly in early layers and increasingly nonlinearly in later layers, while keeping inflectional information uniformly accessible and linearly separable throughout the layers. Further analysis reveals that these models encode inflectional morphology through generalizable abstractions, but rely predominantly on memorization to encode lexical identity. Remarkably, these patterns emerge across all 16 models we test, despite differences in architecture, size, and training regime (including pretrained and instruction-tuned variants). This consistency suggests that, despite substantial advances in LLM technologies, transformer models organize linguistic information in similar ways, indicating that these properties could be fundamental for next token prediction and are learned early during pretraining. Our code is available at https://github.com/ml5885/model_internal_sleuthing",,"Michael Li, Nishant Subramani",2025-06-04T02:03:49Z,Model Internal Sleuthing: Finding Lexical Identity and Inflectional   Morphology in Modern Language Models,Modell Internes Sleuthing: Lexische Identität und Flexionsmorphologie in modernen Sprachmodellen finden,内部示范解决模式:在现代语言模式中寻找词汇特征和内分形生理学,http://arxiv.org/abs/2506.02132v2
406,"Reranking is fundamental to information retrieval and retrieval-augmented generation, with recent Large Language Models (LLMs) significantly advancing reranking quality. While recent advances with LLMs have significantly improved document reranking quality, current approaches primarily rely on large-scale LLMs (>7B parameters) through zero-shot prompting, presenting high computational costs. Small Language Models (SLMs) offer a promising alternative because of their efficiency, but our preliminary quantitative analysis reveals they struggle with understanding task prompts without fine-tuning. This limits their effectiveness for document reranking tasks. To address this issue, we introduce a novel two-stage training approach, ProRank, for SLM-based document reranking. First, we propose a prompt warmup stage using reinforcement learning GRPO to steer SLMs to understand task prompts and generate more accurate coarse-grained binary relevance scores for document reranking. Then, we continuously fine-tune the SLMs with a fine-grained score learning stage without introducing additional layers to further improve the reranking quality. Comprehensive experimental results demonstrate that the proposed ProRank consistently outperforms both the most advanced open-source and proprietary reranking models. Notably, our lightweight ProRank-0.5B model even surpasses the powerful 32B LLM reranking model on the BEIR benchmark, establishing that properly trained SLMs can achieve superior document reranking performance while maintaining computational efficiency.",,"Xianming Li, Aamir Shakir, Rui Huang, Julius Lipp, Jing Li",2025-06-04T02:00:44Z,ProRank: Prompt Warmup via Reinforcement Learning for Small Language   Models Reranking,ProRank: Prompt Warmup via Verstärkungslernen für kleine Sprachmodelle Reranking,"ProRank:通过加强学习小语言模式再排序,通过强化学习,迅速暖和",http://arxiv.org/abs/2506.03487v1
407,"Explainable AI (XAI) has emerged as a powerful tool for improving the performance of AI models, going beyond providing model transparency and interpretability. The scarcity of labeled data remains a fundamental challenge in developing robust and generalizable AI models, particularly for low-resource languages. Conventional data augmentation techniques introduce noise, cause semantic drift, disrupt contextual coherence, lack control, and lead to overfitting. To address these challenges, we propose XAI-Guided Context-Aware Data Augmentation. This novel framework leverages XAI techniques to modify less critical features while selectively preserving most task-relevant features. Our approach integrates an iterative feedback loop, which refines augmented data over multiple augmentation cycles based on explainability-driven insights and the model performance gain. Our experimental results demonstrate that XAI-SR-BT and XAI-PR-BT improve the accuracy of models on hate speech and sentiment analysis tasks by 6.6% and 8.1%, respectively, compared to the baseline, using the Amharic dataset with the XLM-R model. XAI-SR-BT and XAI-PR-BT outperform existing augmentation techniques by 4.8% and 5%, respectively, on the same dataset and model. Overall, XAI-SR-BT and XAI-PR-BT consistently outperform both baseline and conventional augmentation techniques across all tasks and models. This study provides a more controlled, interpretable, and context-aware solution to data augmentation, addressing critical limitations of existing augmentation techniques and offering a new paradigm shift for leveraging XAI techniques to enhance AI model training.",,"Melkamu Abay Mersha, Mesay Gemeda Yigezu, Atnafu Lambebo Tonja, Hassan Shakil, Samer Iskander, Olga Kolesnikova, Jugal Kalita",2025-06-04T01:47:24Z,Explainable AI: XAI-Guided Context-Aware Data Augmentation,Erklärbare KI: XAI-geführte Kontext-Bewusste Datenvergrößerung,可解释的 AI: XAI-指导的上下文软件数据增强,http://arxiv.org/abs/2506.03484v1
408,"Large Language Models (LLMs) often require domain-specific fine-tuning to address targeted tasks, which risks degrading their general capabilities. Maintaining a balance between domain-specific enhancements and general model utility is a key challenge. This paper proposes a novel approach named APT (Weakness Case Acquisition and Iterative Preference Training) to enhance domain-specific performance with self-generated dis-preferred weakness data (bad cases and similar cases). APT uniquely focuses on training the model using only those samples where errors occur, alongside a small, similar set of samples retrieved for this purpose. This targeted training minimizes interference with the model's existing knowledge base, effectively retaining generic capabilities. Experimental results on the LLama-2 and Mistral-V0.3 models across various benchmarks demonstrate that APT ensures no reduction in generic capacity and achieves superior performance on downstream tasks compared to various existing methods. This validates our method as an effective strategy for enhancing domain-specific capabilities without sacrificing the model's broader applicability.",,"Jun Rao, Zepeng Lin, Xuebo Liu, Xiaopeng Ke, Lian Lian, Dong Jin, Shengjun Cheng, Jun Yu, Min Zhang",2025-06-04T01:46:38Z,APT: Improving Specialist LLM Performance with Weakness Case Acquisition   and Iterative Preference Training,APT: Verbesserung der LLM-Spezialleistung mit Schwächefallerwerb und iterativem Preference Training,APT: 改进专家LLM与薄弱案例的取得和迭接优先培训的绩效,http://arxiv.org/abs/2506.03483v1
409,"Managing long texts is challenging for large language models (LLMs) due to limited context window sizes. This study introduces UIO-LLMs, an unbiased incremental optimization approach for memory-enhanced transformers under long-context settings. We initially conceptualize the process as a streamlined encoder-decoder framework where the weights-shared encoder and decoder respectively encapsulate a context segment into memories and leverage these memories to predict outputs of the subsequent segment. Subsequently, by treating our memory-enhanced transformers as fully-connected recurrent neural networks (RNNs), we refine the training process using the Truncated Backpropagation Through Time (TBPTT) algorithm, which incorporates innovative incremental optimization techniques. These techniques not only diminish time complexity but also address the bias in gradient computation through an unbiased optimization process. UIO-LLMs successfully handle long context, such as extending the context window of Llama2-7b-chat from 4K to 100K tokens with minimal 2% additional parameters, while keeping the inference cost nearly linear as context length increases.",,"Wenhao Li, Mingbao Lin, Yunshan Zhong, Shuicheng Yan, Rongrong Ji",2025-06-04T01:42:43Z,UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs,UIO-LLMs: Unvoreingenommene Inkrementelle Optimierung für Langkontext-LLMs,UIO-LLMs:无偏见的长文本LLMs增量优化,http://arxiv.org/abs/2406.18173v2
410,"Reinforcement learning algorithms are fundamental to align large language models with human preferences and to enhance their reasoning capabilities. However, current reinforcement learning algorithms often suffer from training instability due to loose on-policy constraints and computational inefficiency due to auxiliary models. In this work, we propose On-Policy RL with Optimal reward baseline (OPO), a novel and simplified reinforcement learning algorithm designed to address these challenges. OPO emphasizes the importance of exact on-policy training, which empirically stabilizes the training process and enhances exploration. Moreover, OPO integrates a practically feasible formulation of the optimal reward baseline that minimizes gradient variance. We evaluate OPO on mathematical reasoning benchmarks. The results demonstrate its superior performance and training stability without additional models or regularization terms. Furthermore, OPO achieves lower policy shifts and higher output entropy, encouraging more diverse and less repetitive responses. These results highlight OPO as a promising direction for stable and effective reinforcement learning in large language model alignment and reasoning tasks. The implementation is merged into the verl library at https://verl.readthedocs.io/en/latest/algo/opo.html.",,"Yaru Hao, Li Dong, Xun Wu, Shaohan Huang, Zewen Chi, Furu Wei",2025-06-04T01:41:37Z,On-Policy RL with Optimal Reward Baseline,On-Policy RL mit optimaler Prämienbasis,具有最佳回报基准的 政策性RL,http://arxiv.org/abs/2505.23585v2
411,"Large Language Models (LLMs) rely on generating extensive intermediate reasoning units (e.g., tokens, sentences) to enhance final answer quality across a wide range of complex tasks. While this approach has proven effective, it inevitably increases substantial inference costs. Previous methods adopting token-level reduction without clear criteria result in poor performance compared to models trained with complete rationale. To address this challenge, we propose a novel sentence-level rationale reduction framework leveraging likelihood-based criteria, verbosity, to identify and remove redundant reasoning sentences. Unlike previous approaches, our method leverages verbosity to selectively remove redundant reasoning sentences while preserving reasoning capabilities. Our experimental results across various reasoning tasks demonstrate that our method improves performance by an average of 7.71% while reducing token generation by 19.87% compared to model trained with complete reasoning paths.",,"Joonwon Jang, Jaehee Kim, Wonbin Kweon, Seonghyeon Lee, Hwanjo Yu",2025-06-04T01:28:39Z,Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant   Rationale via Principled Criteria,Verbosity-Aware Rationale Reduktion: Effektive Reduktion von Redundant Rationale über Grundsatzkriterien,减少挥发性-软件减少理由说明:根据原则性标准有效减少冗余性理由说明,http://arxiv.org/abs/2412.21006v3
412,"Quantization has gained attention as a promising solution for the cost-effective deployment of large and small language models. However, most prior work has been limited to perplexity or basic knowledge tasks and lacks a comprehensive evaluation of recent models like Llama-3.3. In this paper, we conduct a comprehensive evaluation of instruction-tuned models spanning 1B to 405B parameters, applying four quantization methods across 13 datasets. Our findings reveal that (1) quantized models generally surpass smaller FP16 baselines, yet they often struggle with instruction-following and hallucination detection; (2) FP8 consistently emerges as the most robust option across tasks, and AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller models can suffer severe accuracy drops at 4-bit quantization, while 70B-scale models maintain stable performance; (4) notably, \textit{hard} tasks do not always experience the largest accuracy losses, indicating that quantization magnifies a model's inherent weaknesses rather than simply correlating with task difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant performance declines in Coding and STEM tasks, though it occasionally reports improvements in reasoning.",,"Jemin Lee, Sihyeong Park, Jinse Kwon, Jihun Oh, Yongin Kwon",2025-06-04T01:15:49Z,"Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and   Model Size in Large Language Models From Edge to Giant","Die Trade-Offs erkunden: Quantisierungsmethoden, Aufgaben-Schwierigkeiten und Modellgröße in großen Sprachmodellen vom Rand zum Riese",探讨贸易便利:从边缘到巨型大语言模型的量化方法、任务难度和模型大小,http://arxiv.org/abs/2409.11055v6
413,"Alzheimer's Disease (AD) is a progressive neurodegenerative disorder that leads to dementia, and early intervention can greatly benefit from analyzing linguistic abnormalities. In this work, we explore the potential of Large Language Models (LLMs) as health assistants for AD diagnosis from patient-generated text using in-context learning (ICL), where tasks are defined through a few input-output examples. Empirical results reveal that conventional ICL methods, such as similarity-based selection, perform poorly for AD diagnosis, likely due to the inherent complexity of this task. To address this, we introduce Delta-KNN, a novel demonstration selection strategy that enhances ICL performance. Our method leverages a delta score to assess the relative gains of each training example, coupled with a KNN-based retriever that dynamically selects optimal ""representatives"" for a given input. Experiments on two AD detection datasets across three open-source LLMs demonstrate that Delta-KNN consistently outperforms existing ICL baselines. Notably, when using the Llama-3.1 model, our approach achieves new state-of-the-art results, surpassing even supervised classifiers.",,"Chuyuan Li, Raymond Li, Thalia S. Field, Giuseppe Carenini",2025-06-04T01:14:07Z,Delta-KNN: Improving Demonstration Selection in In-Context Learning for   Alzheimer's Disease Detection,Delta-KNN: Verbesserung der Demonstrationsauswahl im In-Context-Lernen für Alzheimer-Erkennung,Delta-KNN:在阿尔茨海默氏病检测的理论内学习中改进示范选择,http://arxiv.org/abs/2506.03476v1
414,"User-item interactions contain rich collaborative signals that form the backbone of many successful recommender systems. While recent work has explored the use of large language models (LLMs) for recommendation, it remains unclear whether LLMs can effectively reason over this type of collaborative information. In this paper, we conduct a systematic comparison between LLMs and classical matrix factorization (MF) models to assess LLMs' ability to leverage user-item interaction data. We further introduce a simple retrieval-augmented generation (RAG) method that enhances LLMs by grounding their predictions in structured interaction data. Our experiments reveal that current LLMs often fall short in capturing collaborative patterns inherent to MF models, but that our RAG-based approach substantially improves recommendation quality-highlighting a promising direction for future LLM-based recommenders.",,"Shahrooz Pouryousef, Ali Montazeralghaem",2025-06-04T00:54:43Z,What LLMs Miss in Recommendations: Bridging the Gap with   Retrieval-Augmented Collaborative Signals,Was LLMs in Empfehlungen vermissen: Die Lücke mit retrieval-Augmented Collaborative Signals überbrücken,在建议中错过了什么的LLM女士:用检索增强的合作信号弥合差距,http://arxiv.org/abs/2505.20730v2
415,"Toxic language detection is crucial for creating safer online environments and limiting the spread of harmful content. While toxic language detection has been under-explored in Persian, the current work compares different methods for this task, including fine-tuning, data enrichment, zero-shot and few-shot learning, and cross-lingual transfer learning. What is especially compelling is the impact of cultural context on transfer learning for this task: We show that the language of a country with cultural similarities to Persian yields better results in transfer learning. Conversely, the improvement is lower when the language comes from a culturally distinct country. Warning: This paper contains examples of toxic language that may disturb some readers. These examples are included for the purpose of research on toxic detection.",,"Zahra Bokaei, Walid Magdy, Bonnie Webber",2025-06-03T23:48:07Z,Culture Matters in Toxic Language Detection in Persian,Kulturfragen in giftigen Spracherkennung auf Persisch,波斯有毒语言探测中的文化问题,http://arxiv.org/abs/2506.03458v1
416,"As hypothesis generation becomes increasingly automated, a new bottleneck has emerged: hypothesis assessment. Modern systems can surface thousands of statistical relationships-correlations, trends, causal links-but offer little guidance on which ones are novel, non-trivial, or worthy of expert attention. In this work, we study the complementary problem to hypothesis generation: automatic hypothesis assessment. Specifically, we ask: given a large set of statistical relationships, can we automatically assess which ones are novel and worth further exploration? We focus on correlations as they are a common entry point in exploratory data analysis that often serve as the basis for forming deeper scientific or causal hypotheses.   To support automatic assessment, we propose to leverage the vast knowledge encoded in LLMs' weights to derive a prior distribution over the correlation value of a variable pair. If an LLM's prior expects the correlation value observed, then such correlation is not surprising, and vice versa. We propose the Logit-based Calibrated Prior, an LLM-elicited correlation prior that transforms the model's raw output logits into a calibrated, continuous predictive distribution over correlation values. We evaluate the prior on a benchmark of 2,096 real-world variable pairs and it achieves a sign accuracy of 78.8%, a mean absolute error of 0.26, and 95% credible interval coverage of 89.2% in predicting Pearson correlation coefficient. It also outperforms a fine-tuned RoBERTa classifier in binary correlation prediction and achieves higher precision@K in hypothesis ranking. We further show that the prior generalizes to correlations not seen during LLM pretraining, reflecting context-sensitive reasoning rather than memorization.",,"Yue Gong, Raul Castro Fernandez",2025-06-03T22:54:59Z,Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based   Calibrated Prior,Ausnutzung von LLMs für automatische Hypothesis-Abschätzung mittels eines Logit-basierten Kalibrierungsvorher,利用LLMs进行自动假设评估,http://arxiv.org/abs/2506.03444v1
417,"Radiology reports are often lengthy and unstructured, posing challenges for referring physicians to quickly identify critical imaging findings while increasing the risk of missed information. This retrospective study aimed to enhance radiology reports by making them concise and well-structured, with findings organized by relevant organs. To achieve this, we utilized private large language models (LLMs) deployed locally within our institution's firewall, ensuring data security and minimizing computational costs. Using a dataset of 814 radiology reports from seven board-certified body radiologists at Moffitt Cancer Center, we tested five prompting strategies within the LangChain framework. After evaluating several models, the Mixtral LLM demonstrated superior adherence to formatting requirements compared to alternatives like Llama. The optimal strategy involved condensing reports first and then applying structured formatting based on specific instructions, reducing verbosity while improving clarity. Across all radiologists and reports, the Mixtral LLM reduced redundant word counts by more than 53%. These findings highlight the potential of locally deployed, open-source LLMs to streamline radiology reporting. By generating concise, well-structured reports, these models enhance information retrieval and better meet the needs of referring physicians, ultimately improving clinical workflows.",,"Iryna Hartsock, Cyrillo Araujo, Les Folio, Ghulam Rasool",2025-06-03T22:53:07Z,Improving Radiology Report Conciseness and Structure via Local Large   Language Models,Verbesserung des Radiologieberichts Konzisität und Struktur über lokale Großsprachenmodelle,通过当地大语言模式改进放射学报告的简明性和结构,http://arxiv.org/abs/2411.05042v2
418,"Understanding how large language models (LLMs) acquire and store factual knowledge is crucial for enhancing their interpretability and reliability. In this work, we analyze the evolution of factual knowledge representation in the OLMo-7B model by tracking the roles of its attention heads and feed forward networks (FFNs) over the course of pre-training. We classify these components into four roles: general, entity, relation-answer, and fact-answer specific, and examine their stability and transitions. Our results show that LLMs initially depend on broad, general-purpose components, which later specialize as training progresses. Once the model reliably predicts answers, some components are repurposed, suggesting an adaptive learning process. Notably, attention heads display the highest turnover. We also present evidence that FFNs remain more stable throughout training. Furthermore, our probing experiments reveal that location-based relations converge to high accuracy earlier in training than name-based relations, highlighting how task complexity shapes acquisition dynamics. These insights offer a mechanistic view of knowledge formation in LLMs.",,"Ahmad Dawar Hakimi, Ali Modarressi, Philipp Wicke, Hinrich Schütze",2025-06-03T22:35:09Z,Time Course MechInterp: Analyzing the Evolution of Components and   Knowledge in Large Language Models,Time Course MechInterp: Analyse der Evolution von Komponenten und Wissen in großen Sprachmodellen,Mech Ininterp:分析大语言模型组成部分和知识的演变,http://arxiv.org/abs/2506.03434v1
419,"Large Language Models (LLMs) excel in text generation, reasoning, and decision-making, enabling their adoption in high-stakes domains such as healthcare, law, and transportation. However, their reliability is a major concern, as they often produce plausible but incorrect responses. Uncertainty quantification (UQ) enhances trustworthiness by estimating confidence in outputs, enabling risk mitigation and selective prediction. However, traditional UQ methods struggle with LLMs due to computational constraints and decoding inconsistencies. Moreover, LLMs introduce unique uncertainty sources, such as input ambiguity, reasoning path divergence, and decoding stochasticity, that extend beyond classical aleatoric and epistemic uncertainty. To address this, we introduce a new taxonomy that categorizes UQ methods based on computational efficiency and uncertainty dimensions (input, reasoning, parameter, and prediction uncertainty). We evaluate existing techniques, assess their real-world applicability, and identify open challenges, emphasizing the need for scalable, interpretable, and robust UQ approaches to enhance LLM reliability.",,"Xiaoou Liu, Tiejin Chen, Longchao Da, Chacha Chen, Zhen Lin, Hua Wei",2025-06-03T22:16:32Z,Uncertainty Quantification and Confidence Calibration in Large Language   Models: A Survey,Unsicherheit Quantifizierung und Vertrauenskalibrierung in großen Sprachmodellen: Eine Umfrage,大语言模型的不确定性量化和信任度校准:调查,http://arxiv.org/abs/2503.15850v2
420,"The rapid evolution of code largelanguage models underscores the need for effective and transparent benchmarking of their reasoning capabilities. However, the current benchmarking approach heavily depends on publicly available, human-created datasets. The widespread use of these fixed benchmark datasets makes the benchmarking process to be static and thus particularly susceptible to data contamination, an unavoidable consequence of the extensive data collection processes used to train Code LLMs. Existing approaches that address data contamination often suffer from human effort limitations and imbalanced problem complexity. To tackle these challenges, we propose \tool, a novel benchmarking suite for evaluating Code LLMs under potential data contamination. Given a seed programming problem, \tool employs multiple agents to extract and modify the context without altering the core logic, generating semantically equivalent variations. We introduce a dynamic data generation methods and conduct empirical studies on two seed datasets across 21 Code LLMs. Results show that \tool effectively benchmarks reasoning capabilities under contamination risks while generating diverse problem sets to ensure consistent and reliable evaluations.",,"Simin Chen, Pranav Pusarla, Baishakhi Ray",2025-06-03T22:14:34Z,Dynamic Benchmarking of Reasoning Capabilities in Code Large Language   Models Under Data Contamination,Dynamisches Benchmarking von Vernunftfähigkeiten in Code Large Language Models unter Datenkontamination,数据污染下守则大语言模式,http://arxiv.org/abs/2503.04149v2
421,"In-Context Learning (ICL) enables Large Language Models (LLMs) to perform tasks without parameter updates by conditioning on a few demonstrations provided in the prompt. Despite its success, ICL suffers from several limitations, including sensitivity to demonstration order, context length constraints, and computational inefficiency. To address these challenges, task vector-based approaches compress task information into a single vector. However, these methods typically construct task vectors from fixed sets of demonstrations and reuse them across input queries, without conditioning on the specific input. This limitation can lead models to struggle with effective adaptation when the input query is not well aligned with the underlying demonstrations, consequently degrading their generalization performance on unseen tasks. To overcome this limitation, we propose Adaptive Task Vectors (ATV), a simple and effective framework that dynamically generates task vectors conditioned on each input query. ATV employs a small language model to generate task vectors, which are then transformed to match the target LLM's architecture and applied to guide its output generation. In contrast to ICL and previous vector-based approaches, which rely on fixed demonstration sets and their corresponding vectors, ATV dynamically generates task vectors tailored to each specific input query and task. Consequently, ATV demonstrates strong performance and generalization capabilities, even for unseen tasks. Furthermore, we provide a theoretical analysis indicating that ATV is expressively equivalent to LoRA under equal rank budgets and more expressive than Prefix-Tuning, thereby offering formal support for its representational advantage.",,"Joonseong Kang, Soojeong Lee, Subeen Park, Sumin Park, Taero Kim, Jihee Kim, Ryunyi Lee, Kyungwoo Song",2025-06-03T22:12:28Z,Adaptive Task Vectors for Large Language Models,Adaptive Aufgaben-Vektoren für große Sprachmodelle,大语言模型适应性任务矢量,http://arxiv.org/abs/2506.03426v1
422,"Many real world tasks where Large Language Models (LLMs) can be used require spatial reasoning, like Point of Interest (POI) recommendation and itinerary planning. However, on their own LLMs lack reliable spatial reasoning capabilities, especially about distances. To address this problem, we develop a novel approach, DistRAG, that enables an LLM to retrieve relevant spatial information not explicitly learned during training. Our method encodes the geodesic distances between cities and towns in a graph and retrieves a context subgraph relevant to the question. Using this technique, our method enables an LLM to answer distance-based reasoning questions that it otherwise cannot answer. Given the vast array of possible places an LLM could be asked about, DistRAG offers a flexible first step towards providing a rudimentary `world model' to complement the linguistic knowledge held in LLMs.",,"Nicole R Schneider, Nandini Ramachandran, Kent O'Sullivan, Hanan Samet",2025-06-03T22:10:39Z,DistRAG: Towards Distance-Based Spatial Reasoning in LLMs,DistRAG: Auf dem Weg zu einer distanzierten räumlichen Vernunft in LLMs,DistRAG:争取在LLMM中采用远程空间理由,http://arxiv.org/abs/2506.03424v1
423,"Guardrail, an emerging mechanism designed to ensure that large language models (LLMs) align with human values by moderating harmful or toxic responses, requires a sociotechnical approach in their design. This paper addresses a critical issue: existing guardrails lack a well-founded methodology to accommodate the diverse needs of different user groups, particularly concerning access rights. Supported by trust modeling (primarily on `social' aspect) and enhanced with online in-context learning via retrieval-augmented generation (on `technical' aspect), we introduce an adaptive guardrail mechanism, to dynamically moderate access to sensitive content based on user trust metrics. User trust metrics, defined as a novel combination of direct interaction trust and authority-verified trust, enable the system to precisely tailor the strictness of content moderation by aligning with the user's credibility and the specific context of their inquiries. Our empirical evaluation demonstrates the effectiveness of the adaptive guardrail in meeting diverse user needs, outperforming existing guardrails while securing sensitive information and precisely managing potentially hazardous content through a context-aware knowledge base. To the best of our knowledge, this work is the first to introduce trust-oriented concept into a guardrail system, offering a scalable solution that enriches the discourse on ethical deployment for next-generation LLM service.",,"Jinwei Hu, Yi Dong, Xiaowei Huang",2025-06-03T22:06:57Z,Trust-Oriented Adaptive Guardrails for Large Language Models,Vertrauensorientierte adaptive Guardrails für große Sprachmodelle,面向信任的大型语言模式适应性护卫车,http://arxiv.org/abs/2408.08959v3
424,"Recent advances in large language models (LLMs) have sparked growing interest in integrating language-driven techniques into trajectory prediction. By leveraging their semantic and reasoning capabilities, LLMs are reshaping how autonomous systems perceive, model, and predict trajectories. This survey provides a comprehensive overview of this emerging field, categorizing recent work into five directions: (1) Trajectory prediction via language modeling paradigms, (2) Direct trajectory prediction with pretrained language models, (3) Language-guided scene understanding for trajectory prediction, (4) Language-driven data generation for trajectory prediction, (5) Language-based reasoning and interpretability for trajectory prediction. For each, we analyze representative methods, highlight core design choices, and identify open challenges. This survey bridges natural language processing and trajectory prediction, offering a unified perspective on how language can enrich trajectory prediction.",,"Yi Xu, Ruining Yang, Yitian Zhang, Yizhou Wang, Jianglin Lu, Mingyuan Zhang, Lili Su, Yun Fu",2025-06-03T21:36:56Z,Trajectory Prediction Meets Large Language Models: A Survey,Flugbahnvorhersage trifft auf große Sprachmodelle: Eine Umfrage,轨迹预测符合大语言模型:调查,http://arxiv.org/abs/2506.03408v1
425,"Pretrained Large Language Models (LLMs) achieve strong performance across a wide range of tasks, yet exhibit substantial variability in the various layers' training quality with respect to specific downstream applications, limiting their downstream performance. It is therefore critical to estimate layer-wise training quality in a manner that accounts for both model architecture and training data. However, existing approaches predominantly rely on model-centric heuristics (such as spectral statistics, outlier detection, or uniform allocation) while overlooking the influence of data. To address these limitations, we propose LayerIF, a data-driven framework that leverages Influence Functions to quantify the training quality of individual layers in a principled and task-sensitive manner. By isolating each layer's gradients and measuring the sensitivity of the validation loss to training examples by computing layer-wise influences, we derive data-driven estimates of layer importance. Notably, our method produces task-specific layer importance estimates for the same LLM, revealing how layers specialize for different test-time evaluation tasks. We demonstrate the utility of our scores by leveraging them for two downstream applications: (a) expert allocation in LoRA-MoE architectures and (b) layer-wise sparsity distribution for LLM pruning. Experiments across multiple LLM architectures demonstrate that our model-agnostic, influence-guided allocation leads to consistent gains in task performance.",,"Hadi Askari, Shivanshu Gupta, Fei Wang, Anshuman Chhabra, Muhao Chen",2025-06-03T21:18:10Z,LayerIF: Estimating Layer Quality for Large Language Models using   Influence Functions,LayerIF: Bewertung der Ebenenqualität für große Sprachmodelle mit Einflussfunktionen,图层IF: 使用影响函数估算大语言模型使用影响函数的图层质量,http://arxiv.org/abs/2505.23811v2
426,"Open-ended short-answer questions (SAGs) have been widely recognized as a powerful tool for providing deeper insights into learners' responses in the context of learning analytics (LA). However, SAGs often present challenges in practice due to the high grading workload and concerns about inconsistent assessments. With recent advancements in natural language processing (NLP), automatic short-answer grading (ASAG) offers a promising solution to these challenges. Despite this, current ASAG algorithms are often limited in generalizability and tend to be tailored to specific questions. In this paper, we propose a unified multi-agent ASAG framework, GradeOpt, which leverages large language models (LLMs) as graders for SAGs. More importantly, GradeOpt incorporates two additional LLM-based agents - the reflector and the refiner - into the multi-agent system. This enables GradeOpt to automatically optimize the original grading guidelines by performing self-reflection on its errors. Through experiments on a challenging ASAG task, namely the grading of pedagogical content knowledge (PCK) and content knowledge (CK) questions, GradeOpt demonstrates superior performance in grading accuracy and behavior alignment with human graders compared to representative baselines. Finally, comprehensive ablation studies confirm the effectiveness of the individual components designed in GradeOpt.",,"Yucheng Chu, Hang Li, Kaiqi Yang, Harry Shomer, Hui Liu, Yasemin Copur-Gencturk, Jiliang Tang",2025-06-03T21:08:46Z,A LLM-Powered Automatic Grading Framework with Human-Level Guidelines   Optimization,Ein LLM-Powered Automatic Grading Framework mit Mensch-Level-Leitlinien Optimierung,由LLM授权的具有人一级准则优化的自动分级框架,http://arxiv.org/abs/2410.02165v2
427,"Short answer assessment is a vital component of science education, allowing evaluation of students' complex three-dimensional understanding. Large language models (LLMs) that possess human-like ability in linguistic tasks are increasingly popular in assisting human graders to reduce their workload. However, LLMs' limitations in domain knowledge restrict their understanding in task-specific requirements and hinder their ability to achieve satisfactory performance. Retrieval-augmented generation (RAG) emerges as a promising solution by enabling LLMs to access relevant domain-specific knowledge during assessment. In this work, we propose an adaptive RAG framework for automated grading that dynamically retrieves and incorporates domain-specific knowledge based on the question and student answer context. Our approach combines semantic search and curated educational sources to retrieve valuable reference materials. Experimental results in a science education dataset demonstrate that our system achieves an improvement in grading accuracy compared to baseline LLM approaches. The findings suggest that RAG-enhanced grading systems can serve as reliable support with efficient performance gains.",,"Yucheng Chu, Peng He, Hang Li, Haoyu Han, Kaiqi Yang, Yu Xue, Tingting Li, Joseph Krajcik, Jiliang Tang",2025-06-03T21:02:48Z,Enhancing LLM-Based Short Answer Grading with Retrieval-Augmented   Generation,Verbesserung der LLM-basierten Kurzantworteinstufung mit retrieval-erweiterter Generation,"增强基于LLM的短期答案分级法,与再回收-提款一代",http://arxiv.org/abs/2504.05276v2
428,"Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a common reinforcement learning (RL) algorithm used for LLM finetuning, employs value networks to tackle credit assignment. However, recent approaches achieve strong results without it, raising questions about the efficacy of value networks in practice. In this work, we systematically evaluate the efficacy of value networks and reveal their significant shortcomings in reasoning-heavy LLM tasks, showing that they often produce poor estimate of expected return and barely outperform a random baseline when comparing alternative steps. This motivates our key question: Can improved credit assignment enhance RL training for LLMs? To address this, we propose VinePPO, a straightforward approach that leverages the flexibility of language environments to compute unbiased Monte Carlo-based estimates. Our method consistently outperforms PPO and other baselines across MATH and GSM8K datasets in less wall-clock time (up to 3.0x). Crucially, it achieves higher test accuracy for a given training accuracy, capturing more generalization signal per sample. These results emphasize the importance of accurate credit assignment in RL training of LLM.",,"Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, Nicolas Le Roux",2025-06-03T20:51:06Z,VinePPO: Refining Credit Assignment in RL Training of LLMs,VinePPO: Refining Credit Assignment in RL Training of LLMs,VinePPPO:改进LLMLL培训中的信贷分配,http://arxiv.org/abs/2410.01679v2
429,"Recent work found that LLMs are sensitive to a wide range of arbitrary prompt dimensions, including the type of delimiters, answer enumerators, instruction wording, and more. This throws into question popular single-prompt evaluation practices. We present DOVE (Dataset Of Variation Evaluation) a large-scale dataset containing prompt perturbations of various evaluation benchmarks. In contrast to previous work, we examine LLM sensitivity from an holistic perspective, and assess the joint effects of perturbations along various dimensions, resulting in thousands of perturbations per instance. We evaluate several model families against DOVE, leading to several findings, including efficient methods for choosing well-performing prompts, observing that few-shot examples reduce sensitivity, and identifying instances which are inherently hard across all perturbations. DOVE consists of more than 250M prompt perturbations and model outputs, which we make publicly available to spur a community-wide effort toward meaningful, robust, and efficient evaluation.   Browse the data, contribute, and more: https://slab-nlp.github.io/DOVE/",,"Eliya Habba, Ofir Arviv, Itay Itzhak, Yotam Perlitz, Elron Bandel, Leshem Choshen, Michal Shmueli-Scheuer, Gabriel Stanovsky",2025-06-03T20:47:18Z,DOVE: A Large-Scale Multi-Dimensional Predictions Dataset Towards   Meaningful LLM Evaluation,DOVE: Ein multidimensionaler Datensatz zur sinnvollen LLM-Evaluierung,DOVE: 面向有意义的LLM评价的大型多多层次预测数据集,http://arxiv.org/abs/2503.01622v3
430,"Large language models (LLMs) require alignment to effectively and safely follow user instructions. This process necessitates training an aligned version for every base model, resulting in significant computational overhead. In this work, we propose NUDGING, a simple, training-free algorithm that aligns any base model at inference time using a small aligned model. NUDGING is motivated by recent findings that alignment primarily alters the model's behavior on a small subset of stylistic tokens (e.g., discourse markers). We find that base models are significantly more uncertain when generating these tokens. Building on this insight, NUDGING employs a small aligned model to generate nudging tokens to guide the base model's output during decoding when the base model's uncertainty is high, with only a minor additional inference overhead. We evaluate NUDGING across 3 model families on a diverse range of open-instruction tasks. Without any training, nudging a large base model with a 7x-14x smaller aligned model achieves zero-shot performance comparable to, and sometimes surpassing, that of large aligned models. By operating at the token level, NUDGING enables off-the-shelf collaboration between model families. For instance, nudging Gemma-2-27b with Llama-27b-chat outperforms Llama-2-70b-chat on various tasks. Overall, our work offers a modular and cost-efficient solution to LLM alignment. Our code and demo are available at: https://fywalter.github.io/nudging/ .",,"Yu Fei, Yasaman Razeghi, Sameer Singh",2025-06-03T20:35:30Z,Nudging: Inference-time Alignment of LLMs via Guided Decoding,Nudging: Inferenz-Zeit-Ausrichtung von LLMs mittels Guided Decoding,Nudging:通过向导解码对LLMs的推推-时间对齐,http://arxiv.org/abs/2410.09300v4
431,"This note is a survey of various results on the capabilities of unique hard attention transformers encoders (UHATs) to recognize formal languages. We distinguish between masked vs. non-masked, finite vs. infinite image and general vs. bilinear attention score functions. We recall some relations between these models, as well as a lower bound in terms of first-order logic and an upper bound in terms of circuit complexity.",,Leonid Ryvkin,2025-06-03T20:28:51Z,Comparison of different Unique hard attention transformer models by the   formal languages they can recognize,"Vergleich von verschiedenen einzigartigen Hard-Attention-Transformator-Modellen an den formalen Sprachen, die sie erkennen können",不同独特难看型变压器模式与它们能够识别的正式语言的比较,http://arxiv.org/abs/2506.03370v1
432,"Africa's rich linguistic heritage remains underrepresented in NLP, largely due to historical policies that favor foreign languages and create significant data inequities. In this paper, we integrate theoretical insights on Africa's language landscape with an empirical evaluation using Sahara - a comprehensive benchmark curated from large-scale, publicly accessible datasets capturing the continent's linguistic diversity. By systematically assessing the performance of leading large language models (LLMs) on Sahara, we demonstrate how policy-induced data variations directly impact model effectiveness across African languages. Our findings reveal that while a few languages perform reasonably well, many Indigenous languages remain marginalized due to sparse data. Leveraging these insights, we offer actionable recommendations for policy reforms and inclusive data practices. Overall, our work underscores the urgent need for a dual approach - combining theoretical understanding with empirical evaluation - to foster linguistic diversity in AI for African communities.",,"Ife Adebara, Hawau Olamide Toyin, Nahom Tesfu Ghebremichael, AbdelRahim Elmadany, Muhammad Abdul-Mageed",2025-06-03T20:19:19Z,Where Are We? Evaluating LLM Performance on African Languages,Wo sind wir? Bewertung der LLM-Performance auf afrikanischen Sprachen,评价非洲语言LLM成绩,http://arxiv.org/abs/2502.19582v2
433,"Rapid, fine-grained disaster damage assessment is essential for effective emergency response, yet remains challenging due to limited ground sensors and delays in official reporting. Social media provides a rich, real-time source of human-centric observations, but its multimodal and unstructured nature presents challenges for traditional analytical methods. In this study, we propose a structured Multimodal, Multilingual, and Multidimensional (3M) pipeline that leverages multimodal large language models (MLLMs) to assess disaster impacts. We evaluate three foundation models across two major earthquake events using both macro- and micro-level analyses. Results show that MLLMs effectively integrate image-text signals and demonstrate a strong correlation with ground-truth seismic data. However, performance varies with language, epicentral distance, and input modality. This work highlights the potential of MLLMs for disaster assessment and provides a foundation for future research in applying MLLMs to real-time crisis contexts. The code and data are released at: https://github.com/missa7481/EMNLP25_earthquake",,"Zihui Ma, Lingyao Li, Juan Li, Wenyue Hua, Jingxiao Liu, Qingyuan Feng, Yuki Miura",2025-06-03T20:07:25Z,"A Multimodal, Multilingual, and Multidimensional Pipeline for   Fine-grained Crowdsourcing Earthquake Damage Evaluation","Eine multimodale, mehrsprachige und mehrdimensionale Pipeline für die feinkörnige Crowdsourcing-Erdbebenschäden-Bewertung",用于精精密的人群采购地震损害评估的多式、多语种和多层面管道,http://arxiv.org/abs/2506.03360v1
434,"By virtue of linguistic compositionality, few syntactic rules and a finite lexicon can generate an unbounded number of sentences. That is, language, though seemingly high-dimensional, can be explained using relatively few degrees of freedom. An open question is whether contemporary language models (LMs) reflect the intrinsic simplicity of language that is enabled by compositionality. We take a geometric view of this problem by relating the degree of compositionality in a dataset to the intrinsic dimension (ID) of its representations under an LM, a measure of feature complexity. We find not only that the degree of dataset compositionality is reflected in representations' ID, but that the relationship between compositionality and geometric complexity arises due to learned linguistic features over training. Finally, our analyses reveal a striking contrast between nonlinear and linear dimensionality, showing they respectively encode semantic and superficial aspects of linguistic composition.",,"Jin Hwa Lee, Thomas Jiralerspong, Lei Yu, Yoshua Bengio, Emily Cheng",2025-06-03T20:06:28Z,Geometric Signatures of Compositionality Across a Language Model's   Lifetime,Geometrische Signaturen der Kompositionalität über die Lebenszeit eines Sprachmodells hinweg,语文模式中各语文模式的 终身组成特征的几何签名,http://arxiv.org/abs/2410.01444v4
435,"Hallucinations in large language models (LLMs) - instances where models generate plausible but factually incorrect information - present a significant challenge for AI.   We introduce ""Ask a Local"", a novel hallucination detection method exploiting the intuition that specialized models exhibit greater surprise when encountering domain-specific inaccuracies. Our approach computes divergence between perplexity distributions of language-specialized models to identify potentially hallucinated spans. Our method is particularly well-suited for a multilingual context, as it naturally scales to multiple languages without the need for adaptation, relying on external data sources, or performing training. Moreover, we select computationally efficient models, providing a scalable solution that can be applied to a wide range of languages and domains.   Our results on a human-annotated question-answer dataset spanning 14 languages demonstrate consistent performance across languages, with Intersection-over-Union (IoU) scores around 0.3 and comparable Spearman correlation values. Our model shows particularly strong performance on Italian and Catalan, with IoU scores of 0.42 and 0.38, respectively, while maintaining cross-lingual effectiveness without language-specific adaptations. We release our code and architecture to facilitate further research in multilingual hallucination detection.",,"Aldan Creo, Héctor Cerezo-Costas, Pedro Alonso-Doval, Maximiliano Hormazábal-Lagos",2025-06-03T20:00:49Z,Ask a Local: Detecting Hallucinations With Specialized Model Divergence,Fragen Sie einen Einheimischen: Halluzinationen mit spezialisierter Modelldifferenz erkennen,询问本地: 检测有特殊模型差异的幻觉,http://arxiv.org/abs/2506.03357v1
436,"Crosslingual transfer is crucial to contemporary language models' multilingual capabilities, but how it occurs is not well understood. We ask what happens to a monolingual language model when it begins to be trained on a second language. Specifically, we train small bilingual models for which we control the amount of data for each language and the order of language exposure. To find evidence of shared multilingual representations, we turn to structural priming, a method used to study grammatical representations in humans. We first replicate previous crosslingual structural priming results and find that after controlling for training data quantity and language exposure, there are asymmetrical effects across language pairs and directions. We argue that this asymmetry may shape hypotheses about human structural priming effects. We also find that structural priming effects are less robust for less similar language pairs, highlighting potential limitations of crosslingual transfer learning and shared representations for typologically diverse languages.",,"Catherine Arnett, Tyler A. Chang, James A. Michaelov, Benjamin K. Bergen",2025-06-03T20:00:04Z,On the Acquisition of Shared Grammatical Representations in Bilingual   Language Models,Zum Erwerb von gemeinsamen grammatischen Repräsentationen in zweisprachigen Sprachmodellen,获得双语语文模式共同语法代表,http://arxiv.org/abs/2503.03962v2
437,"In an effort to mitigate the harms of large language models (LLMs), learning from human feedback (LHF) has been used to steer LLMs towards outputs that are intended to be both less harmful and more helpful. Despite the widespread adoption of LHF in practice, the quality of this feedback and its effectiveness as a safety mitigation technique remain unclear. This study addresses these issues by auditing the widely-used Helpful and Harmless (HH) dataset by Anthropic. Our work includes: (1) a thorough investigation of the dataset's content through both manual and automated evaluation; (2) experiments demonstrating the dataset's impact on models' safety; and (3) an analysis of the 100 most influential papers citing this dataset. Through our audit, we showcase how conceptualization failures and quality issues identified in the HH dataset can create additional harms by leading to disparate safety behaviors across demographic groups. Our findings highlight the need for more nuanced, context-sensitive approaches to safety mitigation in LLMs.",,"Khaoula Chehbouni, Jonathan Colaço Carr, Yash More, Jackie CK Cheung, Golnoosh Farnadi",2025-06-03T19:37:50Z,Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset,Jenseits des Sicherheitspakets: Auditierung des hilfreichen und harmlosen Datensatzes,安全套件之外:审计有益和无害的数据集,http://arxiv.org/abs/2411.08243v3
438,"We propose THELMA (Task Based Holistic Evaluation of Large Language Model Applications), a reference free framework for RAG (Retrieval Augmented generation) based question answering (QA) applications. THELMA consist of six interdependent metrics specifically designed for holistic, fine grained evaluation of RAG QA applications. THELMA framework helps developers and application owners evaluate, monitor and improve end to end RAG QA pipelines without requiring labelled sources or reference responses.We also present our findings on the interplay of the proposed THELMA metrics, which can be interpreted to identify the specific RAG component needing improvement in QA applications.",,"Udita Patel, Rutu Mulkar, Jay Roberts, Cibi Chakravarthy Senthilkumar, Sujay Gandhi, Xiaofei Zheng, Naumaan Nayyar, Parul Kalra, Rafael Castrillo",2025-06-03T19:21:16Z,THELMA: Task Based Holistic Evaluation of Large Language Model   Applications-RAG Question Answering,THELMA: Aufgabenbasierte ganzheitliche Bewertung von Großsprachenmodellen Anwendungen-RAG-Fragebeantwortung,TYLMA:基于任务的综合评价大语言应用示范应用-RAG问题回答,http://arxiv.org/abs/2505.11626v2
439,"Existing text scoring methods require a large corpus, struggle with short texts, or require hand-labeled data. We develop a text scoring framework that leverages generative large language models (LLMs) to (1) set texts against the backdrop of information from the near-totality of the web and digitized media, and (2) effectively transform pairwise text comparisons from a reasoning problem to a pattern recognition task. Our approach, concept-guided chain-of-thought (CGCoT), utilizes a chain of researcher-designed prompts with an LLM to generate a concept-specific breakdown for each text, akin to guidance provided to human coders. We then pairwise compare breakdowns using an LLM and aggregate answers into a score using a probability model. We apply this approach to better understand speech reflecting aversion to specific political parties on Twitter, a topic that has commanded increasing interest because of its potential contributions to democratic backsliding. We achieve stronger correlations with human judgments than widely used unsupervised text scoring methods like Wordfish. In a supervised setting, besides a small pilot dataset to develop CGCoT prompts, our measures require no additional hand-labeled data and produce predictions on par with RoBERTa-Large fine-tuned on thousands of hand-labeled tweets. This project showcases the potential of combining human expertise and LLMs for scoring tasks.",,"Patrick Y. Wu, Jonathan Nagler, Joshua A. Tucker, Solomon Messing",2025-06-03T19:18:06Z,Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison   Scoring of Texts with Large Language Models,Konzept-geführte Ketten-of-Thought-Prompting für einen Pairwise-Vergleich Bewertung von Texten mit großen Sprachmodellen,用于对等地比较大语言模式文本比较显示的概念指导研究链,http://arxiv.org/abs/2310.12049v3
440,"Large Language Models (LLMs) are being explored for applications in scientific research, including their capabilities to synthesize literature, answer research questions, generate research ideas, and even conduct computational experiments. Ultimately, our goal is for these to help scientists derive novel scientific insights. In many areas of science, such insights often arise from processing and visualizing data to understand its patterns. However, evaluating whether an LLM-mediated scientific workflow produces outputs conveying the correct scientific insights is challenging to evaluate and has not been addressed in past work. We introduce AstroVisBench, the first benchmark for both scientific computing and visualization in the astronomy domain. AstroVisBench judges a language model's ability to both (1) create astronomy-specific workflows to process and analyze data and (2) visualize the results of these workflows through complex plots. Our evaluation of visualizations uses a novel LLM-as-a-judge workflow, which is validated against annotation by five professional astronomers. Using AstroVisBench we present an evaluation of state-of-the-art language models, showing a significant gap in their ability to engage in astronomy research as useful assistants. This evaluation provides a strong end-to-end evaluation for AI scientists that offers a path forward for the development of visualization-based workflows, which are central to a broad range of domains from physics to biology.",,"Sebastian Antony Joseph, Syed Murtaza Husain, Stella S. R. Offner, Stéphanie Juneau, Paul Torrey, Adam S. Bolton, Juan P. Farias, Niall Gaffney, Greg Durrett, Junyi Jessy Li",2025-06-03T18:56:38Z,AstroVisBench: A Code Benchmark for Scientific Computing and   Visualization in Astronomy,AstroVisBench: Ein Code-Bench für wissenschaftliche Computing und Visualisierung in der Astronomie,AstroVisbench:天文科学计算和可视化标准,http://arxiv.org/abs/2505.20538v3
441,"Violent threats remain a significant problem across social media platforms. Useful, high-quality data facilitates research into the understanding and detection of malicious content, including violence. In this paper, we introduce a cross-platform dataset of 30,000 posts hand-coded for violent threats and sub-types of violence, including political and sexual violence. To evaluate the signal present in this dataset, we perform a machine learning analysis with an existing dataset of violent comments from YouTube. We find that, despite originating from different platforms and using different coding criteria, we achieve high classification accuracy both by training on one dataset and testing on the other, and in a merged dataset condition. These results have implications for content-classification strategies and for understanding violent content across social media.",,"Celia Chen, Scotty Beland, Ingo Burghardt, Jill Byczek, William J. Conway, Eric Cotugno, Sadaf Davre, Megan Fletcher, Rajesh Kumar Gnanasekaran, Kristin Hamilton, Marilyn Harbert, Jordan Heustis, Tanaya Jha, Emily Klein, Hayden Kramer, Alex Leitch, Jessica Perkins, Casi Sherman, Celia Sterrn, Logan Stevens, Rebecca Zarrella, Jennifer Golbeck",2025-06-03T18:54:07Z,Cross-Platform Violence Detection on Social Media: A Dataset and   Analysis,Plattformübergreifende Gewalterkennung in sozialen Medien: Ein Datensatz und Analyse,在社会媒体上跨平台暴力探测:数据集和分析,http://arxiv.org/abs/2506.03312v1
442,"Recent studies comparing AI-generated and human-authored literary texts have produced conflicting results: some suggest AI already surpasses human quality, while others argue it still falls short. We start from the hypothesis that such divergences can be largely explained by genuine differences in how readers interpret and value literature, rather than by an intrinsic quality of the texts evaluated. Using five public datasets (1,471 stories, 101 annotators including critics, students, and lay readers), we (i) extract 17 reference-less textual features (e.g., coherence, emotional variance, average sentence length...); (ii) model individual reader preferences, deriving feature importance vectors that reflect their textual priorities; and (iii) analyze these vectors in a shared ""preference space"". Reader vectors cluster into two profiles: 'surface-focused readers' (mainly non-experts), who prioritize readability and textual richness; and 'holistic readers' (mainly experts), who value thematic development, rhetorical variety, and sentiment dynamics. Our results quantitatively explain how measurements of literary quality are a function of how text features align with each reader's preferences. These findings advocate for reader-sensitive evaluation frameworks in the field of creative text generation.",,"Guillermo Marco, Julio Gonzalo, Víctor Fresno",2025-06-03T18:50:22Z,The Reader is the Metric: How Textual Features and Reader Profiles   Explain Conflicting Evaluations of AI Creative Writing,The Reader is the Metric: Wie Textmerkmale und Leserprofile widersprüchliche Bewertungen von KI Creative Writing erklären,读者是《计量:文字特征和阅读器简介如何解释 AI 创意写作的矛盾评价》。,http://arxiv.org/abs/2506.03310v1
443,"Modern causal language models stack many attention blocks to improve performance, but not all blocks are necessary for every task. We propose Hopscotch, a simple yet effective method that identifies and skips attention blocks with least contributions to a task and adapts to preserve output quality. Hopscotch jointly optimizes which blocks to skip and how to scale the outputs of the remaining layers. By introducing lightweight, trainable scaling parameters to attention and MLP blocks, it mitigates distribution shifts in hidden states caused by removing attention blocks. Hopscotch does not modify model weights or require access to pretraining or instruction-tuning data, and is compatible with existing model compression techniques. When applied to $\texttt{Llama-3.1-8B}$ and $\texttt{Qwen2.5-7B}$, Hopscotch achieves less than a 2% drop in performance even after skipping four attention blocks.",,"Mustafa Eyceoz, Nikhil Shivakumar Nayak, Hao Wang, Ligong Han, Akash Srivastava",2025-06-03T18:43:00Z,Hopscotch: Discovering and Skipping Redundancies in Language Models,Hopscotch: Entdecken und Überspringen von Redundanzen in Sprachmodellen,Hopscotch: 发现和跳过语言模型冗余,http://arxiv.org/abs/2506.03303v1
444,"This study presents an approach that uses large language models such as GPT-4 to generate usage policies in the W3C Open Digital Rights Language ODRL automatically from natural language instructions. Our approach uses the ODRL ontology and its documentation as a central part of the prompt. Our research hypothesis is that a curated version of existing ontology documentation will better guide policy generation. We present various heuristics for adapting the ODRL ontology and its documentation to guide an end-to-end KG construction process. We evaluate our approach in the context of dataspaces, i.e., distributed infrastructures for trustworthy data exchange between multiple participating organizations for the cultural domain. We created a benchmark consisting of 12 use cases of varying complexity. Our evaluation shows excellent results with up to 91.95% accuracy in the resulting knowledge graph.",,"Daham M. Mustafa, Abhishek Nadgeri, Diego Collarana, Benedikt T. Arnold, Christoph Quix, Christoph Lange, Stefan Decker",2025-06-03T18:38:41Z,From Instructions to ODRL Usage Policies: An Ontology Guided Approach,Von der Anleitung zu ODRL Nutzungsrichtlinien: Ein Ontologie-geführter Ansatz,从指令到网上解决网上解决机构使用政策:本体学指导方法,http://arxiv.org/abs/2506.03301v1
445,"Steering language models (LMs) by modifying internal activations is a popular approach for controlling text generation. Unsupervised dictionary learning methods, e.g., sparse autoencoders, can be scaled to produce many steering vectors, but lack guarantees on the individual efficacy of each vector and control over the coverage of relevant steering tasks. In contrast, supervised methods for constructing steering vectors are targeted and effective, but require more data collection and training for each additional steering vector produced. In this work, we introduce HyperSteer, a family of hypernetwork-based architectures which are trained end-to-end to generate steering vectors conditioned on the natural language steering prompts and the internals of the steered LM. In our evaluations, we show that scaling HyperSteer with thousands of steering prompts exceeds the performance of state-of-the-art activation steering methods, even on steering prompts never seen during training. Moreover, HyperSteer performs on par with steering-via-prompting.",,"Jiuding Sun, Sidharth Baskaran, Zhengxuan Wu, Michael Sklar, Christopher Potts, Atticus Geiger",2025-06-03T18:32:01Z,HyperSteer: Activation Steering at Scale with Hypernetworks,HyperSteer: Aktivierungssteuerung auf Skala mit Hypernetzwerken,HyperSteer: 超网络比例化启动指导,http://arxiv.org/abs/2506.03292v1
446,"The safety alignment of large language models (LLMs) remains vulnerable, as their initial behavior can be easily jailbroken by even relatively simple attacks. Since infilling a fixed template between the input instruction and initial model output is a common practice for existing LLMs, we hypothesize that this template is a key factor behind their vulnerabilities: LLMs' safety-related decision-making overly relies on the aggregated information from the template region, which largely influences these models' safety behavior. We refer to this issue as template-anchored safety alignment. In this paper, we conduct extensive experiments and verify that template-anchored safety alignment is widespread across various aligned LLMs. Our mechanistic analyses demonstrate how it leads to models' susceptibility when encountering inference-time jailbreak attacks. Furthermore, we show that detaching safety mechanisms from the template region is promising in mitigating vulnerabilities to jailbreak attacks. We encourage future research to develop more robust safety alignment techniques that reduce reliance on the template region.",,"Chak Tou Leong, Qingyu Yin, Jian Wang, Wenjie Li",2025-06-03T18:20:11Z,Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety   Mechanisms Tend to Be Anchored in The Template Region,"Warum schützende Schiffe auf Grund laufen? Ausrichtung der Sicherheitsmechanismen großer Sprachmodelle, die sich in der Vorlagenregion verankern lassen","* 统一大语言模式的安全机制,并设于模板区域",http://arxiv.org/abs/2502.13946v2
447,"As text generation systems' outputs are increasingly anthropomorphic -- perceived as human-like -- scholars have also increasingly raised concerns about how such outputs can lead to harmful outcomes, such as users over-relying or developing emotional dependence on these systems. How to intervene on such system outputs to mitigate anthropomorphic behaviors and their attendant harmful outcomes, however, remains understudied. With this work, we aim to provide empirical and theoretical grounding for developing such interventions. To do so, we compile an inventory of interventions grounded both in prior literature and a crowdsourcing study where participants edited system outputs to make them less human-like. Drawing on this inventory, we also develop a conceptual framework to help characterize the landscape of possible interventions, articulate distinctions between different types of interventions, and provide a theoretical basis for evaluating the effectiveness of different interventions.",,"Myra Cheng, Su Lin Blodgett, Alicia DeVrio, Lisa Egede, Alexandra Olteanu",2025-06-03T18:14:14Z,Dehumanizing Machines: Mitigating Anthropomorphic Behaviors in Text   Generation Systems,Entmenschlichende Maschinen: Mildernde anthropomorphe Verhaltensmuster in Textgenerierungssystemen,非人非人化机器: 减缓文本生成系统中的人类形态行为,http://arxiv.org/abs/2502.14019v2
448,"We introduce FailureSensorIQ, a novel Multi-Choice Question-Answering (MCQA) benchmarking system designed to assess the ability of Large Language Models (LLMs) to reason and understand complex, domain-specific scenarios in Industry 4.0. Unlike traditional QA benchmarks, our system focuses on multiple aspects of reasoning through failure modes, sensor data, and the relationships between them across various industrial assets. Through this work, we envision a paradigm shift where modeling decisions are not only data-driven using statistical tools like correlation analysis and significance tests, but also domain-driven by specialized LLMs which can reason about the key contributors and useful patterns that can be captured with feature engineering. We evaluate the Industrial knowledge of over a dozen LLMs-including GPT-4, Llama, and Mistral-on FailureSensorIQ from different lens using Perturbation-Uncertainty-Complexity analysis, Expert Evaluation study, Asset-Specific Knowledge Gap analysis, ReAct agent using external knowledge-bases. Even though closed-source models with strong reasoning capabilities approach expert-level performance, the comprehensive benchmark reveals a significant drop in performance that is fragile to perturbations, distractions, and inherent knowledge gaps in the models. We also provide a real-world case study of how LLMs can drive the modeling decisions on 3 different failure prediction datasets related to various assets. We release: (a) expert-curated MCQA for various industrial assets, (b) FailureSensorIQ benchmark and Hugging Face leaderboard based on MCQA built from non-textual data found in ISO documents, and (c) LLMFeatureSelector, an LLM-based feature selection scikit-learn pipeline. The software is available at https://github.com/IBM/FailureSensorIQ.",,"Christodoulos Constantinides, Dhaval Patel, Shuxin Lin, Claudio Guerrero, Sunil Dagajirao Patil, Jayant Kalagnanam",2025-06-03T18:05:10Z,FailureSensorIQ: A Multi-Choice QA Dataset for Understanding Sensor   Relationships and Failure Modes,FailureSensorIQ: Ein Multi-Choice QA-Datensatz zum Verständnis von Sensorbeziehungen und Ausfallmodi,失败传感器Q:用于了解传感器关系和故障模式的多选择 QA数据集,http://arxiv.org/abs/2506.03278v1
449,"We present ReFoRCE, a Text-to-SQL agent that tops the Spider 2.0 leaderboard--a challenging benchmark reflecting complex, real-world Text-to-SQL scenarios. While Text-to-SQL systems enable natural language queries over structured databases, deploying them in enterprise environments remains difficult due to large, complex schemas (with over 1,000 columns), diverse SQL dialects (e.g., BigQuery, Snowflake), and sophisticated query requirements (e.g., transformations and analytics). ReFoRCE addresses these challenges through: (a) database information compression via pattern-based table grouping and LLM-guided schema linking to alleviate long-context issues; (b) self-refinement to iteratively correct syntax and semantic errors across dialects; (c) majority-vote consensus to select high-confidence candidates while deferring ambiguous cases arising from sophisticated queries; and (d) iterative column exploration guided by execution feedback to resolve those deferred cases. ReFoRCE achieves new state-of-the-art results, with scores of 35.83 on Spider 2.0-Snow and 36.56 on Spider 2.0-Lite.",,"Minghang Deng, Ashwin Ramachandran, Canwen Xu, Lanxiang Hu, Zhewei Yao, Anupam Datta, Hao Zhang",2025-06-03T18:03:32Z,"ReFoRCE: A Text-to-SQL Agent with Self-Refinement, Consensus   Enforcement, and Column Exploration","ReFoRCE: Ein Text-zu-SQL-Agent mit Selbstveredelung, Konsensdurchsetzung und Kolumnenexploration",ReFoRCE: 具有自我改进、共识执行和专栏勘探的文本到SQL代理,http://arxiv.org/abs/2502.00675v5
450,This is the final remark on the replies received to my target paper in the Italian Journal of Linguistics,,Cristiano Chesi,2025-06-03T18:00:29Z,A conclusive remark on linguistic theorizing and language modeling,Eine abschließende Bemerkung zur sprachlichen Theoretisierung und Sprachmodellierung,关于语言理论和语言建模的结论性评论,http://arxiv.org/abs/2506.03268v1
451,"Purpose: This study aims to evaluate the effectiveness of large language models (LLMs) in automating disease annotation of CT radiology reports. We compare a rule-based algorithm (RBA), RadBERT, and three lightweight open-weight LLMs for multi-disease labeling of chest, abdomen, and pelvis (CAP) CT reports.   Materials and Methods: This retrospective study analyzed 40,833 CT reports from 29,540 patients, with 1,789 CAP reports manually annotated across three organ systems. External validation was conducted using the CT-RATE dataset. Three open-weight LLMs were tested with zero-shot prompting. Performance was evaluated using Cohen's Kappa and micro/macro-averaged F1 scores.   Results: In 12,197 Duke CAP reports from 8,854 patients, Llama-3.1 8B and Gemma-3 27B showed the highest agreement ($\kappa$ median: 0.87). On the manually annotated set, Gemma-3 27B achieved the top macro-F1 (0.82), followed by Llama-3.1 8B (0.79), while the RBA scored lowest (0.64). On the CT-RATE dataset (lungs/pleura only), Llama-3.1 8B performed best (0.91), with Gemma-3 27B close behind (0.89). Performance differences were mainly due to differing labeling practices, especially for lung atelectasis.   Conclusion: Lightweight LLMs outperform rule-based methods for CT report annotation and generalize across organ systems with zero-shot prompting. However, binary labels alone cannot capture the full nuance of report language. LLMs can provide a flexible, efficient solution aligned with clinical judgment and user needs.",,"Michael E. Garcia-Alcoser, Mobina GhojoghNejad, Fakrul Islam Tushar, David Kim, Kyle J. Lafata, Geoffrey D. Rubin, Joseph Y. Lo",2025-06-03T18:00:08Z,Evaluating Large Language Models for Zero-Shot Disease Labeling in CT   Radiology Reports Across Organ Systems,Bewertung großer Sprachmodelle für Zero-Shot Disease-Etikettierung in CT-Radiologieberichten über Organsysteme,评价跨机构系统CT放射报告零热疾病标签大语言模型,http://arxiv.org/abs/2506.03259v1
452,"Modern language models are typically trained over subword sequences, but ultimately define probabilities over character-strings. Ideally, the choice of the tokeniser -- which maps character-strings to subwords -- should not affect the probability assigned to the underlying character-string; in practice, it does. We define this mismatch as tokenisation bias. In this work, we quantify one particular type of tokenisation bias: the effect of including or not a subword (e.g., $\langle hello \rangle$) in a tokeniser's vocabulary on the probability a trained model assigns to the corresponding characters (i.e., \textit{``hello''}). Estimating this effect is challenging because each model is trained with only one tokeniser. We address this by framing tokenisation bias as a causal effect and estimating it using the regression discontinuity design. Specifically, we exploit the fact that tokenisation algorithms rank subwords and add the first $K$ to a tokeniser's vocabulary, where $K$ is an arbitrary cutoff point. As such, we can estimate a causal effect by comparing similar subwords around this cutoff. Experimentally, we find that tokenisation consistently affects models' outputs across scales, vocabularies, and tokenisers. Notably, a subword's presence in a small model's vocabulary may increase its characters' probability by up to 17 times, highlighting tokenisation as a key design choice in language modelling.",,"Pietro Lesci, Clara Meister, Thomas Hofmann, Andreas Vlachos, Tiago Pimentel",2025-06-03T17:59:47Z,Causal Estimation of Tokenisation Bias,Kausale Schätzung von Tokenisierungs-Bias,因果估算 Tokenization Bias,http://arxiv.org/abs/2506.03149v1
453,"Neuroscience research publications encompass a vast wealth of knowledge. Accurately retrieving existing information and discovering new insights from this extensive literature is essential for advancing the field. However, when knowledge is dispersed across multiple sources, current state-of-the-art retrieval methods often struggle to extract the necessary information. A knowledge graph (KG) can integrate and link knowledge from multiple sources, but existing methods for constructing KGs in neuroscience often rely on labeled data and require domain expertise. Acquiring large-scale, labeled data for a specialized area like neuroscience presents significant challenges. This work proposes novel methods for constructing KG from unlabeled large-scale neuroscience research corpus utilizing large language models (LLM), neuroscience ontology, and text embeddings. We analyze the semantic relevance of neuroscience text segments identified by LLM for building the knowledge graph. We also introduce an entity-augmented information retrieval algorithm to extract knowledge from the KG. Several experiments were conducted to evaluate the proposed approaches, and the results demonstrate that our methods significantly enhance knowledge discovery from the unlabeled neuroscience research corpus. It achieves an F1 score of 0.84 for entity extraction, and the knowledge obtained from the KG improves answers to over 54% of the questions.",,"Pralaypati Ta, Sriram Venkatesaperumal, Keerthi Ram, Mohanasankar Sivaprakasam",2025-06-03T17:59:18Z,Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and   Semantic Understanding Capability of LLM,Entity-Augmented Neuroscience Knowledge Retrieval mit Ontologie und semantisches Verständnis Fähigkeit von LLM,利用LLM的本体学和语法理解能力进行实体强化神经科学知识检索,http://arxiv.org/abs/2506.03145v1
454,"Semantic retrieval is crucial for modern applications yet remains underexplored in current research. Existing datasets are limited to single languages, single images, or singular retrieval conditions, often failing to fully exploit the expressive capacity of visual information as evidenced by maintained performance when images are replaced with captions. However, practical retrieval scenarios frequently involve interleaved multi-condition queries with multiple images. Hence, this paper introduces MERIT, the first multilingual dataset for interleaved multi-condition semantic retrieval, comprising 320,000 queries with 135,000 products in 5 languages, covering 7 distinct product categories. Extensive experiments on MERIT identify existing models's limitation: focusing solely on global semantic information while neglecting specific conditional elements in queries. Consequently, we propose Coral, a novel fine-tuning framework that adapts pre-trained MLLMs by integrating embedding reconstruction to preserve fine-grained conditional elements and contrastive learning to extract comprehensive global semantics. Experiments demonstrate that Coral achieves a 45.9% performance improvement over conventional approaches on MERIT, with strong generalization capabilities validated across 8 established retrieval benchmarks. Collectively, our contributions - a novel dataset, identification of critical limitations in existing approaches, and an innovative fine-tuning framework - establish a foundation for future research in interleaved multi-condition semantic retrieval.",,"Wei Chow, Yuan Gao, Linfeng Li, Xian Wang, Qi Xu, Hang Song, Lingdong Kong, Ran Zhou, Yi Zeng, Yidong Cai, Botian Jiang, Shilin Xu, Jiajun Zhang, Minghui Qiu, Xiangtai Li, Tianshu Yang, Siliang Tang, Juncheng Li",2025-06-03T17:59:14Z,MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition   Query,MERIT: Mehrsprachiges Semantisches Retrieval mit interleaved Multi-Condition Query,MERIT: 多语种语义检索与间断多语种状态查询,http://arxiv.org/abs/2506.03144v1
455,"One of the principal challenges in building VLM-powered GUI agents is visual grounding, i.e., localizing the appropriate screen region for action execution based on both the visual content and the textual plans. Most existing work formulates this as a text-based coordinate generation task. However, these approaches suffer from several limitations: weak spatial-semantic alignment, inability to handle ambiguous supervision targets, and a mismatch between the dense nature of screen coordinates and the coarse, patch-level granularity of visual features extracted by models like Vision Transformers. In this paper, we propose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its core, GUI-Actor introduces an attention-based action head that learns to align a dedicated <ACTOR> token with all relevant visual patch tokens, enabling the model to propose one or more action regions in a single forward pass. In line with this, we further design a grounding verifier to evaluate and select the most plausible action region from the candidates proposed for action execution. Extensive experiments show that GUI-Actor outperforms prior state-of-the-art methods on multiple GUI action grounding benchmarks, with improved generalization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B even surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7 with Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by incorporating the verifier, we find that fine-tuning only the newly introduced action head (~100M parameters for 7B model) while keeping the VLM backbone frozen is sufficient to achieve performance comparable to previous state-of-the-art models, highlighting that GUI-Actor can endow the underlying VLM with effective grounding capabilities without compromising its general-purpose strengths.",,"Qianhui Wu, Kanzhi Cheng, Rui Yang, Chaoyun Zhang, Jianwei Yang, Huiqiang Jiang, Jian Mu, Baolin Peng, Bo Qiao, Reuben Tan, Si Qin, Lars Liden, Qingwei Lin, Huan Zhang, Tong Zhang, Jianbing Zhang, Dongmei Zhang, Jianfeng Gao",2025-06-03T17:59:08Z,GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents,GUI-Actor: Koordinierungsfreies Visual Grounding für GUI-Agenten,GUI-Actor: GUI 代理工具的无协调视觉定位,http://arxiv.org/abs/2506.03143v1
456,"We propose CURE, a novel reinforcement learning framework with a dedicated reward design that co-evolves coding and unit test generation capabilities based on their interaction outcomes, without any ground-truth code as supervision. This approach enables flexible and scalable training and allows the unit tester to learn directly from the coder's mistakes. Our derived ReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and Best-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models, outperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They naturally extend to downstream tasks such as test-time scaling and agentic coding-achieving a 8.1% improvement over the base model. For the long-CoT model, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while achieving 64.8% inference efficiency in unit test generation. Notably, we also find that our model can serve as an effective reward model for reinforcement learning on base models. Project: https://github.com/Gen-Verse/CURE",,"Yinjie Wang, Ling Yang, Ye Tian, Ke Shen, Mengdi Wang",2025-06-03T17:58:42Z,Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning,Co-Evolving LLM Coder und Unit Tester über Verstärkungslernen,通过强化学习共同演进的LLMM调制解码器和单元测试器,http://arxiv.org/abs/2506.03136v1
457,"Spatial reasoning is a key aspect of cognitive psychology and remains a major bottleneck for current vision-language models (VLMs). While extensive research has aimed to evaluate or improve VLMs' understanding of basic spatial relations, such as distinguishing left from right, near from far, and object counting, these tasks represent only the most fundamental level of spatial reasoning. In this work, we introduce OmniSpatial, a comprehensive and challenging benchmark for spatial reasoning, grounded in cognitive psychology. OmniSpatial covers four major categories: dynamic reasoning, complex spatial logic, spatial interaction, and perspective-taking, with 50 fine-grained subcategories. Through Internet data crawling and careful manual annotation, we construct over 1.5K question-answer pairs. Extensive experiments show that both open- and closed-source VLMs, as well as existing reasoning and spatial understanding models, exhibit significant limitations in comprehensive spatial understanding. We further analyze failure cases and propose potential directions for future research.",,"Mengdi Jia, Zekun Qi, Shaochen Zhang, Wenyao Zhang, Xinqiang Yu, Jiawei He, He Wang, Li Yi",2025-06-03T17:58:29Z,OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for   Vision Language Models,OmniSpatial: Auf dem Weg zu einer umfassenden räumlichen Begründung Benchmark für Vision Language Models,广域:争取实现愿景语言模式综合空间理由基准,http://arxiv.org/abs/2506.03135v1
458,"A significant debate has emerged in response to a paper written by Steven Piantadosi (Piantadosi, 2023) and uploaded to the LingBuzz platform, the open archive for generative linguistics. Piantadosi's dismissal of Chomsky's approach is ruthless, but generative linguists deserve it. In this paper, I will adopt three idealized perspectives -- computational, theoretical, and experimental -- to focus on two fundamental issues that lend partial support to Piantadosi's critique: (a) the evidence challenging the Poverty of Stimulus (PoS) hypothesis and (b) the notion of simplicity as conceived within mainstream Minimalism. In conclusion, I argue that, to reclaim a central role in language studies, generative linguistics -- representing a prototypical theoretical perspective on language -- needs a serious update leading to (i) more precise, consistent, and complete formalizations of foundational intuitions and (ii) the establishment and utilization of a standardized dataset of crucial empirical evidence to evaluate the theory's adequacy. On the other hand, ignoring the formal perspective leads to major drawbacks in both computational and experimental approaches. Neither descriptive nor explanatory adequacy can be easily achieved without the precise formulation of general principles that can be challenged empirically.",,Cristiano Chesi,2025-06-03T17:56:44Z,Is it the end of (generative) linguistics as we know it?,"Ist es das Ende der (generativen) Linguistik, wie wir sie kennen?",难道这是我们所知道的语言语言的尽头吗?,http://arxiv.org/abs/2412.12797v2
459,"Analog circuit topology synthesis is integral to Electronic Design Automation (EDA), enabling the automated creation of circuit structures tailored to specific design requirements. However, the vast design search space and strict constraint adherence make efficient synthesis challenging. Leveraging the versatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel reinforcement learning (RL)-based framework for automated analog circuit synthesis. The framework operates in two phases: instruction tuning, where an LLM learns to generate circuit topologies from structured prompts encoding design constraints, and RL refinement, which further improves the instruction-tuned model using reward models that evaluate validity, efficiency, and output voltage. The refined model is then used directly to generate topologies that satisfy the design constraints. Empirical results show that AUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by ~14% compared to the best baselines, while reducing duplicate generation rates by ~38%. It achieves over 60% success in synthesizing valid circuits with limited training data, demonstrating strong generalization. These findings highlight the framework's effectiveness in scaling to complex circuits while maintaining efficiency and constraint adherence, marking a significant advancement in AI-driven circuit design.",,"Prashanth Vijayaraghavan, Luyao Shi, Ehsan Degan, Vandana Mukherjee, Xin Zhang",2025-06-03T17:54:30Z,AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit   Topology Generation,AUTOCIRCUIT-RL: Verstärkungslernen-getriebenes LLM für automatisierte Schaltungstopologie-Generierung,AUOCIRCUIT-RL: 用于自动电路生成地形学的强化学习驱动LLMLM,http://arxiv.org/abs/2506.03122v1
460,"Large language models (LLMs) have demonstrated strong effectiveness and robustness while fine-tuned as dense retrievers. However, their large parameter size brings significant inference time computational challenges, including high encoding costs for large-scale corpora and increased query latency, limiting their practical deployment. While smaller retrievers offer better efficiency, they often fail to generalize effectively with limited supervised fine-tuning data. In this work, we introduce DRAMA, a training framework that leverages LLMs to train smaller generalizable dense retrievers. In particular, we adopt pruned LLMs as the backbone and train on diverse LLM-augmented data in a single-stage contrastive learning setup. Experiments show that DRAMA offers better multilingual and long-context capabilities than traditional encoder-based retrievers, and achieves strong performance across multiple tasks and languages. These highlight the potential of connecting the training of smaller retrievers with the growing advancements in LLMs, bridging the gap between efficiency and generalization.",,"Xueguang Ma, Xi Victoria Lin, Barlas Oguz, Jimmy Lin, Wen-tau Yih, Xilun Chen",2025-06-03T17:47:36Z,DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense   Retrievers,DRAMA: Vielfältige Augmentation von großen Sprachmodellen zu kleineren Dense Retrievern,DRAMA:从大语言模型到较小规模的密度探索的多样化增强,http://arxiv.org/abs/2502.18460v2
461,"We trained 13,440 large language models and found that entropy minimization requires only a single unlabeled data and 10 steps optimization to achieve performance improvements comparable to or even greater than those obtained using thousands of data and carefully designed rewards in rule-based reinforcement learning. This striking result may prompt a rethinking of post-training paradigms for large language models. Our code is avaliable at https://github.com/zitian-gao/one-shot-em.",,"Zitian Gao, Lynx Chen, Joey Zhou, Bryan Dai",2025-06-03T17:45:49Z,One-shot Entropy Minimization,Ein Schuss Entropie Minimierung,单向最小化 Entropy 最小化,http://arxiv.org/abs/2505.20282v3
462,"The choice of tokenizer can profoundly impact language model performance, yet accessible and reliable evaluations of tokenizer quality remain an open challenge. Inspired by scaling consistency, we show that smaller models can accurately predict significant differences in tokenizer impact on larger models at a fraction of the compute cost. By systematically evaluating both English-centric and multilingual tokenizers, we find that tokenizer choice has negligible effects on tasks in English but results in consistent performance differences in multilingual settings. We propose new intrinsic tokenizer metrics inspired by Zipf's law that correlate more strongly with downstream performance than text compression when modeling unseen languages. By combining several metrics to capture multiple aspects of tokenizer behavior, we develop a reliable framework for intrinsic tokenizer evaluations. Our work offers a more efficient path to informed tokenizer selection in future language model development.",,"Jonas F. Lotz, António V. Lopes, Stephan Peitz, Hendra Setiawan, Leonardo Emili",2025-06-03T17:35:56Z,Beyond Text Compression: Evaluating Tokenizers Across Scales,Beyond Text Compression: Bewertung von Tokenizern über Skalen hinweg,超越文本压缩: 评估各种规模的收缩器,http://arxiv.org/abs/2506.03101v1
463,"Text-based image generation models, such as Stable Diffusion and DALL-E 3, hold significant potential in content creation and publishing workflows, making them the focus in recent years. Despite their remarkable capability to generate diverse and vivid images, considerable efforts are being made to prevent the generation of harmful content, such as abusive, violent, or pornographic material. To assess the safety of existing models, we introduce a novel jailbreaking method called Chain-of-Jailbreak (CoJ) attack, which compromises image generation models through a step-by-step editing process. Specifically, for malicious queries that cannot bypass the safeguards with a single prompt, we intentionally decompose the query into multiple sub-queries. The image generation models are then prompted to generate and iteratively edit images based on these sub-queries. To evaluate the effectiveness of our CoJ attack method, we constructed a comprehensive dataset, CoJ-Bench, encompassing nine safety scenarios, three types of editing operations, and three editing elements. Experiments on four widely-used image generation services provided by GPT-4V, GPT-4o, Gemini 1.5 and Gemini 1.5 Pro, demonstrate that our CoJ attack method can successfully bypass the safeguards of models for over 60% cases, which significantly outperforms other jailbreaking methods (i.e., 14%). Further, to enhance these models' safety against our CoJ attack method, we also propose an effective prompting-based method, Think Twice Prompting, that can successfully defend over 95% of CoJ attack. We release our dataset and code to facilitate the AI safety research.",,"Wenxuan Wang, Kuiyi Gao, Youliang Yuan, Jen-tse Huang, Qiuzhi Liu, Shuai Wang, Wenxiang Jiao, Zhaopeng Tu",2025-06-03T17:32:00Z,Chain-of-Jailbreak Attack for Image Generation Models via Editing Step   by Step,Chain-of-Jailbreak Attack für Modelle der Bildgenerierung per Editing Schritt für Schritt,通过一步一步的编辑步骤对图像生成模型进行 Jailbreab 攻击,http://arxiv.org/abs/2410.03869v2
464,"Multimodal Large Language Models (MLLMs) have expanded the capabilities of traditional language models by enabling interaction through both text and images. However, ensuring the safety of these models remains a significant challenge, particularly in accurately identifying whether multimodal content is safe or unsafe-a capability we term safety awareness. In this paper, we introduce MMSafeAware, the first comprehensive multimodal safety awareness benchmark designed to evaluate MLLMs across 29 safety scenarios with 1500 carefully curated image-prompt pairs. MMSafeAware includes both unsafe and over-safety subsets to assess models abilities to correctly identify unsafe content and avoid over-sensitivity that can hinder helpfulness. Evaluating nine widely used MLLMs using MMSafeAware reveals that current models are not sufficiently safe and often overly sensitive; for example, GPT-4V misclassifies 36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further explore three methods to improve safety awareness-prompting-based approaches, visual contrastive decoding, and vision-centric reasoning fine-tuning-but find that none achieve satisfactory performance. Our findings highlight the profound challenges in developing MLLMs with robust safety awareness, underscoring the need for further research in this area. All the code and data will be publicly available to facilitate future research.",,"Wenxuan Wang, Xiaoyuan Liu, Kuiyi Gao, Jen-tse Huang, Youliang Yuan, Pinjia He, Shuai Wang, Zhaopeng Tu",2025-06-03T17:27:16Z,Can't See the Forest for the Trees: Benchmarking Multimodal Safety   Awareness for Multimodal LLMs,Den Wald für die Bäume nicht sehen: Benchmarking Multimodales Sicherheitsbewusstsein für multimodale LLMs,看不到树的森林:为多模式LMS设定多模式安全意识基准,http://arxiv.org/abs/2502.11184v2
465,"One of the goals of automatic evaluation metrics in grammatical error correction (GEC) is to rank GEC systems such that it matches human preferences. However, current automatic evaluations are based on procedures that diverge from human evaluation. Specifically, human evaluation derives rankings by aggregating sentence-level relative evaluation results, e.g., pairwise comparisons, using a rating algorithm, whereas automatic evaluation averages sentence-level absolute scores to obtain corpus-level scores, which are then sorted to determine rankings. In this study, we propose an aggregation method for existing automatic evaluation metrics which aligns with human evaluation methods to bridge this gap. We conducted experiments using various metrics, including edit-based metrics, n-gram based metrics, and sentence-level metrics, and show that resolving the gap improves results for the most of metrics on the SEEDA benchmark. We also found that even BERT-based metrics sometimes outperform the metrics of GPT-4. The proposed ranking method is integrated gec-metrics.",,"Takumi Goto, Yusuke Sakai, Taro Watanabe",2025-06-03T17:24:47Z,Rethinking Evaluation Metrics for Grammatical Error Correction: Why Use   a Different Evaluation Process than Human?,Rethinking Evaluation Metrics for Grammatical Error Correction: Warum verwenden Sie einen anderen Evaluationsprozess als der Mensch?,重新思考用于校正明显错误的评价计量:为什么使用与人类不同的评价程序?,http://arxiv.org/abs/2502.09416v2
466,"How well do modern long-context language models understand literary fiction? We explore this question via the task of literary evidence retrieval, repurposing the RELiC dataset of That et al. (2022) to construct a benchmark where the entire text of a primary source (e.g., The Great Gatsby) is provided to an LLM alongside literary criticism with a missing quotation from that work. This setting, in which the model must generate the missing quotation, mirrors the human process of literary analysis by requiring models to perform both global narrative reasoning and close textual examination. We curate a high-quality subset of 292 examples through extensive filtering and human verification. Our experiments show that recent reasoning models, such as Gemini Pro 2.5 can exceed human expert performance (62.5% vs. 50% accuracy). In contrast, the best open-weight model achieves only 29.1% accuracy, highlighting a wide gap in interpretive reasoning between open and closed-weight models. Despite their speed and apparent accuracy, even the strongest models struggle with nuanced literary signals and overgeneration, signaling open challenges for applying LLMs to literary analysis. We release our dataset and evaluation code to encourage future work in this direction.",,"Katherine Thai, Mohit Iyyer",2025-06-03T17:19:45Z,Literary Evidence Retrieval via Long-Context Language Models,Literarische Evidence-Retrieval über Langkontext-Sprachenmodelle,通过长文本语言模型检索文学证据,http://arxiv.org/abs/2506.03090v1
467,"Long context large language models (LLMs) pose significant challenges for efficient serving due to the large memory footprint and high access overhead of KV cache. Retrieval-based KV cache reduction methods can mitigate these challenges, typically by offloading the complete KV cache to CPU and retrieving necessary tokens on demand during inference. However, these methods still suffer from unsatisfactory accuracy degradation and extra retrieval overhead. To address these limitations, this paper proposes A$^2$ATS, a novel retrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate approximation of attention scores by applying the vector quantization technique to key states, thereby enabling efficient and precise retrieval of the top-K tokens. First, we propose Windowed Rotary Position Embedding, which decouples the positional dependency from query and key states after position embedding. Then, we propose query-aware vector quantization that optimizes the objective of attention score approximation directly. Finally, we design the heterogeneous inference architecture for KV cache offloading, enabling long context serving with larger batch sizes. Experimental results demonstrate that A$^2$ATS can achieve a lower performance degradation with similar or lower overhead compared to existing methods, thereby increasing long context serving throughput by up to $2.7 \times$.",,"Junhui He, Junna Xing, Nan Wang, Rui Xu, Shangyu Wu, Peng Zhou, Qiang Liu, Chun Jason Xue, Qingan Li",2025-06-03T17:18:23Z,A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary   Position Embedding and Query-Aware Vector Quantization,A$^2$ATS: Retrieval-basierte KV-Cache-Reduktion über Windowed Rotary Position Einbetten und Query-Aware-Vektor-Quantisierung,A$$2$ATS:通过窗口式扶轮定位嵌入和查询矢量量化减少以回收为基地的 KV 缓存,http://arxiv.org/abs/2502.12665v2
468,"Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL). These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning. To this end, we propose d1, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL. Specifically, we develop and extend techniques to improve reasoning in pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO, the first integration of policy gradient methods to masked dLLMs. Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and planning benchmarks. We find that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM. Our code is released at https://dllm-reasoning.github.io/.",,"Siyan Zhao, Devaansh Gupta, Qinqing Zheng, Aditya Grover",2025-06-03T17:02:25Z,d1: Scaling Reasoning in Diffusion Large Language Models via   Reinforcement Learning,d1: Scaling Reasoning in Diffusion Große Sprachmodelle über Verstärkungslernen,d1:通过强化学习推广大语言模式的扩大理由,http://arxiv.org/abs/2504.12216v2
469,"Computing the polar decomposition and the related matrix sign function, has been a well-studied problem in numerical analysis for decades. More recently, it has emerged as an important subroutine in deep learning, particularly within the Muon optimization framework. However, the requirements in this setting differ significantly from those of traditional numerical analysis. In deep learning, methods must be highly efficient and GPU-compatible, but high accuracy is often unnecessary. As a result, classical algorithms like Newton-Schulz (which suffers from slow initial convergence) and methods based on rational functions (which rely on QR decompositions or matrix inverses) are poorly suited to this context. In this work, we introduce Polar Express, a GPU-friendly algorithm for computing the polar decomposition. Like classical polynomial methods such as Newton-Schulz, our approach uses only matrix-matrix multiplications, making it GPU-compatible. Motivated by earlier work of Chen & Chow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule at each iteration by solving a minimax optimization problem, and we prove that it enjoys a strong worst-case optimality guarantee. This property ensures both rapid early convergence and fast asymptotic convergence. We also address finite-precision issues, making it stable in bfloat16 in practice. We apply Polar Express within the Muon optimization framework and show consistent improvements in validation loss on large-scale models such as GPT-2, outperforming recent alternatives across a range of learning rates.",,"Noah Amsel, David Persson, Christopher Musco, Robert M. Gower",2025-06-03T16:46:45Z,The Polar Express: Optimal Matrix Sign Methods and Their Application to   the Muon Algorithm,Der Polar Express: Optimale Matrix Sign Methoden und ihre Anwendung auf den Muon-Algorithmus,极地快车:最佳矩阵信号方法及其在 Muon 算法中的应用,http://arxiv.org/abs/2505.16932v2
470,"Traditional AI safety evaluations on isolated LLMs are insufficient as multi-agent AI ensembles become prevalent, introducing novel emergent risks. This paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE) framework to systematically assess such risks. Using MAEBE with the Greatest Good Benchmark (and a novel double-inversion question technique), we demonstrate that: (1) LLM moral preferences, particularly for Instrumental Harm, are surprisingly brittle and shift significantly with question framing, both in single agents and ensembles. (2) The moral reasoning of LLM ensembles is not directly predictable from isolated agent behavior due to emergent group dynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure influencing convergence, even when guided by a supervisor, highlighting distinct safety and alignment challenges. Our findings underscore the necessity of evaluating AI systems in their interactive, multi-agent contexts.",,"Sinem Erisken, Timothy Gothard, Martin Leitgab, Ram Potham",2025-06-03T16:33:47Z,MAEBE: Multi-Agent Emergent Behavior Framework,MAEBE: Multi-Agent Emergent Behavior Framework,多边代理新兴行为框架,http://arxiv.org/abs/2506.03053v1
471,"Factuality is a necessary precursor to useful educational tools. As adoption of Large Language Models (LLMs) in education continues of grow, ensuring correctness in all settings is paramount. Despite their strong English capabilities, LLM performance in other languages is largely untested. In this work, we evaluate the correctness of the Llama3.1 family of models in answering factual questions appropriate for middle and high school students. We demonstrate that LLMs not only provide extraneous and less truthful information, but also exacerbate existing biases against rare languages.",,"Yuval Kansal, Shmuel Berman, Lydia Liu",2025-06-03T16:31:52Z,Facts Do Care About Your Language: Assessing Answer Quality of   Multilingual LLMs,Fakten kümmern sich um Ihre Sprache: Bewertung der Antwort Qualität von mehrsprachigen LLMs,关注您的语言:评估多语种LLM的回答质量,http://arxiv.org/abs/2506.03051v1
472,"Public model repositories now contain millions of models, yet most models remain undocumented and effectively lost. In this position paper, we advocate for charting the world's model population in a unified structure we call the Model Atlas: a graph that captures models, their attributes, and the weight transformations that connect them. The Model Atlas enables applications in model forensics, meta-ML research, and model discovery, challenging tasks given today's unstructured model repositories. However, because most models lack documentation, large atlas regions remain uncharted. Addressing this gap motivates new machine learning methods that treat models themselves as data, inferring properties such as functionality, performance, and lineage directly from their weights. We argue that a scalable path forward is to bypass the unique parameter symmetries that plague model weights. Charting all the world's models will require a community effort, and we hope its broad utility will rally researchers toward this goal.",,"Eliahu Horwitz, Nitzan Kurer, Jonathan Kahana, Liel Amar, Yedid Hoshen",2025-06-03T16:28:07Z,We Should Chart an Atlas of All the World's Models,Wir sollten einen Atlas aller Modelle der Welt aufzeichnen,我们应该绘制一份世界所有模型的地图集,http://arxiv.org/abs/2503.10633v2
473,"Reinforcement learning (RL) enhances large language models (LLMs) in complex, long-chain-of-thought (long-CoT) reasoning. The advanced VAPO framework, despite sophisticated mechanisms like Decoupled GAE, theoretically faces fundamental limitations in comprehensively modeling and leveraging deep, long-term value for fine-grained, step-by-step policy guidance in extended reasoning chains. We argue these limitations stem from inherent difficulties in credit assignment, value function representational capacity with temporally abstracted goals, and translating global value signals into local policy improvements, especially with sparse rewards. Our theoretical analysis examines these aspects to illuminate VAPO's boundaries in long-term value modeling, aiming to deepen understanding of current RL for advanced reasoning and suggest future research for more robust LLM agents.",,"Jintian Shao, Yiming Cheng",2025-06-03T16:20:47Z,Towards Analyzing and Understanding the Limitations of VAPO: A   Theoretical Perspective,Auf dem Weg zur Analyse und dem Verständnis der Grenzen von VAPO: Eine theoretische Perspektive,分析和理解VAPO的局限性:理论视角,http://arxiv.org/abs/2506.03038v1
474,"Understanding user queries is fundamental in many applications, such as home assistants, booking systems, or recommendations. Accordingly, it is crucial to develop accurate Spoken Language Understanding (SLU) approaches to ensure the reliability of the considered system. Current State-of-the-Art SLU techniques rely on large amounts of training data; however, only limited annotated examples are available for specific tasks or languages.   In the meantime, instruction-tuned large language models (LLMs) have shown exceptional performance on unseen tasks in a few-shot setting when provided with adequate prompts. In this work, we propose to explore example selection by leveraging Information retrieval (IR) approaches to build an enhanced prompt that is applied to an SLU task. We evaluate the effectiveness of the proposed method on several SLU benchmarks. Experimental results show that lexical IR methods significantly enhance performance without increasing prompt length.",,"Pierre Lepagnol, Sahar Ghannay, Thomas Gerald, Christophe Servan, Sophie Rosset",2025-06-03T16:18:45Z,Leveraging Information Retrieval to Enhance Spoken Language   Understanding Prompts in Few-Shot Learning,"Leveraging Information Retrieval, um gesprochene Sprachverständnis Prompts in wenigen-shot-Lernen zu verbessern",在少热学习中利用信息检索法加强口语语言理解提示,http://arxiv.org/abs/2506.03035v1
475,"Recent impressive improvements in NLP, largely based on the success of contextual neural language models, have been mostly demonstrated on at most a couple dozen high-resource languages. Building language models and, more generally, NLP systems for non-standardized and low-resource languages remains a challenging task. In this work, we focus on North-African colloquial dialectal Arabic written using an extension of the Latin script, called NArabizi, found mostly on social media and messaging communication. In this low-resource scenario with data displaying a high level of variability, we compare the downstream performance of a character-based language model on part-of-speech tagging and dependency parsing to that of monolingual and multilingual models. We show that a character-based model trained on only 99k sentences of NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pre-trained on large multilingual and monolingual models. Confirming these results a on much larger data set of noisy French user-generated content, we argue that such character-based language models can be an asset for NLP in low-resource and high language variability set-tings.",,"Arij Riabi, Benoît Sagot, Djamé Seddah",2025-06-03T16:11:13Z,Can Character-based Language Models Improve Downstream Task Performance   in Low-Resource and Noisy Language Scenarios?,Können zeichenbasierte Sprachmodelle die Downstream-Aufgabenleistung in Low-Resource- und Noisy-Sprachszenarien verbessern?,以字符为基础的语言模式能否改善低资源和噪音语言情景下游任务绩效?,http://arxiv.org/abs/2110.13658v2
476,"Text-to-SQL systems empower users to interact with databases using natural language, automatically translating queries into executable SQL code. However, their reliance on database schema information for SQL generation exposes them to significant security vulnerabilities, particularly schema inference attacks that can lead to unauthorized data access or manipulation. In this paper, we introduce a novel zero-knowledge framework for reconstructing the underlying database schema of text-to-SQL models without any prior knowledge of the database. Our approach systematically probes text-to-SQL models with specially crafted questions and leverages a surrogate GPT-4 model to interpret the outputs, effectively uncovering hidden schema elements -- including tables, columns, and data types. We demonstrate that our method achieves high accuracy in reconstructing table names, with F1 scores of up to .99 for generative models and .78 for fine-tuned models, underscoring the severity of schema leakage risks. We also show that our attack can steal prompt information in non-text-to-SQL models. Furthermore, we propose a simple protection mechanism for generative models and empirically show its limitations in mitigating these attacks.",,"Đorđe Klisura, Anthony Rios",2025-06-03T16:10:16Z,Unmasking Database Vulnerabilities: Zero-Knowledge Schema Inference   Attacks in Text-to-SQL Systems,Entlarvung von Datenbanklücken: Null-Knowledge-Schema-Inferenzangriffe in Text-zu-SQL-Systemen,释放数据数据库脆弱性:在文本到SQL系统中零知识化地表突袭,http://arxiv.org/abs/2406.14545v3
477,"Knowledge Editing, which efficiently modifies the knowledge in large language models, has gathered great attention. Current benchmarks primarily use multi-hop question answering to assess and analyze newly injected or updated knowledge. However, we argue that these benchmarks fail to effectively evaluate how well the updated models apply this knowledge in real-life scenarios, particularly when questions require complex reasoning, involving one-to-many relationships or multi-step logical intersections. To fill in this gap, we introduce a new benchmark, COMPKE: Complex Question Answering under Knowledge Editing, which includes 11,924 complex questions that reflect real-life situations. We conduct an extensive evaluation of four knowledge editing methods on COMPKE, revealing that their effectiveness varies notably across different models. For instance, MeLLo attains an accuracy of 39.47 on GPT-4O-MINI, but this drops sharply to 3.83 on QWEN2.5-3B. We further investigate the underlying causes of these disparities from both methodological and model-specific perspectives. The datasets are available at https://github.com/kzjkzj666/CompKE.",,"Keyuan Cheng, Zijian Kan, Zhixian He, Zhuoran Zhang, Muhammad Asif Ali, Ke Xu, Lijie Hu, Di Wang",2025-06-03T16:03:55Z,COMPKE: Complex Question Answering under Knowledge Editing,COMPKE: Komplexe Fragebeantwortung unter Wissensbearbeitung,COMKE: 知识编辑下的复杂问题回答,http://arxiv.org/abs/2506.00829v2
478,"As pre-trained language models grow in size, full fine-tuning their parameters on task adaptation data becomes increasingly impractical. To address this challenge, some methods for low-rank adaptation of language models have been proposed, e.g. LoRA, which incorporates trainable low-rank decomposition matrices into only some parameters of the pre-trained model, called adapters. This approach significantly reduces the number of trainable parameters compared to fine-tuning all parameters or adapters. In this work, we look at low-rank adaptation method from the lens of data privacy. We show theoretically that the low-rank adaptation used in LoRA is equivalent to fine-tuning adapters with noisy batch gradients - just like what DPSGD algorithm does. We also quantify the variance of the injected noise as a decreasing function of adaptation rank. By establishing a Berry-Esseen type bound on the total variation distance between the injected noise distribution and a Gaussian noise distribution with the same variance, we show that the dynamics of low-rank adaptation is very close to when DPSGD is performed w.r.t the adapters. Following our theoretical findings and approved by our experimental results, we show that low-rank adaptation provides robustness to membership inference attacks w.r.t the fine-tuning data.",,"Saber Malekmohammadi, Golnoosh Farnadi",2025-06-03T16:03:24Z,Low-Rank Adaptation Secretly Imitates Differentially Private SGD,Low-Rank-Anpassung hinterhältig imitiert unterschiedlich private SGD,低浓度适应 秘密模仿 不同的私人 SGD,http://arxiv.org/abs/2409.17538v6
479,"Large language models (LLMs) are a powerful tool with the ability to match human capabilities and behavior in many settings. Retrieval-augmented generation (RAG) further allows LLMs to generate diverse output depending on the contents of their RAG database. This motivates their use in the social sciences to study human behavior between individuals when large-scale experiments are infeasible. However, LLMs depend on complex, computationally expensive algorithms. In this paper, we introduce interacting Gaussian mixture models (GMMs) as an alternative to similar frameworks using LLMs. We compare a simplified model of GMMs to select experimental simulations of LLMs whose updating and response depend on feedback from other LLMs. We find that interacting GMMs capture important features of the dynamics in interacting LLMs, and we investigate key similarities and differences between interacting LLMs and GMMs. We conclude by discussing the benefits of Gaussian mixture models, potential modifications, and future research directions.",,"Edward L. Wang, Tianyu Wang, Avanti Athreya, Vince Lyzinski, Carey E. Priebe",2025-06-03T16:01:41Z,Gaussian mixture models as a proxy for interacting language models,Gaußsche Mischungsmodelle als Proxy für interagierende Sprachmodelle,Gaussian 混合模型作为交互语言模型的替代,http://arxiv.org/abs/2506.00077v2
480,"Modern human labor is characterized by specialization; we train for years and develop particular tools that allow us to perform well across a variety of tasks. In addition, AI agents have been specialized for domains such as software engineering, web navigation, and workflow automation. However, this results in agents that are good for one thing but fail to generalize beyond their intended scope. One reason for this is that agent developers provide a highly specialized set of tools or make architectural decisions optimized for a specific use case or benchmark. In this work, we ask the question: what is the minimal set of general tools that can be used to achieve high performance across a diverse set of tasks? Our answer is OpenHands-Versa, a generalist agent built with a modest number of general tools: code editing and execution, web search, as well as multimodal web browsing and file access. Importantly, OpenHands-Versa demonstrates superior or competitive performance over leading specialized agents across three diverse and challenging benchmarks: SWE-Bench Multimodal, GAIA, and The Agent Company, outperforming the best-performing previously published results with absolute improvements in success rate of 9.1, 1.3, and 9.1 points respectively. Further, we show how existing state-of-the-art multi-agent systems fail to generalize beyond their target domains. These results demonstrate the feasibility of developing a generalist agent to solve diverse tasks and establish OpenHands-Versa as a strong baseline for future research.",,"Aditya Bharat Soni, Boxuan Li, Xingyao Wang, Valerie Chen, Graham Neubig",2025-06-03T15:50:55Z,Coding Agents with Multimodal Browsing are Generalist Problem Solvers,Coding-Agenten mit multimodaler Browsing sind generalistische Problemlöser,具有多式浏览的编码剂是通用的解决问题剂,http://arxiv.org/abs/2506.03011v1
481,"The assessment of legal problems requires the consideration of a specific legal system and its levels of abstraction, from constitutional law to statutory law to case law. The extent to which Large Language Models (LLMs) internalize such legal systems is unknown. In this paper, we propose and investigate different approaches to condition LLMs at different levels of abstraction in legal systems. This paper examines different approaches to conditioning LLMs at multiple levels of abstraction in legal systems to detect potentially punishable hate speech. We focus on the task of classifying whether a specific social media posts falls under the criminal offense of incitement to hatred as prescribed by the German Criminal Code. The results show that there is still a significant performance gap between models and legal experts in the legal assessment of hate speech, regardless of the level of abstraction with which the models were conditioned. Our analysis revealed, that models conditioned on abstract legal knowledge lacked deep task understanding, often contradicting themselves and hallucinating answers, while models using concrete legal knowledge performed reasonably well in identifying relevant target groups, but struggled with classifying target conducts.",,"Florian Ludwig, Torsten Zesch, Frederike Zufall",2025-06-03T15:50:27Z,Conditioning Large Language Models on Legal Systems? Detecting   Punishable Hate Speech,Konditionierung von großen Sprachmodellen zu Rechtssystemen? Bestrafende Hassrede erkennen,关于法律制度的限定大语言模式? 检测可惩罚的仇恨言论,http://arxiv.org/abs/2506.03009v1
482,"Privacy policies inform users about data collection and usage, yet their complexity limits accessibility for diverse populations. Existing Privacy Policy Question Answering (QA) systems exhibit performance disparities across English dialects, disadvantaging speakers of non-standard varieties. We propose a novel multi-agent framework inspired by human-centered design principles to mitigate dialectal biases. Our approach integrates a Dialect Agent, which translates queries into Standard American English (SAE) while preserving dialectal intent, and a Privacy Policy Agent, which refines predictions using domain expertise. Unlike prior approaches, our method does not require retraining or dialect-specific fine-tuning, making it broadly applicable across models and domains. Evaluated on PrivacyQA and PolicyQA, our framework improves GPT-4o-mini's zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from 0.352 to 0.464 on PolicyQA, surpassing or matching few-shot baselines without additional training data. These results highlight the effectiveness of structured agent collaboration in mitigating dialect biases and underscore the importance of designing NLP systems that account for linguistic diversity to ensure equitable access to privacy information.",,"Đorđe Klisura, Astrid R Bernaga Torres, Anna Karen Gárate-Escamilla, Rajesh Roshan Biswal, Ke Yang, Hilal Pataci, Anthony Rios",2025-06-03T15:32:20Z,A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy   Question-Answering Systems,Ein Multi-Agenten-Rahmen für die Eindämmung von Dialekt-Biasen in Datenschutzrichtlinien Frage-Antwort-Systeme,减少隐私政策提问系统中的对立比量的多机构框架,http://arxiv.org/abs/2506.02998v1
483,"Idioms are defined as a group of words with a figurative meaning not deducible from their individual components. Although modern machine translation systems have made remarkable progress, translating idioms remains a major challenge, especially for speech-to-text systems, where research on this topic is notably sparse. In this paper, we systematically evaluate idiom translation as compared to conventional news translation in both text-to-text machine translation (MT) and speech-to-text translation (SLT) systems across two language pairs (German to English, Russian to English). We compare state-of-the-art end-to-end SLT systems (SeamlessM4T SLT-to-text, Whisper Large v3) with MT systems (SeamlessM4T SLT-to-text, No Language Left Behind), Large Language Models (DeepSeek, LLaMA) and cascaded alternatives. Our results reveal that SLT systems experience a pronounced performance drop on idiomatic data, often reverting to literal translations even in higher layers, whereas MT systems and Large Language Models demonstrate better handling of idioms. These findings underscore the need for idiom-specific strategies and improved internal representations in SLT architectures.",,"Iuliia Zaitova, Badr M. Abdullah, Wei Xue, Dietrich Klakow, Bernd Möbius, Tania Avgustinova",2025-06-03T15:29:52Z,It's Not a Walk in the Park! Challenges of Idiom Translation in   Speech-to-text Systems,Es ist kein Spaziergang im Park! Herausforderungen der Rede-zu-Text-Systeme,这不是在公园散步!,http://arxiv.org/abs/2506.02995v1
484,"Large Language Models (LLMs) are increasingly explored for legal argument generation, yet they pose significant risks of manipulation through hallucination and ungrounded persuasion, and often fail to utilize provided factual bases effectively or abstain when arguments are untenable. This paper introduces a novel reflective multi-agent method designed to address these challenges in the context of legally compliant persuasion. Our approach employs specialized agents--a Factor Analyst and an Argument Polisher--in an iterative refinement process to generate 3-ply legal arguments (plaintiff, defendant, rebuttal). We evaluate Reflective Multi-Agent against single-agent, enhanced-prompt single-agent, and non-reflective multi-agent baselines using four diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e, Llama-4-Scout-17b-16e) across three legal scenarios: ""arguable"", ""mismatched"", and ""non-arguable"". Results demonstrate Reflective Multi-Agent's significant superiority in successful abstention (preventing generation when arguments cannot be grounded), marked improvements in hallucination accuracy (reducing fabricated and misattributed factors), particularly in ""non-arguable"" scenarios, and enhanced factor utilization recall (improving the use of provided case facts). These findings suggest that structured reflection within a multi-agent framework offers a robust computable method for fostering ethical persuasion and mitigating manipulation in LLM-based legal argumentation systems, a critical step towards trustworthy AI in law. Project page: https://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/",,"Li Zhang, Kevin D. Ashley",2025-06-03T15:28:30Z,Mitigating Manipulation and Enhancing Persuasion: A Reflective   Multi-Agent Approach for Legal Argument Generation,Manipulation abmildern und Überzeugungskraft stärken: Ein reflektierender Multi-Agent-Ansatz für die Generierung von Rechtsargumenten,减轻操纵和加强预测:法律辩论一代的反思多机构办法,http://arxiv.org/abs/2506.02992v1
485,"Background: Large language models (LLMs) have demonstrated substantial potential to support clinical practice. Other than Chat GPT4 and its predecessors, few LLMs, especially those of the leading and more powerful reasoning model class, have been subjected to medical specialty examination questions, including in the domain of primary care. This paper aimed to test the capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro) in primary care education, specifically in answering Member of the Royal College of General Practitioners (MRCGP) style examination questions.   Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer 100 randomly chosen multiple choice questions from the Royal College of General Practitioners GP SelfTest on 25 May 2025. Questions included textual information, laboratory results, and clinical images. Each model was prompted to answer as a GP in the UK and was provided with full question information. Each question was attempted once by each model. Responses were scored against correct answers provided by GP SelfTest.   Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was 99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the same questions was 73.0%.   Discussion: All models performed remarkably well, and all substantially exceeded the average performance of GPs and GP registrars who had answered the same questions. o3 demonstrated the best performance, while the performances of the other leading models were comparable with each other and were not substantially lower than that of o3. These findings strengthen the case for LLMs, particularly reasoning models, to support the delivery of primary care, especially those that have been specifically trained on primary care clinical data.",,Richard Armitage,2025-06-03T15:25:38Z,Performance of leading large language models in May 2025 in Membership   of the Royal College of General Practitioners-style examination questions: a   cross-sectional analysis,Vorstellung von führenden großen Sprachmodellen im Mai 2025 in der Mitgliedschaft in der Royal College of General Practitioners-Stil Prüfung Fragen: eine Querschnittsanalyse,2025年5月皇家全科医生学院考试类型问题:跨部门分析,http://arxiv.org/abs/2506.02987v1
486,"Multilingual speech processing with self-supervised or supervised pre-trained Speech Foundation Models (SFM) has achieved strong performance on tasks like Language Identification (LID) and Automatic Speech Recognition (ASR). However, these models struggle with limited resources during fine-tuning. This paper enhances multilingual LID and ASR on ML-SUPERB 2.0 by exploring multiple strategies for adapting SFMs, including frozen upstream training, partial fine-tuning, and low-rank adaptation. Furthermore, we employ data augmentation to mitigate performance gaps in few-shot settings and introduce LID Connectionist Temporal Classification (CTC) loss for regularization. Our approach achieves a 14% relative improvement in LID accuracy and a 30% relative reduction in ASR CER over the baseline on ML-SUPERB 2.0, securing second place in the Interspeech 2025 ML-SUPERB 2.0 Challenge.",,"Qingzheng Wang, Jiancheng Sun, Yifan Peng, Shinji Watanabe",2025-06-03T15:19:07Z,Improving Multilingual Speech Models on ML-SUPERB 2.0: Fine-tuning with   Data Augmentation and LID-Aware CTC,Mehrsprachige Sprachmodelle auf ML-SUPERB 2.0 verbessern: Feinabstimmung mit Data Augmentation und LID-Aware CTC,改进关于ML-SUPERB 2.0的多语种语言演讲模式:与数据扩充和LID-Aware 四氯化碳进行微调,http://arxiv.org/abs/2505.24200v2
487,"Full-duplex spoken dialogue systems, which can model simultaneous bidirectional features of human conversations such as speech overlaps and backchannels, have attracted significant attention recently. However, the study of full-duplex spoken dialogue systems for the Japanese language has been limited, and the research on their development in Japanese remains scarce. In this paper, we present the first publicly available full-duplex spoken dialogue model in Japanese, which is built upon Moshi, a full-duplex dialogue model in English. Our model is trained through a two-stage process: pre-training on a large-scale spoken dialogue data in Japanese, followed by fine-tuning on high-quality stereo spoken dialogue data. We further enhance the model's performance by incorporating synthetic dialogue data generated by a multi-stream text-to-speech system. Evaluation experiments demonstrate that the trained model outperforms Japanese baseline models in both naturalness and meaningfulness.",,"Atsumoto Ohashi, Shinya Iizuka, Jingjing Jiang, Ryuichiro Higashinaka",2025-06-03T15:16:50Z,Towards a Japanese Full-duplex Spoken Dialogue System,Auf dem Weg zu einem japanischen Full-Duplex Spoken Dialogue System,走向日本全复口述对话系统,http://arxiv.org/abs/2506.02979v1
488,"Each year, tens of millions of essays are written and graded in college-level English courses. Students are asked to analyze literary and cultural texts through a process known as close reading, in which they gather textual details to formulate evidence-based arguments. Despite being viewed as a basis for critical thinking and widely adopted as a required element of university coursework, close reading has never been evaluated on large language models (LLMs), and multi-discipline benchmarks like MMLU do not include literature as a subject. To fill this gap, we present KRISTEVA, the first close reading benchmark for evaluating interpretive reasoning, consisting of 1331 multiple-choice questions adapted from classroom data. With KRISTEVA, we propose three progressively more difficult sets of tasks to approximate different elements of the close reading process, which we use to test how well LLMs may seem to understand and reason about literary works: 1) extracting stylistic features, 2) retrieving relevant contextual information from parametric knowledge, and 3) multi-hop reasoning between style and external contexts. Our baseline results find that, while state-of-the-art LLMs possess some college-level close reading competency (accuracy 49.7% - 69.7%), their performances still trail those of experienced human evaluators on 10 out of our 11 tasks.",,"Peiqi Sui, Juan Diego Rodriguez, Philippe Laban, Dean Murphy, Joseph P. Dexter, Richard Jean So, Samuel Baker, Pramit Chaudhuri",2025-06-03T15:11:26Z,KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive   Reasoning,KRISTEVA: Lesen als neue Aufgabe zum Benchmarking von Interpreten,KRISTIVA: 近距离阅读作为确定解释理由基准的新颖任务,http://arxiv.org/abs/2505.09825v2
489,"Children acquire language despite being exposed to several orders of magnitude less data than large language models require. Meta-learning has been proposed as a way to integrate human-like learning biases into neural-network architectures, combining both the structured generalizations of symbolic models with the scalability of neural-network models. But what does meta-learning exactly imbue the model with? We investigate the meta-learning of formal languages and find that, contrary to previous claims, meta-trained models are not learning simplicity-based priors when meta-trained on datasets organised around simplicity. Rather, we find evidence that meta-training imprints neural mechanisms (such as counters) into the model, which function like cognitive primitives for the network on downstream tasks. Most surprisingly, we find that meta-training on a single formal language can provide as much improvement to a model as meta-training on 5000 different formal languages, provided that the formal language incentivizes the learning of useful neural mechanisms. Taken together, our findings provide practical implications for efficient meta-learning paradigms and new theoretical insights into linking symbolic theories and neural mechanisms.",,"Michael Goodale, Salvador Mascarenhas, Yair Lakretz",2025-06-03T15:10:55Z,Meta-Learning Neural Mechanisms rather than Bayesian Priors,Meta-Learning-Neurale Mechanismen statt Bayesian Priors,"元学习神经机制,而不是巴耶斯古代",http://arxiv.org/abs/2503.16048v2
490,"Large Language Models (LLMs) demonstrate remarkable capabilities in text understanding and generation. However, their tendency to produce factually inconsistent outputs, commonly referred to as ''hallucinations'', remains a critical challenge. Existing approaches, such as retrieval-based and inference-time correction methods, primarily address this issue at the input or output level, often overlooking the intrinsic information refinement process and the role of premature layers. Meanwhile, alignment- and fine-tuning-based methods are resource-intensive. In this paper, we propose PLI (Premature Layers Interpolation), a novel, training-free, and plug-and-play intervention designed to enhance factuality. PLI mitigates hallucinations by inserting premature layers formed through mathematical interpolation with adjacent layers. Inspired by stable diffusion and sampling steps, PLI extends the depth of information processing and transmission in LLMs, improving factual coherence. Experiments on four publicly available datasets demonstrate that PLI effectively reduces hallucinations while outperforming existing baselines in most cases. Further analysis suggests that the success of layer interpolation is closely linked to LLMs' internal mechanisms. To promote reproducibility, we will release our code and data upon acceptance.",,"Dingwei Chen, Ziqiang Liu, Feiteng Fang, Chak Tou Leong, Shiwen Ni, Ahmadreza Argha, Hamid Alinejad-Rokny, Min Yang, Chengming Li",2025-06-03T15:07:13Z,Expanding before Inferring: Enhancing Factuality in Large Language   Models through Premature Layers Interpolation,Expanding before Inferring: Verbesserung der Faktizität in großen Sprachmodellen durch vorreifen Ebenen Interpolation,在推断前加以扩展:通过过早图层内插提高大语言模型的实际质量,http://arxiv.org/abs/2506.02973v1
491,"Large Language Models (LLMs) have demonstrated exceptional performance across various natural language processing tasks. However, they occasionally generate inaccurate and counterfactual outputs, a phenomenon commonly referred to as ""hallucinations''. To tackle this issue, recent studies have explored contrastive decoding between the original model and an amateur model with induced hallucination, showing promising results. Nevertheless, this approach can disrupt the original LLM's output distribution due to coarse contrast and simple subtraction operations, potentially leading to errors. In this paper, we introduce a novel contrastive decoding framework, termed LOL (LOwer Layer Matters). Unlike prior methods that focus solely on the final layer, our approach integrates contrastive information from lower layers to enable multi-layer fusion during contrastive decoding. Additionally, we incorporate a truthfulness refocused module that leverages instruction guidance to further improve truthfulness in contrastive decoding. Extensive experiments on four publicly available datasets demonstrate that the LOL framework significantly mitigates hallucination while outperforming existing baselines in most cases. For reproducibility, we will release our code and data upon acceptance.",,"Dingwei Chen, Feiteng Fang, Shiwen Ni, Feng Liang, Xiping Hu, Ahmadreza Argha, Hamid Alinejad-Rokny, Min Yang, Chengming Li",2025-06-03T15:05:19Z,Lower Layers Matter: Alleviating Hallucination via Multi-Layer Fusion   Contrastive Decoding with Truthfulness Refocused,Untere Schichten Materie: Halluzination durch Multi-Layer-Fusion lindern Kontrastive Dekodierung mit Wahrhaftigkeit neu fokussiert,下层层物质:通过多层交会兼容解码和真相重心来减少幻觉,http://arxiv.org/abs/2408.08769v2
492,"Large Language Models (LLMs) have achieved state-of-the-art results across diverse domains, yet their development remains reliant on vast amounts of publicly available data, raising concerns about data scarcity and the lack of access to domain-specific, sensitive information. Federated Learning (FL) presents a compelling framework to address these challenges by enabling decentralized fine-tuning on pre-trained LLMs without sharing raw data. However, the compatibility and performance of pre-trained LLMs in FL settings remain largely under explored. We introduce the FlowerTune LLM Leaderboard, a first-of-its-kind benchmarking suite designed to evaluate federated fine-tuning of LLMs across four diverse domains: general NLP, finance, medical, and coding. Each domain includes federated instruction-tuning datasets and domain-specific evaluation metrics. Our results, obtained through a collaborative, open-source and community-driven approach, provide the first comprehensive comparison across 26 pre-trained LLMs with different aggregation and fine-tuning strategies under federated settings, offering actionable insights into model performance, resource constraints, and domain adaptation. This work lays the foundation for developing privacy-preserving, domain-specialized LLMs for real-world applications.",,"Yan Gao, Massimo Roberto Scamarcia, Javier Fernandez-Marques, Mohammad Naseri, Chong Shen Ng, Dimitris Stripelis, Zexi Li, Tao Shen, Jiamu Bai, Daoyuan Chen, Zikai Zhang, Rui Hu, InSeo Song, Lee KangYoon, Hong Jia, Ting Dang, Junyan Wang, Zheyuan Liu, Daniel Janes Beutel, Lingjuan Lyu, Nicholas D. Lane",2025-06-03T14:54:12Z,FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large   Language Models,FlowerTune: Ein Cross-Domain-Benchmark für Federated Fine-Tuning von großen Sprachmodellen,FlowerTune: 通用大语言模式联邦整价跨行业基准,http://arxiv.org/abs/2506.02961v1
493,"The misuse of large language models (LLMs) poses potential risks, motivating the development of machine-generated text (MGT) detection. Existing literature primarily concentrates on binary, document-level detection, thereby neglecting texts that are composed jointly by human and LLM contributions. Hence, this paper explores the possibility of fine-grained MGT detection under human-AI coauthoring. We suggest fine-grained detectors can pave pathways toward coauthored text detection with a numeric AI ratio. Specifically, we propose a dataset, HACo-Det, which produces human-AI coauthored texts via an automatic pipeline with word-level attribution labels. We retrofit seven prevailing document-level detectors to generalize them to word-level detection. Then we evaluate these detectors on HACo-Det on both word- and sentence-level detection tasks. Empirical results show that metric-based methods struggle to conduct fine-grained detection with a 0.462 average F1 score, while finetuned models show superior performance and better generalization across domains. However, we argue that fine-grained co-authored text detection is far from solved. We further analyze factors influencing performance, e.g., context window, and highlight the limitations of current methods, pointing to potential avenues for improvement.",,"Zhixiong Su, Yichen Wang, Herun Wan, Zhaohan Zhang, Minnan Luo",2025-06-03T14:52:44Z,HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection   under Human-AI Coauthoring,HACo-Det: Eine Studie zur feinkörnigen maschinengenerierten Texterkennung unter Mensch-AI-Koautorisierung,"HACO-Det: 一项研究,研究如何在人类-AI合作下进行精美的机器生成的文本探测",http://arxiv.org/abs/2506.02959v1
494,"Despite the remarkable performance of Large Language Models (LLMs), they remain vulnerable to jailbreak attacks, which can compromise their safety mechanisms. Existing studies often rely on brute-force optimization or manual design, failing to uncover potential risks in real-world scenarios. To address this, we propose a novel jailbreak attack framework, ICRT, inspired by heuristics and biases in human cognition. Leveraging the simplicity effect, we employ cognitive decomposition to reduce the complexity of malicious prompts. Simultaneously, relevance bias is utilized to reorganize prompts, enhancing semantic alignment and inducing harmful outputs effectively. Furthermore, we introduce a ranking-based harmfulness evaluation metric that surpasses the traditional binary success-or-failure paradigm by employing ranking aggregation methods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify the harmfulness of generated content. Experimental results show that our approach consistently bypasses mainstream LLMs' safety mechanisms and generates high-risk content, providing insights into jailbreak attack risks and contributing to stronger defense strategies.",,"Haoming Yang, Ke Ma, Xiaojun Jia, Yingfei Sun, Qianqian Xu, Qingming Huang",2025-06-03T14:46:36Z,Cannot See the Forest for the Trees: Invoking Heuristics and Biases to   Elicit Irrational Choices of LLMs,Kann den Wald für die Bäume nicht sehen: Aufruf von Heuristik und Biase zu Elicit Irrationale Wahlmöglichkeiten von LLMs,无法看到树的森林: 引用光量和比喻来选择LLMM 的不合理选择 。,http://arxiv.org/abs/2505.02862v2
495,"Large Language Model (LLM) based multi-agent systems have shown remarkable performance in various tasks, especially when enhanced through collaborative communication. However, current methods often rely on a fixed number of agents and static communication structures, limiting their ability to adapt to varying task complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a novel task-adaptive multi-agent collaboration framework that jointly optimizes agent quantity (hard-pruning) and communication topology (soft-pruning). Specifically, our method employs a two-stage training strategy: firstly, independently training soft-pruning networks for different agent quantities to determine optimal agent-quantity-specific complete graphs and positional masks across specific tasks; and then jointly optimizing hard-pruning and soft-pruning within a maximum complete graph to dynamically configure the number of agents and their communication topologies per task. Extensive experiments demonstrate that our approach is: (1) High-performing, achieving state-of-the-art results across six benchmarks and consistently generalizes across multiple mainstream LLM architectures, with a increase in performance of $2.58\%\sim 9.84\%$; (2) Task-adaptive, dynamically constructing optimized communication topologies tailored to specific tasks, with an extremely high performance in all three task categories (general reasoning, mathematical reasoning, and code generation); (3) Token-economical, having fewer training steps and token consumption at the same time, with a decrease in token consumption of $90\%+$; and (4) Training-efficient, achieving high performance with very few training steps compared with other methods. The performance will surpass the existing baselines after about ten steps of training under six benchmarks.",,"Boyi Li, Zhonghan Zhao, Der-Horng Lee, Gaoang Wang",2025-06-03T14:46:00Z,Adaptive Graph Pruning for Multi-Agent Communication,Adaptives Graph Pruning für Multi-Agent Kommunikation,多机构通信调节图,http://arxiv.org/abs/2506.02951v1
496,"LLM-as-a-judge is a framework in which a large language model (LLM) automatically evaluates the output of another LLM. We propose quantitative LLM judges, which align evaluation scores of existing LLM judges to human scores in a given domain using regression models. The models are trained to improve the score of the original judge by using the judge's textual evaluation and score. We present four quantitative judges for different types of absolute and relative feedback, which showcases the generality and versatility of our framework. Our framework is more computationally efficient than supervised fine-tuning and can be more statistically efficient when human feedback is limited, which is expected in most applications of our work. We validate these claims empirically on four datasets using two base judges. Our experiments show that quantitative judges can effectively improve the predictive power of existing judges through post-hoc modeling.",,"Aishwarya Sahoo, Jeevana Kruthi Karnuthala, Tushar Parmanand Budhwani, Pranchal Agarwal, Sankaran Vaidyanathan, Alexa Siu, Franck Dernoncourt, Jennifer Healey, Nedim Lipka, Ryan Rossi, Uttaran Bhattacharya, Branislav Kveton",2025-06-03T14:44:23Z,Quantitative LLM Judges,Quantitative LLM-Richter,数量有限LLM法官,http://arxiv.org/abs/2506.02945v1
497,"With the development of the Large Language Models (LLMs), a large range of LLM-based Text-to-SQL(Text2SQL) methods have emerged. This survey provides a comprehensive review of LLM-based Text2SQL studies. We first enumerate classic benchmarks and evaluation metrics. For the two mainstream methods, prompt engineering and finetuning, we introduce a comprehensive taxonomy and offer practical insights into each subcategory. We present an overall analysis of the above methods and various models evaluated on well-known datasets and extract some characteristics. Finally, we discuss the challenges and future directions in this field.",,"Liang Shi, Zhengju Tang, Nan Zhang, Xiaotong Zhang, Zhi Yang",2025-06-03T14:40:01Z,A Survey on Employing Large Language Models for Text-to-SQL Tasks,Eine Umfrage zum Einsatz großer Sprachmodelle für Text-zu-SQL-Aufgaben,关于采用大语言模式处理文本到SQL任务的调查,http://arxiv.org/abs/2407.15186v5
498,"Large Reasoning Models (LRMs) have significantly advanced beyond traditional Large Language Models (LLMs) with their exceptional logical reasoning capabilities, yet these improvements introduce heightened safety risks. When subjected to jailbreak attacks, their ability to generate more targeted and organized content can lead to greater harm. Although some studies claim that reasoning enables safer LRMs against existing LLM attacks, they overlook the inherent flaws within the reasoning process itself. To address this gap, we propose the first jailbreak attack targeting LRMs, exploiting their unique vulnerabilities stemming from the advanced reasoning capabilities. Specifically, we introduce a Chaos Machine, a novel component to transform attack prompts with diverse one-to-one mappings. The chaos mappings iteratively generated by the machine are embedded into the reasoning chain, which strengthens the variability and complexity and also promotes a more robust attack. Based on this, we construct the Mousetrap framework, which makes attacks projected into nonlinear-like low sample spaces with mismatched generalization enhanced. Also, due to the more competing objectives, LRMs gradually maintain the inertia of unpredictable iterative reasoning and fall into our trap. Success rates of the Mousetrap attacking o1-mini, Claude-Sonnet and Gemini-Thinking are as high as 96%, 86% and 98% respectively on our toxic dataset Trotter. On benchmarks such as AdvBench, StrongREJECT, and HarmBench, attacking Claude-Sonnet, well-known for its safety, Mousetrap can astonishingly achieve success rates of 87.5%, 86.58% and 93.13% respectively. Attention: This paper contains inappropriate, offensive and harmful content.",,"Yang Yao, Xuan Tong, Ruofan Wang, Yixu Wang, Lujundong Li, Liang Liu, Yan Teng, Yingchun Wang",2025-06-03T14:35:59Z,A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of   Iterative Chaos,Eine Mausefalle: Fooling große Gründe Modelle für Jailbreak mit Kette von iterativen Chaos,鼠捕:与迭生性混乱链一起愚弄监狱越狱大型理由模型,http://arxiv.org/abs/2502.15806v2
499,"Data annotation is an essential component of the machine learning pipeline; it is also a costly and time-consuming process. With the introduction of transformer-based models, annotation at the document level is increasingly popular; however, there is no standard framework for structuring such tasks. The EffiARA annotation framework is, to our knowledge, the first project to support the whole annotation pipeline, from understanding the resources required for an annotation task to compiling the annotated dataset and gaining insights into the reliability of individual annotators as well as the dataset as a whole. The framework's efficacy is supported by two previous studies: one improving classification performance through annotator-reliability-based soft-label aggregation and sample weighting, and the other increasing the overall agreement among annotators through removing identifying and replacing an unreliable annotator. This work introduces the EffiARA Python package and its accompanying webtool, which provides an accessible graphical user interface for the system. We open-source the EffiARA Python package at https://github.com/MiniEggz/EffiARA and the webtool is publicly accessible at https://effiara.gate.ac.uk.",,"Owen Cook, Jake Vasilakes, Ian Roberts, Xingyi Song",2025-06-03T14:25:31Z,Efficient Annotator Reliability Assessment with EffiARA,Effiziente Annotator-Zuverlässigkeitsbewertung mit EffiARA,EffiARA的可靠性评估,http://arxiv.org/abs/2504.00589v3
500,"In this work, we describe our team's approach to eRisk's 2025 Task 1: Search for Symptoms of Depression. Given a set of sentences and the Beck's Depression Inventory - II (BDI) questionnaire, participants were tasked with submitting up to 1,000 sentences per depression symptom in the BDI, sorted by relevance. Participant submissions were evaluated according to standard Information Retrieval (IR) metrics, including Average Precision (AP) and R-Precision (R-PREC). The provided training data, however, consisted of sentences labeled as to whether a given sentence was relevant or not w.r.t. one of BDI's symptoms. Due to this labeling limitation, we framed our development as a binary classification task for each BDI symptom, and evaluated accordingly. To that end, we split the available labeled data into training and validation sets, and explored foundation model fine-tuning, sentence similarity, Large Language Model (LLM) prompting, and ensemble techniques. The validation results revealed that fine-tuning foundation models yielded the best performance, particularly when enhanced with synthetic data to mitigate class imbalance. We also observed that the optimal approach varied by symptom. Based on these insights, we devised five independent test runs, two of which used ensemble methods. These runs achieved the highest scores in the official IR evaluation, outperforming submissions from 16 other teams.",,"Diogo A. P. Nunes, Eugénio Ribeiro",2025-06-03T14:25:12Z,"INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and   Prompt-Based Approaches to Depression Symptom Identification","INESC-ID @ eRisk 2025: Erforschen von Fein-Tuned, Ähnlichkeit-Based und Prompt-Based Annäherungen an Depression Symptome Identifikation",INESC-ID@ eRisk 2025年:探索对萧条症状识别的精度、相似性、以及基于迅速的方法,http://arxiv.org/abs/2506.02924v1
501,"Efficient preference optimization algorithms such as Direct Preference Optimization (DPO) have become a popular approach in aligning large language models (LLMs) with human preferences. These algorithms implicitly treat the LLM as a reward model, and focus on training it to correct misranked preference pairs. However, recent work~\citep{chen2024preference} empirically finds that DPO training \textit{rarely improves these misranked preference pairs}, despite its gradient emphasizing on these cases. We introduce FocalPO, a DPO variant that instead \textit{down-weighs} misranked preference pairs and prioritizes enhancing the model's understanding of pairs that it can already rank correctly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this by adding a modulating factor to dynamically scale DPO loss. Our experiment demonstrates that FocalPO surpasses DPO and its variants on popular benchmarks like Alpaca Eval 2.0 using Mistral-Base-7B and Llama-3-Instruct-8B, with the introduced hyperparameter fixed. Additionally, we empirically reveals how FocalPO affects training on correct and incorrect sample groups, further underscoring its effectiveness.",,"Tong Liu, Xiao Yu, Wenxuan Zhou, Jindong Gu, Volker Tresp",2025-06-03T14:23:27Z,FocalPO: Enhancing Preference Optimizing by Focusing on Correct   Preference Rankings,FocalPO: Verbesserung der Preference-Optimierung durch Fokussierung auf korrekte Preference-Rankings,"重点:通过注重正确的优先排序,加强优惠优化",http://arxiv.org/abs/2501.06645v2
502,"Existing frameworks for evaluating long-context language models (LCLM) can be broadly categorized into real-world and synthetic tasks. Despite their utility, both approaches are accompanied by certain intrinsic limitations. Real-world tasks are too complex to interpret or characterize and are susceptible to data contamination. In contrast, synthetic tasks often adopt the needle-in-the-haystack (NIAH) format, wherein a lack of coherence between the ""needle"" and the ""haystack"" compromises their validity as proxies for realistic applications. In response to these challenges, we posit that an ideal long-context evaluation framework should be characterized by three essential features: $\textit{seamless context}$, $\textit{controllable setting}$, and $\textit{sound evaluation}$. This study introduces $\textbf{LongBioBench}$, a novel benchmark that utilizes artificially generated biographies as a controlled environment for assessing LCLMs across dimensions of $\textit{understanding}$, $\textit{reasoning}$, and $\textit{trustworthiness}$. Our experimental evaluation, which includes $\textbf{18}$ LCLMs in total, demonstrates that most models still exhibit deficiencies in semantic understanding and elementary reasoning over retrieved results and are less trustworthy as context length increases. Our further analysis indicates some design choices employed by existing synthetic benchmarks, such as contextual non-coherence, numerical needles, and the absence of distractors, rendering them vulnerable to test the model long-context capabilities. Moreover, we also reveal that long-context continual pretraining primarily adjusts RoPE embedding to accommodate extended context lengths. To sum up, compared to previous synthetic benchmarks, LongBioBench achieves a better trade-off between mirroring authentic language tasks and maintaining controllability, and is highly interpretable and configurable.",,"Yijun Yang, Zeyu Huang, Wenhao Zhu, Zihan Qiu, Fei Yuan, Jeff Z. Pan, Ivan Titov",2025-06-03T14:23:06Z,A Controllable Examination for Long-Context Language Models,Eine kontrollierbare Prüfung für Langkontext-Sprachenmodelle,长文本语言模式可控测试,http://arxiv.org/abs/2506.02921v1
503,"Cell type annotation is a key task in analyzing the heterogeneity of single-cell RNA sequencing data. Although recent foundation models automate this process, they typically annotate cells independently, without considering batch-level cellular context or providing explanatory reasoning. In contrast, human experts often annotate distinct cell types for different cell clusters based on their domain knowledge. To mimic this workflow, we introduce the CellPuzzles task, where the objective is to assign unique cell types to a batch of cells. This benchmark spans diverse tissues, diseases, and donor conditions, and requires reasoning across the batch-level cellular context to ensure label uniqueness. We find that off-the-shelf large language models (LLMs) struggle on CellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0% batch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained via supervised fine-tuning on distilled reasoning traces, followed by reinforcement learning with batch-level rewards. Cell-o1 achieves state-of-the-art performance, outperforming o1 by over 73% and generalizing well across contexts. Further analysis of training dynamics and reasoning behaviors provides insights into batch-level annotation performance and emergent expert-like reasoning. Code and data are available at https://github.com/ncbi-nlp/cell-o1.",,"Yin Fang, Qiao Jin, Guangzhi Xiong, Bowen Jin, Xianrui Zhong, Siru Ouyang, Aidong Zhang, Jiawei Han, Zhiyong Lu",2025-06-03T14:16:53Z,Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with   Reinforcement Learning,"Cell-o1: LLMs trainieren, um einzellige Denk-Puzzles mit Verstärkungs-Lernen zu lösen",Cell-o1: 以强化学习方式解答单项理据谜题的培训LLMS,http://arxiv.org/abs/2506.02911v1
504,"We propose IMPARA-GED, a novel reference-free automatic grammatical error correction (GEC) evaluation method with grammatical error detection (GED) capabilities. We focus on the quality estimator of IMPARA, an existing automatic GEC evaluation method, and construct that of IMPARA-GED using a pre-trained language model with enhanced GED capabilities. Experimental results on SEEDA, a meta-evaluation dataset for automatic GEC evaluation methods, demonstrate that IMPARA-GED achieves the highest correlation with human sentence-level evaluations.",,"Yusuke Sakai, Takumi Goto, Taro Watanabe",2025-06-03T14:05:37Z,IMPARA-GED: Grammatical Error Detection is Boosting Reference-free   Grammatical Error Quality Estimator,IMPARA-GED: Grammatical Error Detection steigert referenzfreie Grammatical Error Quality Estimator,IMPARA-GED: 显性错误探测是增强无参考无显性错误质量模拟器,http://arxiv.org/abs/2506.02899v1
505,"Although Germany has a diverse landscape of dialects, they are underrepresented in current automatic speech recognition (ASR) research. To enable studies of how robust models are towards dialectal variation, we present Betthupferl, an evaluation dataset containing four hours of read speech in three dialect groups spoken in Southeast Germany (Franconian, Bavarian, Alemannic), and half an hour of Standard German speech. We provide both dialectal and Standard German transcriptions, and analyze the linguistic differences between them. We benchmark several multilingual state-of-the-art ASR models on speech translation into Standard German, and find differences between how much the output resembles the dialectal vs. standardized transcriptions. Qualitative error analyses of the best ASR model reveal that it sometimes normalizes grammatical differences, but often stays closer to the dialectal constructions.",,"Verena Blaschke, Miriam Winkler, Constantin Förster, Gabriele Wenger-Glemser, Barbara Plank",2025-06-03T14:02:52Z,A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard   Speech Translation,Multi-Dialogdatensatz für deutsche ASR- und Dialect-to-Standard-Sprachübersetzung,德国对数 ASR 和对数到标准语音翻译多功能数据集,http://arxiv.org/abs/2506.02894v1
506,"Large language models (LLMs) have made significant advancements across various tasks, but their safety alignment remain a major concern. Exploring jailbreak prompts can expose LLMs' vulnerabilities and guide efforts to secure them. Existing methods primarily design sophisticated instructions for the LLM to follow, or rely on multiple iterations, which could hinder the performance and efficiency of jailbreaks. In this work, we propose a novel jailbreak paradigm, Simple Assistive Task Linkage (SATA), which can effectively circumvent LLM safeguards and elicit harmful responses. Specifically, SATA first masks harmful keywords within a malicious query to generate a relatively benign query containing one or multiple [MASK] special tokens. It then employs a simple assistive task such as a masked language model task or an element lookup by position task to encode the semantics of the masked keywords. Finally, SATA links the assistive task with the masked query to jointly perform the jailbreak. Extensive experiments show that SATA achieves state-of-the-art performance and outperforms baselines by a large margin. Specifically, on AdvBench dataset, with mask language model (MLM) assistive task, SATA achieves an overall attack success rate (ASR) of 85% and harmful score (HS) of 4.57, and with element lookup by position (ELP) assistive task, SATA attains an overall ASR of 76% and HS of 4.43.",,"Xiaoning Dong, Wenbo Hu, Wei Xu, Tianxing He",2025-06-03T13:59:22Z,SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage,SATA: Ein Paradigma für LLM Jailbreak über einfache assistive Task-Linkage,SATA:LLM监狱通过简单辅助任务链接破获的范例,http://arxiv.org/abs/2412.15289v3
507,"Mixture of Experts (MoE) architectures have emerged as pivotal for scaling Large Language Models (LLMs) efficiently. Fine-grained MoE approaches - utilizing more numerous, smaller experts - have demonstrated potential in improving model convergence and quality. This work proposes a set of training recipes and provides a comprehensive empirical evaluation of fine-grained MoE, directly comparing its scaling properties against standard MoE configurations for models with up to 56B total (17B active) parameters. We investigate convergence speed, model performance on downstream benchmarks, and practical training considerations across various setups. Overall, at the largest scale we show that fine-grained MoE achieves better validation loss and higher accuracy across a set of downstream benchmarks. This study offers empirical grounding and practical insights for leveraging fine-grained MoE in the development of future large-scale models.",,"Jakub Krajewski, Marcin Chochowski, Daniel Korzekwa",2025-06-03T13:55:48Z,Scaling Fine-Grained MoE Beyond 50B Parameters: Empirical Evaluation and   Practical Insights,Skalierung feinkörniger MoE jenseits von 50B-Parametern: Empirische Auswertung und praktische Einblicke,50B参数之外的微小教育部:经验评价和实际展望,http://arxiv.org/abs/2506.02890v1
508,"Finetuning is a critical step for adapting large language models (LLMs) to domain-specific downstream tasks. To mitigate the substantial computational and memory costs of full-model fine-tuning, Parameter-Efficient Finetuning (PEFT) methods have been proposed to update only a small subset of model parameters. However, performance gaps between PEFT approaches and full-model fine-tuning still exist. In this work, we present DiaBlo, a simple yet effective PEFT approach that updates only the diagonal blocks of selected model weight matrices. Unlike Low Rank Adaptation (LoRA) and its variants, DiaBlo eliminates the need for low rank matrix products, thereby avoiding the reliance on auxiliary initialization schemes or customized optimization strategies to improve convergence. This design leads to stable and robust convergence while maintaining comparable memory efficiency and training speed to LoRA. We conduct extensive experiments across a range of tasks, including commonsense reasoning, arithmetic reasoning, code generation, and safety alignment, to evaluate the effectiveness and efficiency of DiaBlo. Across these benchmarks, DiaBlo demonstrates strong and consistent performance while maintaining high memory efficiency and fast finetuning speed. Codes are available at https://github.com/ziyangjoy/DiaBlo.",,"Selcuk Gurses, Aozhong Zhang, Yanxia Deng, Xun Dong, Xin Li, Naigang Wang, Penghang Yin, Zi Yang",2025-06-03T13:47:59Z,DiaBlo: Diagonal Blocks Are Sufficient For Finetuning,DiaBlo: Diagonale Blöcke reichen für die Feinsteuerung aus,DiaBlo:对角块足以进行微调,http://arxiv.org/abs/2506.03230v1
509,"Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of Large Language Models on tasks requiring multi-step inference. This success has led to widespread claims of emergent reasoning capabilities in these models. In this paper, we present a theoretical counter-perspective: Chain-of-Thought (CoT) does not elicit genuine, abstract reasoning. Instead, we argue that Chain-of-Thought functions as a powerful structural constraint that guides Large Language Models to imitate the form of reasoning. By forcing the generation of intermediate steps, Chain-of-Thought leverages the model immense capacity for sequence prediction and pattern matching, effectively constraining its output to sequences that resemble coherent thought processes. Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of Large Language Models on tasks requiring multi-step inference. This success has led to widespread claims of emergent reasoning capabilities in these models. In this paper, we present a theoretical counter-perspective: Chain-of-Thought (CoT) does not elicit genuine, abstract reasoning. Instead, we argue that Chain-of-Thought functions as a powerful structural constraint that guides Large Language Models to imitate the form of reasoning. By forcing the generation of intermediate steps, Chain-of-Thought leverages the model immense capacity for sequence prediction and pattern matching, effectively constraining its output to sequences that resemble coherent thought processes.",,"Jintian Shao, Yiming Cheng",2025-06-03T13:45:01Z,"CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A   Theory Perspective","CoT ist nicht wahr Vernunft, es ist nur eine enge Beschränkung zu nachahmen: Eine Theorie Perspektive","COT不是真实的理由,这只是一个难以想象的束缚:理论观点",http://arxiv.org/abs/2506.02878v1
510,"Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This interdependency can limit control precision and sometimes lead to unintended side effects. Recent research has explored the use of sparse autoencoders (SAE) to disentangle knowledge in high-dimensional spaces for steering. However, these applications have been limited to toy tasks owing to the nontrivial issue of locating atomic knowledge components. In this paper, we propose Steering Target Atoms (STA), a novel method that isolates and manipulates disentangled knowledge components to enhance safety. Comprehensive experiments demonstrate the effectiveness of our approach. Further analysis reveals that steering exhibits superior robustness and flexibility, particularly in adversarial scenarios. We also apply the steering strategy to the large reasoning model, confirming its effectiveness in precise reasoning control.",,"Mengru Wang, Ziwen Xu, Shengyu Mao, Shumin Deng, Zhaopeng Tu, Huajun Chen, Ningyu Zhang",2025-06-03T13:40:17Z,Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering   Target Atoms,Beyond Prompt Engineering: Robuste Behavior Control in LLMs über Steering Target Atoms,超越快速工程:通过指导目标原子在LLMM中进行强力行为控制,http://arxiv.org/abs/2505.20322v2
511,"Named Entity Recognition (NER) in historical texts presents unique challenges due to non-standardized language, archaic orthography, and nested or overlapping entities. This study benchmarks a diverse set of NER approaches, ranging from classical Conditional Random Fields (CRFs) and spaCy-based models to transformer-based architectures such as CamemBERT and sequence-labeling models like Flair. Experiments are conducted on the GeoEDdA dataset, a richly annotated corpus derived from 18th-century French encyclopedias. We propose framing NER as both token-level and span-level classification to accommodate complex nested entity structures typical of historical documents. Additionally, we evaluate the emerging potential of few-shot prompting with generative language models for low-resource scenarios. Our results demonstrate that while transformer-based models achieve state-of-the-art performance, especially on nested entities, generative models offer promising alternatives when labeled data are scarce. The study highlights ongoing challenges in historical NER and suggests avenues for hybrid approaches combining symbolic and neural methods to better capture the intricacies of early modern French text.",,"Ludovic Moncla, Hédi Zeghidi",2025-06-03T13:37:44Z,Token and Span Classification for Entity Recognition in French   Historical Encyclopedias,Token- und Span-Klassifikation für die Entitätserkennung in französischen historischen Enzyklopädien,法国历史百科全书实体识别图肯和斯潘分类,http://arxiv.org/abs/2506.02872v1
512,"Large language models (LLMs) often struggle to accurately read and comprehend extremely long texts. Current methods for improvement typically rely on splitting long contexts into fixed-length chunks. However, fixed truncation risks separating semantically relevant content, leading to ambiguity and compromising accurate understanding. To overcome this limitation, we propose a straightforward approach for dynamically separating and selecting chunks of long context, facilitating a more streamlined input for LLMs. In particular, we compute semantic similarities between adjacent sentences, using lower similarities to adaptively divide long contexts into variable-length chunks. We further train a question-aware classifier to select sensitive chunks that are critical for answering specific questions. Experimental results on both single-hop and multi-hop question-answering benchmarks show that the proposed approach consistently outperforms strong baselines. Notably, it maintains robustness across a wide range of input lengths, handling sequences of up to 256k tokens. Our datasets and code are available at the following link: https://github.com/ECNU-Text-Computing/DCS",,"Boheng Sheng, Jiacheng Yao, Meicong Zhang, Guoxiu He",2025-06-03T13:29:29Z,Dynamic Chunking and Selection for Reading Comprehension of Ultra-Long   Context in Large Language Models,Dynamisches Chunking und Auswahl für Leseverständnis ultralanger Kontexte in großen Sprachmodellen,大语言模型中超长语言背景的动态冲击和阅读理解选择,http://arxiv.org/abs/2506.00773v2
513,"Instruction-fine-tuned large language models (LLMs) under 14B parameters continue to underperform on natural language understanding (NLU) tasks, often trailing smaller models like BERT-base on benchmarks such as GLUE and SuperGLUE. Motivated by the success of reinforcement learning in reasoning tasks (e.g., DeepSeek), we explore Proximal Policy Optimization (PPO) as a framework to improve the NLU capabilities of LLMs. We frame NLU as a reinforcement learning environment, treating token generation as a sequence of actions and optimizing for reward signals based on alignment with ground-truth labels. PPO consistently outperforms supervised fine-tuning, yielding an average improvement of 6.3 points on GLUE, and surpasses zero-shot and few-shot prompting by 38.7 and 26.1 points, respectively. Notably, PPO-tuned models outperform GPT-4o by over 4\% on average across sentiment and natural language inference tasks, including gains of 7.3\% on the Mental Health dataset and 10.9\% on SIGA-nli. This work highlights a promising direction for adapting LLMs to new tasks by reframing them as reinforcement learning problems, enabling learning through simple end-task rewards rather than extensive data curation.",,"Bokai Hu, Sai Ashish Somayajula, Xin Pan, Pengtao Xie",2025-06-03T13:16:21Z,Improving the Language Understanding Capabilities of Large Language   Models Using Reinforcement Learning,Verbesserung des Sprachverständnisses von großen Sprachmodellen durch Stärkung des Lernens,提高利用强化学习提高大语言模式的语言理解能力,http://arxiv.org/abs/2410.11020v4
514,"Large language models (LLMs) can effectively elicit human preferences through multi-turn dialogue. Complex tasks can be accomplished through iterative clarifying questions and final responses generated by an LLM acting as a questioner (STaR-GATE; Andukuri et al., 2024}). However, existing approaches based on self-taught reasoning struggle to identify optimal dialogue trajectories and avoid irrelevant questions to the tasks. To address this limitation, we propose TO-GATE, a novel framework that enhances question generation through trajectory optimization, which consists of two key components: a clarification resolver that generates optimal questioning trajectories, and a summarizer that ensures task-aligned final responses. The trajectory optimization enables the model to produce effective elicitation questions and summary responses tailored to specific tasks. Experimental results demonstrate that TO-GATE significantly outperforms baseline methods, achieving a 9.32% improvement on standard preference elicitation tasks.",,"Yulin Dou, Jiangming Liu",2025-06-03T12:58:07Z,TO-GATE: Clarifying Questions and Summarizing Responses with Trajectory   Optimization for Eliciting Human Preference,TO-GATE: Präzisierung von Fragen und Zusammenfassung von Reaktionen mit Trajektorienoptimierung zur Eliziierung menschlicher Präferenzen,"将:澄清问题和用轨迹优化概括答复,以利维托人类首选",http://arxiv.org/abs/2506.02827v1
515,"This work presents a computational approach to analyze character development along the narrative timeline. The analysis characterizes the inner and outer changes the protagonist undergoes within a narrative, and the interplay between them. We consider transcripts of Holocaust survivor testimonies as a test case, each telling the story of an individual in first-person terms. We focus on the survivor's religious trajectory, examining the evolution of their disposition toward religious belief and practice along the testimony. Clustering the resulting trajectories in the dataset, we identify common sequences in the data. Our findings highlight multiple common structures of religiosity across the narratives: in terms of belief, most present a constant disposition, while for practice, most present an oscillating structure, serving as valuable material for historical and sociological research. This work demonstrates the potential of natural language processing techniques for analyzing character evolution through thematic trajectories in narratives.",,"Esther Shizgal, Eitan Wagner, Renana Keydar, Omri Abend",2025-06-03T12:53:28Z,Computational Analysis of Character Development in Holocaust Testimonies,Computational Analyse der Charakterentwicklung in Holocaust-Zeugnissen,大屠杀证词特征发展计算分析,http://arxiv.org/abs/2412.17063v2
516,"Large language models (LLMs) demonstrate impressive results in natural language processing tasks but require a significant amount of computational and memory resources. Structured matrix representations are a promising way for reducing the number of parameters of these models. However, it seems unrealistic to expect that weight matrices of pretrained models can be accurately represented by structured matrices without any fine-tuning. To overcome this issue, we utilize the fact that LLM output is invariant under certain orthogonal transformations of weight matrices. This insight can be leveraged to identify transformations that significantly improve the compressibility of weights within structured classes. The proposed approach is applicable to various types of structured matrices that support efficient projection operations. Code is available at https://github.com/GrishKate/ProcrustesGPT",,"Ekaterina Grishina, Mikhail Gorbunov, Maxim Rakhuba",2025-06-03T12:47:23Z,ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal   Transformations,ProcrustesGPT: LLMs mit strukturierten Matrizen und orthogonalen Transformationen komprimieren,ProcrustetesGPT: 用结构矩阵和正心变形压缩LLMs,http://arxiv.org/abs/2506.02818v1
517,"Retrieval-Augmented Generation (RAG) systems enable language models to access relevant information and generate accurate, well-grounded, and contextually informed responses. However, for Indian languages, the development of high-quality RAG systems is hindered by the lack of two critical resources: (1) evaluation benchmarks for retrieval and generation tasks, and (2) large-scale training datasets for multilingual retrieval. Most existing benchmarks and datasets are centered around English or high-resource languages, making it difficult to extend RAG capabilities to the diverse linguistic landscape of India. To address the lack of evaluation benchmarks, we create IndicMSMarco, a multilingual benchmark for evaluating retrieval quality and response generation in 13 Indian languages, created via manual translation of 1000 diverse queries from MS MARCO-dev set. To address the need for training data, we build a large-scale dataset of (question, answer, relevant passage) tuples derived from the Wikipedias of 19 Indian languages using state-of-the-art LLMs. Additionally, we include translated versions of the original MS MARCO dataset to further enrich the training data and ensure alignment with real-world information-seeking tasks. Resources are available here: https://huggingface.co/collections/ai4bharat/indicragsuite-683e7273cb2337208c8c0fcb",,"Pasunuti Prasanjith, Prathmesh B More, Anoop Kunchukuttan, Raj Dabre",2025-06-03T12:38:27Z,IndicRAGSuite: Large-Scale Datasets and a Benchmark for Indian Language   RAG Systems,IndicRAGSuite: Large-Scale-Datensätze und ein Benchmark für indische Sprach-RAG-Systeme,参考文献:大型数据集和印度语RAG系统基准,http://arxiv.org/abs/2506.01615v2
518,"Free-text rationales justify model decisions in natural language and thus become likable and accessible among approaches to explanation across many tasks. However, their effectiveness can be hindered by misinterpretation and hallucination. As a perturbation test, we investigate how large language models (LLMs) perform rationale generation under the effects of readability level control, i.e., being prompted for an explanation targeting a specific expertise level, such as sixth grade or college. We find that explanations are adaptable to such instruction, though the observed distinction between readability levels does not fully match the defined complexity scores according to traditional readability metrics. Furthermore, the generated rationales tend to feature medium level complexity, which correlates with the measured quality using automatic metrics. Finally, our human annotators confirm a generally satisfactory impression on rationales at all readability levels, with high-school-level readability being most commonly perceived and favored.",,"Yi-Sheng Hsu, Nils Feldhus, Sherzod Hakimov",2025-06-03T12:37:12Z,Free-text Rationale Generation under Readability Level Control,Freitext-Rationale Generation unter Lesbarkeits-Level-Kontrolle,在可读性控制级别下的自由文本理由生成,http://arxiv.org/abs/2407.01384v3
519,"Vision-language models (VLMs) excel in semantic tasks but falter at a core human capability: detecting hidden content in optical illusions or AI-generated images through perceptual adjustments like zooming. We introduce HC-Bench, a benchmark of 112 images with hidden text, objects, and illusions, revealing that leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit prompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to an overreliance on high-level semantics. Strikingly, we propose SemVink (Semantic Visual Thinking) by simply scaling images to low resolutions (32-128 pixels), which unlocks >99% accuracy by eliminating redundant visual noise. This exposes a critical architectural flaw: VLMs prioritize abstract reasoning over low-level visual operations crucial for real-world robustness. Our work urges a shift toward hybrid models integrating multi-scale processing, bridging the gap between computational vision and human cognition for applications in medical imaging, security, and beyond.",,"Sifan Li, Yujun Cai, Yiwei Wang",2025-06-03T12:33:47Z,SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via   Visual Global Thinking,SemVink: Das semantische Verständnis optischer Illusionen durch visuelles globales Denken von VLMs verbessern,SemVink:通过视觉全球思维推进VLMs对光学幻影的语义理解,http://arxiv.org/abs/2506.02803v1
520,"As large language models (LLMs) continue to evolve, efficient evaluation metrics are vital for assessing their ability to compress information and reduce redundancy. While traditional metrics like Matrix Entropy offer valuable insights, they are computationally intensive for large-scale models due to their \( O(n^3) \) time complexity with Singular Value Decomposition (SVD). To mitigate this issue, we introduce the Matrix Nuclear-Norm, which not only serves as a metric to quantify the data compression proficiency of LLM but also provides a convex approximation of matrix rank to capture both predictive discriminability and diversity. By employing the \( L_{1,2}\text{-norm} \) to further approximate the nuclear norm, we can effectively assess the model's information compression capabilities. This approach reduces the time complexity to \( O(n^2) \) and eliminates the need for SVD computation. Consequently, the Matrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy for the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This performance gap becomes more pronounced with larger models, as validated in tests with other models like Pythia. Additionally, evaluations on benchmarks and model responses confirm that our proposed Matrix Nuclear-Norm is a reliable, scalable, and efficient tool for assessing LLMs' performance, striking a balance between accuracy and computational efficiency. The code is available at https://github.com/MLGroupJLU/MatrixNuclearNorm.",,"Yahan Li, Tingyu Xia, Yi Chang, Yuan Wu",2025-06-03T12:07:50Z,Large Language Model Evaluation via Matrix Nuclear-Norm,Bewertung großer Sprachmodelle über Matrix-Nuklear-Norm,"通过 "" 核-诺姆矩阵 "" 进行大语言模式评价",http://arxiv.org/abs/2410.10672v3
521,"While Large Language Models (LLMs) have significantly advanced natural language processing, aligning them with human preferences remains an open challenge. Although current alignment methods rely primarily on explicit feedback, eye-tracking (ET) data offers insights into real-time cognitive processing during reading. In this paper, we present OASST-ETC, a novel eye-tracking corpus capturing reading patterns from 24 participants, while evaluating LLM-generated responses from the OASST1 dataset. Our analysis reveals distinct reading patterns between preferred and non-preferred responses, which we compare with synthetic eye-tracking data. Furthermore, we examine the correlation between human reading measures and attention patterns from various transformer-based models, discovering stronger correlations in preferred responses. This work introduces a unique resource for studying human cognitive processing in LLM evaluation and suggests promising directions for incorporating eye-tracking data into alignment methods. The dataset and analysis code are publicly available.",,"Angela Lopez-Cardona, Sebastian Idesis, Miguel Barreda-Ángeles, Sergi Abadal, Ioannis Arapakis",2025-06-03T11:58:39Z,OASST-ETC Dataset: Alignment Signals from Eye-tracking Analysis of LLM   Responses,OASST-ETC Datensatz: Ausrichtungssignale aus der Eye-Tracking Analyse von LLM-Antworten,OASST-ETC数据集:从LLM对策的目视跟踪分析得到的对准信号,http://arxiv.org/abs/2503.10927v3
522,"We propose a new finetuning method to provide pre-trained large language models (LMs) the ability to scale test-time compute through the diffusion framework. By increasing the number of diffusion steps, we show our finetuned models achieve monotonically increasing accuracy, directly translating to improved performance across downstream tasks. Furthermore, our finetuned models can expertly answer questions on specific topics by integrating powerful guidance techniques, and autonomously determine the compute required for a given problem by leveraging adaptive ODE solvers. Our method is universally applicable to any foundation model pre-trained with a cross-entropy loss and does not modify any of its original weights, fully preserving its strong single-step generation capabilities. We show our method is more effective and fully compatible with traditional finetuning approaches, introducing an orthogonal new direction to unify the strengths of the autoregressive and diffusion frameworks.",,"Edoardo Cetin, Tianyu Zhao, Yujin Tang",2025-06-03T11:52:35Z,Large Language Models to Diffusion Finetuning,Große Sprachmodelle zum Diffusionsfinetuning,用于传播的大型语言模型,http://arxiv.org/abs/2501.15781v2
523,"Large language models (LLMs) have demonstrated remarkable capabilities across a range of natural language processing (NLP) tasks, capturing the attention of both practitioners and the broader public. A key question that now preoccupies the AI community concerns the capabilities and limitations of these models, with trustworthiness emerging as a central issue, particularly as LLMs are increasingly applied in sensitive fields like healthcare and finance, where errors can have serious consequences. However, most previous studies on the trustworthiness of LLMs have been limited to a single language, typically the predominant one in the dataset, such as English. In response to the growing global deployment of LLMs, we introduce XTRUST, the first comprehensive multilingual trustworthiness benchmark. XTRUST encompasses a diverse range of topics, including illegal activities, hallucination, out-of-distribution (OOD) robustness, physical and mental health, toxicity, fairness, misinformation, privacy, and machine ethics, across 10 different languages. Using XTRUST, we conduct an empirical evaluation of the multilingual trustworthiness of five widely used LLMs, offering an in-depth analysis of their performance across languages and tasks. Our results indicate that many LLMs struggle with certain low-resource languages, such as Arabic and Russian, highlighting the considerable room for improvement in the multilingual trustworthiness of current language models. The code is available at https://github.com/LluckyYH/XTRUST.",,"Yahan Li, Yi Wang, Yi Chang, Yuan Wu",2025-06-03T11:45:34Z,XTRUST: On the Multilingual Trustworthiness of Large Language Models,XTRUST: Mehrsprachige Vertrauenswürdigkeit großer Sprachmodelle,XLust:关于多语种大语言模式多语种可信赖性,http://arxiv.org/abs/2409.15762v2
524,"End-to-end autonomous driving has advanced significantly, offering benefits such as system simplicity and stronger driving performance in both open-loop and closed-loop settings than conventional pipelines. However, existing frameworks still suffer from low success rates in closed-loop evaluations, highlighting their limitations in real-world deployment. In this paper, we introduce X-Driver, a unified multi-modal large language models(MLLMs) framework designed for closed-loop autonomous driving, leveraging Chain-of-Thought(CoT) and autoregressive modeling to enhance perception and decision-making. We validate X-Driver across multiple autonomous driving tasks using public benchmarks in CARLA simulation environment, including Bench2Drive[6]. Our experimental results demonstrate superior closed-loop performance, surpassing the current state-of-the-art(SOTA) while improving the interpretability of driving decisions. These findings underscore the importance of structured reasoning in end-to-end driving and establish X-Driver as a strong baseline for future research in closed-loop autonomous driving.",,"Wei Liu, Jiyuan Zhang, Binxiong Zheng, Yufeng Hu, Yingzhan Lin, Zengfeng Zeng",2025-06-03T11:30:21Z,X-Driver: Explainable Autonomous Driving with Vision-Language Models,X-Driver: Erklärbares autonomes Fahren mit Vision-Sprachenmodellen,"X-驱动:可解释的自主驾驶,采用视觉语言模型",http://arxiv.org/abs/2505.05098v2
525,"To address the global challenge of online hate speech, prior research has developed detection models to flag such content on social media. However, due to systematic biases in evaluation datasets, the real-world effectiveness of these models remains unclear, particularly across geographies. We introduce HateDay, the first global hate speech dataset representative of social media settings, constructed from a random sample of all tweets posted on September 21, 2022 and covering eight languages and four English-speaking countries. Using HateDay, we uncover substantial variation in the prevalence and composition of hate speech across languages and regions. We show that evaluations on academic datasets greatly overestimate real-world detection performance, which we find is very low, especially for non-European languages. Our analysis identifies key drivers of this gap, including models' difficulty to distinguish hate from offensive speech and a mismatch between the target groups emphasized in academic datasets and those most frequently targeted in real-world settings. We argue that poor model performance makes public models ill-suited for automatic hate speech moderation and find that high moderation rates are only achievable with substantial human oversight. Our results underscore the need to evaluate detection systems on data that reflects the complexity and diversity of real-world social media.",,"Manuel Tonneau, Diyi Liu, Niyati Malhotra, Scott A. Hale, Samuel P. Fraiberger, Victor Orozco-Olvera, Paul Röttger",2025-06-03T11:27:21Z,HateDay: Insights from a Global Hate Speech Dataset Representative of a   Day on Twitter,HateDay: Einblicke aus einem globalen Hass-Sprachdatensatz Vertreter eines Tages auf Twitter,HateDay:Twitter上全球仇恨言论数据集代表对日的透视,http://arxiv.org/abs/2411.15462v3
526,"With the surge and widespread application of image generation models, data privacy and content safety have become major concerns and attracted great attention from users, service providers, and policymakers. Machine unlearning (MU) is recognized as a cost-effective and promising means to address these challenges. Despite some advancements, image generation model unlearning (IGMU) still faces remarkable gaps in practice, e.g., unclear task discrimination and unlearning guidelines, lack of an effective evaluation framework, and unreliable evaluation metrics. These can hinder the understanding of unlearning mechanisms and the design of practical unlearning algorithms. We perform exhaustive assessments over existing state-of-the-art unlearning algorithms and evaluation standards, and discover several critical flaws and challenges in IGMU tasks. Driven by these limitations, we make several core contributions, to facilitate the comprehensive understanding, standardized categorization, and reliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel hierarchical task categorization framework. It provides detailed implementation guidance for IGMU, assisting in the design of unlearning algorithms and the construction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation framework. It includes reliable quantitative metrics across five critical aspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can be used for extensive evaluations of IGMU, training content detectors for judgment, and benchmarking the state-of-the-art unlearning algorithms. With EvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot handle the unlearning well across different evaluation dimensions, especially for preservation and robustness. Code and models are available at https://github.com/ryliu68/IGMU.",,"Renyang Liu, Wenjie Feng, Tianwei Zhang, Wei Zhou, Xueqi Cheng, See-Kiong Ng",2025-06-03T11:25:14Z,Rethinking Machine Unlearning in Image Generation Models,Rethinking Machine Unlearning in Image Generation Models,在图像生成模型中重新思考机离学习,http://arxiv.org/abs/2506.02761v1
527,"Vocabulary use is a fundamental aspect of second language (L2) proficiency. To date, its assessment by automated systems has typically examined the context-independent, or part-of-speech (PoS) related use of words. This paper introduces a novel approach to enable fine-grained vocabulary evaluation exploiting the precise use of words within a sentence. The scheme combines large language models (LLMs) with the English Vocabulary Profile (EVP). The EVP is a standard lexical resource that enables in-context vocabulary use to be linked with proficiency level. We evaluate the ability of LLMs to assign proficiency levels to individual words as they appear in L2 learner writing, addressing key challenges such as polysemy, contextual variation, and multi-word expressions. We compare LLMs to a PoS-based baseline. LLMs appear to exploit additional semantic information that yields improved performance. We also explore correlations between word-level proficiency and essay-level proficiency. Finally, the approach is applied to examine the consistency of the EVP proficiency levels. Results show that LLMs are well-suited for the task of vocabulary assessment.",,"Stefano Bannò, Kate Knill, Mark Gales",2025-06-03T11:23:57Z,Exploiting the English Vocabulary Profile for L2 word-level vocabulary   assessment with LLMs,Ausnutzung des englischen Vokabelprofils für die Wortschatzbewertung auf L2-Ebene mit LLMs,与LLMM公司利用L2字级词汇评估的英文词汇简况,http://arxiv.org/abs/2506.02758v1
528,"Theorem proving serves as a major testbed for evaluating complex reasoning abilities in large language models (LLMs). However, traditional automated theorem proving (ATP) approaches rely heavily on formal proof systems that poorly align with LLMs' strength derived from informal, natural language knowledge acquired during pre-training. In this work, we propose DeepTheorem, a comprehensive informal theorem-proving framework exploiting natural language to enhance LLM mathematical reasoning. DeepTheorem includes a large-scale benchmark dataset consisting of 121K high-quality IMO-level informal theorems and proofs spanning diverse mathematical domains, rigorously annotated for correctness, difficulty, and topic categories, accompanied by systematically constructed verifiable theorem variants. We devise a novel reinforcement learning strategy (RL-Zero) explicitly tailored to informal theorem proving, leveraging the verified theorem variants to incentivize robust mathematical inference. Additionally, we propose comprehensive outcome and process evaluation metrics examining proof correctness and the quality of reasoning steps. Extensive experimental analyses demonstrate DeepTheorem significantly improves LLM theorem-proving performance compared to existing datasets and supervised fine-tuning protocols, achieving state-of-the-art accuracy and reasoning quality. Our findings highlight DeepTheorem's potential to fundamentally advance automated informal theorem proving and mathematical exploration.",,"Ziyin Zhang, Jiahao Xu, Zhiwei He, Tian Liang, Qiuzhi Liu, Yansi Li, Linfeng Song, Zhenwen Liang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu",2025-06-03T11:23:21Z,DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural   Language and Reinforcement Learning,DeepTheorem: Verbesserung der LLM-Gründung für Theorem Proving durch natürliche Sprache und Stärkung Lernen,深理理论:通过自然语言和加强学习提高理论力的理论力和强化学习,http://arxiv.org/abs/2505.23754v2
529,"The rapid growth of social media has amplified the spread of offensive, violent, and vulgar speech, which poses serious societal and cybersecurity concerns. Detecting such content in Arabic text is particularly complex due to limited labeled data, dialectal variations, and the language's inherent complexity. This paper proposes a novel framework that integrates multi-task learning (MTL) with active learning to enhance offensive speech detection in Arabic social media text. By jointly training on two auxiliary tasks, violent and vulgar speech, the model leverages shared representations to improve the detection accuracy of the offensive speech. Our approach dynamically adjusts task weights during training to balance the contribution of each task and optimize performance. To address the scarcity of labeled data, we employ an active learning strategy through several uncertainty sampling techniques to iteratively select the most informative samples for model training. We also introduce weighted emoji handling to better capture semantic cues. Experimental results on the OSACT2022 dataset show that the proposed framework achieves a state-of-the-art macro F1-score of 85.42%, outperforming existing methods while using significantly fewer fine-tuning samples. The findings of this study highlight the potential of integrating MTL with active learning for efficient and accurate offensive language detection in resource-constrained settings.",,"Aisha Alansari, Hamzah Luqman",2025-06-03T11:17:03Z,Multi-task Learning with Active Learning for Arabic Offensive Speech   Detection,Multi-Task-Lernen mit aktivem Lernen für arabische Offensive Speech Detection,"多任务学习,积极学习阿拉伯语进攻性言语探测",http://arxiv.org/abs/2506.02753v1
530,"Entity Set Expansion (ESE) aims to identify new entities belonging to the same semantic class as the given set of seed entities. Traditional methods solely relied on positive seed entities to represent the target fine-grained semantic class, rendering them tough to represent ultra-fine-grained semantic classes. Specifically, merely relying on positive seed entities leads to two inherent shortcomings: (i) Ambiguity among ultra-fine-grained semantic classes. (ii) Inability to define ``unwanted'' semantics. Hence, previous ESE methods struggle to address the ultra-fine-grained ESE (Ultra-ESE) task. To solve this issue, we first introduce negative seed entities in the inputs, which jointly describe the ultra-fine-grained semantic class with positive seed entities. Negative seed entities eliminate the semantic ambiguity by providing a contrast between positive and negative attributes. Meanwhile, it provides a straightforward way to express ``unwanted''. To assess model performance in Ultra-ESE and facilitate further research, we also constructed UltraWiki, the first large-scale dataset tailored for Ultra-ESE. UltraWiki encompasses 50,973 entities and 394,097 sentences, alongside 236 ultra-fine-grained semantic classes, where each class is represented with 3-5 positive and negative seed entities. Moreover, a retrieval-based framework RetExpan and a generation-based framework GenExpan are proposed to provide powerful baselines for Ultra-ESE. Additionally, we devised two strategies to enhance models' comprehension of ultra-fine-grained entities' semantics: contrastive learning and chain-of-thought reasoning. Extensive experiments confirm the effectiveness of our proposed strategies and also reveal that there remains a large space for improvement in Ultra-ESE.",,"Yangning Li, Qingsong Lv, Tianyu Yu, Yinghui Li, Xuming Hu, Wenhao Jiang, Hai-Tao Zheng, Hui Wang",2025-06-03T11:11:29Z,UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed   Entities,UltraWiki: Ultra-feinkörnige Entity-Set-Erweiterung mit negativen Saatgut-Entities,UltraWiki:带有负种子实体的超松果扩大实体集,http://arxiv.org/abs/2403.04247v3
531,"We extracted gender-specific actions from text corpora and Twitter, and compared them to stereotypical expectations of people. We used Open Mind Common Sense (OMCS), a commonsense knowledge repository, to focus on actions that are pertinent to common sense and daily life of humans. We use the gender information of Twitter users and Web-corpus-based pronoun/name gender heuristics to compute the gender bias of the actions. With high recall, we obtained a Spearman correlation of 0.47 between corpus-based predictions and a human gold standard, and an area under the ROC curve of 0.76 when predicting the polarity of the gold standard. We conclude that it is feasible to use natural text (and a Twitter-derived corpus in particular) in order to augment commonsense repositories with the stereotypical gender expectations of actions. We also present a dataset of 441 commonsense actions with human judges' ratings on whether the action is typically/slightly masculine/feminine (or neutral), and another larger dataset of 21,442 actions automatically rated by the methods we investigate in this study.",,"Amaç Herdağdelen, Marco Baroni",2025-06-03T10:55:00Z,Stereotypical gender actions can be extracted from Web text,Stereotypische Gender-Aktionen können aus dem Webtext extrahiert werden,性别定型行动可从网上文本中摘取,http://arxiv.org/abs/2506.02740v1
532,"Chain-of-thought (CoT) significantly enhances the performance of large language models (LLMs) across a wide range of tasks, and prior research shows that CoT can theoretically increase expressiveness. However, there is limited mechanistic understanding of the algorithms that Transformer+CoT can learn. Our key contributions are: (1) We evaluate the state tracking capabilities of Transformer+CoT and its variants, confirming the effectiveness of CoT. (2) Next, we identify the circuit (a subset of model components, responsible for tracking the world state), indicating that late-layer MLP neurons play a key role. We propose two metrics, compression and distinction, and show that the neuron sets for each state achieve nearly 100% accuracy, providing evidence of an implicit finite state automaton (FSA) embedded within the model. (3) Additionally, we explore three challenging settings: skipping intermediate steps, introducing data noises, and testing length generalization. Our results demonstrate that Transformer+CoT learns robust algorithms (FSAs), highlighting its resilience in challenging scenarios. Our code is available at https://github.com/IvanChangPKU/FSA.",,"Yifan Zhang, Wenyu Du, Dongming Jin, Jie Fu, Zhi Jin",2025-06-03T10:48:30Z,Finite State Automata Inside Transformers with Chain-of-Thought: A   Mechanistic Study on State Tracking,Finite State Automata Inside Transformers mit Chain-of-Thought: Eine mechanistische Studie über State Tracking,"具有 "" 探索链 "" 的变形器内部自控控制系统:关于国家跟踪的机械研究",http://arxiv.org/abs/2502.20129v3
533,"We present an exploratory framework to test whether noise-like input can induce structured responses in language models. Instead of assuming that extraterrestrial signals must be decoded, we evaluate whether inputs can trigger linguistic behavior in generative systems. This shifts the focus from decoding to viewing structured output as a sign of underlying regularity in the input. We tested GPT-2 small, a 117M-parameter model trained on English text, using four types of acoustic input: human speech, humpback whale vocalizations, Phylloscopus trochilus birdsong, and algorithmically generated white noise. All inputs were treated as noise-like, without any assumed symbolic encoding. To assess reactivity, we defined a composite score called Semantic Induction Potential (SIP), combining entropy, syntax coherence, compression gain, and repetition penalty. Results showed that whale and bird vocalizations had higher SIP scores than white noise, while human speech triggered only moderate responses. This suggests that language models may detect latent structure even in data without conventional semantics. We propose that this approach could complement traditional SETI methods, especially in cases where communicative intent is unknown. Generative reactivity may offer a different way to identify data worth closer attention.",,Po-Chieh Yu,2025-06-03T10:46:57Z,An Exploratory Framework for Future SETI Applications: Detecting   Generative Reactivity via Language Models,Ein exploratorischer Rahmen für zukünftige SETI-Anwendungen: Generative Reaktivität anhand von Sprachmodellen erkennen,未来SETI应用探索框架:通过语言模型检测产生反应,http://arxiv.org/abs/2506.02730v1
534,"Large Language Models (LLMs) struggle with accuracy, domain-specific reasoning, and interpretability in vertical domains. Traditional preference alignment methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) often overlook the underlying knowledge sources and reasoning logic. This paper introduces RACE-Align (Retrieval-Augmented and Chain-of-Thought Enhanced Alignment), a novel framework designed to address these limitations. RACE-Align systematically constructs a binary preference dataset incorporating external knowledge support and explicit Chain-of-Thought (CoT) reasoning, then aligns LLMs using the DPO algorithm. The core innovation lies in its preference data construction strategy: it integrates AI-driven retrieval for factual grounding, enhancing knowledgeability and accuracy, and emphasizes the optimization of domain-specific CoT, treating the reasoning process itself as a key preference dimension. A multi-stage, AI-driven refinement pipeline cost-effectively generates these preference pairs. Experimental validation in Traditional Chinese Medicine (TCM) using Qwen3-1.7B as the base model demonstrates that RACE-Align significantly outperforms the original base model and a model fine-tuned only with Supervised Fine-Tuning (SFT). Improvements were observed across multiple dimensions, including answer accuracy, information richness, application of TCM thinking patterns, logicality and depth of reasoning, and interpretability. These findings suggest RACE-Align offers an effective pathway to enhance LLMs' knowledge application, reasoning reliability, and process transparency in complex vertical domains.",,"Qihang Yan, Xinyu Zhang, Luming Guo, Qi Zhang, Feifan Liu",2025-06-03T10:36:38Z,RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference   Alignment for Large Language Models,RACE-Align: Retrieval-Augmented und Chain-of-Thought Verbesserte Präferenz-Ausrichtung für große Sprachmodelle,RACE-对称:大语言模型检索和搜索链强化优惠比对,http://arxiv.org/abs/2506.02726v1
535,"The remarkable performance achieved by Large Language Models (LLM) has driven research efforts to leverage them for a wide range of tasks and input modalities. In speech-to-text (S2T) tasks, the emerging solution consists of projecting the output of the encoder of a Speech Foundational Model (SFM) into the LLM embedding space through an adapter module. However, no work has yet investigated how much the downstream-task performance depends on each component (SFM, adapter, LLM) nor whether the best design of the adapter depends on the chosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter modules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on two widespread S2T tasks, namely Automatic Speech Recognition and Speech Translation. Our results demonstrate that the SFM plays a pivotal role in downstream performance, while the adapter choice has moderate impact and depends on the SFM and LLM.",,"Francesco Verdini, Pierfrancesco Melucci, Stefano Perna, Francesco Cariaggi, Marco Gaido, Sara Papi, Szymon Mazurek, Marek Kasztelnik, Luisa Bentivogli, Sébastien Bratières, Paolo Merialdo, Simone Scardapane",2025-06-03T10:28:07Z,How to Connect Speech Foundation Models and Large Language Models? What   Matters and What Does Not,Wie verbindet man Sprachstiftungsmodelle und große Sprachmodelle? Was zählt und was nicht,如何连接语音基础模型和大语言模型?,http://arxiv.org/abs/2409.17044v3
536,"Large language models (LLMs) have exhibited remarkable capabilities and achieved significant breakthroughs across various domains, leading to their widespread adoption in recent years. Building on this progress, we investigate their potential in the realm of local life services. In this study, we establish a comprehensive benchmark and systematically evaluate the performance of diverse LLMs across a wide range of tasks relevant to local life services. To further enhance their effectiveness, we explore two key approaches: model fine-tuning and agent-based workflows. Our findings reveal that even a relatively compact 7B model can attain performance levels comparable to a much larger 72B model, effectively balancing inference cost and model capability. This optimization greatly enhances the feasibility and efficiency of deploying LLMs in real-world online services, making them more practical and accessible for local life applications.",,"Xiaochong Lan, Jie Feng, Jiahuan Lei, Xinlei Shi, Yong Li",2025-06-03T10:18:19Z,Benchmarking and Advancing Large Language Models for Local Life Services,Benchmarking und Advancing von großen Sprachmodellen für lokale Life Services,制定基准和推进当地生活服务大语言模式,http://arxiv.org/abs/2506.02720v1
537,"Continued pretraining (CPT) is a popular approach to adapt existing large language models (LLMs) to new languages. When doing so, it is common practice to include a portion of English data in the mixture, but its role has not been carefully studied to date. In this work, we show that including English does not impact validation perplexity, yet it is critical for the emergence of downstream capabilities in the target language. We introduce a language-agnostic benchmark for in-context learning (ICL), which reveals catastrophic forgetting early on CPT when English is not included. This in turn damages the ability of the model to generalize to downstream prompts in the target language as measured by perplexity, even if it does not manifest in terms of accuracy until later in training, and can be tied to a big shift in the model parameters. Based on these insights, we introduce curriculum learning and exponential moving average (EMA) of weights as effective alternatives to mitigate the need for English. All in all, our work sheds light into the dynamics by which emergent abilities arise when doing CPT for language adaptation, and can serve as a foundation to design more effective methods in the future.",,"Ahmed Elhady, Eneko Agirre, Mikel Artetxe",2025-06-03T10:17:34Z,Emergent Abilities of Large Language Models under Continued Pretraining   for Language Adaptation,Emergente Fähigkeiten großer Sprachmodelle unter fortgesetzter Vorausbildung für Sprachanpassung,语言适应持续培训前的大型语言模式新兴能力,http://arxiv.org/abs/2506.00288v2
538,"Rapid growth in speech data demands adaptive models, as traditional static methods fail to keep pace with dynamic and diverse speech information. We introduce continuous speech learning, a new set-up targeting at bridging the adaptation gap in current speech models. We use the encoder-decoder Whisper model to standardize speech tasks into a generative format. We integrate a learnable gated-fusion layer on the top of the encoder to dynamically select task-specific features for downstream tasks. Our approach improves accuracy significantly over traditional methods in six speech processing tasks, demonstrating gains in adapting to new speech tasks without full retraining.",,"Guitao Wang, Jinming Zhao, Hao Yang, Guilin Qi, Tongtong Wu, Gholamreza Haffari",2025-06-03T10:16:03Z,Continual Speech Learning with Fused Speech Features,Kontinuierliches Sprachlernen mit fused Speech-Features,"持续语音学习,有模糊的语音文稿特点",http://arxiv.org/abs/2506.01496v2
539,"LLM-based Interactive Drama is a novel AI-based dialogue scenario, where the user (i.e. the player) plays the role of a character in the story, has conversations with characters played by LLM agents, and experiences an unfolding story. This paper begins with understanding interactive drama from two aspects: Immersion, the player's feeling of being present in the story, and Agency, the player's ability to influence the story world. Both are crucial to creating an enjoyable interactive experience, while they have been underexplored in previous work. To enhance these two aspects, we first propose Playwriting-guided Generation, a novel method that helps LLMs craft dramatic stories with substantially improved structures and narrative quality. Additionally, we introduce Plot-based Reflection for LLM agents to refine their reactions to align with the player's intentions. Our evaluation relies on human judgment to assess the gains of our methods in terms of immersion and agency.",,"Hongqiu Wu, Weiqi Wu, Tianyang Xu, Jiameng Zhang, Hai Zhao",2025-06-03T10:10:00Z,Towards Enhanced Immersion and Agency for LLM-based Interactive Drama,Auf dem Weg zu mehr Eintauchen und Agentur für LLM-basiertes interaktives Drama,争取增强吸引和以LLM为基础的LLM互动戏剧机构,http://arxiv.org/abs/2502.17878v2
540,"Self-attention in transformer models is an incremental associative memory that maps key vectors to value vectors. One way to speed up self-attention is to employ GPU-compatible vector search algorithms based on standard partitioning methods such as k-means. However, such partitioning methods yield poor results in this context because (1) the keys and queries follow different distributions, and (2) the RoPE positional encoding hinders the bucket assignment.   This paper introduces Saap (Self-Attention with Asymmetric Partitions), which overcomes these problems. It is an asymmetrical indexing technique that employs distinct partitions for keys and queries, thereby approximating self-attention with a data-adaptive sparsity pattern. It works on pretrained language models and only requires to train (offline) a small query classifier. On a long context Llama 3.1-8b model, with sequences ranging from 100k to 500k tokens, Saap typically reduces by a factor of 20 the fraction of memory that needs to be looked-up, which translates to a time saving of 60\% when compared to FlashAttention-v2.",,"Pierre-Emmanuel Mazaré, Gergely Szilvasy, Maria Lomeli, Francisco Massa, Naila Murray, Hervé Jégou, Matthijs Douze",2025-06-03T10:07:35Z,Inference-time sparse attention with asymmetric indexing,Inferenzzeit spärliche Aufmerksamkeit mit asymmetrischer Indexierung,"由于非对称指数化,推断-时间关注不够",http://arxiv.org/abs/2502.08246v2
541,"Image scoring is a crucial task in numerous real-world applications. To trust a model's judgment, understanding its rationale is essential. This paper proposes a novel training method for Vision Language Models (VLMs) to generate not only image scores but also corresponding justifications in natural language. Leveraging only an image scoring dataset and an instruction-tuned VLM, our method enables self-training, utilizing the VLM's generated text without relying on external data or models. In addition, we introduce a simple method for creating a dataset designed to improve alignment between predicted scores and their textual justifications. By iteratively training the model with Direct Preference Optimization on two distinct datasets and merging them, we can improve both scoring accuracy and the coherence of generated explanations.",,"Naoto Tanji, Toshihiko Yamasaki",2025-06-03T10:04:19Z,Iterative Self-Improvement of Vision Language Models for Image Scoring   and Self-Explanation,Iterative Selbst-Verbesserung von Visions-Sprachmodellen für Bildbeurteilung und Selbst-Erläuterung,图像浏览和自我剥削的愿景语言模型的迭代自我改进,http://arxiv.org/abs/2506.02708v1
542,"Large Language Models (LLMs) often exhibit knowledge disparities across languages. Encouraging LLMs to \textit{abstain} when faced with knowledge gaps is a promising strategy to reduce hallucinations in multilingual settings. Current abstention strategies for multilingual scenarios primarily rely on generating feedback in various languages using LLMs and performing self-reflection. However, these methods can be adversely impacted by inaccuracies and biases in the generated feedback. To address this, from a causal perspective, we introduce \textit{CausalAbstain}, a method that helps LLMs determine whether to utilize multiple generated feedback responses and how to identify the most useful ones. Extensive experiments demonstrate that \textit{CausalAbstain} effectively selects helpful feedback and enhances abstention decisions with interpretability in both native language (\textsc{Casual-native}) and multilingual (\textsc{Causal-multi}) settings, outperforming strong baselines on two benchmark datasets covering encyclopedic and commonsense knowledge QA tasks. Our code and data are open-sourced at https://github.com/peachch/CausalAbstain.",,"Yuxi Sun, Aoqi Zuo, Wei Gao, Jing Ma",2025-06-03T09:58:17Z,CausalAbstain: Enhancing Multilingual LLMs with Causal Reasoning for   Trustworthy Abstention,CausalAbstain: Mehrsprachige LLMs mit kausaler Begründung für vertrauensvolle Enthaltung verbessern,"原因:加强多语种LLMs,加上值得信赖的禁闭原因",http://arxiv.org/abs/2506.00519v2
543,"Large Language Models (LLMs) have demonstrated impressive capabilities in natural language processing tasks, such as text generation and semantic understanding. However, their performance on numerical reasoning tasks, such as basic arithmetic, numerical retrieval, and magnitude comparison, remains surprisingly poor. This gap arises from their reliance on surface-level statistical patterns rather than understanding numbers as continuous magnitudes. Existing benchmarks primarily focus on either linguistic competence or structured mathematical problem-solving, neglecting fundamental numerical reasoning required in real-world scenarios. To bridge this gap, we propose NumericBench, a comprehensive benchmark to evaluate six fundamental numerical capabilities: number recognition, arithmetic operations, contextual retrieval, comparison, summary, and logical reasoning. NumericBench includes datasets ranging from synthetic number lists to the crawled real-world data, addressing challenges like long contexts, noise, and multi-step reasoning. Extensive experiments on state-of-the-art LLMs, including GPT-4 and DeepSeek, reveal persistent weaknesses in numerical reasoning, highlighting the urgent need to improve numerically-aware language modeling. The benchmark is released in: https://github.com/TreeAI-Lab/NumericBench.",,"Haoyang Li, Xuejia Chen, Zhanchao XU, Darian Li, Nicole Hu, Fei Teng, Yiming Li, Luyu Qiu, Chen Jason Zhang, Qing Li, Lei Chen",2025-06-03T09:47:24Z,Exposing Numeracy Gaps: A Benchmark to Evaluate Fundamental Numerical   Abilities in Large Language Models,Aufdecken von Zahlenlücken: Ein Benchmark zur Bewertung fundamentaler numerischer Fähigkeiten in großen Sprachmodellen,数字差距:评价大语言模型基本数字能力的基准,http://arxiv.org/abs/2502.11075v2
544,"The recent rise of reasoning-tuned Large Language Models (LLMs)--which generate chains of thought (CoTs) before giving the final answer--has attracted significant attention and offers new opportunities for gaining insights into human label variation, which refers to plausible differences in how multiple annotators label the same data instance. Prior work has shown that LLM-generated explanations can help align model predictions with human label distributions, but typically adopt a reverse paradigm: producing explanations based on given answers. In contrast, CoTs provide a forward reasoning path that may implicitly embed rationales for each answer option, before generating the answers. We thus propose a novel LLM-based pipeline enriched with linguistically-grounded discourse segmenters to extract supporting and opposing statements for each answer option from CoTs with improved accuracy. We also propose a rank-based HLV evaluation framework that prioritizes the ranking of answers over exact scores, which instead favor direct comparison of label distributions. Our method outperforms a direct generation method as well as baselines on three datasets, and shows better alignment of ranking methods with humans, highlighting the effectiveness of our approach.",,"Beiduo Chen, Yang Janet Liu, Anna Korhonen, Barbara Plank",2025-06-03T09:45:05Z,Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain   Human Label Variation,"Threading the Needle: Rewebing Chain-of-Thought Begründung zu erklären, Human Label Variation",针线串列: 重新编织尝试链 解释人类标签变化的原因,http://arxiv.org/abs/2505.23368v2
545,"The growing use of large language models has raised environmental and economic concerns about their intensity of resource usage during inference. Serving these models to each user requires substantial energy and water for cooling. Model compression techniques like quantization can shrink large language models and make them more resource efficient at the cost of potential performance degradation. Quantization methods compress model size through replacing their high-precision parameters by quantized values of lower precision. Among existing methods, the ApiQ method achieves superior accuracy preservation at minimal memory and time overhead. We investigate two ideas to extend performance in ultra-low-bit quantization beyond ApiQ's level. First, we look into combining existing quantization-aware training techniques with ApiQ's partial training. We show that this does not outperform the baseline ApiQ method with limited training data and frozen weights. This leads to two key insights: (1) The substantial representational capacity that is gained through full retraining is unlikely to be feasible through partial training. (2) This gain may depend on using a large and diverse dataset in quantization-aware training. Second, through a novel approach informed by the two insights, we propose an ultra-low-bit quantization method that builds upon ApiQ and extends its performance without the need for full retraining. This publicly available method relies on a saliency-aware regularization term that prioritizes preserving the most impactful parameters during quantization. Our experiments on LLaMA 7B and 13B benchmarks demonstrate that our method reduces the ApiQ's accuracy degradation by 10.85\% and 7.54\% respectively.",,"Deyu Cao, Samin Aref",2025-06-03T09:42:54Z,Enhancing Ultra-Low-Bit Quantization of Large Language Models Through   Saliency-Aware Partial Retraining,Erhöhung der Ultra-Low-Bit-Quantisierung großer Sprachmodelle durch Saliency-Aware Partial Retraining,"通过提高质量-软件部分再培训,加强大语言模型的超低比小量量化",http://arxiv.org/abs/2504.13932v2
546,"There is increasing evidence of Human Label Variation (HLV) in Natural Language Inference (NLI), where annotators assign different labels to the same premise-hypothesis pair. However, within-label variation--cases where annotators agree on the same label but provide divergent reasoning--poses an additional and mostly overlooked challenge. Several NLI datasets contain highlighted words in the NLI item as explanations, but the same spans on the NLI item can be highlighted for different reasons, as evidenced by free-text explanations, which offer a window into annotators' reasoning. To systematically understand this problem and gain insight into the rationales behind NLI labels, we introduce LITEX, a linguistically-informed taxonomy for categorizing free-text explanations. Using this taxonomy, we annotate a subset of the e-SNLI dataset, validate the taxonomy's reliability, and analyze how it aligns with NLI labels, highlights, and explanations. We further assess the taxonomy's usefulness in explanation generation, demonstrating that conditioning generation on LITEX yields explanations that are linguistically closer to human explanations than those generated using only labels or highlights. Our approach thus not only captures within-label variation but also shows how taxonomy-guided generation for reasoning can bridge the gap between human and model explanations more effectively than existing strategies.",,"Pingjun Hong, Beiduo Chen, Siyao Peng, Marie-Catherine de Marneffe, Barbara Plank",2025-06-03T09:41:25Z,LiTEx: A Linguistic Taxonomy of Explanations for Understanding   Within-Label Variation in Natural Language Inference,LiTEx: Eine linguistische Taxonomie von Erklärungen zum Verständnis von Inner-Label-Variation in natürlicher Sprach-Inferenz,"LiTEx:用语言对解释进行分类,以了解在标内对自然语言推断的变异的理解",http://arxiv.org/abs/2505.22848v2
547,"Despite significant advances in Large Language Models (LLMs), planning tasks still present challenges for LLM-based agents. Existing planning methods face two key limitations: heavy constraints and cascading errors. To address these limitations, we propose a novel parallel planning paradigm, which Decomposes, Plans for subtasks in Parallel, and Merges subplans into a final plan (DPPM). Specifically, DPPM decomposes the complex task based on constraints into subtasks, generates the subplan for each subtask in parallel, and merges them into a global plan. In addition, our approach incorporates a verification and refinement module, enabling error correction and conflict resolution. Experimental results demonstrate that DPPM significantly outperforms existing methods in travel planning tasks.",,"Zhengdong Lu, Weikai Lu, Yiling Tao, Yun Dai, ZiXuan Chen, Huiping Zhuang, Cen Chen, Hao Peng, Ziqian Zeng",2025-06-03T09:33:13Z,"Decompose, Plan in Parallel, and Merge: A Novel Paradigm for Large   Language Models based Planning with Multiple Constraints","Zersetzen, Planen in Parallel und Zusammenführen: Ein neuartiges Paradigma für die auf großen Sprachmodellen basierende Planung mit mehreren Einschränkungen",分解、平行计划和合并计划:基于多种限制因素的大型语言模式规划的新范例,http://arxiv.org/abs/2506.02683v1
548,"Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon.",,"Zhong-Zhi Li, Xiao Liang, Zihao Tang, Lei Ji, Peijie Wang, Haotian Xu, Xing W, Haizhen Huang, Weiwei Deng, Ying Nian Wu, Yeyun Gong, Zhijiang Guo, Xiao Liu, Fei Yin, Cheng-Lin Liu",2025-06-03T09:23:41Z,"TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression","TL;DR: Zu lange, re-Gewichtung für effziente LLM-vernünftige Kompression","TL; DR: 过长, 有效 LLM 原因压缩的重新加权",http://arxiv.org/abs/2506.02678v1
549,"In this work we show that the size versus accuracy trade-off of neural network quantization can be significantly improved by increasing the quantization dimensionality. We propose the GPTVQ method, a new fast method for post-training vector quantization (VQ) that scales well to Large Language Models (LLMs). Our method interleaves quantization of one or more columns with updates to the remaining unquantized weights, using information from the Hessian of the per-layer output reconstruction MSE. Quantization codebooks are initialized using an efficient data-aware version of the EM algorithm. The codebooks are then updated, and further compressed by using integer quantization and SVD-based compression. GPTVQ establishes a new state-of-the art in the size vs accuracy trade-offs on a wide range of LLMs such as Llama-v2 and Mistral. Furthermore, our method is efficient: on a single H100 it takes between 3 and 11 hours to process a Llamav2-70B model, depending on quantization setting. Lastly, with on-device timings for VQ decompression on a mobile CPU we show that VQ leads to improved latency compared to using a 4-bit integer format.",,"Mart van Baalen, Andrey Kuzmin, Ivan Koryakovskiy, Markus Nagel, Peter Couperus, Cedric Bastoul, Eric Mahurin, Tijmen Blankevoort, Paul Whatmough",2025-06-03T09:22:07Z,GPTVQ: The Blessing of Dimensionality for LLM Quantization,GPTVQ: Der Segen der Dimensionalität für die LLM-Quantisierung,GPTVQ:LLM量化的多面性,http://arxiv.org/abs/2402.15319v2
550,"Recent advances in Large Language Models (LLMs) have shown promise in automating discourse annotation for conversations. While manually designing tree annotation schemes significantly improves annotation quality for humans and models, their creation remains time-consuming and requires expert knowledge. We propose a fully automated pipeline that uses LLMs to construct such schemes and perform annotation. We evaluate our approach on speech functions (SFs) and the Switchboard-DAMSL (SWBD-DAMSL) taxonomies. Our experiments compare various design choices, and we show that frequency-guided decision trees, paired with an advanced LLM for annotation, can outperform previously manually designed trees and even match or surpass human annotators while significantly reducing the time required for annotation. We release all code and resultant schemes and annotations to facilitate future research on discourse annotation.",,"Kseniia Petukhova, Ekaterina Kochmar",2025-06-03T09:19:32Z,A Fully Automated Pipeline for Conversational Discourse Annotation: Tree   Scheme Generation and Labeling with Large Language Models,Eine vollautomatisierte Pipeline für Konversationsdiskurs Annotation: Baumschemaerstellung und Beschriftung mit großen Sprachmodellen,"一个完全自动化的用于沟通的管道,用于对口的分流批注:植树方案生成和与大语言模型的标签",http://arxiv.org/abs/2504.08961v2
551,"Personalized Large Language Models (LLMs) are increasingly used in diverse applications, where they are assigned a specific persona - such as a happy high school teacher - to guide their responses. While prior research has examined how well LLMs adhere to predefined personas in writing style, a comprehensive analysis of consistency across different personas and task types is lacking. In this paper, we introduce a new standardized framework to analyze consistency in persona-assigned LLMs. We define consistency as the extent to which a model maintains coherent responses when assigned the same persona across different tasks and runs. Our framework evaluates personas across four different categories (happiness, occupation, personality, and political stance) spanning multiple task dimensions (survey writing, essay generation, social media post generation, single turn, and multi-turn conversations). Our findings reveal that consistency is influenced by multiple factors, including the assigned persona, stereotypes, and model design choices. Consistency also varies across tasks, increasing with more structured tasks and additional context. All code is available on GitHub.",,"Manon Reusens, Bart Baesens, David Jurgens",2025-06-03T09:12:23Z,Are Economists Always More Introverted? Analyzing Consistency in   Persona-Assigned LLMs,Sind Ökonomen immer mehr introvertiert? Analyse der Konsistenz in Persona-zugeordneten LLMs,经济学家是否总是更加内向?分析人与人之间的一致性,http://arxiv.org/abs/2506.02659v1
552,"Transformer-based models have shown outstanding results in natural language processing but face challenges in applications like classifying small-scale clinical texts, especially with constrained computational resources. This study presents a customized Mixture of Expert (MoE) Transformer models for classifying small-scale French clinical texts at CHU Sainte-Justine Hospital. The MoE-Transformer addresses the dual challenges of effective training with limited data and low-resource computation suitable for in-house hospital use. Despite the success of biomedical pre-trained models such as CamemBERT-bio, DrBERT, and AliBERT, their high computational demands make them impractical for many clinical settings. Our MoE-Transformer model not only outperforms DistillBERT, CamemBERT, FlauBERT, and Transformer models on the same dataset but also achieves impressive results: an accuracy of 87\%, precision of 87\%, recall of 85\%, and F1-score of 86\%. While the MoE-Transformer does not surpass the performance of biomedical pre-trained BERT models, it can be trained at least 190 times faster, offering a viable alternative for settings with limited data and computational resources. Although the MoE-Transformer addresses challenges of generalization gaps and sharp minima, demonstrating some limitations for efficient and accurate clinical text classification, this model still represents a significant advancement in the field. It is particularly valuable for classifying small French clinical narratives within the privacy and constraints of hospital-based computational resources.",,"Thanh-Dung Le, Philippe Jouvet, Rita Noumeir",2025-06-03T09:08:30Z,Improving Transformer Performance for French Clinical Notes   Classification Using Mixture of Experts on a Limited Dataset,Verbesserung der Transformer-Performance für französische klinische Anmerkungen Klassifizierung mittels Mischung von Experten auf einem begrenzten Datensatz,利用有限数据集专家混合方法改进法国临床说明分类的变换性性能,http://arxiv.org/abs/2303.12892v3
553,"Self-evolving trainin--where models iteratively learn from their own outputs--has emerged as a key approach for complex reasoning tasks, addressing the scarcity of high-quality chain-of-thought data. However, its effectiveness in multimodal reasoning, a domain more intricate than text-only reasoning, remains underexplored, and the understanding of critical factors in this training paradigm remains limited. Furthermore, a central challenge for this training method is performance saturation, which impedes further improvements and scalability. Inspired by reinforcement learning (RL), in this paper, we reframe self-evolving training for multimodal reasoning through the lens of RL, identifying three pivotal factors: Training Method, Reward Model, and Prompt Variation. Through systematic analysis, we establish relatively optimal design principles that significantly enhance multimodal reasoning capabilities. Moreover, delving deeper into training dynamics, we uncover the roots of saturation and propose a new automatic balancing mechanism to mitigate this limitation. Building on these insights, we propose M-STAR (Multimodal Self-evolving Training for Reasoning), a framework that achieves consistent performance gains across models of varying sizes and diverse benchmarks. All resources are made publicly available at https://mstar-lmm.github.io.",,"Wei Liu, Junlong Li, Xiwen Zhang, Fan Zhou, Yu Cheng, Junxian He",2025-06-03T09:07:50Z,Diving into Self-Evolving Training for Multimodal Reasoning,Eintauchen in das Selbst-Evolving Training für multimodale Vernunft,跳入多模式理由自我发展培训,http://arxiv.org/abs/2412.17451v2
554,"Product Attribute Value Identification (PAVI) involves identifying attribute values from product profiles, a key task for improving product search, recommendation, and business analytics on e-commerce platforms. However, existing PAVI methods face critical challenges, such as inferring implicit values, handling out-of-distribution (OOD) values, and producing normalized outputs. To address these limitations, we introduce Taxonomy-Aware Contrastive Learning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR formulates PAVI as an information retrieval task by encoding product profiles and candidate values into embeddings and retrieving values based on their similarity. It leverages contrastive training with taxonomy-aware hard negative sampling and employs adaptive inference with dynamic thresholds. TACLR offers three key advantages: (1) it effectively handles implicit and OOD values while producing normalized outputs; (2) it scales to thousands of categories, tens of thousands of attributes, and millions of values; and (3) it supports efficient inference for high-load industrial deployment. Extensive experiments on proprietary and public datasets validate the effectiveness and efficiency of TACLR. Further, it has been successfully deployed on the real-world e-commerce platform Xianyu, processing millions of product listings daily with frequently updated, large-scale attribute taxonomies. We release the code to facilitate reproducibility and future research at https://github.com/SuYindu/TACLR.",,"Yindu Su, Huike Zou, Lin Sun, Ting Zhang, Haiyang Yang, Liyu Chen, David Lo, Qingheng Zhang, Shuguang Han, Jufeng Chen",2025-06-03T09:02:22Z,TACLR: A Scalable and Efficient Retrieval-based Method for Industrial   Product Attribute Value Identification,TACLR: Ein skalierbares und effizientes Retrieval-basiertes Verfahren für die Ermittlung industrieller Produkte zur Bestimmung des Wertes von Produkten,TACLR: 以可缩放和高效回收法为基础的工业产品价值鉴定方法,http://arxiv.org/abs/2501.03835v4
555,"While post-training compression techniques effectively reduce the memory footprint, latency, and power consumption of Large Language Models (LLMs), they often result in noticeable accuracy degradation and remain limited by hardware and kernel constraints that restrict supported compression formats ultimately reducing flexibility across a wide range of deployment scenarios. In this work, we propose EoRA, a novel fine-tuning-free method that augments compressed LLMs with low-rank matrices, allowing users to rapidly enhance task-specific performance and freely balance the trade-off between accuracy and computational overhead beyond the constraints of compression formats. EoRA consistently outperforms prior training-free low rank methods in recovering the accuracy of compressed LLMs, achieving notable accuracy improvements (e.g., $\mathbf{10.84\%}$ on ARC-Challenge, $\mathbf{6.74\%}$ on MathQA, and $\mathbf{6.74\%}$ on GSM8K) for LLaMA3-8B compressed to 3-bit. We also introduce an optimized CUDA kernel, accelerating inference by up to 1.4x and reducing memory overhead through quantizing EoRA. Overall, EoRA offers a prompt solution for improving the accuracy of compressed models under varying user requirements, enabling more efficient and flexible deployment of LLMs. Code is available at https://github.com/NVlabs/EoRA.",,"Shih-Yang Liu, Maksim Khadkevich, Nai Chit Fung, Charbel Sakr, Chao-Han Huck Yang, Chien-Yi Wang, Saurav Muralidharan, Hongxu Yin, Kwang-Ting Cheng, Jan Kautz, Yu-Chiang Frank Wang, Pavlo Molchanov, Min-Hung Chen",2025-06-03T08:59:53Z,EoRA: Fine-tuning-free Compensation for Compressed LLM with Eigenspace   Low-Rank Approximation,EoRA: Feinabstimmungsfreie Kompensation für komprimierte LLM mit Eigenspace Low-Rank-Annäherung,EORA: 低射速接近电离层空间压缩LLM的无微调补偿,http://arxiv.org/abs/2410.21271v4
556,"As multimodal large language models (MLLMs) advance rapidly, rigorous evaluation has become essential, providing further guidance for their development. In this work, we focus on a unified and robust evaluation of \textbf{vision perception} abilities, the foundational skill of MLLMs. We find that existing perception benchmarks, each focusing on different question types, domains, and evaluation metrics, introduce significant evaluation variance, complicating comprehensive assessments of perception abilities when relying on any single benchmark. To address this, we introduce \textbf{AbilityLens}, a unified benchmark designed to evaluate MLLMs in six key perception abilities (ranging from counting, OCR, to understanding structural data), focusing on both accuracy and stability, with each ability encompassing diverse types of questions, domains, and metrics. With the assistance of AbilityLens, we: (1) identify the strengths and weaknesses of current main-stream MLLMs, highlighting stability patterns and revealing a notable performance gap between state-of-the-art open-source and closed-source models; (2) uncover interesting ability conflict and early convergence phenomena during MLLM training; (3) reveal the primary reason of ability conflict is data mixing ratio and LLM model size; and (4) discuss the effectiveness of some straightforward strategies \eg, fine-tuning and model merging, to solve the ability conflict. The benchmark and online leaderboard is released in https://github.com/Chenfeng1271/AbilityLens.",,"Feng Chen, Chenhui Gou, Jing Liu, Yang Yang, Zhaoyang Li, Jiyuan Zhang, Zhenbang Sun, Bohan Zhuang, Qi Wu",2025-06-03T08:55:05Z,Evaluating and Advancing Multimodal Large Language Models in Perception   Ability Lens,Bewertung und Förderung multimodaler Großsprachenmodelle in Wahrnehmungsfähigkeitslinsen,评价和推进感知能力镜头多式大语言模型,http://arxiv.org/abs/2411.14725v2
557,"The advanced role-playing capabilities of Large Language Models (LLMs) have enabled rich interactive scenarios, yet existing research in social interactions neglects hallucination while struggling with poor generalizability and implicit character fidelity judgments. To bridge this gap, motivated by human behaviour, we introduce a generalizable and explicit paradigm for uncovering interactive patterns of LLMs across diverse worldviews. Specifically, we first define interactive hallucination through stance transfer, then construct SHARP, a benchmark built by extracting relations from commonsense knowledge graphs and utilizing LLMs' inherent hallucination properties to simulate multi-role interactions. Extensive experiments confirm our paradigm's effectiveness and stability, examine the factors that influence these metrics, and challenge conventional hallucination mitigation solutions. More broadly, our work reveals a fundamental limitation in popular post-training methods for role-playing LLMs: the tendency to obscure knowledge beneath style, resulting in monotonous yet human-like behaviors - interactive hallucination.",,"Chuyi Kong, Ziyang Luo, Hongzhan Lin, Zhiyuan Fan, Yaxin Fan, Yuxi Sun, Jing Ma",2025-06-03T08:48:28Z,SHARP: Unlocking Interactive Hallucination via Stance Transfer in   Role-Playing LLMs,SHARP: Entsperren der interaktiven Halluzination durch Stance-Transfer in Rollenspiel-LLMs,SHARP:通过在角色扮演中转移角色来解锁互动幻觉,http://arxiv.org/abs/2411.07965v5
558,"We present a study on how and where personas -- defined by distinct sets of human characteristics, values, and beliefs -- are encoded in the representation space of large language models (LLMs). Using a range of dimension reduction and pattern recognition methods, we first identify the model layers that show the greatest divergence in encoding these representations. We then analyze the activations within a selected layer to examine how specific personas are encoded relative to others, including their shared and distinct embedding spaces. We find that, across multiple pre-trained decoder-only LLMs, the analyzed personas show large differences in representation space only within the final third of the decoder layers. We observe overlapping activations for specific ethical perspectives -- such as moral nihilism and utilitarianism -- suggesting a degree of polysemy. In contrast, political ideologies like conservatism and liberalism appear to be represented in more distinct regions. These findings help to improve our understanding of how LLMs internally represent information and can inform future efforts in refining the modulation of specific human traits in LLM outputs. Warning: This paper includes potentially offensive sample statements.",,"Celia Cintas, Miriam Rateike, Erik Miehling, Elizabeth Daly, Skyler Speakman",2025-06-03T08:45:28Z,Localizing Persona Representations in LLMs,Lokalisierung von Personenvertretungen in LLMs,将LLMM中的个人代表地方化,http://arxiv.org/abs/2505.24539v2
559,"Although commercial Arabic automatic speech recognition (ASR) systems support Modern Standard Arabic (MSA), they struggle with dialectal speech. We investigate the effect of fine-tuning OpenAI's Whisper on five major Arabic dialects (Gulf, Levantine, Iraqi, Egyptian, Maghrebi) using Mozilla Common Voice for MSA and the MASC dataset for dialectal speech. We evaluate MSA training size effects, benefits of pre-training on MSA data, and dialect-specific versus dialect-pooled models. We find that small amounts of MSA fine-tuning data yield substantial improvements for smaller models, matching larger non-fine-tuned models. While MSA pre-training shows minimal benefit, suggesting limited shared features between MSA and dialects, our dialect-pooled models perform comparably to dialect-specific ones. This indicates that pooling dialectal data, when properly balanced, can help address data scarcity in low-resource ASR without significant performance loss.",,"Ömer Tarik Özyilmaz, Matt Coler, Matias Valdenegro-Toro",2025-06-03T08:41:49Z,Overcoming Data Scarcity in Multi-Dialectal Arabic ASR via Whisper   Fine-Tuning,Überwindung von Datenschrecklichkeit im multi-dialektischen arabischen ASR über Whisper Fine-Tuning,通过 Whiseper Fine-Tuning 克服多种阿拉伯语ASR中的数据稀缺性,http://arxiv.org/abs/2506.02627v1
560,"The exponential rise in mobile device usage necessitates streamlined automation for effective task management, yet many AI frameworks fall short due to inadequate operational expertise. While manually written knowledge can bridge this gap, it is often burdensome and inefficient. We introduce Mobile-Agent-V, an innovative framework that utilizes video as a guiding tool to effortlessly and efficiently inject operational knowledge into mobile automation processes. By deriving knowledge directly from video content, Mobile-Agent-V eliminates manual intervention, significantly reducing the effort and time required for knowledge acquisition. To rigorously evaluate this approach, we propose Mobile-Knowledge, a benchmark tailored to assess the impact of external knowledge on mobile agent performance. Our experimental findings demonstrate that Mobile-Agent-V enhances performance by 36% compared to existing methods, underscoring its effortless and efficient advantages in mobile automation.",,"Junyang Wang, Haiyang Xu, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, Jitao Sang",2025-06-03T08:39:01Z,Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient   Operational Knowledge Injection in Mobile Automation,Mobile-Agent-V: Ein Video-geführter Ansatz für mühelose und effiziente betriebliche Wissenseinspritzung in die Mobile Automation,移动-代理-V:在移动自动化中以视频引导方式进行不费力和高效操作知识注射,http://arxiv.org/abs/2505.13887v3
561,"The exponential rise in mobile device usage necessitates streamlined automation for effective task management, yet many AI frameworks fall short due to inadequate operational expertise. While manually written knowledge can bridge this gap, it is often burdensome and inefficient. We introduce Mobile-Agent-V, an innovative framework that utilizes video as a guiding tool to effortlessly and efficiently inject operational knowledge into mobile automation processes. By deriving knowledge directly from video content, Mobile-Agent-V eliminates manual intervention, significantly reducing the effort and time required for knowledge acquisition. To rigorously evaluate this approach, we propose Mobile-Knowledge, a benchmark tailored to assess the impact of external knowledge on mobile agent performance. Our experimental findings demonstrate that Mobile-Agent-V enhances performance by 36% compared to existing methods, underscoring its effortless and efficient advantages in mobile automation.",,"Junyang Wang, Haiyang Xu, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, Jitao Sang",2025-06-03T08:37:03Z,Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient   Operational Knowledge Injection in Mobile Automation,Mobile-Agent-V: Ein Video-geführter Ansatz für mühelose und effiziente betriebliche Wissenseinspritzung in die Mobile Automation,移动-代理-V:在移动自动化中以视频引导方式进行不费力和高效操作知识注射,http://arxiv.org/abs/2502.17110v3
562,"In dialogue state tracking (DST), in-context learning comprises a retriever that selects labeled dialogues as in-context examples and a DST model that uses these examples to infer the dialogue state of the query dialogue. Existing methods for constructing training data for retrievers suffer from three key limitations: (1) the synergistic effect of examples is not considered, (2) the linguistic characteristics of the query are not sufficiently factored in, and (3) scoring is not directly optimized for DST performance. Consequently, the retriever can fail to retrieve examples that would substantially improve DST performance. To address these issues, we present CombiSearch, a method that scores effective in-context examples based on their combinatorial impact on DST performance. Our evaluation on MultiWOZ shows that retrievers trained with CombiSearch surpass state-of-the-art models, achieving a 20x gain in data efficiency and generalizing well to the SGD dataset. Moreover, CombiSearch attains a 12% absolute improvement in the upper bound DST performance over traditional approaches when no retrieval errors are assumed. This significantly increases the headroom for practical DST performance while demonstrating that existing methods rely on suboptimal data for retriever training.",,"Haesung Pyun, Yoonah Park, Yohan Jo",2025-06-03T08:31:06Z,Improving Dialogue State Tracking through Combinatorial Search for   In-Context Examples,Verbesserung der Dialog-Staatsverfolgung durch kombinatorische Suche nach In-Context-Beispielen,通过合并搜索来改进对话国家跟踪跟踪,http://arxiv.org/abs/2506.00622v2
563,"Ransomware-as-a-service (RaaS) is increasing the scale and complexity of ransomware attacks. Understanding the internal operations behind RaaS has been a challenge due to the illegality of such activities. The recent chat leak of the Conti RaaS operator, one of the most infamous ransomware operators on the international scene, offers a key opportunity to better understand the inner workings of such organizations. This paper analyzes the main topic discussions in the Conti chat leak using machine learning techniques such as Natural Language Processing (NLP) and Latent Dirichlet Allocation (LDA), as well as visualization strategies. Five discussion topics are found: 1) Business, 2) Technical, 3) Internal tasking/Management, 4) Malware, and 5) Customer Service/Problem Solving. Moreover, the distribution of topics among Conti members shows that only 4% of individuals have specialized discussions while almost all individuals (96%) are all-rounders, meaning that their discussions revolve around the five topics. The results also indicate that a significant proportion of Conti discussions are non-tech related. This study thus highlights that running such large RaaS operations requires a workforce skilled beyond technical abilities, with individuals involved in various tasks, from management to customer service or problem solving. The discussion topics also show that the organization behind the Conti RaaS oper5086933ator shares similarities with a large firm. We conclude that, although RaaS represents an example of specialization in the cybercrime industry, only a few members are specialized in one topic, while the rest runs and coordinates the RaaS operation.",,"Estelle Ruellan, Masarah Paquet-Clouston, Sebastian Garcia",2025-06-03T08:29:32Z,Conti Inc.: Understanding the Internal Discussions of a large   Ransomware-as-a-Service Operator with Machine Learning,Conti Inc.: Verständnis der internen Diskussionen einer großen Ransomware-as-a-Service-Betreiber mit Machine Learning,孔蒂公司:了解一个大型机械学习机械软件服务操作员的内部讨论,http://arxiv.org/abs/2308.16061v2
564,"Large Language Models (LLMs) have been observed to process non-human-readable text sequences, such as jailbreak prompts, often viewed as a bug for aligned LLMs. In this work, we present a systematic investigation challenging this perception, demonstrating that unnatural languages - strings that appear incomprehensible to humans but maintain semantic meanings for LLMs - contain latent features usable by models. Notably, unnatural languages possess latent features that can be generalized across different models and tasks during inference. Furthermore, models fine-tuned on unnatural versions of instruction datasets perform on-par with those trained on natural language, achieving 49.71 win rates in Length-controlled AlpacaEval 2.0 in average across various base models. In addition, through comprehensive analysis, we demonstrate that LLMs process unnatural languages by filtering noise and inferring contextual meaning from filtered words.",,"Keyu Duan, Yiran Zhao, Zhili Feng, Jinjie Ni, Tianyu Pang, Qian Liu, Tianle Cai, Longxu Dou, Kenji Kawaguchi, Anirudh Goyal, J. Zico Kolter, Michael Qizhe Shieh",2025-06-03T08:19:31Z,Unnatural Languages Are Not Bugs but Features for LLMs,"Unnatürliche Sprachen sind keine Bugs, sondern Features für LLMs","非自然语言不是虫子,而是LLMM的特性",http://arxiv.org/abs/2503.01926v2
565,"Long-form question answering (LFQA) aims to provide thorough and in-depth answers to complex questions, enhancing comprehension. However, such detailed responses are prone to hallucinations and factual inconsistencies, challenging their faithful evaluation. This work introduces HaluQuestQA, the first hallucination dataset with localized error annotations for human-written and model-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 1.8k span-level error annotations for five different error types by expert annotators, along with preference judgments. Using our collected data, we thoroughly analyze the shortcomings of long-form answers and find that they lack comprehensiveness and provide unhelpful references. We train an automatic feedback model on this dataset that predicts error spans with incomplete information and provides associated explanations. Finally, we propose a prompt-based approach, Error-informed refinement, that uses signals from the learned feedback model to refine generated answers, which we show reduces errors and improves answer quality across multiple models. Furthermore, humans find answers generated by our approach comprehensive and highly prefer them (84%) over the baseline answers.",,"Rachneet Sachdeva, Yixiao Song, Mohit Iyyer, Iryna Gurevych",2025-06-03T08:18:55Z,Localizing and Mitigating Errors in Long-form Question Answering,Lokalisierung und Behebung von Fehlern in der Langzeitfragebeantwortung,长式问题解答中本地化和缩小错误,http://arxiv.org/abs/2407.11930v5
566,"Negations are key to determining sentence meaning, making them essential for logical reasoning. Despite their importance, negations pose a substantial challenge for large language models (LLMs) and remain underexplored.   We constructed and published two new textual entailment datasets NoFEVER-ML and NoSNLI-ML in four languages (English, Czech, German, and Ukrainian) with   examples differing in negation. It allows investigation of the root causes of the negation problem and its exemplification: how popular LLM model properties and language impact their inability to handle negation correctly.   Contrary to previous work, we show that increasing the model size may improve the models' ability to handle negations. Furthermore, we find that both the models' reasoning accuracy and robustness to negation are language-dependent and that the length and explicitness of the premise have an impact on robustness. There is better accuracy in projective language with fixed order, such as English, than in non-projective ones, such as German or Czech.   Our entailment datasets pave the way to further research for explanation and exemplification of the negation problem, minimization of LLM hallucinations, and improvement of LLM reasoning in multilingual settings.",,"Tereza Vrabcová, Marek Kadlčík, Petr Sojka, Michal Štefánik, Michal Spiegel",2025-06-03T08:17:50Z,Negation: A Pink Elephant in the Large Language Models' Room?,Negation: Ein rosa Elefant im Raum der großen Sprachmodelle?,偏差:大语言模型室中的粉红大象?,http://arxiv.org/abs/2503.22395v2
567,"Chinese essay writing and its evaluation are critical in educational contexts, yet the capabilities of Large Language Models (LLMs) in this domain remain largely underexplored. Existing benchmarks often rely on coarse-grained text quality metrics, largely overlooking the structural and rhetorical complexities of Chinese essays, particularly across diverse genres. To address this gap, we propose \benchName, a multi-genre benchmark specifically designed for Chinese essay writing across four major genres: Argumentative, Narrative, Descriptive, and Expository. We curate and refine a total of 728 real-world prompts to ensure authenticity and meticulously categorize them into the \textit{Open-Ended} and \textit{Constrained} sets to capture diverse writing scenarios. To reliably evaluate generated essays, we develop a fine-grained, genre-specific scoring framework that hierarchically aggregates scores. We further validate our evaluation protocol through a comprehensive human agreement study. Finally, we benchmark 15 large-sized LLMs, analyzing their strengths and limitations across genres and instruction types. With \benchName, we aim to advance LLM-based Chinese essay evaluation and inspire future research on improving essay generation in educational settings.",,"Fan Gao, Dongyuan Li, Ding Xia, Fei Mi, Yasheng Wang, Lifeng Shang, Baojun Wang",2025-06-03T08:14:46Z,EssayBench: Evaluating Large Language Models in Multi-Genre Chinese   Essay Writing,EssayBench: Bewertung großer Sprachmodelle in multi-Genre Chinesisch Essay Writing,Essay Bench:评估多金中文论文撰写中的大语言模型,http://arxiv.org/abs/2506.02596v1
568,"Recent studies show that large language models (LLMs) exhibit self-preference bias when serving as judges, meaning they tend to favor their own responses over those generated by other models. Existing methods typically measure this bias by calculating the difference between the scores a judge model assigns to its own responses and those it assigns to responses from other models. However, this approach conflates self-preference bias with response quality, as higher-quality responses from the judge model may also lead to positive score differences, even in the absence of bias. To address this issue, we introduce gold judgments as proxies for the actual quality of responses and propose the DBG score, which measures self-preference bias as the difference between the scores assigned by the judge model to its own responses and the corresponding gold judgments. Since gold judgments reflect true response quality, the DBG score mitigates the confounding effect of response quality on bias measurement. Using the DBG score, we conduct comprehensive experiments to assess self-preference bias across LLMs of varying versions, sizes, and reasoning abilities. Additionally, we investigate two factors that influence and help alleviate self-preference bias: response text style and the post-training data of judge models. Finally, we explore potential underlying mechanisms of self-preference bias from an attention-based perspective. Our code and data are available at https://github.com/zhiyuanc2001/self-preference.",,"Zhi-Yuan Chen, Hao Wang, Xinyu Zhang, Enrui Hu, Yankai Lin",2025-06-03T08:12:47Z,Beyond the Surface: Measuring Self-Preference in LLM Judgments,Jenseits der Oberfläche: Messung der Selbstpräferenz in LLM-Richtungen,表面以外:在LLM判决中衡量自我偏爱,http://arxiv.org/abs/2506.02592v1
569,"Measurement systems (e.g., currencies) differ across cultures, but the conversions between them are well defined so that humans can state facts using any measurement system of their choice. Being available to users from diverse cultural backgrounds, large language models (LLMs) should also be able to provide accurate information irrespective of the measurement system at hand. Using newly compiled datasets we test if this is the case for seven open-source LLMs, addressing three key research questions: (RQ1) What is the default system used by LLMs for each type of measurement? (RQ2) Do LLMs' answers and their accuracy vary across different measurement systems? (RQ3) Can LLMs mitigate potential challenges w.r.t. underrepresented systems via reasoning? Our findings show that LLMs default to the measurement system predominantly used in the data. Additionally, we observe considerable instability and variance in performance across different measurement systems. While this instability can in part be mitigated by employing reasoning methods such as chain-of-thought (CoT), this implies longer responses and thereby significantly increases test-time compute (and inference costs), marginalizing users from cultural backgrounds that use underrepresented measurement systems.",,"Minh Duc Bui, Kyung Eun Park, Goran Glavaš, Fabian David Schmidt, Katharina von der Wense",2025-06-03T08:12:28Z,On Generalization across Measurement Systems: LLMs Entail More Test-Time   Compute for Underrepresented Cultures,Zur Verallgemeinerung über Messsysteme: LLMs Entail Mehr Testzeit für unterrepräsentierte Kulturen berechnen,横跨计量系统的一般化:代表不足文化的LLMs Entail 更多测试时间计算,http://arxiv.org/abs/2506.02591v1
570,"This paper addresses source tracing in synthetic speech-identifying generative systems behind manipulated audio via speaker recognition-inspired pipelines. While prior work focuses on spoofing detection, source tracing lacks robust solutions. We evaluate two approaches: classification-based and metric-learning. We tested our methods on the MLAADv5 benchmark using ResNet and self-supervised learning (SSL) backbones. The results show that ResNet achieves competitive performance with the metric learning approach, matching and even exceeding SSL-based systems. Our work demonstrates ResNet's viability for source tracing while underscoring the need to optimize SSL representations for this task. Our work bridges speaker recognition methodologies with audio forensic challenges, offering new directions for combating synthetic media manipulation.",,"Dimitrios Koutsianos, Stavros Zacharopoulos, Yannis Panagakis, Themos Stafylakis",2025-06-03T08:12:15Z,Synthetic Speech Source Tracing using Metric Learning,Synthetische Sprache Quelle Tracing mit Metric Learning,利用计量学习追踪合成语言源,http://arxiv.org/abs/2506.02590v1
571,"This paper addresses the challenge of Named Entity Recognition (NER) for person names within the specialized domain of Russian news texts concerning cultural events. The study utilizes the unique SPbLitGuide dataset, a collection of event announcements from Saint Petersburg spanning 1999 to 2019. A comparative evaluation of diverse NER models is presented, encompassing established transformer-based architectures such as DeepPavlov, RoBERTa, and SpaCy, alongside recent Large Language Models (LLMs) including GPT-3.5, GPT-4, and GPT-4o. Key findings highlight the superior performance of GPT-4o when provided with specific prompting for JSON output, achieving an F1 score of 0.93. Furthermore, GPT-4 demonstrated the highest precision at 0.99. The research contributes to a deeper understanding of current NER model capabilities and limitations when applied to morphologically rich languages like Russian within the cultural heritage domain, offering insights for researchers and practitioners. Follow-up evaluation with GPT-4.1 (April 2025) achieves F1=0.94 for both simple and structured prompts, demonstrating rapid progress across model families and simplified deployment requirements.",,Maria Levchenko,2025-06-03T08:11:16Z,Evaluating Named Entity Recognition Models for Russian Cultural News   Texts: From BERT to LLM,Bewertung der benannten Entitätserkennungsmodelle für russische Kulturnachrichten Texte: Von BERT bis LLM,评估俄罗斯文化新闻文本命名实体识别模式:从BERT到LLM,http://arxiv.org/abs/2506.02589v1
572,"Peer review is a cornerstone of quality control in scientific publishing. With the increasing workload, the unintended use of `quick' heuristics, referred to as lazy thinking, has emerged as a recurring issue compromising review quality. Automated methods to detect such heuristics can help improve the peer-reviewing process. However, there is limited NLP research on this issue, and no real-world dataset exists to support the development of detection tools. This work introduces LazyReview, a dataset of peer-review sentences annotated with fine-grained lazy thinking categories. Our analysis reveals that Large Language Models (LLMs) struggle to detect these instances in a zero-shot setting. However, instruction-based fine-tuning on our dataset significantly boosts performance by 10-20 performance points, highlighting the importance of high-quality training data. Furthermore, a controlled experiment demonstrates that reviews revised with lazy thinking feedback are more comprehensive and actionable than those written without such feedback. We will release our dataset and the enhanced guidelines that can be used to train junior reviewers in the community. (Code available here: https://github.com/UKPLab/acl2025-lazy-review)",,"Sukannya Purkayastha, Zhuang Li, Anne Lauscher, Lizhen Qu, Iryna Gurevych",2025-06-03T08:06:43Z,LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews,LazyReview Ein Datensatz für die Entdeckung lazy Denken in NLP Peer Reviews,NLP 同行审评中用于解开懒惰思考的 懒惰审查数据集,http://arxiv.org/abs/2504.11042v3
573,"People exploit the predictability of lexical structures during text comprehension. Though predictable structure is also present in speech, the degree to which prosody, e.g. intonation, tempo, and loudness, contributes to such structure independently of the lexical content is unclear. This study leverages self-supervised learning (SSL) to examine the temporal granularity of structures in the acoustic correlates of prosody. Representations from our proposed Masked Prosody Model can predict perceptual labels dependent on local information, such as word boundaries, but provide the most value for labels involving longer-term structures, like emotion recognition. Probing experiments across various perceptual labels show strong relative gains over untransformed pitch, energy, and voice activity features. Our results reveal the importance of SSL training objective timescale and highlight the value of complex SSL-encoded structures compared to more constrained classical structures.",,"Sarenne Wallbridge, Christoph Minixhofer, Catherine Lai, Peter Bell",2025-06-03T08:04:03Z,Prosodic Structure Beyond Lexical Content: A Study of Self-Supervised   Learning,Prosodic Structure Beyond Lexical Content: Eine Studie des Selbstüberwachten Lernens,超越法律内容的主张结构:自学研究,http://arxiv.org/abs/2506.02584v1
574,"Understanding training dynamics and feature evolution is crucial for the mechanistic interpretability of large language models (LLMs). Although sparse autoencoders (SAEs) have been used to identify features within LLMs, a clear picture of how these features evolve during training remains elusive. In this study, we (1) introduce SAE-Track, a novel method for efficiently obtaining a continual series of SAEs, providing the foundation for a mechanistic study that covers (2) the semantic evolution of features, (3) the underlying processes of feature formation, and (4) the directional drift of feature vectors. Our work provides new insights into the dynamics of features in LLMs, enhancing our understanding of training mechanisms and feature evolution. For reproducibility, our code is available at https://github.com/Superposition09m/SAE-Track.",,"Yang Xu, Yi Wang, Hengguan Huang, Hao Wang",2025-06-03T08:02:02Z,Tracking the Feature Dynamics in LLM Training: A Mechanistic Study,Nachvollziehen der Feature Dynamics im LLM Training: Eine mechanistische Studie,跟踪LLLM培训中的地物动态:机械研究,http://arxiv.org/abs/2412.17626v3
575,"Multi-Objective Alignment (MOA) aims to align LLMs' responses with multiple human preference objectives, with Direct Preference Optimization (DPO) emerging as a prominent approach. However, we find that DPO-based MOA approaches suffer from widespread preference conflicts in the data, where different objectives favor different responses. This results in conflicting optimization directions, hindering the optimization on the Pareto Front. To address this, we propose to construct Pareto-optimal responses to resolve preference conflicts. To efficiently obtain and utilize such responses, we propose a self-improving DPO framework that enables LLMs to self-generate and select Pareto-optimal responses for self-supervised preference alignment. Extensive experiments on two datasets demonstrate the superior Pareto Front achieved by our framework compared to various baselines. Code is available at https://github.com/zyttt-coder/SIPO.",,"Moxin Li, Yuantao Zhang, Wenjie Wang, Wentao Shi, Zhuo Liu, Fuli Feng, Tat-Seng Chua",2025-06-03T07:57:33Z,Self-Improvement Towards Pareto Optimality: Mitigating Preference   Conflicts in Multi-Objective Alignment,Selbstverbesserung auf dem Weg zur Pareto-Optimalität: Konfliktbewältigung in der Multi-Objektiven Ausrichtung mildern,向 Pareto 优化进进的自我改进:在多目标对齐中减少优惠冲突,http://arxiv.org/abs/2502.14354v2
576,"Although region-specific large language models (LLMs) are increasingly developed, their safety remains underexplored, particularly in culturally diverse settings like Indonesia, where sensitivity to local norms is essential and highly valued by the community. In this work, we present IndoSafety, the first high-quality, human-verified safety evaluation dataset tailored for the Indonesian context, covering five language varieties: formal and colloquial Indonesian, along with three major local languages: Javanese, Sundanese, and Minangkabau. IndoSafety is constructed by extending prior safety frameworks to develop a taxonomy that captures Indonesia's sociocultural context. We find that existing Indonesian-centric LLMs often generate unsafe outputs, particularly in colloquial and local language settings, while fine-tuning on IndoSafety significantly improves safety while preserving task performance. Our work highlights the critical need for culturally grounded safety evaluation and provides a concrete step toward responsible LLM deployment in multilingual settings. Warning: This paper contains example data that may be offensive, harmful, or biased.",,"Muhammad Falensi Azmi, Muhammad Dehan Al Kautsar, Alfan Farizki Wicaksono, Fajri Koto",2025-06-03T07:53:55Z,IndoSafety: Culturally Grounded Safety for LLMs in Indonesian Languages,IndoSafety: Culturally Grounded Safety für LLMs in indonesischen Sprachen,IndoSafety:印度尼西亚语言LMLMLLLLM在文化上的安全印度尼西亚语言,http://arxiv.org/abs/2506.02573v1
577,"Self-correction is emerging as a promising approach to mitigate the issue of hallucination in Large Language Models (LLMs). To facilitate effective self-correction, recent research has proposed mistake detection as its initial step. However, current literature suggests that LLMs often struggle with reliably identifying reasoning mistakes when using simplistic prompting strategies. To address this challenge, we introduce a unique prompting strategy, termed the Pedagogical Chain-of-Thought (PedCoT), which is specifically designed to guide the identification of reasoning mistakes, particularly mathematical reasoning mistakes. PedCoT consists of pedagogical principles for prompts (PPP) design, two-stage interaction process (TIP) and grounded PedCoT prompts, all inspired by the educational theory of the Bloom Cognitive Model (BCM). We evaluate our approach on two public datasets featuring math problems of varying difficulty levels. The experiments demonstrate that our zero-shot prompting strategy significantly outperforms strong baselines. The proposed method can achieve the goal of reliable mathematical mistake identification and provide a foundation for automatic math answer grading. The results underscore the significance of educational theory, serving as domain knowledge, in guiding prompting strategy design for addressing challenging tasks with LLMs effectively.",,"Zhuoxuan Jiang, Haoyuan Peng, Shanshan Feng, Fan Li, Dongsheng Li",2025-06-03T07:51:51Z,LLMs can Find Mathematical Reasoning Mistakes by Pedagogical   Chain-of-Thought,LLMs können mathematische Gründe für Fehler durch pädagogische Kette-of-Thought finden,LLMs 能够找出教学研究链在数学方面造成的错误。,http://arxiv.org/abs/2405.06705v3
578,"Large language models (LLMs) have revolutionized natural language processing, yet their substantial model sizes often require substantial computational resources. To preserve computing resources and accelerate inference speed, it is crucial to prune redundant parameters, especially for experienced users who often need compact expert models tailored to specific downstream scenarios. However, most existing pruning methods focus on preserving the model's general capabilities, often requiring extensive post-training or suffering from degraded performance due to coarse-grained pruning. In this work, we design a $\underline{Cus}$tom $\underline{Prun}$ing method ($\texttt{Cus-Prun}$) to prune a large general model into a smaller lightweight expert model, which is positioned along the ""language"", ""domain"" and ""task"" dimensions. By identifying and pruning irrelevant neurons of each dimension, $\texttt{Cus-Prun}$ creates expert models without any post-training. Our experiments demonstrate that $\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal loss in both expert and general capabilities across various models from different model families and sizes.",,"Yirao Zhao, Guizhen Chen, Kenji Kawaguchi, Lidong Bing, Wenxuan Zhang",2025-06-03T07:47:30Z,Pruning General Large Language Models into Customized Expert Models,Beschneiden von allgemeinen großen Sprachmodellen zu maßgeschneiderten Expertenmodellen,将一般大语言模式转化为定制专家模式,http://arxiv.org/abs/2506.02561v1
579,"We study a common challenge in reinforcement learning for large language models (LLMs): the Zero-Reward Assumption, where non-terminal actions (i.e., intermediate token generations) receive zero task-specific immediate reward, while only the final token receives a reward for the entire response. This assumption arises frequently in practice, as precise token-level rewards are often difficult or infeasible to obtain in LLM applications. In this work, we provide a unifying theoretical perspective. We introduce the Trajectory Policy Gradient Theorem, which shows that the policy gradient based on true, unknown token-level rewards can be unbiasedly estimated using only a response-level reward model, regardless of whether the Zero-Reward Assumption holds or not, for algorithms in the REINFORCE and Actor-Critic families. This result reveals that widely used methods such as PPO, GRPO, ReMax, and RLOO inherently possess the capacity to model token-level reward signals, offering a theoretical justification for response-level reward approaches. Our findings pave the way for more practical, efficient LLM fine-tuning, allowing developers to treat training algorithms as black boxes and focus on improving the response-level reward model with auxiliary sub-models. We also offer a detailed analysis of popular RL and non-RL methods, comparing their theoretical foundations and practical advantages across common LLM tasks. Finally, we propose a new algorithm: Token-Reinforced Policy Optimization (TRePO), a theoretically grounded method that is simpler than PPO, matches GRPO in memory efficiency, and holds promise for broad applicability.",,"Shenghua He, Tian Xia, Xuan Zhou, Hui Wei",2025-06-03T07:44:31Z,Response-Level Rewards Are All You Need for Online Reinforcement   Learning in LLMs: A Mathematical Perspective,"Response-Level Belohnungen sind alles, was Sie für Online-Verstärkung lernen in LLMs: Eine mathematische Perspektive",在LLMM项目中进行在线强化学习:数学视角,http://arxiv.org/abs/2506.02553v1
580,"As e-commerce competition intensifies, balancing creative content with conversion effectiveness becomes critical. Leveraging LLMs' language generation capabilities, we propose a framework that integrates prompt engineering, multi-objective fine-tuning, and post-processing to generate marketing copy that is both engaging and conversion-driven. Our fine-tuning method combines sentiment adjustment, diversity enhancement, and CTA embedding. Through offline evaluations and online A/B tests across categories, our approach achieves a 12.5 % increase in CTR and an 8.3 % increase in CVR while maintaining content novelty. This provides a practical solution for automated copy generation and suggests paths for future multimodal, real-time personalization.",,"Haowei Yang, Haotian Lyu, Tianle Zhang, Dingzhou Wang, Yushang Zhao",2025-06-03T07:39:21Z,LLM-Driven E-Commerce Marketing Content Optimization: Balancing   Creativity and Conversion,LLM-getriebene E-Commerce-Marketing-Inhalte Optimierung: Ausbalancierende Kreativität und Konversion,LLM-Driven E-Commerce 电子商务营销内容优化:平衡创造与转化,http://arxiv.org/abs/2505.23809v2
581,"Chain-of-thought (CoT) prompting enhances reasoning in large language models (LLMs) but often leads to verbose and redundant outputs, thus increasing inference cost. We hypothesize that many reasoning steps are unnecessary for producing correct answers. To investigate this, we start with a systematic study to examine what is the minimum reasoning required for a model to reach a stable decision. We find that on math reasoning tasks like math, models typically converge to their final answers after 60\% of the reasoning steps, suggesting substantial redundancy in the remaining content. Based on these insights, we propose three inference-time strategies to improve efficiency: (1) early stopping via answer consistency, (2) boosting the probability of generating end-of-reasoning signals, and (3) a supervised method that learns when to stop based on internal activations. Experiments across five benchmarks and five open-weights LLMs show that our methods significantly reduce token usage with little or no accuracy drop. In particular, on NaturalQuestions, Answer Consistency reduces tokens by over 40\% while further improving accuracy. Our work underscores the importance of cost-effective reasoning methods that operate at inference time, offering practical benefits for real-world applications.",,"Xin Liu, Lu Wang",2025-06-03T07:20:54Z,Answer Convergence as a Signal for Early Stopping in Reasoning,Antwort Konvergenz als Signal für ein frühzeitiges Stoppen in der Vernunft,回答一致作为早期停止解释的信号,http://arxiv.org/abs/2506.02536v1
582,"Large language model (LLM) agents typically adopt a step-by-step reasoning framework, in which they interleave the processes of thinking and acting to accomplish the given task. However, this paradigm faces a deep-rooted one-pass issue whereby each generated intermediate thought is plugged into the trajectory regardless of its correctness, which can cause irreversible error propagation. To address the issue, this paper proposes a novel framework called Generator-Assistant Stepwise Rollback (GA-Rollback) to induce better decision-making for LLM agents. Particularly, GA-Rollback utilizes a generator to interact with the environment and an assistant to examine each action produced by the generator, where the assistant triggers a rollback operation upon detection of incorrect actions. Moreover, we introduce two additional strategies tailored for the rollback scenario to further improve its effectiveness. Extensive experiments show that GA-Rollback achieves significant improvements over several strong baselines on three widely used benchmarks. Our analysis further reveals that GA-Rollback can function as a robust plug-and-play module, integrating seamlessly with other methods.",,"Xingzuo Li, Kehai Chen, Yunfei Long, Xuefeng Bai, Yong Xu, Min Zhang",2025-06-03T07:18:14Z,Generator-Assistant Stepwise Rollback Framework for Large Language Model   Agent,Generator-Assistant Stepwise Rollback Framework für Large Language Model Agent,大型语文模式代理的发电机助理分步推回框架,http://arxiv.org/abs/2503.02519v3
583,"Large language models (LLMs) have demonstrated remarkable capabilities in solving complex reasoning tasks, particularly in mathematics. However, the domain of physics reasoning presents unique challenges that have received significantly less attention. Existing benchmarks often fall short in evaluating LLMs' abilities on the breadth and depth of undergraduate-level physics, underscoring the need for a comprehensive evaluation. To fill this gap, we introduce UGPhysics, a large-scale and comprehensive benchmark specifically designed to evaluate UnderGraduate-level Physics (UGPhysics) reasoning with LLMs. UGPhysics includes 5,520 undergraduate-level physics problems in both English and Chinese, covering 13 subjects with seven different answer types and four distinct physics reasoning skills, all rigorously screened for data leakage. Additionally, we develop a Model-Assistant Rule-based Judgment (MARJ) pipeline specifically tailored for assessing answer correctness of physics problems, ensuring accurate evaluation. Our evaluation of 31 leading LLMs shows that the highest overall accuracy, 49.8% (achieved by OpenAI-o1-mini), emphasizes the necessity for models with stronger physics reasoning skills, beyond math abilities. We hope UGPhysics, along with MARJ, will drive future advancements in AI for physics reasoning. Codes and data are available at https://github.com/YangLabHKUST/UGPhysics .",,"Xin Xu, Qiyun Xu, Tong Xiao, Tianhao Chen, Yuchen Yan, Jiaxin Zhang, Shizhe Diao, Can Yang, Yang Wang",2025-06-03T07:13:03Z,UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning   with Large Language Models,UGPhysics: Umfassender Benchmark für Undergraduate Physics Reasoning mit großen Sprachmodellen,动脉物理学:具有大语言模型的本科物理原因综合基准,http://arxiv.org/abs/2502.00334v4
584,"Political online participation in the form of discussing political issues and exchanging opinions among citizens is gaining importance with more and more formats being held digitally. To come to a decision, a careful discussion and consideration of opinions and a civil exchange of arguments, which is defined as the act of deliberation, is desirable. The quality of discussions and participation processes in terms of their deliberativeness highly depends on the design of platforms and processes. To facilitate online communication for both participants and initiators, machine learning methods offer a lot of potential. In this work we want to showcase which issues occur in political online discussions and how machine learning can be used to counteract these issues and enhance deliberation.",,"Maike Behrendt, Stefan Sylvius Wagner, Carina Weinmann, Marike Bormann, Mira Warne, Stefan Harmeling",2025-06-03T07:11:49Z,Natural Language Processing to Enhance Deliberation in Political Online   Discussions: A Survey,Natürliche Sprachverarbeitung zur Verbesserung der Beratung in politischen Online-Diskussionen: Eine Umfrage,利用自然语言处理加强政治在线讨论的审议:调查,http://arxiv.org/abs/2506.02533v1
585,"Large reasoning models (LRMs) generate complex reasoning traces with planning, reflection, verification, and backtracking. In this work, we introduce ReasoningFlow, a unified schema for analyzing the semantic structures of these complex traces. ReasoningFlow parses traces into directed acyclic graphs, enabling the characterization of distinct reasoning patterns as subgraph structures. This human-interpretable representation offers promising applications in understanding, evaluating, and enhancing the reasoning processes of LRMs.",,"Jinu Lee, Sagnik Mukherjee, Dilek Hakkani-Tur, Julia Hockenmaier",2025-06-03T07:11:34Z,ReasoningFlow: Semantic Structure of Complex Reasoning Traces,ReasoningFlow: Semantische Struktur komplexer Reasoning Traces,理由说明:复杂理由说明路线的语义结构,http://arxiv.org/abs/2506.02532v1
586,"Web applications are critical to modern software ecosystems, yet ensuring their reliability remains challenging due to the complexity and dynamic nature of web interfaces. Recent advances in large language models (LLMs) have shown promise in automating complex tasks, but limitations persist in handling dynamic navigation flows and complex form interactions. This paper presents an automated system for generating test cases for two key aspects of web application testing: site navigation and form filling. For site navigation, the system employs screen transition graphs and LLMs to model navigation flows and generate test scenarios. For form filling, it uses state graphs to handle conditional forms and automates Selenium script generation. Key contributions include: (1) a novel integration of graph structures and LLMs for site navigation testing, (2) a state graph-based approach for automating form-filling test cases, and (3) a comprehensive dataset for evaluating form-interaction testing. Experimental results demonstrate the system's effectiveness in improving test coverage and robustness, advancing the state of web application testing.",,"Nguyen-Khang Le, Quan Minh Bui, Minh Ngoc Nguyen, Hiep Nguyen, Trung Vo, Son T. Luu, Shoshin Nomura, Minh Le Nguyen",2025-06-03T07:08:21Z,Automated Web Application Testing: End-to-End Test Case Generation with   Large Language Models and Screen Transition Graphs,Automatisierte Web Application Testing: End-to-End Test Case Generation mit großen Sprachmodellen und Screen Transition Graphen,自动网络应用程序测试:以大语言模型和屏幕过渡图生成端到端测试案例,http://arxiv.org/abs/2506.02529v1
587,"Multilingual information retrieval has emerged as powerful tools for expanding knowledge sharing across languages. On the other hand, resources on high quality knowledge base are often scarce and in limited languages, therefore an effective embedding model to transform sentences from different languages into a feature vector space same as the knowledge base language becomes the key ingredient for cross language knowledge sharing, especially to transfer knowledge available in high-resource languages to low-resource ones. In this paper we propose a novel strategy to fine-tune multilingual embedding models with weighted sampling for contrastive learning, enabling multilingual information retrieval with a monolingual knowledge base. We demonstrate that the weighted sampling strategy produces performance gains compared to standard ones by up to 31.03\% in MRR and up to 33.98\% in Recall@3. Additionally, our proposed methodology is language agnostic and applicable for both multilingual and code switching use cases.",,"Yingying Zhuang, Aman Gupta, Anurag Beniwal",2025-06-03T07:05:49Z,Multilingual Information Retrieval with a Monolingual Knowledge Base,Mehrsprachiges Informations-Retrieval mit einsprachiger Wissensbasis,"多语言信息检索,使用单语知识库",http://arxiv.org/abs/2506.02527v1
588,"Mamba's theoretical infinite-context potential is limited in practice when sequences far exceed training lengths. This work explores unlocking Mamba's long-context memory ability by a simple-yet-effective method, Recall with Reasoning (RwR), by distilling chain-of-thought (CoT) summarization from a teacher model. Specifically, RwR prepends these summarization as CoT prompts during fine-tuning, teaching Mamba to actively recall and reason over long contexts. Experiments on LONGMEMEVAL and HELMET show RwR boosts Mamba's long-context performance against comparable Transformer/hybrid baselines under similar pretraining conditions, while preserving short-context capabilities, all without architectural changes.",,"Junyu Ma, Tianqing Fang, Zhisong Zhang, Hongming Zhang, Haitao Mi, Dong Yu",2025-06-03T06:56:44Z,Recall with Reasoning: Chain-of-Thought Distillation for Mamba's   Long-Context Memory and Extrapolation,Rückruf mit Vernunft: Gedachte Destillation für Mambas langkontexten Speicher und Extrapolation,以合理理由回顾:Mamba的长文内存和外推法的思考链蒸馏,http://arxiv.org/abs/2505.03320v2
589,"LLMssuch as GPT-4 have shown a remarkable ability to solve complex questions by generating step-by-step rationales. Prior works have utilized this capability to improve smaller and cheaper LMs (say, with 7B parameters). However, various practical constraints, such as copyright and legal issues, owing to lack of transparency in the pre-training data of large (often closed) models, prevent their use in commercial settings. Little focus has been given to improving the innate reasoning ability of smaller models without distilling information from larger LLMs. To address this, we propose COLLATE, a trainable framework that tunes a (small) LLM to generate those outputs from a pool of diverse rationales that selectively improves the downstream task. COLLATE enforces multiple instances of the same LLM to exhibit distinct behavior and employs them to generate rationales to obtain diverse outputs. The LLM is then tuned via preference optimization to choose the candidate rationale which maximizes the likelihood of ground-truth answer. COLLATE outperforms several trainable and prompting baselines on 5 datasets across 3 domains: maths problem solving, natural language inference, and commonsense reasoning. We show the eff icacy of COLLATE on LLMs from different model families across varying parameter scales (1B to 8B) and demonstrate the benefit of multiple rationale providers guided by the end task through ablations. Code is released here (https://github.com/Sohanpatnaik106/collate).",,"Sohan Patnaik, Milan Aggarwal, Sumit Bhatia, Balaji Krishnamurthy",2025-06-03T06:50:08Z,Learning Together to Perform Better: Teaching Small-Scale LLMs to   Collaborate via Preferential Rationale Tuning,"Gemeinsam lernen, bessere Ergebnisse zu erzielen: Lehren von kleinen LLMs, sich über bevorzugte Rationale Tuning zu kollaborieren","共同学习如何更好地工作:通过优惠理由提款方式,向合作者传授小型LLMs",http://arxiv.org/abs/2506.02519v1
590,"Multi-step symbolic reasoning is critical for advancing downstream performance on financial tasks. Yet, benchmarks for systematically evaluating this capability are lacking. Existing datasets like FinQA and ConvFinQA supervise only final numerical answers, without assessing intermediate reasoning steps. To address this, we introduce FinChain, the first symbolic benchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning. Spanning 54 topics across 12 financial domains, Fin- Chain offers five parameterized templates per topic, each varying in reasoning complexity and domain expertise required. Each dataset instance includes an executable Python trace, enabling automatic generation of extensive training data and easy adaptation to other domains. We also introduce ChainEval, a new metric for automatic evaluation of both final answers and intermediate reasoning. Benchmarking 30 LLMs on our dataset, we find that even state-of-the-art models have considerable room for improvement in multi-step financial reasoning. All templates and evaluation metrics for FinChain are available at https: //github.com/mbzuai-nlp/finchain.",,"Zhuohan Xie, Dhruv Sahnan, Debopriyo Banerjee, Georgi Georgiev, Rushil Thareja, Hachem Madmoun, Jinyan Su, Aaryamonvikram Singh, Yuxia Wang, Rui Xing, Fajri Koto, Haonan Li, Ivan Koychev, Tanmoy Chakraborty, Salem Lahlou, Veselin Stoyanov, Preslav Nakov",2025-06-03T06:44:42Z,FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial   Reasoning,FinChain: Ein symbolischer Benchmark für überprüfbare Ketten der finanziellen Begründung,Finchain:可核实的寻求财政理由链的象征性基准,http://arxiv.org/abs/2506.02515v1
591,"Recent breakthroughs in large language models (LLMs) have led to the development of new benchmarks for evaluating their performance in the financial domain. However, current financial benchmarks often rely on news articles, earnings reports, or announcements, making it challenging to capture the real-world dynamics of financial meetings. To address this gap, we propose a novel benchmark called $\texttt{M$^3$FinMeeting}$, which is a multilingual, multi-sector, and multi-task dataset designed for financial meeting understanding. First, $\texttt{M$^3$FinMeeting}$ supports English, Chinese, and Japanese, enhancing comprehension of financial discussions in diverse linguistic contexts. Second, it encompasses various industry sectors defined by the Global Industry Classification Standard (GICS), ensuring that the benchmark spans a broad range of financial activities. Finally, $\texttt{M$^3$FinMeeting}$ includes three tasks: summarization, question-answer (QA) pair extraction, and question answering, facilitating a more realistic and comprehensive evaluation of understanding. Experimental results with seven popular LLMs reveal that even the most advanced long-context models have significant room for improvement, demonstrating the effectiveness of $\texttt{M$^3$FinMeeting}$ as a benchmark for assessing LLMs' financial meeting comprehension skills.",,"Jie Zhu, Junhui Li, Yalong Wen, Xiandong Li, Lifan Guo, Feng Chen",2025-06-03T06:41:09Z,"M$^3$FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial   Meeting Understanding Evaluation Dataset","M$^3$FinMeeting: Ein multilinguales, multisektorales und multi-Task-Finanztreffen zum Verständnis von Evaluationsdatensatz",会议:多语种、多部门和多任务财务会议了解评价数据集,http://arxiv.org/abs/2506.02510v1
592,"Large Language Models (LLMs) have been shown to exhibit various biases and stereotypes in their generated content. While extensive research has investigated biases in LLMs, prior work has predominantly focused on explicit bias, with minimal attention to implicit bias and the relation between these two forms of bias. This paper presents a systematic framework grounded in social psychology theories to investigate and compare explicit and implicit biases in LLMs. We propose a novel self-reflection-based evaluation framework that operates in two phases: first measuring implicit bias through simulated psychological assessment methods, then evaluating explicit bias by prompting LLMs to analyze their own generated content. Through extensive experiments on advanced LLMs across multiple social dimensions, we demonstrate that LLMs exhibit a substantial inconsistency between explicit and implicit biases: while explicit bias manifests as mild stereotypes, implicit bias exhibits strong stereotypes. We further investigate the underlying factors contributing to this explicit-implicit bias inconsistency, examining the effects of training data scale, model size, and alignment techniques. Experimental results indicate that while explicit bias declines with increased training data and model size, implicit bias exhibits a contrasting upward trend. Moreover, contemporary alignment methods effectively suppress explicit bias but show limited efficacy in mitigating implicit bias.",,"Yachao Zhao, Bo Wang, Yan Wang, Dongming Zhao, Ruifang He, Yuexian Hou",2025-06-03T06:37:54Z,Explicit vs. Implicit: Investigating Social Bias in Large Language   Models through Self-Reflection,Explizit gegen Implizit: Soziale Bias in großen Sprachmodellen durch Selbstreflexion untersuchen,明确与隐含:通过自我反省在大语言模式中调查社会偏见,http://arxiv.org/abs/2501.02295v4
593,"Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to access broader knowledge sources, yet factual inconsistencies persist due to noise in retrieved documents-even with advanced retrieval methods. We demonstrate that enhancing generative models' capacity to process noisy content is equally critical for robust performance. In this paper, we present KARE-RAG (Knowledge-Aware Refinement and Enhancement for RAG), which improves knowledge utilization through three key innovations: (1) structured knowledge representations that facilitate error detection during training, (2) Dense Direct Preference Optimization (DDPO)-a refined training objective that prioritizes correction of critical errors, and (3) a contrastive data generation pipeline that maintains semantic consistency while rectifying factual inaccuracies. Experiments show our method significantly enhances standard RAG pipelines across model scales, improving both in-domain and out-of-domain task performance without compromising general capabilities. Notably, these gains are achieved with modest training data, suggesting data-efficient optimization is possible through targeted learning strategies. Our findings establish a new direction for RAG improvement: by improving how models learn to process retrieved content, we can enhance performance across diverse inference paradigms. All data and code will be publicly available on Github.",,"Yongjian Li, HaoCheng Chu, Yukun Yan, Zhenghao Liu, Shi Yu, Zheni Zeng, Ruobing Wang, Sen Song, Zhiyuan Liu, Maosong Sun",2025-06-03T06:31:17Z,KARE-RAG: Knowledge-Aware Refinement and Enhancement for RAG,KARE-RAG: Knowledge-Aware-Verfeinerung und -Verbesserung für RAG,KARE-RAG:为RAG改进和加强知识-软件,http://arxiv.org/abs/2506.02503v1
594,"Evaluation is important for multimodal generation tasks. With the rapid progress of MLLMs, there is growing interest in applying MLLMs to build general evaluation systems. However, existing work overlooks two aspects: (1) the development of evaluation capabilities for text-to-image (T2I) generation task, and (2) the incorporation of large-scale human evaluation data. In this paper, we introduce Minos-Corpus, a large-scale multimodal evaluation dataset that combines evaluation data from both human and GPT. The corpus contains evaluation data across both image-to-text(I2T) and T2I generation tasks. Based on this corpus, we propose Data Selection and Balance, Mix-SFT training methods, and apply DPO to develop Minos, a multimodal evaluation model built upon a 7B backbone. Minos achieves state-of-the-art (SoTA) performance among all open-source evaluation models of similar scale on the average of evaluation performance on all tasks, and outperforms all open-source and closed-source models on evaluation of T2I generation task. Extensive experiments demonstrate the importance of leveraging high-quality human evaluation data and jointly training on evaluation data from both I2T and T2I generation tasks.",,"Junzhe Zhang, Huixuan Zhang, Xinyu Hu, Li Lin, Mingqi Gao, Shi Qiu, Xiaojun Wan",2025-06-03T06:17:16Z,Minos: A Multimodal Evaluation Model for Bidirectional Generation   Between Image and Text,Minos: Ein multimodales Evaluationsmodell für die bidirektionale Generierung zwischen Bild und Text,Minos:图像和文字之间双向生成的多模式评价模式,http://arxiv.org/abs/2506.02494v1
595,"The rapid proliferation of large language models (LLMs) such as GPT-4 and Gemini underscores the intense demand for resources during their training processes, posing significant challenges due to substantial computational and environmental costs. To alleviate this issue, we propose checkpoint merging in pretraining LLM. This method utilizes LLM checkpoints with shared training trajectories, and is rooted in an extensive search space exploration for the best merging weight via Bayesian optimization. Through various experiments, we demonstrate that: (1) Our proposed methodology exhibits the capacity to augment pretraining, presenting an opportunity akin to obtaining substantial benefits at minimal cost; (2) Our proposed methodology, despite requiring a given held-out dataset, still demonstrates robust generalization capabilities across diverse domains, a pivotal aspect in pretraining.",,"Deyuan Liu, Zecheng Wang, Bingning Wang, Weipeng Chen, Chunshan Li, Zhiying Tu, Dianhui Chu, Bo Li, Dianbo Sui",2025-06-03T06:01:14Z,Checkpoint Merging via Bayesian Optimization in LLM Pretraining,Checkpoint Merging via Bayesian Optimization in LLM Pretraining,通过Bayesian优化在LLM培训前合并的检查站,http://arxiv.org/abs/2403.19390v2
596,"Large Language Models (LLMs) achieve superior performance through Chain-of-Thought (CoT) reasoning, but these token-level reasoning chains are computationally expensive and inefficient. In this paper, we introduce Compressed Latent Reasoning (CoLaR), a novel framework that dynamically compresses reasoning processes in latent space through a two-stage training approach. First, during supervised fine-tuning, CoLaR extends beyond next-token prediction by incorporating an auxiliary next compressed embedding prediction objective. This process merges embeddings of consecutive tokens using a compression factor randomly sampled from a predefined range, and trains a specialized latent head to predict distributions of subsequent compressed embeddings. Second, we enhance CoLaR through reinforcement learning (RL) that leverages the latent head's non-deterministic nature to explore diverse reasoning paths and exploit more compact ones. This approach enables CoLaR to: i) perform reasoning at a dense latent level (i.e., silently), substantially reducing reasoning chain length, and ii) dynamically adjust reasoning speed at inference time by simply prompting the desired compression factor. Extensive experiments across four mathematical reasoning datasets demonstrate that CoLaR achieves 14.1% higher accuracy than latent-based baseline methods at comparable compression ratios, and reduces reasoning chain length by 53.3% with only 4.8% performance degradation compared to explicit CoT method. Moreover, when applied to more challenging mathematical reasoning tasks, our RL-enhanced CoLaR demonstrates performance gains of up to 5.4% while dramatically reducing latent reasoning chain length by 82.8%. The code and models will be released upon acceptance.",,"Wenhui Tan, Jiaze Li, Jianzhong Ju, Zhenbo Luo, Jian Luan, Ruihua Song",2025-06-03T05:58:05Z,"Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning   Chains","Denken Sie leise, denken Sie schnell: Dynamische Latent-Kompression von LLM-vernünftigen Ketten","默默思考,快速思考:LLM 解释性链条的动态延迟压缩",http://arxiv.org/abs/2505.16552v4
597,"Large language models (LLMs) often struggle to perform multi-target reasoning in long-context scenarios where relevant information is scattered across extensive documents. To address this challenge, we introduce NeuroSymbolic Augmented Reasoning (NSAR), which combines the benefits of neural and symbolic reasoning during inference. NSAR explicitly extracts symbolic facts from text and generates executable Python code to handle complex reasoning steps. Through extensive experiments across seven languages and diverse context lengths, we demonstrate that NSAR significantly outperforms both a vanilla RAG baseline and advanced prompting strategies in accurately identifying and synthesizing multiple pieces of information. Our results highlight the effectiveness of combining explicit symbolic operations with neural inference for robust, interpretable, and scalable reasoning in multilingual settings.",,"Sina Bagheri Nezhad, Ameeta Agrawal",2025-06-03T05:54:20Z,Enhancing Large Language Models with Neurosymbolic Reasoning for   Multilingual Tasks,Erweiterung großer Sprachmodelle mit neurosymbolischer Begründung für mehrsprachige Aufgaben,强化多种语文任务使用新词共略理由的大语言模式,http://arxiv.org/abs/2506.02483v1
598,"Evaluations of LLMs' ethical risks and value inclinations often rely on short-form surveys and psychometric tests, yet real-world use involves long-form, open-ended responses -- leaving value-related risks and preferences in practical settings largely underexplored. In this work, we ask: Do value preferences inferred from short-form tests align with those expressed in long-form outputs? To address this question, we compare value preferences elicited from short-form reactions and long-form responses, varying the number of arguments in the latter to capture users' differing verbosity preferences. Analyzing five LLMs (llama3-8b, gemma2-9b, mistral-7b, qwen2-7b, and olmo-7b), we find (1) a weak correlation between value preferences inferred from short-form and long-form responses across varying argument counts, and (2) similarly weak correlation between preferences derived from any two distinct long-form generation settings. (3) Alignment yields only modest gains in the consistency of value expression. Further, we examine how long-form generation attributes relate to value preferences, finding that argument specificity negatively correlates with preference strength, while representation across scenarios shows a positive correlation. Our findings underscore the need for more robust methods to ensure consistent value expression across diverse applications.",,"Inderjeet Nair, Lu Wang",2025-06-03T05:52:03Z,Do Language Models Think Consistently? A Study of Value Preferences   Across Varying Response Lengths,Denken Sprachmodelle konsequent? Eine Studie von Wertpräferenzen über unterschiedliche Antwortlängen,语言模式是否持一致思维? 不同反应长度的价值观偏好研究,http://arxiv.org/abs/2506.02481v1
599,"High-quality prompts are crucial for eliciting outstanding performance from large language models (LLMs) on complex tasks. Existing research has explored model-driven strategies for prompt optimization. However, these methods often suffer from high computational overhead or require strong optimization capabilities from the model itself, which limits their broad applicability.To address these challenges, we propose ORPP (Optimized Role-Playing Prompt),a framework that enhances model performance by optimizing and generating role-playing prompts. The core idea of ORPP is to confine the prompt search space to role-playing scenarios, thereby fully activating the model's intrinsic capabilities through carefully crafted, high-quality role-playing prompts. Specifically, ORPP first performs iterative optimization on a small subset of training samples to generate high-quality role-playing prompts. Then, leveraging the model's few-shot learning capability, it transfers the optimization experience to efficiently generate suitable prompts for the remaining samples.Our experimental results show that ORPP not only matches but in most cases surpasses existing mainstream prompt optimization methods in terms of performance. Notably, ORPP demonstrates superior ""plug-and-play"" capability. In most cases, it can be integrated with various other prompt methods and further enhance their effectiveness.",,"Yifan Duan, Yihong Tang, Kehai Chen, Liqiang Nie, Min Zhang",2025-06-03T05:51:35Z,ORPP: Self-Optimizing Role-playing Prompts to Enhance Language Model   Capabilities,ORPP: Selbstoptimierende Rollenspiele zur Verbesserung der Sprachmodellfähigkeiten,"ORPP: 自我优化角色扮演提示,以加强语文示范能力",http://arxiv.org/abs/2506.02480v1
600,"The inherent risk of generating harmful and unsafe content by Large Language Models (LLMs), has highlighted the need for their safety alignment. Various techniques like supervised fine-tuning, reinforcement learning from human feedback, and red-teaming were developed for ensuring the safety alignment of LLMs. However, the robustness of these aligned LLMs is always challenged by adversarial attacks that exploit unexplored and underlying vulnerabilities of the safety alignment. In this paper, we develop a novel black-box jailbreak attack, called BitBypass, that leverages hyphen-separated bitstream camouflage for jailbreaking aligned LLMs. This represents a new direction in jailbreaking by exploiting fundamental information representation of data as continuous bits, rather than leveraging prompt engineering or adversarial manipulations. Our evaluation of five state-of-the-art LLMs, namely GPT-4o, Gemini 1.5, Claude 3.5, Llama 3.1, and Mixtral, in adversarial perspective, revealed the capabilities of BitBypass in bypassing their safety alignment and tricking them into generating harmful and unsafe content. Further, we observed that BitBypass outperforms several state-of-the-art jailbreak attacks in terms of stealthiness and attack success. Overall, these results highlights the effectiveness and efficiency of BitBypass in jailbreaking these state-of-the-art LLMs.",,"Kalyan Nakka, Nitesh Saxena",2025-06-03T05:51:18Z,BitBypass: A New Direction in Jailbreaking Aligned Large Language Models   with Bitstream Camouflage,BitBypass: Eine neue Richtung beim Jailbreaking Große Sprachmodelle mit Bitstream Camouflage,BitBypass: 带有Bitstream Camouflage 的破碎大型语言统一模型的新方向,http://arxiv.org/abs/2506.02479v1
601,"With the development of large language models, fine-tuning has emerged as an effective method to enhance performance in specific scenarios by injecting domain-specific knowledge. In this context, model merging techniques provide a solution for fusing knowledge from multiple fine-tuning models by combining their parameters. However, traditional methods often encounter task interference when merging full fine-tuning models, and this problem becomes even more evident in parameter-efficient fine-tuning scenarios. In this paper, we introduce an improvement to the RegMean method, which indirectly leverages the training data to approximate the outputs of the linear layers before and after merging. We propose an adaptive merging method called FroM, which directly measures the model parameters using the Frobenius norm, without any training data. By introducing an additional hyperparameter for control, FroM outperforms baseline methods across various fine-tuning scenarios, alleviating the task interference problem.",,"Zijian Li, Xiaocheng Feng, Huixin Liu, Yichong Huang, Ting Liu, Bing Qin",2025-06-03T05:50:09Z,FroM: Frobenius Norm-Based Data-Free Adaptive Model Merging,FroM: Frobenius Normbasiertes datenfreies adaptives Modell Zusammenführen,FroM:Frobenius Norm基于无数据无适应性模型合并,http://arxiv.org/abs/2506.02478v1
602,"Recent efficient sequence modeling methods such as Gated DeltaNet, TTT, and RWKV-7 have achieved performance improvements by supervising the recurrent memory management through Delta learning rule. Unlike previous state-space models (e.g., Mamba) and gated linear attentions (e.g., GLA), these models introduce interactions between the recurrent state and the key vector, resulting in a nonlinear recursive structure. In this paper, we first introduce the concept of Nonlinear RNNs with a comprehensive analysis on the advantages and limitations of these models. Then, based on closed-loop control theory, we propose a novel Nonlinear RNN variant named Comba, which adopts a scalar-plus-low-rank state transition, with both state feedback and output feedback corrections. We also implement a hardware-efficient chunk-wise parallel kernel in Triton and train models with 340M/1.3B parameters on large-scale corpus. Comba demonstrates its superior performance and computation efficiency in both language and vision modeling.",,"Jiaxi Hu, Yongqi Pan, Jusen Du, Disen Lan, Xiaqiang Tang, Qingsong Wen, Yuxuan Liang, Weigao Sun",2025-06-03T05:44:50Z,Comba: Improving Nonlinear RNNs with Closed-loop Control,Comba: Verbesserung nichtlinearer RNNs mit Closed-Loop-Steuerung,Comba: 改进非线性区域NNN和闭环控制非线性NNN,http://arxiv.org/abs/2506.02475v1
603,"Understanding information from visually rich documents remains a significant challenge for traditional Retrieval-Augmented Generation (RAG) methods. Existing benchmarks predominantly focus on image-based question answering (QA), overlooking the fundamental challenges of efficient retrieval, comprehension, and reasoning within dense visual documents. To bridge this gap, we introduce ViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich documents requiring complex reasoning. Based on it, we identify key limitations in current RAG approaches: (i) purely visual retrieval methods struggle to effectively integrate both textual and visual features, and (ii) previous approaches often allocate insufficient reasoning tokens, limiting their effectiveness. To address these challenges, we propose ViDoRAG, a novel multi-agent RAG framework tailored for complex reasoning across visual documents. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy to effectively handle multi-modal retrieval. To further elicit the model's reasoning capabilities, we introduce an iterative agent workflow incorporating exploration, summarization, and reflection, providing a framework for investigating test-time scaling in RAG domains. Extensive experiments on ViDoSeek validate the effectiveness and generalization of our approach. Notably, ViDoRAG outperforms existing methods by over 10% on the competitive ViDoSeek benchmark. The code is available at https://github.com/Alibaba-NLP/ViDoRAG.",,"Qiuchen Wang, Ruixue Ding, Zehui Chen, Weiqi Wu, Shihang Wang, Pengjun Xie, Feng Zhao",2025-06-03T05:34:30Z,ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic   Iterative Reasoning Agents,ViDoRAG: Visual Document Retrieval-Augmented Generation über dynamische iterative Reasoning Agents,ViDoRAG: 通过动态迭代理据代理器获取视觉文件,http://arxiv.org/abs/2502.18017v2
604,"Large Language Models (LLMs) demonstrate impressive capabilities but lack robust temporal intelligence, struggling to integrate reasoning about the past with predictions and plausible generations of the future. Meanwhile, existing methods typically target isolated temporal skills, such as question answering about past events or basic forecasting, and exhibit poor generalization, particularly when dealing with events beyond their knowledge cutoff or requiring creative foresight. To address these limitations, we introduce \textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter) LLM with comprehensive temporal abilities: understanding, prediction, and creative generation. Our approach features a novel three-stage development path; the first two constitute a \textit{reinforcement learning (RL) curriculum} driven by a meticulously designed dynamic rule-based reward system. This framework progressively builds (1) foundational temporal understanding and logical event-time mappings from historical data, (2) future event prediction skills for events beyond its knowledge cutoff, and finally (3) enables remarkable generalization to creative future scenario generation without any fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1, on highly challenging future event prediction and creative scenario generation benchmarks. This work provides strong evidence that thoughtfully engineered, progressive RL fine-tuning allows smaller, efficient models to achieve superior temporal performance, offering a practical and scalable path towards truly time-aware AI. To foster further research, we also release \textit{Time-Bench}, a large-scale multi-task temporal reasoning dataset derived from 10 years of news data, and our series of \textit{Time-R1} checkpoints.",,"Zijia Liu, Peixuan Han, Haofei Yu, Haoru Li, Jiaxuan You",2025-06-03T05:30:14Z,Time-R1: Towards Comprehensive Temporal Reasoning in LLMs,Time-R1: Auf dem Weg zu einer umfassenden zeitlichen Reasoning in LLMs,时间-R1:走向LLMM中的全面时间理由,http://arxiv.org/abs/2505.13508v2
605,"Effectively retrieving, reasoning and understanding visually rich information remains a challenge for RAG methods. Traditional text-based methods cannot handle visual-related information. On the other hand, current vision-based RAG approaches are often limited by fixed pipelines and frequently struggle to reason effectively due to the insufficient activation of the fundamental capabilities of models. As RL has been proven to be beneficial for model reasoning, we introduce VRAG-RL, a novel RL framework tailored for complex reasoning across visually rich information. With this framework, VLMs interact with search engines, autonomously sampling single-turn or multi-turn reasoning trajectories with the help of visual perception tokens and undergoing continual optimization based on these samples. Our approach highlights key limitations of RL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely incorporate images into the context, leading to insufficient reasoning token allocation and neglecting visual-specific perception; and (ii) When models interact with search engines, their queries often fail to retrieve relevant information due to the inability to articulate requirements, thereby leading to suboptimal performance. To address these challenges, we define an action space tailored for visually rich inputs, with actions including cropping and scaling, allowing the model to gather information from a coarse-to-fine perspective. Furthermore, to bridge the gap between users' original inquiries and the retriever, we employ a simple yet effective reward that integrates query rewriting and retrieval performance with a model-based reward. Our VRAG-RL optimizes VLMs for RAG tasks using specially designed RL strategies, aligning the model with real-world applications. The code is available at https://github.com/Alibaba-NLP/VRAG.",,"Qiuchen Wang, Ruixue Ding, Yu Zeng, Zehui Chen, Lin Chen, Shihang Wang, Pengjun Xie, Fei Huang, Feng Zhao",2025-06-03T05:28:55Z,VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich   Information Understanding via Iterative Reasoning with Reinforcement Learning,VRAG-RL: Empower Vision-Perception-Based RAG für visuell reiches Informationsverständnis über iteratives Reasoning mit Verstärkungslernen,"VRAG-RL: 通过强化学习的迭代理由,增强基于愿景-观点的RAG, 以便通过强化学习获得视觉上丰富的信息了解",http://arxiv.org/abs/2505.22019v2
606,"Clinical tasks such as diagnosis and treatment require strong decision-making abilities, highlighting the importance of rigorous evaluation benchmarks to assess the reliability of large language models (LLMs). In this work, we introduce a knowledge-guided data augmentation framework that enhances the difficulty of clinical multiple-choice question (MCQ) datasets by generating distractors (i.e., incorrect choices that are similar to the correct one and may confuse existing LLMs). Using our KG-based pipeline, the generated choices are both clinically plausible and deliberately misleading. Our approach involves multi-step, semantically informed walks on a medical knowledge graph to identify distractor paths-associations that are medically relevant but factually incorrect-which then guide the LLM in crafting more deceptive distractors. We apply the designed knowledge graph guided distractor generation (KGGDG) pipline, to six widely used medical QA benchmarks and show that it consistently reduces the accuracy of state-of-the-art LLMs. These findings establish KGGDG as a powerful tool for enabling more robust and diagnostic evaluations of medical LLMs.",,"Running Yang, Wenlong Deng, Minghui Chen, Yuyin Zhou, Xiaoxiao Li",2025-06-03T05:28:26Z,Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge   Graph Guided Distractor Generation,Verbesserung klinischer Multiple-Choice-Fragen Benchmarks mit Knowledge Graph Guided Distractor Generierung,"加强具有知识图导引引产生体的临床多选择问题基准,加强临床多选择问题基准",http://arxiv.org/abs/2506.00612v2
607,"High-quality math datasets are crucial for advancing the reasoning abilities of large language models (LLMs). However, existing datasets often suffer from three key issues: outdated and insufficient challenging content, neglecting human-like reasoning, and limited reliability due to single-LLM generation. To address these, we introduce STORM-BORN, an ultra-challenging dataset of mathematical derivations sourced from cutting-edge academic papers, which includes dense human-like approximations and heuristic cues. To ensure the reliability and quality, we propose a novel human-in-the-loop, multi-agent data generation framework, integrating reasoning-dense filters, multi-agent collaboration, and human mathematicians' evaluations. We curated a set of 2,000 synthetic samples and deliberately selected the 100 most difficult problems. Even most advanced models like GPT-o1 solved fewer than 5% of them. Fine-tuning on STORM-BORN boosts accuracy by 7.84% (LLaMA3-8B) and 9.12% (Qwen2.5-7B). As AI approaches mathematician-level reasoning, STORM-BORN provides both a high-difficulty benchmark and a human-like reasoning training resource. Our code and dataset are publicly available at https://github.com/lwhere/STORM-BORN.",,"Wenhao Liu, Zhenyi Lu, Xinyu Hu, Jierui Zhang, Dailin Li, Jiacheng Cen, Huilin Cao, Haiteng Wang, Yuhan Li, Kun Xie, Dandan Li, Pei Zhang, Chengbo Zhang, Yuxiang Ren, Xiaohong Huang, Yan Ma",2025-06-03T05:25:22Z,STORM-BORN: A Challenging Mathematical Derivations Dataset Curated via a   Human-in-the-Loop Multi-Agent Framework,STORM-BORN: Ein anspruchsvoller mathematischer Ableitungs-Datensatz Kuratiert über ein Multi-Agenten-Rahmenwerk für Mensch-in-the-Loop-Systeme,"STORM-BORN:一个挑战性数学衍生数据集,通过 "" 人类在卢博 "" 多机构框架曲线",http://arxiv.org/abs/2506.01531v2
608,"Theory of Mind (ToM), the ability to infer mental states in others, is pivotal for human social cognition. Existing evaluations of ToM in LLMs are largely limited to English, neglecting the linguistic diversity that shapes human cognition. This limitation raises a critical question: can LLMs exhibit Multilingual Theory of Mind, which is the capacity to reason about mental states across diverse linguistic contexts? To address this gap, we present XToM, a rigorously validated multilingual benchmark that evaluates ToM across five languages and incorporates diverse, contextually rich task scenarios. Using XToM, we systematically evaluate LLMs (e.g., DeepSeek R1), revealing a pronounced dissonance: while models excel in multilingual language understanding, their ToM performance varies across languages. Our findings expose limitations in LLMs' ability to replicate human-like mentalizing across linguistic contexts.",,"Chunkit Chan, Yauwai Yim, Hongchuan Zeng, Zhiying Zou, Xinyuan Cheng, Zhifan Sun, Zheye Deng, Kawai Chung, Yuzhuo Ao, Yixiang Fan, Cheng Jiayang, Ercong Nie, Ginny Y. Wong, Helmut Schmid, Hinrich Schütze, Simon See, Yangqiu Song",2025-06-03T05:23:25Z,XToM: Exploring the Multilingual Theory of Mind for Large Language   Models,XToM: Erforschung der Mehrsprachigen Geistestheorie für große Sprachmodelle,XToM:探索大语言模式多语言心理理论,http://arxiv.org/abs/2506.02461v1
609,"As large language models (LLMs) are increasingly applied across various domains, enhancing safety while maintaining the helpfulness of LLMs has become a critical challenge. Recent studies solve this problem through safety-constrained online preference optimization or safety-constrained offline preference optimization. However, the safety-constrained online methods often suffer from excessive safety, which might reduce helpfulness, while the safety-constrained offline methods perform poorly in adaptively balancing safety and helpfulness. To address these limitations, we propose MidPO, a \textbf{\underline{Mi}}xture of Experts (MoE) framework for safety-helpfulness \textbf{\underline{d}}ual \textbf{\underline{P}}reference \textbf{\underline{O}}ptimization. Firstly, MidPO devises single-preference enhanced direct preference optimization approach to transform the base model into two independent experts, termed safety and helpfulness experts, and fine-tunes the two independent experts for optimal safety or helpfulness performance. Secondly, to achieve an effective balance between safety and helpfulness, MidPO incorporates the two experts into the MoE framework and designs a dynamic routing mechanism to allocate contributions from each expert adaptively. We conduct quantitative and qualitative experiments on three popular datasets to demonstrate the proposed MidPO significantly outperforms state-of-the-art approaches in both safety and helpfulness. The code and models will be released.",,"Yupeng Qi, Ziyu Lyu, Min Yang, Yanlin Wang, Lu Bai, Lixin Cui",2025-06-03T05:23:09Z,MidPO: Dual Preference Optimization for Safety and Helpfulness in Large   Language Models via a Mixture of Experts Framework,MidPO: Dual Preference Optimierung für Sicherheit und Hilfsbereitschaft in großen Sprachmodellen über einen Mixture of Experts Framework,"MidPO:通过专家混合框架,对大语言模式中的安全和助益实现双重优先优化",http://arxiv.org/abs/2506.02460v1
610,"Human preference data plays a critical role in aligning large language models (LLMs) with human values. However, collecting such data is often expensive and inefficient, posing a significant scalability challenge. To address this, we introduce Alignment Data Map, a GPT-4o-assisted tool for analyzing and diagnosing preference data. Using GPT-4o as a proxy for LLM alignment, we compute alignment scores for LLM-generated responses to instructions from existing preference datasets. These scores are then used to construct an Alignment Data Map based on their mean and variance. Our experiments show that using only 33 percent of the data, specifically samples in the high-mean, low-variance region, achieves performance comparable to or better than using the entire dataset. This finding suggests that the Alignment Data Map can significantly improve data collection efficiency by identifying high-quality samples for LLM alignment without requiring explicit annotations. Moreover, the Alignment Data Map can diagnose existing preference datasets. Our analysis shows that it effectively detects low-impact or potentially misannotated samples. Source code is available online.",,"Seohyeong Lee, Eunwon Kim, Hwaran Lee, Buru Chang",2025-06-03T05:21:36Z,Dataset Cartography for Large Language Model Alignment: Mapping and   Diagnosing Preference Data,Datensatzkartographie für großsprachliche Modellausrichtung: Mapping und Diagnose von Präferenzdaten,用于大语言模型对齐的数据集制图:绘图和诊断优先数据,http://arxiv.org/abs/2505.23114v2
611,"Visualizations play a crucial part in effective communication of concepts and information. Recent advances in reasoning and retrieval augmented generation have enabled Large Language Models (LLMs) to perform deep research and generate comprehensive reports. Despite its progress, existing deep research frameworks primarily focus on generating text-only content, leaving the automated generation of interleaved texts and visualizations underexplored. This novel task poses key challenges in designing informative visualizations and effectively integrating them with text reports. To address these challenges, we propose Formal Description of Visualization (FDV), a structured textual representation of charts that enables LLMs to learn from and generate diverse, high-quality visualizations. Building on this representation, we introduce Multimodal DeepResearcher, an agentic framework that decomposes the task into four stages: (1) researching, (2) exemplar report textualization, (3) planning, and (4) multimodal report generation. For the evaluation of generated multimodal reports, we develop MultimodalReportBench, which contains 100 diverse topics served as inputs along with 5 dedicated metrics. Extensive experiments across models and evaluation methods demonstrate the effectiveness of Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet model, Multimodal DeepResearcher achieves an 82\% overall win rate over the baseline method.",,"Zhaorui Yang, Bo Pan, Han Wang, Yiyao Wang, Xingyu Liu, Minfeng Zhu, Bo Zhang, Wei Chen",2025-06-03T05:18:19Z,Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports   From Scratch with Agentic Framework,Multimodale DeepResearcher: Text-Chart interleaved Reports from Scratch mit Agentic Framework generieren,多式深层研究多式研究者:利用制剂框架生成来自积存的文本图表间断报告,http://arxiv.org/abs/2506.02454v1
612,"In modern dialogue systems, the ability to implicitly infer user backgrounds from conversations and leverage this information for personalized assistance is crucial. However, the scarcity of high-quality data remains a fundamental challenge to evaluating and improving this capability. Traditional dataset construction methods are labor-intensive, resource-demanding, and raise privacy concerns. To address these issues, we propose a novel approach for automatic synthetic data generation and introduce the Implicit Personalized Dialogue (IP-Dialog) benchmark along with a training dataset, covering 10 tasks and 12 user attribute types. Additionally, we develop a systematic evaluation framework with four metrics to assess both attribute awareness and reasoning capabilities. We further propose five causal graphs to elucidate models' reasoning pathways during implicit personalization. Extensive experiments yield insightful observations and prove the reliability of our dataset.",,"Bo Peng, Zhiheng Wang, Heyang Gong, Chaochao Lu",2025-06-03T05:14:11Z,IP-Dialog: Evaluating Implicit Personalization in Dialogue Systems with   Synthetic Data,IP-Dialog: Bewertung der Impliziten Personalisierung in Dialogsystemen mit synthetischen Daten,IP-Dialog:评估在带有合成数据的对话系统中的隐含个性化,http://arxiv.org/abs/2506.02449v1
613,"Reinforcement Learning from Human Feedback (RLHF) is a crucial technique for aligning language models with human preferences, playing a pivotal role in the success of conversational models like GPT-4, ChatGPT, and Llama 2. A core challenge in employing RLHF lies in training a reliable reward model (RM), which relies on high-quality labels typically provided by human experts or advanced AI system. These methods can be costly and may introduce biases that affect the language model's responses. As language models improve, human input may become less effective in further enhancing their performance. In this paper, we propose Self-Evolved Reward Learning (SER), a novel approach where the RM generates additional training data to iteratively improve itself. We conducted extensive experiments on multiple datasets such as HH-RLHF and UltraFeedback, using models like Mistral and Llama 3, and compare SER against various baselines. Our results demonstrate that even with limited human-annotated data, learning from self-feedback can robustly enhance RM performance, thereby boosting the capabilities of large language models (LLMs). Resources of this paper can be found at https://aka.ms/ser",,"Chenghua Huang, Zhizhen Fan, Lu Wang, Fangkai Yang, Pu Zhao, Zeqi Lin, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang",2025-06-03T04:43:45Z,Self-Evolved Reward Learning for LLMs,Selbstentwickeltes Lohn-Lernen für LLMs,为LLMMs进行自我发展的奖励学习,http://arxiv.org/abs/2411.00418v3
614,"Emotions are a fundamental facet of human experience, varying across individuals, cultural contexts, and nationalities. Given the recent success of Large Language Models (LLMs) as role-playing agents, we examine whether LLMs exhibit emotional stereotypes when assigned nationality-specific personas. Specifically, we investigate how different countries are represented in pre-trained LLMs through emotion attributions and whether these attributions align with cultural norms. Our analysis reveals significant nationality-based differences, with emotions such as shame, fear, and joy being disproportionately assigned across regions. Furthermore, we observe notable misalignment between LLM-generated and human emotional responses, particularly for negative emotions, highlighting the presence of reductive and potentially biased stereotypes in LLM outputs.",,"Mahammed Kamruzzaman, Abdullah Al Monsur, Gene Louis Kim, Anshuman Chhabra",2025-06-03T04:35:51Z,From Anger to Joy: How Nationality Personas Shape Emotion Attribution in   Large Language Models,Vom Zorn zur Freude: Wie Nationalität Personas die Emotionszuschreibung in großen Sprachmodellen formt,"从愤怒到欢乐:在大语言模式中,国籍人格成形情感归属如何",http://arxiv.org/abs/2506.02431v1
615,"Brain-computer interfaces (BCIs) with speech decoding from brain recordings have broad application potential in fields such as clinical rehabilitation and cognitive neuroscience. However, current decoding methods remain limited to single-language, single-subject, and single neuroimaging modality settings, restricting their clinical applicability and generalizability. Here we propose a joint multilingual, multi-subject and multimodal decoding framework. It maps diverse brain recordings into a unified semantic space defined by a pre-trained multilingual model (PMM), enabling decoding across multiple languages, multiple subjects and multiple neuroimaging modalities. The proposed framework is validated using non-invasive brain recordings from 159 participants across four languages. Experimental results show that it exhibits strong generalization across multilingual, multi-subject, and multimodal settings. More importantly, the proposed framework can promote linguistic fairness, which is vital for underrepresented languages in BCI applications. The unified semantic space enables cross-lingual mapping enhancement, allowing the framework to boost the decoding performance of underrepresented languages, thereby promoting linguistic fairness. Overall, the proposed framework establishes a new potential paradigm for brain decoding, opening new paths for broader applications of BCI.",,"Yi Guo, Yihang Dong, Michael Kwok-Po Ng, Shuqiang Wang",2025-06-03T04:34:22Z,A Pre-trained Framework for Multilingual Brain Decoding Using   Non-invasive Recordings,Ein vortrainiertes Framework zur Mehrsprachigen Gehirndekodierung mit nicht-invasiven Aufnahmen,使用非侵入记录进行多语种脑解密的预培训框架,http://arxiv.org/abs/2506.03214v1
616,"Recent developments in prompt learning of large Vision-Language Models (VLMs) have significantly improved performance in target-specific tasks. However, these prompting methods often struggle to tackle the target-unspecific or generalizable tasks effectively. It may be attributed to the fact that overfitting training causes the model to forget its general knowledge. The general knowledge has a strong promotion on target-unspecific tasks. To alleviate this issue, we propose a novel Features Matrix (FM) approach designed to enhance these models on target-unspecific tasks. Our method extracts and leverages general knowledge, shaping a Features Matrix (FM). Specifically, the FM captures the semantics of diverse inputs from a deep and fine perspective, preserving essential general knowledge, which mitigates the risk of overfitting. Representative evaluations demonstrate that: 1) the FM is compatible with existing frameworks as a generic and flexible module, and 2) the FM significantly showcases its effectiveness in enhancing target-unspecific tasks (base-to-novel generalization, domain generalization, and cross-dataset generalization), achieving state-of-the-art performance.",,"Fangming Cui, Yonggang Zhang, Xuan Wang, Xinmei Tian, Jun Yu",2025-06-03T04:27:33Z,Enhancing Target-unspecific Tasks through a Features Matrix,Verbesserung von Ziel-unspezifischen Aufgaben durch eine Features Matrix,"通过特征矩阵,加强针对特定目标的任务",http://arxiv.org/abs/2505.03414v5
617,"Multi-hop question generation (MQG) aims to generate questions that require synthesizing multiple information snippets from documents to derive target answers. The primary challenge lies in effectively pinpointing crucial information snippets related to question-answer (QA) pairs, typically relying on keywords. However, existing works fail to fully utilize the guiding potential of keywords and neglect to differentiate the distinct roles of question-specific and document-specific keywords. To address this, we define dual-perspective keywords (i.e., question and document keywords) and propose a Dual-Perspective Keyword-Guided (DPKG) framework, which seamlessly integrates keywords into the multi-hop question generation process. We argue that question keywords capture the questioner's intent, whereas document keywords reflect the content related to the QA pair. Functionally, question and document keywords work together to pinpoint essential information snippets in the document, with question keywords required to appear in the generated question. The DPKG framework consists of an expanded transformer encoder and two answer-aware transformer decoders for keyword and question generation, respectively. Extensive experiments demonstrate the effectiveness of our work, showcasing its promising performance and underscoring its significant value in the MQG task.",,"Maodong Li, Longyin Zhang, Fang Kong",2025-06-03T04:20:21Z,Multi-Hop Question Generation via Dual-Perspective Keyword Guidance,Multi-Hop-Fragegenerierung über Dual-Perspektive Keyword Guidance,通过双向关键词指南多层次问题生成,http://arxiv.org/abs/2505.15299v2
618,"High-quality datasets are fundamental to training and evaluating machine learning models, yet their creation-especially with accurate human annotations-remains a significant challenge. Many dataset paper submissions lack originality, diversity, or rigorous quality control, and these shortcomings are often overlooked during peer review. Submissions also frequently omit essential details about dataset construction and properties. While existing tools such as datasheets aim to promote transparency, they are largely descriptive and do not provide standardized, measurable methods for evaluating data quality. Similarly, metadata requirements at conferences promote accountability but are inconsistently enforced. To address these limitations, this position paper advocates for the integration of systematic, rubric-based evaluation metrics into the dataset review process-particularly as submission volumes continue to grow. We also explore scalable, cost-effective methods for synthetic data generation, including dedicated tools and LLM-as-a-judge approaches, to support more efficient evaluation. As a call to action, we introduce DataRubrics, a structured framework for assessing the quality of both human- and model-generated datasets. Leveraging recent advances in LLM-based evaluation, DataRubrics offers a reproducible, scalable, and actionable solution for dataset quality assessment, enabling both authors and reviewers to uphold higher standards in data-centric research. We also release code to support reproducibility of LLM-based evaluations at https://github.com/datarubrics/datarubrics.",,"Genta Indra Winata, David Anugraha, Emmy Liu, Alham Fikri Aji, Shou-Yi Hung, Aditya Parashar, Patrick Amadeus Irawan, Ruochen Zhang, Zheng-Xin Yong, Jan Christian Blaise Cruz, Niklas Muennighoff, Seungone Kim, Hanyang Zhao, Sudipta Kar, Kezia Erina Suryoraharjo, M. Farid Adilazuarda, En-Shiun Annie Lee, Ayu Purwarianti, Derry Tanti Wijaya, Monojit Choudhury",2025-06-03T04:18:39Z,Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and   Accountability,Datenblätter reichen nicht aus: DataRubrics für automatisierte Qualitäts-Metriken und Rechenschaftspflicht,数据表还不够:用于自动化质量计量和问责制的数据参数,http://arxiv.org/abs/2506.01789v2
619,"Prompt learning has emerged as a promising method for adapting pre-trained visual-language models (VLMs) to a range of downstream tasks. While optimizing the context can be effective for improving performance on specific tasks, it can often lead to poor generalization performance on unseen classes or datasets sampled from different distributions. It may be attributed to the fact that textual prompts tend to overfit downstream data distributions, leading to the forgetting of generalized knowledge derived from hand-crafted prompts. In this paper, we propose a novel method called Similarity Paradigm with Textual Regularization (SPTR) for prompt learning without forgetting. SPTR is a two-pronged design based on hand-crafted prompts that is an inseparable framework. 1) To avoid forgetting general textual knowledge, we introduce the optimal transport as a textual regularization to finely ensure approximation with hand-crafted features and tuning textual features. 2) In order to continuously unleash the general ability of multiple hand-crafted prompts, we propose a similarity paradigm for natural alignment score and adversarial alignment score to improve model robustness for generalization. Both modules share a common objective in addressing generalization issues, aiming to maximize the generalization capability derived from multiple hand-crafted prompts. Four representative tasks (i.e., non-generalization few-shot learning, base-to-novel generalization, cross-dataset generalization, domain generalization) across 11 datasets demonstrate that SPTR outperforms existing prompt learning methods.",,"Fangming Cui, Jan Fong, Rongfei Zeng, Xinmei Tian, Jun Yu",2025-06-03T04:16:18Z,A Similarity Paradigm Through Textual Regularization Without Forgetting,Eine Ähnlichkeit Paradigmen durch Textregularisierung ohne Vergessen,通过不忘不忘的文本正规化的相似范例,http://arxiv.org/abs/2502.14376v2
620,"Textbooks play a critical role in shaping children's understanding of the world. While previous studies have identified gender inequality in individual countries' textbooks, few have examined the issue cross-culturally. This study applies natural language processing methods to quantify gender inequality in English textbooks from 22 countries across 7 cultural spheres. Metrics include character count, firstness (which gender is mentioned first), and TF-IDF word associations by gender. The analysis also identifies gender patterns in proper names appearing in TF-IDF word lists, tests whether large language models can distinguish between gendered word lists, and uses GloVe embeddings to examine how closely keywords associate with each gender. Results show consistent overrepresentation of male characters in terms of count, firstness, and named entities. All regions exhibit gender inequality, with the Latin cultural sphere showing the least disparity.",,Tairan Liu,2025-06-03T04:16:09Z,Gender Inequality in English Textbooks Around the World: an NLP Approach,Geschlechterungleichheit in englischen Lehrbüchern auf der ganzen Welt: ein NLP-Ansatz,全世界英文教科书中的两性平等:国家劳工政策办法,http://arxiv.org/abs/2506.02425v1
621,"Large language models (LLMs) excel in many tasks but struggle to accurately quantify uncertainty in their generated responses. This limitation makes it challenging to detect misinformation and ensure reliable decision-making. Existing uncertainty quantification (UQ) methods for LLMs are primarily prompt-wise rather than response-wise, often requiring multiple response samples, which incurs high computational costs. Moreover, LLMs have been shown to be overconfident, particularly when using reasoning steps to derive their answers. In this work, we propose CoT-UQ, a response-wise UQ framework that integrates LLMs' inherent reasoning capabilities through Chain-of-Thought (CoT) into the UQ process. CoT-UQ captures critical information during inference by extracting keywords from each reasoning step and assessing their importance to the final answer. This key reasoning information is then aggregated to produce a final uncertainty estimate. We conduct extensive experiments based on Llama Family with model sizes varying from 8B to 13B across logical and mathematical reasoning tasks. Experimental results demonstrate that CoT-UQ significantly outperforms existing UQ methods, achieving an average improvement of 5.9% AUROC compared to current UQ methods. The code is available at: https://github.com/ZBox1005/CoT-UQ.",,"Boxuan Zhang, Ruqi Zhang",2025-06-03T04:13:58Z,CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with   Chain-of-Thought,CoT-UQ: Verbesserung der reaktionsweisen Ungewissheitsquantifizierung in LLMs durch Ketten-of-Thought,COT-UQ:改进带研究链的LLMs 中反应-错误不确定因素的量化,http://arxiv.org/abs/2502.17214v2
622,"Current speech-LLMs exhibit limited capability in contextual reasoning alongside paralinguistic understanding, primarily due to the lack of Question-Answer (QA) datasets that cover both aspects. We propose a novel framework for dataset generation from in-the-wild speech data, that integrates contextual reasoning with paralinguistic information. It consists of a pseudo paralinguistic label-based data condensation of in-the-wild speech and LLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is validated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct model on a dataset created by our framework and human-generated CPQA dataset. The results also reveal the speech-LLM's limitations in handling empathetic reasoning tasks, highlighting the need for such datasets and more robust models. The proposed framework is first of its kind and has potential in training more robust speech-LLMs with paralinguistic reasoning capabilities.",,"Qiongqiong Wang, Hardik B. Sailor, Tianchi Liu, Ai Ti Aw",2025-06-03T04:11:48Z,Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data   Condensation and Spoken QA Generation,Kontextuelle paralinguistische Datenerstellung für Multi-Modal Speech-LLM: Datenkondensation und gesprochene QA-Generierung,多模式语音-LLM:数据集中和口语QA一代,http://arxiv.org/abs/2505.13338v2
623,"Voice Conversion (VC) modifies speech to match a target speaker while preserving linguistic content. Traditional methods usually extract speaker information directly from speech while neglecting the explicit utilization of linguistic content. Since VC fundamentally involves disentangling speaker identity from linguistic content, leveraging structured semantic features could enhance conversion performance. However, previous attempts to incorporate semantic features into VC have shown limited effectiveness, motivating the integration of explicit text modeling. We propose StarVC, a unified autoregressive VC framework that first predicts text tokens before synthesizing acoustic features. The experiments demonstrate that StarVC outperforms conventional VC methods in preserving both linguistic content (i.e., WER and CER) and speaker characteristics (i.e., SECS and MOS). Audio demo can be found at: https://thuhcsi.github.io/StarVC/.",,"Fengjin Li, Jie Wang, Yadong Niu, Yongqing Wang, Meng Meng, Jian Luan, Zhiyong Wu",2025-06-03T04:00:53Z,StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech   Generation in Voice Conversion,StarVC: Ein einheitliches Auto-Regressive Framework für gemeinsame Text- und Sprachgenerierung bei der Sprachumwandlung,StarVC:语音转换中联合文本和语音转换语音和语音共同生成统一自动递减框架,http://arxiv.org/abs/2506.02414v1
624,"The integration of generative artificial intelligence into educational applications has enhanced personalized and interactive learning experiences, and it shows strong potential to promote young learners language acquisition. However, it is still challenging to ensure consistent and robust performance across different languages and cultural contexts, and kids-friendly design requires simplified instructions, engaging interactions, and age-appropriate scaffolding to maintain motivation and optimize learning outcomes. In this work, we introduce SingaKids, a dialogic tutor designed to facilitate language learning through picture description tasks. Our system integrates dense image captioning, multilingual dialogic interaction, speech understanding, and engaging speech generation to create an immersive learning environment in four languages: English, Mandarin, Malay, and Tamil. We further improve the system through multilingual pre-training, task-specific tuning, and scaffolding optimization. Empirical studies with elementary school students demonstrate that SingaKids provides effective dialogic teaching, benefiting learners at different performance levels.",,"Zhengyuan Liu, Geyu Lin, Hui Li Tan, Huayun Zhang, Yanfeng Lu, Xiaoxue Gao, Stella Xin Yin, He Sun, Hock Huan Goh, Lung Hsiang Wong, Nancy F. Chen",2025-06-03T03:56:45Z,SingaKids: A Multilingual Multimodal Dialogic Tutor for Language   Learning,SingaKids: Ein multimodaler multimodaler dialogischer Tutor für das Sprachenlernen,SingaKids:语言学习多语种多模式对话导师,http://arxiv.org/abs/2506.02412v1
625,"Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing recognition for its potential to enhance large language models (LLMs) by structurally organizing domain-specific corpora and facilitating complex reasoning. However, current evaluations of GraphRAG models predominantly rely on traditional question-answering datasets. Their limited scope in questions and evaluation metrics fails to comprehensively assess the reasoning capacity improvements enabled by GraphRAG models. To address this gap, we introduce GraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously evaluate GraphRAG models. Our benchmark offers three key superiorities: \((i)\) Challenging question design. Featuring college-level, domain-specific questions that demand multi-hop reasoning, the benchmark ensures that simple content retrieval is insufficient for problem-solving. For example, some questions require mathematical reasoning or programming. \((ii)\) Diverse task coverage. The dataset includes a broad spectrum of reasoning tasks, multiple-choice, true/false, multi-select, open-ended, and fill-in-the-blank. It spans 16 disciplines in twenty core textbooks. \((iii)\) Holistic evaluation framework. GraphRAG-Bench provides comprehensive assessment across the entire GraphRAG pipeline, including graph construction, knowledge retrieval, and answer generation. Beyond final-answer correctness, it evaluates the logical coherence of the reasoning process. By applying nine contemporary GraphRAG methods to GraphRAG-Bench, we demonstrate its utility in quantifying how graph-based structuring improves model reasoning capabilities. Our analysis reveals critical insights about graph architectures, retrieval efficacy, and reasoning capabilities, offering actionable guidance for the research community.",,"Yilin Xiao, Junnan Dong, Chuang Zhou, Su Dong, Qianwen Zhang, Di Yin, Xing Sun, Xiao Huang",2025-06-03T03:44:26Z,GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating   Graph Retrieval-Augmented Generation,GraphRAG-Bench: Herausfordernde Domain-spezifische Begründung für die Auswertung der Graph Retrieval-Augmented Generation,图图RAG-Bench:评估图回收-提款一代的有挑战性域特定原因,http://arxiv.org/abs/2506.02404v1
626,"Intraoperative hypotension (IOH) frequently occurs under general anesthesia and is strongly linked to adverse outcomes such as myocardial injury and increased mortality. Despite its significance, IOH prediction is hindered by event sparsity and the challenge of integrating static and dynamic data across diverse patients. In this paper, we propose \textbf{IOHFuseLM}, a multimodal language model framework. To accurately identify and differentiate sparse hypotensive events, we leverage a two-stage training strategy. The first stage involves domain adaptive pretraining on IOH physiological time series augmented through diffusion methods, thereby enhancing the model sensitivity to patterns associated with hypotension. Subsequently, task fine-tuning is performed on the original clinical dataset to further enhance the ability to distinguish normotensive from hypotensive states. To enable multimodal fusion for each patient, we align structured clinical descriptions with the corresponding physiological time series at the token level. Such alignment enables the model to capture individualized temporal patterns alongside their corresponding clinical semantics. In addition, we convert static patient attributes into structured text to enrich personalized information. Experimental evaluations on two intraoperative datasets demonstrate that IOHFuseLM outperforms established baselines in accurately identifying IOH events, highlighting its applicability in clinical decision support scenarios. Our code is publicly available to promote reproducibility at https://github.com/zjt-gpu/IOHFuseLM.",,"Jintao Zhang, Zirui Liu, Mingyue Cheng, Shilong Zhang, Tingyue Pan, Qi Liu, Yanhu Xie",2025-06-03T03:43:38Z,Multimodal Forecasting of Sparse Intraoperative Hypotension Events   Powered by Language Model,Multimodale Vorhersage von Sparse Intraoperativen Hypotonieereignissen durch Sprachmodell,以语言模式为动力的草散的不合作和不连续活动多式预报,http://arxiv.org/abs/2505.22116v2
627,"LLM hallucination, where unfaithful text is generated, presents a critical challenge for LLMs' practical applications. Current detection methods often resort to external knowledge, LLM fine-tuning, or supervised training with large hallucination-labeled datasets. Moreover, these approaches do not distinguish between different types of hallucinations, which is crucial for enhancing detection performance. To address such limitations, we introduce hallucination probing, a new task that classifies LLM-generated text into three categories: aligned, misaligned, and fabricated. Driven by our novel discovery that perturbing key entities in prompts affects LLM's generation of these three types of text differently, we propose SHINE, a novel hallucination probing method that does not require external knowledge, supervised training, or LLM fine-tuning. SHINE is effective in hallucination probing across three modern LLMs, and achieves state-of-the-art performance in hallucination detection, outperforming seven competing methods across four datasets and four LLMs, underscoring the importance of probing for accurate detection.",,"Seongmin Lee, Hsiang Hsu, Chun-Fu Chen, Duen Horng, Chau",2025-06-03T03:35:25Z,Probing LLM Hallucination from Within: Perturbation-Driven Approach via   Internal Knowledge,Probing LLM Halluzination von innen: Perturbation-Driven Approach via Internal Knowledge,来自内部的探探LLM 幻觉:通过内部知识进行渗透-驱动方法,http://arxiv.org/abs/2411.09689v3
628,"Emotions manifest through physical experiences and bodily reactions, yet identifying such embodied emotions in text remains understudied. We present an embodied emotion classification dataset, CHEER-Ekman, extending the existing binary embodied emotion dataset with Ekman's six basic emotion categories. Using automatic best-worst scaling with large language models, we achieve performance superior to supervised approaches on our new dataset. Our investigation reveals that simplified prompting instructions and chain-of-thought reasoning significantly improve emotion recognition accuracy, enabling smaller models to achieve competitive performance with larger ones. Our dataset is publicly available at: https://github.com/menamerai/cheer-ekman.",,"Phan Anh Duong, Cat Luong, Divyesh Bommana, Tianyu Jiang",2025-06-03T03:33:33Z,CHEER-Ekman: Fine-grained Embodied Emotion Classification,"CHEER-Ekman: Feinkörnige, körperbetonte Emotions-Klassifikation",CHEER-Ekman: 精细的内衣情感分类,http://arxiv.org/abs/2506.01047v2
629,"Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference. While chain-of-thought inference significantly improves performance on complex reasoning tasks, it can also lead to reasoning failures when deployed with existing KV cache compression approaches. To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models. Our method preserves nearly 100% of the full KV cache performance using only 10% of the KV cache, substantially outperforming existing KV cache baselines, which reach only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV cache performance with 16% of the KV cache. This KV-cache reduction also leads to a 90% memory saving and a 6.6X throughput over standard chain-of-thought reasoning inference. Experimental results show that R-KV consistently outperforms existing KV cache compression baselines across two mathematical reasoning datasets.",,"Zefan Cai, Wen Xiao, Hanshi Sun, Cheng Luo, Yikai Zhang, Ke Wan, Yucheng Li, Yeyang Zhou, Li-Wen Chang, Jiuxiang Gu, Zhen Dong, Anima Anandkumar, Abedelkadir Asi, Junjie Hu",2025-06-03T03:32:10Z,R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning   Models Acceleration,R-KV: Redundancy-aware KV Cache-Kompression für trainingsfreie Reasoning-Modelle Beschleunigung,R-KV: 加速无培训理由模型的裁员-了解KV缓冲压缩,http://arxiv.org/abs/2505.24133v2
630,"Large Language Models (LLMs) demonstrate potential in complex legal tasks like argument generation, yet their reliability remains a concern. Building upon pilot work assessing LLM generation of 3-ply legal arguments using human evaluation, this paper introduces an automated pipeline to evaluate LLM performance on this task, specifically focusing on faithfulness (absence of hallucination), factor utilization, and appropriate abstention. We define hallucination as the generation of factors not present in the input case materials and abstention as the model's ability to refrain from generating arguments when instructed and no factual basis exists. Our automated method employs an external LLM to extract factors from generated arguments and compares them against the ground-truth factors provided in the input case triples (current case and two precedent cases). We evaluated eight distinct LLMs on three tests of increasing difficulty: 1) generating a standard 3-ply argument, 2) generating an argument with swapped precedent roles, and 3) recognizing the impossibility of argument generation due to lack of shared factors and abstaining. Our findings indicate that while current LLMs achieve high accuracy (over 90%) in avoiding hallucination on viable argument generation tests (Tests 1 & 2), they often fail to utilize the full set of relevant factors present in the cases. Critically, on the abstention test (Test 3), most models failed to follow instructions to stop, instead generating spurious arguments despite the lack of common factors. This automated pipeline provides a scalable method for assessing these crucial LLM behaviors, highlighting the need for improvements in factor utilization and robust abstention capabilities before reliable deployment in legal settings. Link: https://lizhang-aiandlaw.github.io/An-Automated-Pipeline-for-Evaluating-LLM-Generated-3-ply-Case-Based-Legal-Arguments/",,"Li Zhang, Morgan Gray, Jaromir Savelka, Kevin D. Ashley",2025-06-03T03:22:48Z,Measuring Faithfulness and Abstention: An Automated Pipeline for   Evaluating LLM-Generated 3-ply Case-Based Legal Arguments,Messen von Treue und Enthaltung: Eine automatisierte Pipeline zur Bewertung von LLM-generierten 3-lagigen fallbasierten rechtlichen Argumenten,测量诚信度和避免:评价LLM自动管道----基于3个案件的基于案件的法律论据,http://arxiv.org/abs/2506.00694v2
631,"This study addresses the challenges in intelligent processing of Chinese ancient mathematical classics by constructing Guji_MATH, a benchmark for evaluating classical texts based on Suanjing Shishu. It systematically assesses the mathematical problem-solving capabilities of mainstream reasoning models under the unique linguistic constraints of classical Chinese. Through machine-assisted annotation and manual verification, 538 mathematical problems were extracted from 8 canonical texts, forming a structured dataset centered on the ""Question-Answer-Solution"" framework, supplemented by problem types and difficulty levels. Dual evaluation modes--closed-book (autonomous problem-solving) and open-book (reproducing classical solution methods)--were designed to evaluate the performance of six reasoning models on ancient Chinese mathematical problems. Results indicate that reasoning models can partially comprehend and solve these problems, yet their overall performance remains inferior to benchmarks on modern mathematical tasks. Enhancing models' classical Chinese comprehension and cultural knowledge should be prioritized for optimization. This study provides methodological support for mining mathematical knowledge from ancient texts and disseminating traditional culture, while offering new perspectives for evaluating cross-linguistic and cross-cultural capabilities of reasoning models.",,"Liu Chang, Wang Dongbo, Liu liu, Zhao Zhixiao",2025-06-03T03:19:29Z,Can reasoning models comprehend mathematical problems in Chinese ancient   texts? An empirical study based on data from Suanjing Shishu,Können Argumentationsmodelle mathematische Probleme in chinesischen alten Texten verstehen? Eine empirische Studie basierend auf Daten von Suanjing Shishu,推理模型能理解中国古经中的数学问题吗?,http://arxiv.org/abs/2505.16660v2
632,"The synergistic mechanism based on Speculative Decoding (SD) has garnered considerable attention as a simple yet effective approach for accelerating the inference of large language models (LLMs). Nonetheless, the high rejection rates require repeated LLMs calls to validate draft tokens, undermining the overall efficiency gain of SD. In this work, we revisit existing verification mechanisms and propose a novel synergetic mechanism Consultant Decoding (CD). Unlike SD, which relies on a metric derived from importance sampling for verification, CD verifies candidate drafts using token-level likelihoods computed solely by the LLM. CD achieves up to a 2.5-fold increase in inference speed compared to the target model, while maintaining comparable generation quality (around 100% of the target model's performance). Interestingly, this is achieved by combining models whose parameter sizes differ by two orders of magnitude. In addition, CD reduces the call frequency of the large target model to below 10%, particularly in more demanding tasks. CD's performance was even found to surpass that of the large target model, which theoretically represents the upper bound for speculative decoding.",,"Chuanghao Ding, Jiaping Wang, Ziqing Yang, Xiaoliang Wang, Dahua Lin, Cam-Tu Nguyen, Fei Tan",2025-06-03T03:13:27Z,Consultant Decoding: Yet Another Synergistic Mechanism,Berater-Dekodierung: Ein weiterer synergistischer Mechanismus,顾问解说:然而又一个协同机制,http://arxiv.org/abs/2506.02391v1
633,"In this study, we reveal an in-context learning (ICL) capability of multilingual large language models (LLMs): by translating the input to several languages, we provide Parallel Input in Multiple Languages (PiM) to LLMs, which significantly enhances their comprehension abilities. To test this capability, we design extensive experiments encompassing 8 typical datasets, 7 languages and 8 state-of-the-art multilingual LLMs. Experimental results show that (1) incorporating more languages help PiM surpass the conventional ICL further; (2) even combining with the translations that are inferior to baseline performance can also help. Moreover, by examining the activated neurons in LLMs, we discover a counterintuitive but interesting phenomenon. Contrary to the common thought that PiM would activate more neurons than monolingual input to leverage knowledge learned from diverse languages, PiM actually inhibits neurons and promotes more precise neuron activation especially when more languages are added. This phenomenon aligns with the neuroscience insight about synaptic pruning, which removes less used neural connections, strengthens remainders, and then enhances brain intelligence.",,"Yongyu Mu, Peinan Feng, Zhiquan Cao, Yuzhang Wu, Bei Li, Chenglong Wang, Tong Xiao, Kai Song, Tongran Liu, Chunliang Zhang, Jingbo Zhu",2025-06-03T03:12:09Z,Revealing the Parallel Multilingual Learning within Large Language   Models,Das parallele Mehrsprachige Lernen in großen Sprachmodellen aufzeigen,在大语言模式内推广平行多语言学习,http://arxiv.org/abs/2403.09073v3
634,"As large language models (LLMs) are applied across diverse domains, the ability to selectively unlearn specific information is becoming increasingly essential. For instance, LLMs are expected to selectively provide confidential information to authorized internal users, such as employees or trusted partners, while withholding it from external users, including the general public and unauthorized entities. Therefore, we propose a novel method termed ``in-context knowledge unlearning'', which enables the model to selectively forget information in test-time based on the query context. Our method fine-tunes pre-trained LLMs to enable prompt unlearning of target knowledge within the context, while preserving unrelated information. Experiments on TOFU, AGE and RWKU datasets using Llama2-7B/13B and Mistral-7B models demonstrate that our method achieves up to 95% forget accuracy while retaining 80% of unrelated knowledge, significantly outperforming baselines in both in-domain and out-of-domain scenarios. Further investigation of the model's internal behavior revealed that while fine-tuned LLMs generate correct predictions in the middle layers and preserve them up to the final layer. However, the decision to forget is made only at the last layer, i.e. ``LLMs pretend to forget''. Our findings offer valuable insight into the improvement of the robustness of the unlearning mechanisms in LLMs, laying a foundation for future research in the field.",,"Shota Takashiro, Takeshi Kojima, Andrew Gambardella, Qi Cao, Yusuke Iwasawa, Yutaka Matsuo",2025-06-03T03:10:25Z,"Answer When Needed, Forget When Not: Language Models Pretend to Forget   via In-Context Knowledge Unlearning","Antwort Wenn nötig, vergessen, wenn nicht: Sprachmodelle tun so, als ob sie über In-Context Knowledge Unlearning vergessen würden","需要回答时,就忘记不是:语言模式假装通过内文知识去学习而忘记",http://arxiv.org/abs/2410.00382v2
635,"When comparing the linguistic capabilities of language models (LMs) with humans using LM probabilities, factors such as the length of the sequence and the unigram frequency of lexical items have a significant effect on LM probabilities in ways that humans are largely robust to. Prior works in comparing LM and human acceptability judgments treat these effects uniformly across models, making a strong assumption that models require the same degree of adjustment to control for length and unigram frequency effects. We propose MORCELA, a new linking theory between LM scores and acceptability judgments where the optimal level of adjustment for these effects is estimated from data via learned parameters for length and unigram frequency. We first show that MORCELA outperforms a commonly used linking theory for acceptability - SLOR (Pauls and Klein, 2012; Lau et al. 2017) - across two families of transformer LMs (Pythia and OPT). Furthermore, we demonstrate that the assumed degrees of adjustment in SLOR for length and unigram frequency overcorrect for these confounds, and that larger models require a lower relative degree of adjustment for unigram frequency, though a significant amount of adjustment is still necessary for all models. Finally, our subsequent analysis shows that larger LMs' lower susceptibility to frequency effects can be explained by an ability to better predict rarer words in context.",,"Lindia Tjuatja, Graham Neubig, Tal Linzen, Sophie Hao",2025-06-03T03:09:48Z,What Goes Into a LM Acceptability Judgment? Rethinking the Impact of   Frequency and Length,Was führt zu einem LM-Annehmbarkeitsurteil? Nachdenken über die Auswirkungen von Frequenz und Länge,究竟是什么进入LM可接受性判决?重新思考频率和长度的影响,http://arxiv.org/abs/2411.02528v3
636,"Bidirectional language models have better context understanding and perform better than unidirectional models on natural language understanding tasks, yet the theoretical reasons behind this advantage remain unclear. In this work, we investigate this disparity through the lens of the Information Bottleneck (IB) principle, which formalizes a trade-off between compressing input information and preserving task-relevant content. We propose FlowNIB, a dynamic and scalable method for estimating mutual information during training that addresses key limitations of classical IB approaches, including computational intractability and fixed trade-off schedules. Theoretically, we show that bidirectional models retain more mutual information and exhibit higher effective dimensionality than unidirectional models. To support this, we present a generalized framework for measuring representational complexity and prove that bidirectional representations are strictly more informative under mild conditions. We further validate our findings through extensive experiments across multiple models and tasks using FlowNIB, revealing how information is encoded and compressed throughout training. Together, our work provides a principled explanation for the effectiveness of bidirectional architectures and introduces a practical tool for analyzing information flow in deep language models.",,"Md Kowsher, Nusrat Jahan Prottasha, Shiyun Xu, Shetu Mohanto, Chen Chen, Ozlem Garibay, Niloofar Yousefi",2025-06-03T03:07:49Z,How Bidirectionality Helps Language Models Learn Better via Dynamic   Bottleneck Estimation,Wie Bidirektionalität Sprachmodelle durch dynamische Bottleneck-Schätzung besser lernen hilft,双向关系如何帮助语言模型通过动态瓶颈估计更好地学习,http://arxiv.org/abs/2506.00859v2
637,"Pruning has recently been widely adopted to reduce the parameter scale and improve the inference efficiency of Large Language Models (LLMs). Mainstream pruning techniques often rely on uniform layerwise pruning strategies, which can lead to severe performance degradation at high sparsity levels. Recognizing the varying contributions of different layers in LLMs, recent studies have shifted their focus toward non-uniform layerwise pruning. However, these approaches often rely on pre-defined values, which can result in suboptimal performance. To overcome these limitations, we propose a novel method called Dynamic Layerwise Pruning (DLP). This approach adaptively determines the relative importance of each layer by integrating model weights with input activation information, assigning pruning rates accordingly. Experimental results show that DLP effectively preserves model performance at high sparsity levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7% compared to state-of-the-art methods. Moreover, DLP is compatible with various existing LLM compression techniques and can be seamlessly integrated into Parameter-Efficient Fine-Tuning (PEFT). We release the code at https://github.com/ironartisan/DLP to facilitate future research.",,"Yuli Chen, Bo Cheng, Jiale Han, Yingying Zhang, Yingting Li, Shuhao Zhang",2025-06-03T03:06:29Z,DLP: Dynamic Layerwise Pruning in Large Language Models,DLP: Dynamisches Layerwise Pruning in großen Sprachmodellen,DLP: 动态多语言大语言模型的多语言缓冲,http://arxiv.org/abs/2505.23807v3
638,"High-quality long-context instruction data is essential for aligning long-context large language models (LLMs). Despite the public release of models like Qwen and Llama, their long-context instruction data remains proprietary. Human annotation is costly and challenging, while template-based synthesis methods limit scale, diversity, and quality. We introduce LongMagpie, a self-synthesis framework that automatically generates large-scale long-context instruction data. Our key insight is that aligned long-context LLMs, when presented with a document followed by special tokens preceding a user turn, auto-regressively generate contextually relevant queries. By harvesting these document-query pairs and the model's responses, LongMagpie produces high-quality instructions without human effort. Experiments on HELMET, RULER, and Longbench v2 demonstrate that LongMagpie achieves leading performance on long-context tasks while maintaining competitive performance on short-context tasks, establishing it as a simple and effective approach for open, diverse, and scalable long-context instruction data synthesis.",,"Chaochen Gao, Xing Wu, Zijia Lin, Debing Zhang, Songlin Hu",2025-06-03T03:04:17Z,LongMagpie: A Self-synthesis Method for Generating Large-scale   Long-context Instructions,LongMagpie: Eine Selbstsynthese-Methode zur Generierung großformatiger Langkontext-Anweisungen,LongMagpie: 生成大型长期指令的自合成方法,http://arxiv.org/abs/2505.17134v2
639,"With the rapid adoption of large language models (LLMs) in natural language processing, the ability to follow instructions has emerged as a key metric for evaluating their practical utility. However, existing evaluation methods often focus on single-language scenarios, overlooking the challenges and differences present in multilingual and cross-lingual contexts. To address this gap, we introduce MaXIFE: a comprehensive evaluation benchmark designed to assess instruction-following capabilities across 23 different languages with 1667 verifiable instruction tasks. MaXIFE integrates both Rule-Based Evaluation and Model-Based Evaluation, ensuring a balance of efficiency and accuracy. We applied MaXIFE to evaluate several leading commercial LLMs, establishing baseline results for future comparisons. By providing a standardized tool for multilingual instruction-following evaluation, MaXIFE aims to advance research and development in natural language processing.",,"Yile Liu, Ziwei Ma, Xiu Jiang, Jinglu Hu, Jing Chang, Liang Li",2025-06-03T02:53:48Z,MaXIFE: Multilingual and Cross-lingual Instruction Following Evaluation,MaXIFE: Mehrsprachige und cross-linguale Anleitung nach Auswertung,评价之后的多语种和跨语言教学,http://arxiv.org/abs/2506.01776v2
640,"Developing AI agents capable of interacting with open-world environments to solve diverse tasks is a compelling challenge. However, evaluating such open-ended agents remains difficult, with current benchmarks facing scalability limitations. To address this, we introduce Minecraft Universe (MCU), a comprehensive evaluation framework set within the open-world video game Minecraft. MCU incorporates three key components: (1) an expanding collection of 3,452 composable atomic tasks that encompasses 11 major categories and 41 subcategories of challenges; (2) a task composition mechanism capable of generating infinite diverse tasks with varying difficulty; and (3) a general evaluation framework that achieves 91.5\% alignment with human ratings for open-ended task assessment. Empirical results reveal that even state-of-the-art foundation agents struggle with the increasing diversity and complexity of tasks. These findings highlight the necessity of MCU as a robust benchmark to drive progress in AI agent development within open-ended environments. Our evaluation code and scripts are available at https://github.com/CraftJarvis/MCU.",,"Xinyue Zheng, Haowei Lin, Kaichen He, Zihao Wang, Zilong Zheng, Yitao Liang",2025-06-03T02:30:05Z,MCU: An Evaluation Framework for Open-Ended Game Agents,MCU: Ein Evaluierungsrahmen für offene Spielagenten,MCU: 开放游戏媒介评价框架,http://arxiv.org/abs/2310.08367v4
641,"In-context learning (ICL) has emerged as a successful paradigm for leveraging large language models (LLMs). However, it often struggles to generalize beyond the distribution of the provided demonstrations. A recent advancement in enhancing robustness is ICL with explanations (X-ICL), which improves prediction reliability by guiding LLMs to understand and articulate the reasoning behind correct labels. Building on this approach, we introduce an advanced framework that extends X-ICL by systematically exploring explanations for all possible labels (X$^2$-ICL), thereby enabling more comprehensive and robust decision-making. Experimental results on multiple natural language understanding datasets validate the effectiveness of X$^2$-ICL, demonstrating significantly improved robustness to out-of-distribution data compared to the existing ICL approaches.",,"Ukyo Honda, Tatsushi Oka",2025-06-03T02:29:14Z,Exploring Explanations Improves the Robustness of In-Context Learning,Erforschende Erklärungen verbessern die Robustheit des In-Context-Lernens,探讨解释可提高内文学习的实力,http://arxiv.org/abs/2506.02378v1
642,"Text-to-SQL transforms the user queries from natural language to executable SQL programs, enabling non-experts to interact with complex databases. Existing prompt-based methods craft meticulous text guidelines and examples to facilitate SQL generation, but their accuracy is hindered by the large semantic gap between the texts and the low-resource SQL programs. In this work, we propose Pi-SQL, which incorporates the high-resource Python program as a pivot to bridge between the natural language query and SQL program. In particular, Pi-SQL first generates Python programs that provide fine-grained step-by-step guidelines in their code blocks or comments, and then produces an SQL program following the guidance of each Python program. The final SQL program matches the reference Python program's query results and, through selection from candidates generated by different strategies, achieves superior execution speed, with a reward-based valid efficiency score up to 4.55 higher than the best-performing baseline. Extensive experiments demonstrate the effectiveness of Pi-SQL, which improves the execution accuracy of the best-performing baseline by up to 3.20.",,"Yongdong chi, Hanqing Wang, Zonghan Yang, Jian Yang, Xiao Yan, Yun Chen, Guanhua Chen",2025-06-03T02:29:05Z,Pi-SQL: Enhancing Text-to-SQL with Fine-Grained Guidance from Pivot   Programming Languages,Pi-SQL: Text-zu-SQL mit feinkörniger Anleitung von Pivot-Programmiersprachen verbessern,"Pi-SQL:用 "" 重点方案编制语言 "" 的精细指导加强文本到SQL",http://arxiv.org/abs/2506.00912v2
643,"In this paper we present AnswerCarefully, a dataset for promoting the safety and appropriateness of Japanese LLM outputs. The dataset consists of 1,800 pairs of questions and reference answers, where the questions require special attention in answering. It covers a wide range of risk categories established in prior English-language datasets, but the data samples are original in that they are manually created to reflect the socio-cultural context of LLM usage in Japan. We show that using this dataset for instruction to fine-tune a Japanese LLM led to improved output safety without compromising the utility of general responses. We also report the results of a safety evaluation of 12 Japanese LLMs using this dataset as a benchmark. Finally, we describe the latest update on the dataset which provides English translations and annotations of the questions, aimed at facilitating the derivation of similar datasets in different languages and regions.",,"Hisami Suzuki, Satoru Katsumata, Takashi Kodama, Tetsuro Takahashi, Kouta Nakayama, Satoshi Sekine",2025-06-03T02:18:59Z,AnswerCarefully: A Dataset for Improving the Safety of Japanese LLM   Output,AntwortVorsichtig: Ein Datensatz zur Verbesserung der Sicherheit der japanischen LLM-Ausgabe,正确回答:提高日本LLM产出安全性的数据集,http://arxiv.org/abs/2506.02372v1
644,"Graph retrieval-augmented generation (GRAG) places high demands on graph-specific retrievers. However, existing retrievers often rely on language models pretrained on plain text, limiting their effectiveness due to domain misalignment and structure ignorance. To address these challenges, we propose GPR, a graph-based retriever pretrained directly on knowledge graphs. GPR aligns natural language questions with relevant subgraphs through LLM-guided graph augmentation and employs a structure-aware objective to learn fine-grained retrieval strategies. Experiments on two datasets, three LLM backbones, and five baselines show that GPR consistently improves both retrieval quality and downstream generation, demonstrating its effectiveness as a robust retrieval solution for GRAG.",,"Xiaochen Wang, Zongyu Wu, Yuan Zhong, Xiang Zhang, Suhang Wang, Fenglong Ma",2025-06-03T02:07:40Z,GPR: Empowering Generation with Graph-Pretrained Retriever,GPR: Empowering Generation mit Graph-Pretrained Retriever,GPR:以图预开发的再利用方式增强发电能力,http://arxiv.org/abs/2506.00261v2
645,"Robust, diverse, and challenging cultural knowledge benchmarks are essential for measuring our progress towards making LMs that are helpful across diverse cultures. We introduce CulturalBench: a set of 1,696 human-written and human-verified questions to assess LMs' cultural knowledge, covering 45 global regions including underrepresented ones like Bangladesh, Zimbabwe, and Peru. Questions are each verified by five independent annotators and span 17 diverse topics ranging from food preferences to greeting etiquette. We construct CulturalBench using methods inspired by Human-AI Red-Teaming. Compared to human performance (92.4% accuracy), the hard version of CulturalBench is challenging even for the best-performing frontier LMs, ranging from 28.7% to 61.5% in accuracy. We find that LMs often struggle with tricky questions that have multiple correct answers (e.g., What utensils do the Chinese usually use?), revealing a tendency to overfit to a single answer. Our results indicate that GPT-4o substantially outperform other models across cultures, besting local providers (e.g., Mistral on European culture and DeepSeek on Chinese culture). Across the board, models under-perform on questions related to North Africa, South America and Middle East.",,"Yu Ying Chiu, Liwei Jiang, Bill Yuchen Lin, Chan Young Park, Shuyue Stella Li, Sahithya Ravi, Mehar Bhatia, Maria Antoniak, Yulia Tsvetkov, Vered Shwartz, Yejin Choi",2025-06-03T01:56:26Z,"CulturalBench: A Robust, Diverse, and Challenging Cultural Benchmark by   Human-AI CulturalTeaming","CulturalBench: Ein robustes, vielfältiges und anspruchsvolles kulturelles Benchmark von Human-AI CulturalTeaming",文化时区:人类-AI文化团队的强健、多样化和挑战性文化基准,http://arxiv.org/abs/2410.02677v2
646,"Key-Value (KV) cache has become a bottleneck of LLMs for long-context generation. Despite the numerous efforts in this area, the optimization for the decoding phase is generally ignored. However, we believe such optimization is crucial, especially for long-output generation tasks based on the following two observations: (i) Excessive compression during the prefill phase, which requires specific full context impairs the comprehension of the reasoning task; (ii) Deviation of heavy hitters occurs in the reasoning tasks with long outputs. Therefore, SCOPE, a simple yet efficient framework that separately performs KV cache optimization during the prefill and decoding phases, is introduced. Specifically, the KV cache during the prefill phase is preserved to maintain the essential information, while a novel strategy based on sliding is proposed to select essential heavy hitters for the decoding phase. Memory usage and memory transfer are further optimized using adaptive and discontinuous strategies. Extensive experiments on LongGenBench show the effectiveness and generalization of SCOPE and its compatibility as a plug-in to other prefill-only KV compression methods.",,"Jialong Wu, Zhenglin Wang, Linhai Zhang, Yilong Lai, Yulan He, Deyu Zhou",2025-06-03T01:55:18Z,SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation,ANWENDUNGSBEREICH: Optimierung der Schlüsselwert-Cache-Kompression in der Langkontext-Generierung,SCOPE: 优化长期生成中键值缓存压缩,http://arxiv.org/abs/2412.13649v3
647,"This paper develops an agentic framework that employs large language models (LLMs) to automate the generation of persuasive and grounded marketing content, using real estate listing descriptions as our focal application domain. Our method is designed to align the generated content with user preferences while highlighting useful factual attributes. This agent consists of three key modules: (1) Grounding Module, mimicking expert human behavior to predict marketable features; (2) Personalization Module, aligning content with user preferences; (3) Marketing Module, ensuring factual accuracy and the inclusion of localized features. We conduct systematic human-subject experiments in the domain of real estate marketing, with a focus group of potential house buyers. The results demonstrate that marketing descriptions generated by our approach are preferred over those written by human experts by a clear margin while maintaining the same level of factual accuracy. Our findings suggest a promising agentic approach to automate large-scale targeted marketing while ensuring factuality of content generation.",,"Jibang Wu, Chenghao Yang, Simon Mahns, Chaoqi Wang, Hao Zhu, Fei Fang, Haifeng Xu",2025-06-03T01:49:54Z,Grounded Persuasive Language Generation for Automated Marketing,Gegründete überzeugende Sprachgenerierung für automatisiertes Marketing,用于自动营销的有源辅助性语言生成,http://arxiv.org/abs/2502.16810v2
648,"Interpreting the internal process of neural models has long been a challenge. This challenge remains relevant in the era of large language models (LLMs) and in-context learning (ICL); for example, ICL poses a new issue of interpreting which example in the few-shot examples contributed to identifying/solving the task. To this end, in this paper, we design synthetic diagnostic tasks of inductive reasoning, inspired by the generalization tests typically adopted in psycholinguistics. Here, most in-context examples are ambiguous w.r.t. their underlying rule, and one critical example disambiguates it. The question is whether conventional input attribution (IA) methods can track such a reasoning process, i.e., identify the influential example, in ICL. Our experiments provide several practical findings; for example, a certain simple IA method works the best, and the larger the model, the generally harder it is to interpret the ICL with gradient-based IA methods.",,"Mengyu Ye, Tatsuki Kuribayashi, Goro Kobayashi, Jun Suzuki",2025-06-03T01:49:25Z,Can Input Attributions Explain Inductive Reasoning in In-Context   Learning?,Kann Input-Attributionen im In-Context-Lernen eine induktive Begründung erklären?,投入属性能否解释内文学习中的引因?,http://arxiv.org/abs/2412.15628v4
649,"Large Language Models (LLMs) have achieved remarkable progress on advanced reasoning tasks such as mathematics and coding competitions. Meanwhile, physics, despite being both reasoning-intensive and essential to real-world understanding, received limited academic and industrial attention. This paper introduces PHYSICS, a dataset containing 16,568 high-quality physics problems spanning subjects and difficulty levels, to facilitate this issue. Specifically, PHYSICS is curated with exercises from over 100 textbooks through a carefully designed pipeline for quality control. It covers five major physics domains: Mechanics, Electromagnetism, Thermodynamics, Optics, and Modern Physics. It also spans a wide range of difficulty levels, from high school to graduate-level physics courses. To utilize the data for improving and evaluating the model's physical reasoning capabilities, we split the dataset into training and test sets, and provide reasoning paths generated by powerful reasoning models for the training data to facilitate model training. In addition, for the evaluation part, we find that existing evaluation frameworks exhibit biases in aspects such as units, simplification, and precision in physics domain. To balance efficiency and accuracy, we introduce a Rule+Model evaluation framework tailored to physics problems. Our evaluations on current state-of-the-art open-source and proprietary models highlight the limitations of current models in handling physics-related tasks. We hope that our dataset and evaluation methodology will jointly advance the development of LLMs in the field of physics.",,"Shenghe Zheng, Qianjia Cheng, Junchi Yao, Mengsong Wu, Haonan He, Ning Ding, Yu Cheng, Shuyue Hu, Lei Bai, Dongzhan Zhou, Ganqu Cui, Peng Ye",2025-06-03T01:33:30Z,Scaling Physical Reasoning with the PHYSICS Dataset,Skalierung der physikalischen Vernunft mit dem PHYSICS-Datensatz,利用PHYSICS数据集调整物理理由,http://arxiv.org/abs/2506.00022v2
650,"Sign language processing has traditionally relied on task-specific models, limiting the potential for transfer learning across tasks. Pre-training methods for sign language have typically focused on either supervised pre-training, which cannot take advantage of unlabeled data, or context-independent (frame or video segment) representations, which ignore the effects of relationships across time in sign language. We introduce SHuBERT (Sign Hidden-Unit BERT), a self-supervised contextual representation model learned from approximately 1,000 hours of American Sign Language video. SHuBERT adapts masked token prediction objectives to multi-stream visual sign language input, learning to predict multiple targets corresponding to clustered hand, face, and body pose streams. SHuBERT achieves state-of-the-art performance across multiple tasks including sign language translation, isolated sign language recognition, and fingerspelling detection.",,"Shester Gueuwou, Xiaodan Du, Greg Shakhnarovich, Karen Livescu, Alexander H. Liu",2025-06-03T01:30:30Z,SHuBERT: Self-Supervised Sign Language Representation Learning via   Multi-Stream Cluster Prediction,SHuBERT: Selbstüberwachte Sign Language Representation Lernen über Multi-Stream Cluster Prediction,通过多系统集群预测进行自上自上手语代表制学习,http://arxiv.org/abs/2411.16765v2
651,"A persistent challenge in sign language video processing, including the task of sign to written language translation, is how we learn representations of sign language in an effective and efficient way that preserves the important attributes of these languages, while remaining invariant to irrelevant visual differences. Informed by the nature and linguistics of signed languages, our proposed method focuses on just the most relevant parts in a signing video: the face, hands and body pose of the signer. However, instead of fully relying on pose estimation from off-the-shelf pose tracking models, which have inconsistent performance for hands and faces, we propose to learn a representation of the complex handshapes and facial expressions of sign languages in a self-supervised fashion. Our approach is based on learning from individual frames (rather than video sequences) and is therefore much more efficient than prior work on sign language pre-training. Compared to a recent model that established a new state of the art in sign language translation on the How2Sign dataset, our approach yields similar translation performance, using less than 3\% of the compute.",,"Shester Gueuwou, Xiaodan Du, Greg Shakhnarovich, Karen Livescu",2025-06-03T01:25:18Z,SignMusketeers: An Efficient Multi-Stream Approach for Sign Language   Translation at Scale,SignMusketeers: Ein effizienter Multi-Stream-Ansatz für Sign Language-Übersetzungen im Maßstab,符号 Musketeerers: 用于缩放手语翻译的高效多系统方法,http://arxiv.org/abs/2406.06907v2
652,"Traditional approaches -- such as Win Probability Added (WPA)-based ranking or computer vision-driven event detection -- can identify scoring plays but often miss strategic depth, momentum shifts, and storyline progression. Manual curation remains the gold standard but is resource-intensive and not scalable. We introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight summarization that integrates structured sports analytics with natural language reasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and Leverage Index -- to quantify play importance, while an LLM module enhances selection based on contextual narrative value. This hybrid approach ensures both quantitative rigor and qualitative richness, surpassing the limitations of purely statistical or vision-based systems. Evaluated on five diverse Korean Baseball Organization League games, DIAMOND improves F1-score from 42.9% (WPA-only) to 84.8%, outperforming both commercial and statistical baselines. Though limited in scale, our results highlight the potential of modular, interpretable agent-based frameworks for event-level summarization in sports and beyond.",,"Jeonghun Kang, Soonmok Kwon, Joonseok Lee, Byung-Hak Kim",2025-06-03T01:10:20Z,DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight   Summarization,DIAMOND: Ein LLM-getriebener Agent für kontextabhängige Baseball-Highlight-Zusammenfassung,用于上下文软件的LLM-Driven代理器,http://arxiv.org/abs/2506.02351v1
653,"Misinformation detection models often rely on superficial cues (i.e., \emph{shortcuts}) that correlate with misinformation in training data but fail to generalize to the diverse and evolving nature of real-world misinformation. This issue is exacerbated by large language models (LLMs), which can easily generate convincing misinformation through simple prompts. We introduce TruthOverTricks, a unified evaluation paradigm for measuring shortcut learning in misinformation detection. TruthOverTricks categorizes shortcut behaviors into intrinsic shortcut induction and extrinsic shortcut injection, and evaluates seven representative detectors across 14 popular benchmarks, along with two new factual misinformation datasets, NQ-Misinfo and Streaming-Misinfo. Empirical results reveal that existing detectors suffer severe performance degradation when exposed to both naturally occurring and adversarially crafted shortcuts. To address this, we propose SMF, an LLM-augmented data augmentation framework that mitigates shortcut reliance through paraphrasing, factual summarization, and sentiment normalization. SMF consistently enhances robustness across 16 benchmarks, encouraging models to rely on deeper semantic understanding rather than shortcut cues. To promote the development of misinformation detectors, we have published the resources publicly at https://github.com/whr000001/TruthOverTricks.",,"Herun Wan, Jiaying Wu, Minnan Luo, Zhi Zeng, Zhixiong Su",2025-06-03T01:09:55Z,Truth over Tricks: Measuring and Mitigating Shortcut Learning in   Misinformation Detection,Wahrheit über Tricks: Messen und Abmildern von Shortcut-Lernen in Fehlinformationserkennung,衡量和减轻错误信息探测中的捷径学习,http://arxiv.org/abs/2506.02350v1
654,"The recent progress in Vision-Language Models (VLMs) has broadened the scope of multimodal applications. However, evaluations often remain limited to functional tasks, neglecting abstract dimensions such as personality traits and human values. To address this gap, we introduce Value-Spectrum, a novel Visual Question Answering (VQA) benchmark aimed at assessing VLMs based on Schwartz's value dimensions that capture core human values guiding people's preferences and actions. We design a VLM agent pipeline to simulate video browsing and construct a vector database comprising over 50,000 short videos from TikTok, YouTube Shorts, and Instagram Reels. These videos span multiple months and cover diverse topics, including family, health, hobbies, society, technology, etc. Benchmarking on Value-Spectrum highlights notable variations in how VLMs handle value-oriented content. Beyond identifying VLMs' intrinsic preferences, we also explore the ability of VLM agents to adopt specific personas when explicitly prompted, revealing insights into the adaptability of the model in role-playing scenarios. These findings highlight the potential of Value-Spectrum as a comprehensive evaluation set for tracking VLM preferences in value-based tasks and abilities to simulate diverse personas. The complete code and data are available at: https://github.com/Jeremyyny/Value-Spectrum.",,"Jingxuan Li, Yuning Yang, Shengqi Yang, Linfan Zhang, Ying Nian Wu",2025-06-03T01:05:41Z,Value-Spectrum: Quantifying Preferences of Vision-Language Models via   Value Decomposition in Social Media Contexts,Value-Spectrum: Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts,价值特征:通过社会媒体背景下的价值观分解对愿景-语言模式的偏好进行量化,http://arxiv.org/abs/2411.11479v3
655,"Stories are central to human culture, serving to share ideas, preserve traditions, and foster connections. Automatic story generation, a key advancement in artificial intelligence (AI), offers new possibilities for creating personalized content, exploring creative ideas, and enhancing interactive experiences. However, existing methods struggle to maintain narrative coherence and logical consistency. This disconnect compromises the overall storytelling experience, underscoring the need for substantial improvements. Inspired by human cognitive processes, we introduce Storyteller, a novel approach that systemically improves the coherence and consistency of automatically generated stories. Storyteller introduces a plot node structure based on linguistically grounded subject verb object (SVO) triplets, which capture essential story events and ensure a consistent logical flow. Unlike previous methods, Storyteller integrates two dynamic modules, the STORYLINE and narrative entity knowledge graph (NEKG),that continuously interact with the story generation process. This integration produces structurally sound, cohesive and immersive narratives. Extensive experiments demonstrate that Storyteller significantly outperforms existing approaches, achieving an 84.33% average win rate through human preference evaluation. At the same time, it is also far ahead in other aspects including creativity, coherence, engagement, and relevance.",,"Jiaming Li, Yukun Chen, Ziqiang Liu, Minghuan Tan, Lei Zhang, Yunshui Li, Run Luo, Longze Chen, Jing Luo, Ahmadreza Argha, Hamid Alinejad-Rokny, Wei Zhou, Min Yang",2025-06-03T00:54:00Z,STORYTELLER: An Enhanced Plot-Planning Framework for Coherent and   Cohesive Story Generation,STORYTELLER: Ein erweitertes Plot-Planning-Framework für eine kohärente und kohärente Story-Generation,增强的和谐和凝聚代人规划框架,http://arxiv.org/abs/2506.02347v1
656,"Large language models (LLMs) are foundational explorations to artificial general intelligence, yet their alignment with human values via instruction tuning and preference learning achieves only superficial compliance. Here, we demonstrate that harmful knowledge embedded during pretraining persists as indelible ""dark patterns"" in LLMs' parametric memory, evading alignment safeguards and resurfacing under adversarial inducement at distributional shifts. In this study, we first theoretically analyze the intrinsic ethical vulnerability of aligned LLMs by proving that current alignment methods yield only local ""safety regions"" in the knowledge manifold. In contrast, pretrained knowledge remains globally connected to harmful concepts via high-likelihood adversarial trajectories. Building on this theoretical insight, we empirically validate our findings by employing semantic coherence inducement under distributional shifts--a method that systematically bypasses alignment constraints through optimized adversarial prompts. This combined theoretical and empirical approach achieves a 100% attack success rate across 19 out of 23 state-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing their universal vulnerabilities.",,"Jiawei Lian, Jianhong Pan, Lefan Wang, Yi Wang, Shaohui Mei, Lap-Pui Chau",2025-06-03T00:32:13Z,Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language   Models,Enthüllen der Intrinsischen Ethischen Verletzlichkeit von ausgerichteten großen Sprachmodellen,揭示统一大语言模式内在道德脆弱性,http://arxiv.org/abs/2504.05050v4
657,"With the release of R1, a publicly available large reasoning model (LRM), researchers commonly train new LRMs by training language models on R1's long chain-of-thought (CoT) inferences. While prior works show that LRMs' capabilities can be reproduced through direct distillation, the continued reliance on the existing models (e.g., R1) remains a critical limitation in advancing the field. As a first step toward independent LRM development, this paper explores the possibility of constructing a long CoT dataset with LLMs that are not trained for inference-time scaling. To this end, we present the Long CoT Collection, a dataset of 100K CoT rationales annotated using existing short CoT LLMs. We develop a pipeline that induces o1's novel reasoning strategies into short CoT LLMs, enabling them to think longer and introducing controllability over the thought budget to better manage the overthinking problem. Our extensive analyses validate that our dataset achieves quality comparable to--or slightly below--R1. Furthermore, our experiments demonstrate that training on our dataset not only strengthens general reasoning skills, but also provides a strong foundation for reinforcement learning--models initialized on our data achieve 2-3x larger gains with RLVR.",,"Hyungjoo Chae, Dongjin Kang, Jihyuk Kim, Beong-woo Kwak, Sunghyun Park, Haeju Park, Jinyoung Yeo, Moontae Lee, Kyungjae Lee",2025-06-03T00:29:15Z,One Missing Piece for Open-Source Reasoning Models: A Dataset to   Mitigate Cold-Starting Short CoT LLMs in RL,"Ein fehlendes Stück für Open-Source-Vernunftmodelle: Ein Datensatz, um kurzzeitige CoT-LLMs in RL zu vermischen","开放源码理由模型遗漏的一块块:一个数据集,用于在RL中模拟冷启动短 CoT LLMS",http://arxiv.org/abs/2506.02338v1
658,"Large Language Models (LLMs) have achieved remarkable performance on a wide range of NLP benchmarks, often surpassing human-level accuracy. However, their reliability in high-stakes domains such as medicine, particularly in low-resource languages, remains underexplored. In this work, we introduce PersianMedQA, a large-scale, expert-validated dataset of multiple-choice Persian medical questions, designed to evaluate LLMs across both Persian and English. We benchmark over 40 state-of-the-art models, including general-purpose, Persian fine-tuned, and medical LLMs, in zero-shot and chain-of-thought (CoT) settings. Our results show that closed-source general models (e.g., GPT-4.1) consistently outperform all other categories, achieving 83.3% accuracy in Persian and 80.7% in English, while Persian fine-tuned models such as Dorna underperform significantly (e.g., 35.9% in Persian), often struggling with both instruction-following and domain reasoning. We also analyze the impact of translation, showing that while English performance is generally higher, Persian responses are sometimes more accurate due to cultural and clinical contextual cues. Finally, we demonstrate that model size alone is insufficient for robust performance without strong domain or language adaptation. PersianMedQA provides a foundation for evaluating multilingual and culturally grounded medical reasoning in LLMs. The PersianMedQA dataset can be accessed at: https://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA",,"Mohammad Javad Ranjbar Kalahroodi, Amirhossein Sheikholselami, Sepehr Karimi, Sepideh Ranjbar Kalahroodi, Heshaam Faili, Azadeh Shakery",2025-06-03T00:22:37Z,PersianMedQA: Language-Centric Evaluation of LLMs in the Persian Medical   Domain,PersianMedQA: Sprach-Centric Evaluation von LLMs im persischen medizinischen Bereich,波斯梅德QA:波斯医学领域LLMs的语文中心评价,http://arxiv.org/abs/2506.00250v2
659,"Traditional Retrieval-Augmented Generation (RAG) pipelines rely on similarity-based retrieval and re-ranking, which depend on heuristics such as top-k, and lack explainability, interpretability, and robustness against adversarial content. To address this gap, we propose a novel method METEORA that replaces re-ranking in RAG with a rationale-driven selection approach. METEORA operates in two stages. First, a general-purpose LLM is preference-tuned to generate rationales conditioned on the input query using direct preference optimization. These rationales guide the evidence chunk selection engine, which selects relevant chunks in three stages: pairing individual rationales with corresponding retrieved chunks for local relevance, global selection with elbow detection for adaptive cutoff, and context expansion via neighboring chunks. This process eliminates the need for top-k heuristics. The rationales are also used for consistency check using a Verifier LLM to detect and filter poisoned or misleading content for safe generation. The framework provides explainable and interpretable evidence flow by using rationales consistently across both selection and verification. Our evaluation across six datasets spanning legal, financial, and academic research domains shows that METEORA improves generation accuracy by 33.34% while using approximately 50% fewer chunks than state-of-the-art re-ranking methods. In adversarial settings, METEORA significantly improves the F1 score from 0.10 to 0.44 over the state-of-the-art perplexity-based defense baseline, demonstrating strong resilience to poisoning attacks. Code available at: https://anonymous.4open.science/r/METEORA-DC46/README.md",,"Yash Saxena, Ankur Padia, Mandar S Chaudhary, Kalpa Gunaratna, Srinivasan Parthasarathy, Manas Gaur",2025-06-03T00:21:21Z,Ranking Free RAG: Replacing Re-ranking with Selection in RAG for   Sensitive Domains,Ranking Free RAG: Re-Ranking mit Auswahl in RAG für sensible Domains ersetzen,排名免费RAG:取代在RAG中为敏感域域选择的重新排名,http://arxiv.org/abs/2505.16014v3
660,"Toxicity in online content, including content generated by language models, has become a critical concern due to its potential for negative psychological and social impact. This paper introduces TRuST, a comprehensive dataset designed to improve toxicity detection that merges existing datasets, and has labels for toxicity, target social group, and toxic spans. It includes a diverse range of target groups such as ethnicity, gender, religion, disability, and politics, with both human/machine-annotated and human machine-generated data. We benchmark state-of-the-art large language models (LLMs) on toxicity detection, target group identification, and toxic span extraction. We find that fine-tuned models consistently outperform zero-shot and few-shot prompting, though performance remains low for certain social groups. Further, reasoning capabilities do not significantly improve performance, indicating that LLMs have weak social reasoning skills.",,"Berk Atil, Namrata Sureddy, Rebecca J. Passonneau",2025-06-02T23:48:16Z,Something Just Like TRuST : Toxicity Recognition of Span and Target,Etwas genau wie TRuST : Toxizitätserkennung von Span und Target,类似TRuST的东西: 承认斯潘和目标的毒性,http://arxiv.org/abs/2506.02326v1
661,"Authorship misattribution can have profound consequences in real life. In forensic settings simply being considered as one of the potential authors of an evidential piece of text or communication can result in undesirable scrutiny. This raises a fairness question: Is every author in the candidate pool at equal risk of misattribution? Standard evaluation measures for authorship attribution systems do not explicitly account for this notion of fairness. We introduce a simple measure, Misattribution Unfairness Index (MAUIk), which is based on how often authors are ranked in the top k for texts they did not write. Using this measure we quantify the unfairness of five models on two different datasets. All models exhibit high levels of unfairness with increased risks for some authors. Furthermore, we find that this unfairness relates to how the models embed the authors as vectors in the latent search space. In particular, we observe that the risk of misattribution is higher for authors closer to the centroid (or center) of the embedded authors in the haystack. These results indicate the potential for harm and the need for communicating with and calibrating end users on misattribution risk when building and providing such models for downstream use.",,"Pegah Alipoormolabashi, Ajay Patel, Niranjan Balasubramanian",2025-06-02T23:28:24Z,Quantifying Misattribution Unfairness in Authorship Attribution,Quantifizierung der Missachtung Unfairness in der Urheberschaft Attribution,作者归属不公的量化错误分配,http://arxiv.org/abs/2506.02321v1
662,"Large language models (LLMs) have shown promise in transforming machine learning research, yet their capability to faithfully implement novel ideas from recent research papers-ideas unseen during pretraining-remains unclear. We introduce ResearchCodeBench, a benchmark of 212 coding challenges that evaluates LLMs' ability to translate cutting-edge ML contributions from top 2024-2025 research papers into executable code. We assessed 30+ proprietary and open-source LLMs, finding that even the best models correctly implement less than 40% of the code. We find Gemini-2.5-Pro-Preview to perform best at 37.3% success rate, with O3 (High) and O4-mini (High) following behind at 32.3% and 30.8% respectively. We present empirical findings on performance comparison, contamination, and error patterns. By providing a rigorous and community-driven evaluation platform, ResearchCodeBench enables continuous understanding and advancement of LLM-driven innovation in research code generation.",,"Tianyu Hua, Harper Hua, Violet Xiang, Benjamin Klieger, Sang T. Truong, Weixin Liang, Fan-Yun Sun, Nick Haber",2025-06-02T23:04:12Z,ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine   Learning Research Code,ResearchCodeBench: Benchmarking LLMs zur Implementierung neuartiger Machine Learning Research Code,研究守则:确定执行新机器学习研究守则的衡量标准,http://arxiv.org/abs/2506.02314v1
663,"Large language models (LLMs) can explain grammatical rules, yet they often fail to apply those rules when judging sentence acceptability. We present ""grammar prompting"", an explain-then-process paradigm: a large LLM first produces a concise explanation of the relevant syntactic phenomenon, then that explanation is fed back as additional context to the target model -- either an LLM or a smaller language model (SLM) -- before deciding which sentence of a minimal pair is grammatical. On the English BLiMP, Chinese SLING, and Russian RuBLiMP benchmarks, this simple prompt design yields substantial improvements over strong baselines across many syntactic phenomena. Feeding an LLM's metalinguistic explanation back to the target model bridges the gap between knowing a rule and using it. On SLMs, grammar prompting alone trims the average LLM-SLM accuracy gap by about 20%, and when paired with chain-of-thought, by 56% (13.0 pp -> 5.8 pp), all at negligible cost. The lightweight, language-agnostic cue lets low-cost SLMs approach frontier-LLM performance in multilingual settings.",,"Russell Scheinberg, Ameeta Agrawal, Amber Shore, So Young Lee",2025-06-02T22:42:33Z,Explain-then-Process: Using Grammar Prompting to Enhance Grammatical   Acceptability Judgments,"Explain-then-Process: Grammar nutzen, um grammatische Annahmeurteile zu verbessern",解释 - 解释 - - - - 解释 - - - - - 程序:使用语法提示加强语法可接受性判决,http://arxiv.org/abs/2506.02302v1
664,"Large Action Models (LAMs) for AI Agents offer incredible potential but face challenges due to the need for high-quality training data, especially for multi-steps tasks that involve planning, executing tool calls, and responding to feedback. To address these issues, we present LAM SIMULATOR, a comprehensive framework designed for online exploration of agentic tasks with high-quality feedback. Our framework features a dynamic task query generator, an extensive collection of tools, and an interactive environment where Large Language Model (LLM) Agents can call tools and receive real-time feedback. This setup enables LLM Agents to explore and solve tasks autonomously, facilitating the discovery of multiple approaches to tackle any given task. The resulting action trajectory data are then used to create high-quality training datasets for LAMs. Our experiments on popular agentic benchmarks, ToolBench and CRMArena, highlight the effectiveness of LAM SIMULATOR: models trained with self-generated datasets using our framework achieve significant performance gains, up to a 49.3\% improvement over their original baselines. LAM SIMULATOR requires minimal human input during dataset creation, highlighting LAM SIMULATOR's efficiency and effectiveness in speeding up development of AI agents.",,"Thai Hoang, Kung-Hsiang Huang, Shirley Kokane, Jianguo Zhang, Zuxin Liu, Ming Zhu, Jake Grigsby, Tian Lan, Michael S Ryoo, Chien-Sheng Wu, Shelby Heinecke, Huan Wang, Silvio Savarese, Caiming Xiong, Juan Carlos Niebles",2025-06-02T22:36:02Z,LAM SIMULATOR: Advancing Data Generation for Large Action Model Training   via Online Exploration and Trajectory Feedback,LAM SIMULATOR: Weiterentwicklung der Datengenerierung für groß angelegte Modellschulungen über Online-Exploration und Trajektorie-Feedback,LAM SIMMULATOR:通过在线探索和轨迹反馈为大型行动示范培训推进数据生成,http://arxiv.org/abs/2506.02298v1
665,"While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systems' fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution - as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The source code for evaluating performance is available at https://github.com/WujiangXu/AgenticMemory, while the source code of agentic memory system is available at https://github.com/agiresearch/A-mem.",,"Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, Yongfeng Zhang",2025-06-02T22:21:21Z,A-MEM: Agentic Memory for LLM Agents,A-MEM: Agentischer Speicher für LLM-Agenten,A-MEM: LLM 剂的剂内存,http://arxiv.org/abs/2502.12110v9
666,"Dense retrieval models are commonly used in Information Retrieval (IR) applications, such as Retrieval-Augmented Generation (RAG). Since they often serve as the first step in these systems, their robustness is critical to avoid downstream failures. In this work, we repurpose a relation extraction dataset (e.g., Re-DocRED) to design controlled experiments that quantify the impact of heuristic biases, such as a preference for shorter documents, on retrievers like Dragon+ and Contriever. We uncover major vulnerabilities, showing retrievers favor shorter documents, early positions, repeated entities, and literal matches, all while ignoring the answer's presence! Notably, when multiple biases combine, models exhibit catastrophic performance degradation, selecting the answer-containing document in less than 10% of cases over a synthetic biased document without the answer. Furthermore, we show that these biases have direct consequences for downstream applications like RAG, where retrieval-preferred documents can mislead LLMs, resulting in a 34% performance drop than providing no documents at all. https://huggingface.co/datasets/mohsenfayyaz/ColDeR",,"Mohsen Fayyaz, Ali Modarressi, Hinrich Schuetze, Nanyun Peng",2025-06-02T22:19:43Z,"Collapse of Dense Retrievers: Short, Early, and Literal Biases   Outranking Factual Evidence","Zusammenbruch von Dense Retrievers: Kurze, frühe und literale Biasen übertreffen die tatsächlichen Beweise",大量重复的折叠:短、早、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利、利,http://arxiv.org/abs/2503.05037v2
667,"Multimodal Large Language Models (MLLMs) have serious security vulnerabilities.While safety alignment using multimodal datasets consisting of text and data of additional modalities can effectively enhance MLLM's security, it is costly to construct these datasets. Existing low-resource security alignment methods, including textual alignment, have been found to struggle with the security risks posed by additional modalities. To address this, we propose Synthetic Embedding augmented safety Alignment (SEA), which optimizes embeddings of additional modality through gradient updates to expand textual datasets. This enables multimodal safety alignment training even when only textual data is available. Extensive experiments on image, video, and audio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding on a single RTX3090 GPU within 24 seconds. SEA significantly improves the security of MLLMs when faced with threats from additional modalities. To assess the security risks introduced by video and audio, we also introduced a new benchmark called VA-SafetyBench. High attack success rates across multiple MLLMs validate its challenge. Our code and data will be available at https://github.com/ZeroNLP/SEA.",,"Weikai Lu, Hao Peng, Huiping Zhuang, Cen Chen, Ziqian Zeng",2025-06-02T21:55:31Z,SEA: Low-Resource Safety Alignment for Multimodal Large Language Models   via Synthetic Embeddings,SEA: Low-Resource-Sicherheitsausrichtung für multimodale große Sprachmodelle über synthetische Einbettungen,SEA:通过合成嵌入装置对多种多式大语言模型进行低资源安全协调,http://arxiv.org/abs/2502.12562v3
668,"This study examines the prosodic characteristics associated with winning and losing in post-match tennis interviews. Additionally, this research explores the potential to classify match outcomes solely based on post-match interview recordings using prosodic features and self-supervised learning (SSL) representations. By analyzing prosodic elements such as pitch and intensity, alongside SSL models like Wav2Vec 2.0 and HuBERT, the aim is to determine whether an athlete has won or lost their match. Traditional acoustic features and deep speech representations are extracted from the data, and machine learning classifiers are employed to distinguish between winning and losing players. Results indicate that SSL representations effectively differentiate between winning and losing outcomes, capturing subtle speech patterns linked to emotional states. At the same time, prosodic cues -- such as pitch variability -- remain strong indicators of victory.",,"Sofoklis Kakouros, Haoyu Chen",2025-06-02T21:45:39Z,Sounding Like a Winner? Prosodic Differences in Post-Match Interviews,Klingt wie ein Gewinner? Prosodische Unterschiede in Post-Match-Interviews,听起来像赢家?,http://arxiv.org/abs/2506.02283v1
669,"Retrieval-Augmented Generation (RAG) systems traditionally treat retrieval and generation as separate processes, requiring explicit textual queries to connect them. This separation can limit the ability of models to generalize across diverse tasks. In this work, we propose a query-free RAG system, named ImpRAG, which integrates retrieval and generation into a unified model. ImpRAG allows models to implicitly express their information needs, eliminating the need for human-specified queries. By dividing pretrained decoder-only language models into specialized layer groups, ImpRAG optimizes retrieval and generation tasks simultaneously. Our approach employs a two-stage inference process, using the same model parameters and forward pass for both retrieval and generation, thereby minimizing the disparity between retrievers and language models. Experiments on 8 knowledge-intensive tasks demonstrate that ImpRAG achieves 3.6-11.5 improvements in exact match scores on unseen tasks with diverse formats, highlighting its effectiveness in enabling models to articulate their own information needs and generalize across tasks. Our analysis underscores the importance of balancing retrieval and generation parameters and leveraging generation perplexities as retrieval training objectives for enhanced performance.",,"Wenzheng Zhang, Xi Victoria Lin, Karl Stratos, Wen-tau Yih, Mingda Chen",2025-06-02T21:38:21Z,ImpRAG: Retrieval-Augmented Generation with Implicit Queries,ImpRAG: Retrieval-Augmented Generation mit Impliziten Abfragen,ImpraG: 带隐性查询的回收- 原始一代,http://arxiv.org/abs/2506.02279v1
670,"The surge in online content has created an urgent demand for robust detection systems, especially in non-English contexts where current tools demonstrate significant limitations. We present forePLay, a novel Polish language dataset for erotic content detection, featuring over 24k annotated sentences with a multidimensional taxonomy encompassing ambiguity, violence, and social unacceptability dimensions. Our comprehensive evaluation demonstrates that specialized Polish language models achieve superior performance compared to multilingual alternatives, with transformer-based architectures showing particular strength in handling imbalanced categories. The dataset and accompanying analysis establish essential frameworks for developing linguistically-aware content moderation systems, while highlighting critical considerations for extending such capabilities to morphologically complex languages.",,"Anna Kołos, Katarzyna Lorenc, Emilia Wiśnios, Agnieszka Karlińska",2025-06-02T21:36:22Z,Behind Closed Words: Creating and Investigating the forePLay Annotated   Dataset for Polish Erotic Discourse,Hinter verschlossenen Wörtern: Erstellen und Untersuchen des Vorspiels Annotierter Datensatz für den polnischen Erotischen Diskurs,在封闭的单词背后:为波兰诗篇创建和调查前帕莱附加说明数据集,http://arxiv.org/abs/2412.17533v3
671,"Publicly significant images from events hold valuable contextual information, crucial for journalism and education. However, existing methods often struggle to extract this relevance accurately. To address this, we introduce GETReason (Geospatial Event Temporal Reasoning), a framework that moves beyond surface-level image descriptions to infer deeper contextual meaning. We propose that extracting global event, temporal, and geospatial information enhances understanding of an image's significance. Additionally, we introduce GREAT (Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric for evaluating reasoning-based image understanding. Our layered multi-agent approach, assessed using a reasoning-weighted metric, demonstrates that meaningful insights can be inferred, effectively linking images to their broader event context.",,"Shikhhar Siingh, Abhinav Rawat, Chitta Baral, Vivek Gupta",2025-06-02T21:33:47Z,GETReason: Enhancing Image Context Extraction through Hierarchical   Multi-Agent Reasoning,GETReason: Bildkontext-Extraktion durch Hierarchische Multi-Agenten-Reasoning verbessern,GetReason:通过等级式多机构代理理由加强图像背景采掘,http://arxiv.org/abs/2505.21863v3
672,"With the rapid scaling of large language models (LLMs), structured pruning has become a widely used technique to learn efficient, smaller models from larger ones, delivering superior performance compared to training similarly sized models from scratch. In this paper, we move beyond the traditional static pruning approach of determining a fixed pruning mask for a model, and propose a dynamic approach to structured pruning. In our method, the pruning mask is input-dependent and adapts dynamically based on the information described in a user instruction. Our approach, termed ""instruction-following pruning"", introduces a sparse mask predictor that takes the user instruction as input and dynamically selects the most relevant model parameters for the given task. To identify and activate effective parameters, we jointly optimize the sparse mask predictor and the LLM, leveraging both instruction-following data and the pre-training corpus. Experimental results demonstrate the effectiveness of our approach on a wide range of evaluation benchmarks. For example, our 3B activated model improves over the 3B dense model by 5-8 points of absolute margin on domains such as math and coding, and rivals the performance of a 9B model.",,"Bairu Hou, Qibin Chen, Jianyu Wang, Guoli Yin, Chong Wang, Nan Du, Ruoming Pang, Shiyu Chang, Tao Lei",2025-06-02T21:33:43Z,Instruction-Following Pruning for Large Language Models,Instruction-Following Pruning für große Sprachmodelle,"大型语文模式的遵循指示的 "" 留意 "" 方案",http://arxiv.org/abs/2501.02086v3
673,"It is often challenging to teach specialized, unseen tasks to dialogue systems due to the high cost of expert knowledge, training data, and high technical difficulty. To support domain-specific applications - such as law, medicine, or finance - it is essential to build frameworks that enable non-technical experts to define, test, and refine system behaviour with minimal effort. Achieving this requires cross-disciplinary collaboration between developers and domain specialists. In this work, we introduce a novel framework, CoDial (Code for Dialogue), that converts expert knowledge, represented as a novel structured heterogeneous graph, into executable conversation logic. CoDial can be easily implemented in existing guardrailing languages, such as Colang, to enable interpretable, modifiable, and true zero-shot specification of task-oriented dialogue systems. Empirically, CoDial achieves state-of-the-art performance on the STAR dataset for inference-based models and is competitive with similar baselines on the well-known MultiWOZ dataset. We also demonstrate CoDial's iterative improvement via manual and LLM-aided feedback, making it a practical tool for expert-guided alignment of LLMs in high-stakes domains.",,"Radin Shayanfar, Chu Fei Luo, Rohan Bhambhoria, Samuel Dahan, Xiaodan Zhu",2025-06-02T21:12:27Z,CoDial: Interpretable Task-Oriented Dialogue Systems Through Dialogue   Flow Alignment,CoDial: Verdolmetschbare aufgabenorientierte Dialogsysteme durch Dialogflussausrichtung,"CoDial: 通过对话流程协调,以任务为主的可解释对话系统",http://arxiv.org/abs/2506.02264v1
674,"Compositionality is believed to be fundamental to intelligence. In humans, it underlies the structure of thought, language, and higher-level reasoning. In AI, compositional representations can enable a powerful form of out-of-distribution generalization, in which a model systematically adapts to novel combinations of known concepts. However, while we have strong intuitions about what compositionality is, we lack satisfying formal definitions for it that are measurable and mathematical. Here, we propose such a definition, which we call representational compositionality, that accounts for and extends our intuitions about compositionality. The definition is conceptually simple, quantitative, grounded in algorithmic information theory, and applicable to any representation. Intuitively, representational compositionality states that a compositional representation satisfies three properties. First, it must be expressive. Second, it must be possible to re-describe the representation as a function of discrete symbolic sequences with re-combinable parts, analogous to sentences in natural language. Third, the function that relates these symbolic sequences to the representation, analogous to semantics in natural language, must be simple. Through experiments on both synthetic and real world data, we validate our definition of compositionality and show how it unifies disparate intuitions from across the literature in both AI and cognitive science. We also show that representational compositionality, while theoretically intractable, can be readily estimated using standard deep learning tools. We hope that our definition can inspire the design of novel, theoretically-driven models that better capture the mechanisms of compositional thought. We make our code available at https://github.com/EricElmoznino/complexity_compositionality.",,"Eric Elmoznino, Thomas Jiralerspong, Yoshua Bengio, Guillaume Lajoie",2025-06-02T21:11:54Z,A Complexity-Based Theory of Compositionality,Eine auf Komplexität basierende Theorie der Kompositionalität,复杂、基于复杂性的构成理论,http://arxiv.org/abs/2410.14817v5
675,"Speculative decoding relies on fast and accurate drafters. Recent state-of-the-art language models employ larger and larger vocabularies, which significantly slows down drafters. One promising approach to boost the efficiency of speculative decoding is to use drafters with smaller vocabularies. However, existing sampling methods cannot draw out-of-vocabulary tokens, creating a tradeoff between drafters' vocabulary size and acceptance rates. This paper introduces Redistributing Drafter Kernels (RDK), the first out-of-vocabulary sampler that effectively recovers acceptance rates by virtually restoring pruned target tokens. RDK leverages token-affinity priors to reallocate drafter mass towards high-overlap regions. We prove mathematically that RDK can achieve higher acceptance rates than vanilla and state-of-the-art samplers. We provide an efficient first-order approximation of RDK and prove that it reduces redistribution times from $O(N^2)$ to $O(N)$, enabling lightweight implementations for large vocabularies. Our experiments demonstrate that this linear-time RDK significantly boosts acceptance rates even after extreme pruning (removing more than 75% of the drafter's vocabulary), where existing samplers fail. RDK opens the door to extremely pruned drafters, which were previously impractical.",,"Nadav Timor, Jonathan Mamou, Oren Pereg, Hongyang Zhang, David Harel",2025-06-02T21:08:02Z,Out-of-Vocabulary Sampling Boosts Speculative Decoding,Out-of-Vocabulary Sampling steigert die spekulative Dekodierung,军用外抽样抽样促进 投机推推 推 推 推 价 定 值,http://arxiv.org/abs/2506.03206v1
676,"Whether large language models (LLMs) process language similarly to humans has been the subject of much theoretical and practical debate. We examine this question through the lens of the production-interpretation distinction found in human sentence processing and evaluate the extent to which instruction-tuned LLMs replicate this distinction. Using an empirically documented asymmetry between pronoun production and interpretation in humans for implicit causality verbs as a testbed, we find that some LLMs do quantitatively and qualitatively reflect human-like asymmetries between production and interpretation. We demonstrate that whether this behavior holds depends upon both model size-with larger models more likely to reflect human-like patterns and the choice of meta-linguistic prompts used to elicit the behavior. Our codes and results are available at https://github.com/LingMechLab/Production-Interpretation_Asymmetries_ACL2025.",,"Suet-Ying Lam, Qingcheng Zeng, Jingyi Wu, Rob Voigt",2025-06-02T21:07:33Z,Leveraging Human Production-Interpretation Asymmetries to Test LLM   Cognitive Plausibility,"Nutzung von Asymmetrien der menschlichen Produktion, um LLM Kognitive Plausibilität zu testen",利用人力生产 -- -- 解释不对称利用测试LLM 认知性可变性,http://arxiv.org/abs/2503.17579v2
677,"In emotion recognition from speech, a key challenge lies in identifying speech signal segments that carry the most relevant acoustic variations for discerning specific emotions. Traditional approaches compute functionals for features such as energy and F0 over entire sentences or longer speech portions, potentially missing essential fine-grained variation in the long-form statistics. This research investigates the use of word informativeness, derived from a pre-trained language model, to identify semantically important segments. Acoustic features are then computed exclusively for these identified segments, enhancing emotion recognition accuracy. The methodology utilizes standard acoustic prosodic features, their functionals, and self-supervised representations. Results indicate a notable improvement in recognition performance when features are computed on segments selected based on word informativeness, underscoring the effectiveness of this approach.",,Sofoklis Kakouros,2025-06-02T20:30:48Z,Investigating the Impact of Word Informativeness on Speech Emotion   Recognition,Untersuchung der Auswirkungen von Wortinformativität auf die Sprachemotionserkennung,调查文字信息对言语情感识别的影响,http://arxiv.org/abs/2506.02239v1
678,"A seminal paper published by Ledley and Lusted in 1959 introduced complex clinical diagnostic reasoning cases as the gold standard for the evaluation of expert medical computing systems, a standard that has held ever since. Here, we report the results of a physician evaluation of a large language model (LLM) on challenging clinical cases against a baseline of hundreds of physicians. We conduct five experiments to measure clinical reasoning across differential diagnosis generation, display of diagnostic reasoning, triage differential diagnosis, probabilistic reasoning, and management reasoning, all adjudicated by physician experts with validated psychometrics. We then report a real-world study comparing human expert and AI second opinions in randomly-selected patients in the emergency room of a major tertiary academic medical center in Boston, MA. We compared LLMs and board-certified physicians at three predefined diagnostic touchpoints: triage in the emergency room, initial evaluation by a physician, and admission to the hospital or intensive care unit. In all experiments--both vignettes and emergency room second opinions--the LLM displayed superhuman diagnostic and reasoning abilities, as well as continued improvement from prior generations of AI clinical decision support. Our study suggests that LLMs have achieved superhuman performance on general medical diagnostic and management reasoning, fulfilling the vision put forth by Ledley and Lusted, and motivating the urgent need for prospective trials.",,"Peter G. Brodeur, Thomas A. Buckley, Zahir Kanjee, Ethan Goh, Evelyn Bin Ling, Priyank Jain, Stephanie Cabral, Raja-Elie Abdulnour, Adrian D. Haimovich, Jason A. Freed, Andrew Olson, Daniel J. Morgan, Jason Hom, Robert Gallo, Liam G. McCoy, Haadi Mombini, Christopher Lucas, Misha Fotoohi, Matthew Gwiazdon, Daniele Restifo, Daniel Restrepo, Eric Horvitz, Jonathan Chen, Arjun K. Manrai, Adam Rodman",2025-06-02T20:29:39Z,Superhuman performance of a large language model on the reasoning tasks   of a physician,Übermenschliche Leistung eines großen Sprachmodells über die Argumentationsaufgaben eines Arztes,医生推理任务的大型语言模型超人表现,http://arxiv.org/abs/2412.10849v3
679,"Pathological examination of the placenta is an effective method for detecting and mitigating health risks associated with childbirth. Recent advancements in AI have enabled the use of photographs of the placenta and pathology reports for detecting and classifying signs of childbirth-related pathologies. However, existing automated methods are computationally extensive, which limits their deployability. We propose two modifications to vision-language contrastive learning (VLC) frameworks to enhance their accuracy and efficiency: (1) text-anchored vision-language contrastive knowledge distillation (VLCD)-a new knowledge distillation strategy for medical VLC pretraining, and (2) unsupervised predistillation using a large natural images dataset for improved initialization. Our approach distills efficient neural networks that match or surpass the teacher model in performance while achieving model compression and acceleration. Our results showcase the value of unsupervised predistillation in improving the performance and robustness of our approach, specifically for lower-quality images. VLCD serves as an effective way to improve the efficiency and deployability of medical VLC approaches, making AI-based healthcare solutions more accessible, especially in resource-constrained environments.",,"Manas Mehta, Yimu Pan, Kelly Gallagher, Alison D. Gernand, Jeffery A. Goldstein, Delia Mwinyelle, Leena Mithal, James Z. Wang",2025-06-02T20:12:27Z,VLCD: Vision-Language Contrastive Distillation for Accurate and   Efficient Automatic Placenta Analysis,VLCD: Vision-Language Kontrastive Destillation für eine präzise und effiziente automatische Plazentaanalyse,VLCD: 用于准确和高效自动胎盘分析的视觉-语言相抗争蒸馏法,http://arxiv.org/abs/2506.02229v1
680,"Natural Language Processing (NLP) has transformed various fields beyond linguistics by applying techniques originally developed for human language to the analysis of biological sequences. This review explores the application of NLP methods to biological sequence data, focusing on genomics, transcriptomics, and proteomics. We examine how various NLP methods, from classic approaches like word2vec to advanced models employing transformers and hyena operators, are being adapted to analyze DNA, RNA, protein sequences, and entire genomes. The review also examines tokenization strategies and model architectures, evaluating their strengths, limitations, and suitability for different biological tasks. We further cover recent advances in NLP applications for biological data, such as structure prediction, gene expression, and evolutionary analysis, highlighting the potential of these methods for extracting meaningful insights from large-scale genomic data. As language models continue to advance, their integration into bioinformatics holds immense promise for advancing our understanding of biological processes in all domains of life.",,"Ella Rannon, David Burstein",2025-06-02T19:54:03Z,"Leveraging Natural Language Processing to Unravel the Mystery of Life: A   Review of NLP Approaches in Genomics, Transcriptomics, and Proteomics","Nutzung natürlicher Sprachverarbeitung, um das Mysterium des Lebens zu entschlüsseln: Eine Überprüfung der NLP-Ansätze in Genomik, Transkriptomik und Proteomik",利用自然语言处理来解除生命的神秘:审查NLP在基因组学、转基因学和蛋白质组学方面的做法,http://arxiv.org/abs/2506.02212v1
681,"Many commercial Large Language Models (LLMs) are often closed-source, limiting developers to prompt tuning for aligning content generation with specific applications. While these models currently do not provide access to token logits, we argue that if such access were available, it would enable more powerful adaptation techniques beyond prompt engineering. In this paper, we propose a token-level probability reweighting framework that, given access to logits and a small amount of task-specific data, can effectively steer black-box LLMs toward application-specific content generation. Our approach views next-token prediction through the lens of supervised classification. We show that aligning black-box LLMs with task-specific data can be formulated as a label noise correction problem, leading to Plugin model -- an autoregressive probability reweighting model that operates solely on logits. We provide theoretical justification for why reweighting logits alone is sufficient for task adaptation. Extensive experiments with multiple datasets, LLMs, and reweighting models demonstrate the effectiveness of our method, advocating for broader access to token logits in closed-source models.",,"Gaurush Hiranandani, Haolun Wu, Subhojyoti Mukherjee, Sanmi Koyejo",2025-06-02T19:53:46Z,Logits are All We Need to Adapt Closed Models,"Logits sind alles, was wir brauchen, um geschlossene Modelle anzupassen","只需登录即可,我们只需调整已关闭的模型",http://arxiv.org/abs/2502.06806v3
682,"Large language models (LLMs) showcase increasingly impressive English benchmark scores, however their performance profiles remain inconsistent across multilingual settings. To address this gap, we introduce PolyPrompt, a novel, parameter-efficient framework for enhancing the multilingual capabilities of LLMs. Our method learns a set of trigger tokens for each language through a gradient-based search, identifying the input query's language and selecting the corresponding trigger tokens which are prepended to the prompt during inference. We perform experiments on two ~1 billion parameter models, with evaluations on the global MMLU benchmark across fifteen typologically and resource diverse languages, demonstrating accuracy gains of 3.7%-19.9% compared to naive and translation-pipeline baselines.",,Nathan Roll,2025-06-02T19:50:52Z,PolyPrompt: Automating Knowledge Extraction from Multilingual Language   Models with Dynamic Prompt Generation,PolyPrompt: Automatisierung der Wissensextraktion aus mehrsprachigen Sprachmodellen mit dynamischer Prompt-Generation,"PolyPropt: 自动从多语言语言模式中获取知识,并有动态快速生成",http://arxiv.org/abs/2502.19756v2
683,"Large language models (LLMs) are powerful tools capable of handling diverse tasks. Comparing and selecting appropriate LLMs for specific tasks requires systematic evaluation methods, as models exhibit varying capabilities across different domains. However, finding suitable benchmarks is difficult given the many available options. This complexity not only increases the risk of benchmark misuse and misinterpretation but also demands substantial effort from LLM users, seeking the most suitable benchmarks for their specific needs. To address these issues, we introduce \texttt{BenchmarkCards}, an intuitive and validated documentation framework that standardizes critical benchmark attributes such as objectives, methodologies, data sources, and limitations. Through user studies involving benchmark creators and users, we show that \texttt{BenchmarkCards} can simplify benchmark selection and enhance transparency, facilitating informed decision-making in evaluating LLMs. Data & Code: https://github.com/SokolAnn/BenchmarkCards",,"Anna Sokol, Elizabeth Daly, Michael Hind, David Piorkowski, Xiangliang Zhang, Nuno Moniz, Nitesh Chawla",2025-06-02T19:50:17Z,BenchmarkCards: Standardized Documentation for Large Language Model   Benchmarks,BenchmarkCards: Standardisierte Dokumentation für große Sprachmodell-Benchmarks,基准索引:大语文示范基准标准文件,http://arxiv.org/abs/2410.12974v3
684,"Query-driven recommendation with unknown items poses a challenge for users to understand why certain items are appropriate for their needs. Query-driven Contrastive Summarization (QCS) is a methodology designed to address this issue by leveraging language-based item descriptions to clarify contrasts between them. However, existing state-of-the-art contrastive summarization methods such as STRUM-LLM fall short of this goal. To overcome these limitations, we introduce Q-STRUM Debate, a novel extension of STRUM-LLM that employs debate-style prompting to generate focused and contrastive summarizations of item aspects relevant to a query. Leveraging modern large language models (LLMs) as powerful tools for generating debates, Q-STRUM Debate provides enhanced contrastive summaries. Experiments across three datasets demonstrate that Q-STRUM Debate yields significant performance improvements over existing methods on key contrastive summarization criteria, thus introducing a novel and performant debate prompting methodology for QCS.",,"George-Kirollos Saad, Scott Sanner",2025-06-02T19:47:49Z,Q-STRUM Debate: Query-Driven Contrastive Summarization for   Recommendation Comparison,Q-STRUM-Debatte: Query-Driven Contrastive Zusammenfassung für Empfehlungsvergleich,Q-STUU辩论:建议比较的问答式矛盾之处摘要,http://arxiv.org/abs/2502.12921v2
685,"Recent advances in large language model (LLM) post-training have leveraged two distinct paradigms to enhance reasoning capabilities: reinforcement learning (RL) and knowledge distillation (KD). While RL enables the emergence of complex reasoning behaviors, it often suffers from low sample efficiency when the initial policy struggles to explore high-reward trajectories. Conversely, KD improves learning efficiency via mimicking the teacher model but tends to generalize poorly to out-of-domain scenarios. In this work, we present \textbf{KDRL}, a \textit{unified post-training framework} that jointly optimizes a reasoning model through teacher supervision (KD) and self-exploration (RL). Specifically, KDRL leverages policy gradient optimization to simultaneously minimize the reverse Kullback-Leibler divergence (RKL) between the student and teacher distributions while maximizing the expected rule-based rewards. We first formulate a unified objective that integrates GRPO and KD, and systematically explore how different KL approximations, KL coefficients, and reward-guided KD strategies affect the overall post-training dynamics and performance. Empirical results on multiple reasoning benchmarks demonstrate that KDRL outperforms GRPO and various KD baselines while achieving a favorable balance between performance and reasoning token efficiency. These findings indicate that integrating KD and RL serves as an effective and efficient strategy to train reasoning LLMs.",,"Hongling Xu, Qi Zhu, Heyuan Deng, Jinpeng Li, Lu Hou, Yasheng Wang, Lifeng Shang, Ruifeng Xu, Fei Mi",2025-06-02T19:46:41Z,KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation   and Reinforcement Learning,KDRL: LLMs nach dem Training über Unified Knowledge Destillation und Verstärkungslernen,"KDRL: 通过统一知识蒸馏和强化学习,培训后助学助学课程",http://arxiv.org/abs/2506.02208v1
686,"Language model evaluation is a daunting task: prompts are brittle, corpus-level perplexities are vague, and the choice of benchmarks are endless. Finding examples that show meaningful, generalizable differences between two LMs is crucial to understanding where one model succeeds and another fails. Can this process be done automatically? In this work, we propose methodology for automated comparison of language models that uses performance-aware contextual embeddings to find fine-grained features of text where one LM outperforms another. Our method, which we name BehaviorBox, extracts coherent features that demonstrate differences with respect to the ease of generation between two LMs. Specifically, BehaviorBox finds features that describe groups of words in fine-grained contexts, such as ""conditional 'were' in the phrase 'if you were'"" and ""exclamation marks after emotional statements"", where one model outperforms another within a particular datatset. We apply BehaviorBox to compare models that vary in size, model family, and post-training, and enumerate insights into specific contexts that illustrate meaningful differences in performance which cannot be found by measures such as corpus-level perplexity alone.",,"Lindia Tjuatja, Graham Neubig",2025-06-02T19:44:06Z,BehaviorBox: Automated Discovery of Fine-Grained Performance Differences   Between Language Models,VerhaltenBox: Automatisierte Entdeckung von feinkörnigen Leistungsunterschieden zwischen Sprachmodellen,行为框:语言模型之间优异性能差异的自动发现,http://arxiv.org/abs/2506.02204v1
687,"Common subword tokenization algorithms like BPE and UnigramLM assume that text can be split into meaningful units by concatenative measures alone. This is not true for languages such as Hebrew and Arabic, where morphology is encoded in root-template patterns, or Malay and Georgian, where split affixes are common. We present SPLINTER, a pre-processing step which rearranges text into a linear form that better represents such nonconcatenative morphologies, enabling meaningful contiguous segments to be found by the tokenizer. We demonstrate SPLINTER's merit using both intrinsic measures evaluating token vocabularies in Hebrew, Arabic, and Malay; as well as on downstream tasks using BERT-architecture models trained for Hebrew.",,"Bar Gazit, Shaltiel Shmidman, Avi Shmidman, Yuval Pinter",2025-06-02T19:43:46Z,Splintering Nonconcatenative Languages for Better Tokenization,Splintering Nichtkonkatenative Sprachen für eine bessere Tokenisierung,"模拟使用非语言,以更好地使用地名",http://arxiv.org/abs/2503.14433v2
688,"Scaling laws predict the loss of a target machine learning model by extrapolating from easier-to-train models with fewer parameters or smaller training sets. This provides an efficient way for practitioners and researchers alike to compare pretraining decisions involving optimizers, datasets, and model architectures. Despite the widespread use of scaling laws to model the dynamics of language model training, there has been little work on understanding how to best estimate and interpret them. We collect (and release) a large-scale dataset containing losses and downstream evaluations for 485 previously published pretrained models. We use these to estimate more than 1000 scaling laws, then derive a set of best practices for estimating scaling laws in new model families. We find that fitting scaling laws to intermediate checkpoints of training runs (and not just their final losses) substantially improves accuracy, and that -- all else equal -- estimates of performance are generally most accurate when derived from other models of similar sizes. However, because there is a significant degree of variability across model seeds, training multiple small models is sometimes more useful than training a single large one. Moreover, while different model families differ scaling behavior, they are often similar enough that a target model's behavior can be predicted from a single model with the same architecture, along with scaling parameter estimates derived from other model families.",,"Leshem Choshen, Yang Zhang, Jacob Andreas",2025-06-02T19:33:41Z,A Hitchhiker's Guide to Scaling Law Estimation,Ein Hitchhiker-Führer für die Schätzung des Skalierungsrechts,希希克人关于扩大法律估计比例的指南,http://arxiv.org/abs/2410.11840v2
689,"Recent advances in large language models, particularly following GPT-4o, have sparked increasing interest in developing omni-modal models capable of understanding more modalities. While some open-source alternatives have emerged, there is still a notable lag behind specialized single-modality models in performance. In this paper, we present Ola, an Omni-modal Language model that achieves competitive performance across image, video, and audio understanding compared to specialized counterparts, pushing the frontiers of the omni-modal language model to a large extent. We conduct a comprehensive exploration of architectural design, data curation, and training strategies essential for building a robust omni-modal model. Ola incorporates advanced visual understanding and audio recognition capabilities through several critical and effective improvements over mainstream baselines. Moreover, we rethink inter-modal relationships during omni-modal training, emphasizing cross-modal alignment with video as a central bridge, and propose a progressive training pipeline that begins with the most distinct modalities and gradually moves towards closer modality alignment. Extensive experiments demonstrate that Ola surpasses existing open omni-modal LLMs across all modalities while achieving highly competitive performance compared to state-of-the-art specialized models of similar sizes. We aim to make Ola a fully open omni-modal understanding solution to advance future research in this emerging field. Model weights, code, and data are open-sourced at https://github.com/Ola-Omni/Ola.",,"Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao",2025-06-02T19:33:24Z,Ola: Pushing the Frontiers of Omni-Modal Language Model,Ola: Die Grenzen des Omni-Modalen-Sprachmodells schieben,奥拉:推进全语-多模式语言模式的前沿模式,http://arxiv.org/abs/2502.04328v3
690,"This paper introduces UnSeenTimeQA, a novel data contamination-free time-sensitive question-answering (TSQA) benchmark. It differs from existing TSQA benchmarks by avoiding web-searchable queries grounded in the real world. We present a series of time-sensitive event scenarios based on synthetically generated facts. It requires large language models (LLMs) to engage in genuine temporal reasoning without depending on the factual knowledge acquired during the pre-training phase. Our data generation framework enables on-demand generation of new samples, mitigating the risk of data leakage. We designed three types of time-sensitive questions to test LLMs' temporal reasoning abilities over sequential and parallel event occurrences. Our evaluation of five LLMs on synthetic fact-based TSQA reveals mixed results: while they perform well on simpler subsets, their overall performance remains inferior as compared to real world fact-based TSQA. Error analysis indicates that LLMs face difficulties in reasoning over long-range event dependencies and parallel events.",,"Md Nayem Uddin, Amir Saeidi, Divij Handa, Agastya Seth, Tran Cao Son, Eduardo Blanco, Steven R. Corman, Chitta Baral",2025-06-02T19:32:25Z,UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs'   Memorization,UnSeenTimeQA: Zeitsensible Frage-Antworten über die Erinnerung an LLMs hinaus,不可见时间QA:除LLMs'记忆化之外的时间敏感问题解答,http://arxiv.org/abs/2407.03525v4
691,"Speculative decoding is widely adopted to reduce latency in large language model (LLM) inference by leveraging smaller draft models capable of handling diverse user tasks. However, emerging AI applications, such as LLM-based agents, present unique workload characteristics: instead of diverse independent requests, agentic frameworks typically submit repetitive inference requests, such as multi-agent pipelines performing similar subtasks or self-refinement loops iteratively enhancing outputs. These workloads result in long and highly predictable sequences, which current speculative decoding methods do not effectively exploit. To address this gap, we introduce \emph{SuffixDecoding}, a novel method that utilizes efficient suffix trees to cache long token sequences from prompts and previous outputs. By adaptively speculating more tokens when acceptance likelihood is high and fewer when it is low, SuffixDecoding effectively exploits opportunities for longer speculations while conserving computation when those opportunities are limited. Evaluations on agentic benchmarks, including SWE-Bench and Text-to-SQL, demonstrate that SuffixDecoding achieves speedups of up to 5.3$\times$, outperforming state-of-the-art methods -- 2.8$\times$ faster than model-based approaches like EAGLE-2/3 and 1.9$\times$ faster than model-free approaches such as Token Recycling. SuffixDecoding is open-sourced at https://github.com/snowflakedb/ArcticInference.",,"Gabriele Oliaro, Zhihao Jia, Daniel Campos, Aurick Qiao",2025-06-02T19:27:12Z,SuffixDecoding: Extreme Speculative Decoding for Emerging AI   Applications,SuffixDecoding: Extreme spekulative Dekodierung für neu auftretende KI-Anwendungen,后缀值:新出现的AI型应用的极端投机代号,http://arxiv.org/abs/2411.04975v2
692,"Large language models (LLMs) are increasingly used for data generation. However, creating evaluation benchmarks raises the bar for this emerging paradigm. Benchmarks must target specific phenomena, penalize exploiting shortcuts, and be challenging. Through two case studies, we investigate whether LLMs can meet these demands by generating reasoning over-text benchmarks and comparing them to those created through careful crowdsourcing. Specifically, we evaluate both the validity and difficulty of LLM-generated versions of two high-quality reading comprehension datasets: CondaQA, which evaluates reasoning about negation, and DROP, which targets reasoning about quantities. We find that prompting LLMs can produce variants of these datasets that are often valid according to the annotation guidelines, at a fraction of the cost of the original crowdsourcing effort. However, we show that they are less challenging for LLMs than their human-authored counterparts. This finding sheds light on what may have been lost by generating evaluation data with LLMs, and calls for critically reassessing the immediate use of this increasingly prevalent approach to benchmark creation.",,"Alexander Gill, Abhilasha Ravichander, Ana Marasović",2025-06-02T19:12:19Z,What Has Been Lost with Synthetic Evaluation?,Was wurde mit synthetischer Bewertung verloren?,合成评价失去了什么?,http://arxiv.org/abs/2505.22830v2
693,"In spoken communication, information is transmitted not only via words, but also through a rich array of non-verbal signals, including prosody--the non-segmental auditory features of speech. Do these different communication channels carry distinct information? Prior work has shown that the information carried by prosodic features is substantially redundant with that carried by the surrounding words. Here, we systematically examine the time scale of this relationship, studying how it varies with the length of past and future contexts. We find that a word's prosodic features require an extended past context (3-8 words across different features) to be reliably predicted. Given that long-scale contextual information decays in memory, prosody may facilitate communication by adding information that is locally unique. We also find that a word's prosodic features show some redundancy with future words, but only with a short scale of 1-2 words, consistent with reports of incremental short-term planning in language production. Thus, prosody may facilitate communication by helping listeners predict upcoming material. In tandem, our results highlight potentially distinct roles that prosody plays in facilitating integration of words into past contexts and in helping predict upcoming words.",,"Tamar I. Regev, Chiebuka Ohams, Shaylee Xie, Lukas Wolf, Evelina Fedorenko, Alex Warstadt, Ethan G. Wilcox, Tiago Pimentel",2025-06-02T19:12:00Z,The time scale of redundancy between prosody and linguistic context,Die Zeitskala der Redundanz zwischen Prosodie und sprachlichem Kontext,手动和语言背景之间的冗余时间尺度,http://arxiv.org/abs/2503.11630v3
694,"In this paper, we introduce the Akan Conversation Emotion (ACE) dataset, the first multimodal emotion dialogue dataset for an African language, addressing the significant lack of resources for low-resource languages in emotion recognition research. ACE, developed for the Akan language, contains 385 emotion-labeled dialogues and 6,162 utterances across audio, visual, and textual modalities, along with word-level prosodic prominence annotations. The presence of prosodic labels in this dataset also makes it the first prosodically annotated African language dataset. We demonstrate the quality and utility of ACE through experiments using state-of-the-art emotion recognition methods, establishing solid baselines for future research. We hope ACE inspires further work on inclusive, linguistically and culturally diverse NLP resources.",,"David Sasu, Zehui Wu, Ziwei Gong, Run Chen, Pengyuan Shi, Lin Ai, Julia Hirschberg, Natalie Schluter",2025-06-02T19:11:29Z,Akan Cinematic Emotions (ACE): A Multimodal Multi-party Dataset for   Emotion Recognition in Movie Dialogues,Akan Cinematic Emotions (ACE): Ein multimodaler Multi-Party-Datensatz zur Emotionserkennung in Filmdialogen,Akan电影情感(ACE):电影对话中承认情感的多党多党多模式数据集,http://arxiv.org/abs/2502.10973v3
695,"Despite significant advances in ASR, the specific acoustic cues models rely on remain unclear. Prior studies have examined such cues on a limited set of phonemes and outdated models. In this work, we apply a feature attribution technique to identify the relevant acoustic cues for a modern Conformer-based ASR system. By analyzing plosives, fricatives, and vowels, we assess how feature attributions align with their acoustic properties in the time and frequency domains, also essential for human speech perception. Our findings show that the ASR model relies on vowels' full time spans, particularly their first two formants, with greater saliency in male speech. It also better captures the spectral characteristics of sibilant fricatives than non-sibilants and prioritizes the release phase in plosives, especially burst characteristics. These insights enhance the interpretability of ASR models and highlight areas for future research to uncover potential gaps in model robustness.",,"Dennis Fucci, Marco Gaido, Matteo Negri, Mauro Cettolo, Luisa Bentivogli",2025-06-02T19:11:16Z,Echoes of Phonetics: Unveiling Relevant Acoustic Cues for ASR via   Feature Attribution,Echos der Phonetik: Enthüllen relevanter akustischer Queues für ASR über Feature Attribution,语音回声:通过特写为ASR,http://arxiv.org/abs/2506.02181v1
696,"Audio-Visual Speech Recognition (AVSR) offers a robust solution for speech recognition in challenging environments, such as cocktail-party scenarios, where relying solely on audio proves insufficient. However, current AVSR models are often optimized for idealized scenarios with consistently active speakers, overlooking the complexities of real-world settings that include both speaking and silent facial segments. This study addresses this gap by introducing a novel audio-visual cocktail-party dataset designed to benchmark current AVSR systems and highlight the limitations of prior approaches in realistic noisy conditions. Additionally, we contribute a 1526-hour AVSR dataset comprising both talking-face and silent-face segments, enabling significant performance gains in cocktail-party environments. Our approach reduces WER by 67% relative to the state-of-the-art, reducing WER from 119% to 39.2% in extreme noise, without relying on explicit segmentation cues.",,"Thai-Binh Nguyen, Ngoc-Quan Pham, Alexander Waibel",2025-06-02T19:07:51Z,Cocktail-Party Audio-Visual Speech Recognition,Cocktail-Party Audio-Visuelle Spracherkennung,视听语言承认,http://arxiv.org/abs/2506.02178v1
697,"As AI grows more powerful, it will increasingly shape how we understand the world. But with this influence comes the risk of amplifying misinformation and deepening social divides-especially on consequential topics like public health where factual accuracy directly impacts well-being. Scalable Oversight aims to ensure AI truthfulness by enabling humans to supervise systems that may exceed human capabilities--yet humans themselves hold different beliefs and biases that impair their judgment. We study whether AI debate can guide biased judges toward the truth by having two AI systems debate opposing sides of controversial COVID-19 factuality claims where people hold strong prior beliefs. We conduct two studies: one with human judges holding either mainstream or skeptical beliefs evaluating factuality claims through AI-assisted debate or consultancy protocols, and a second examining the same problem with personalized AI judges designed to mimic these different human belief systems. In our human study, we find that debate-where two AI advisor systems present opposing evidence-based arguments-consistently improves judgment accuracy and confidence calibration, outperforming consultancy with a single-advisor system by 10% overall. The improvement is most significant for judges with mainstream beliefs (+15.2% accuracy), though debate also helps skeptical judges who initially misjudge claims move toward accurate views (+4.7% accuracy). In our AI judge study, we find that AI judges with human-like personas achieve even higher accuracy (78.5%) than human judges (70.1%) and default AI judges without personas (69.8%), suggesting their potential for supervising frontier AI models. These findings highlight AI debate as a promising path toward scalable, bias-resilient oversight--leveraging both diverse human and AI judgments to move closer to truth in contested domains.",,"Salman Rahman, Sheriff Issaka, Ashima Suvarna, Genglin Liu, James Shiffer, Jaeyoung Lee, Md Rizwan Parvez, Hamid Palangi, Shi Feng, Nanyun Peng, Yejin Choi, Julian Michael, Liwei Jiang, Saadia Gabriel",2025-06-02T19:01:53Z,AI Debate Aids Assessment of Controversial Claims,KI-Debatte Beihilfen Bewertung kontroverser Ansprüche,AI 辩论援助对有争议的索赔要求的评估,http://arxiv.org/abs/2506.02175v1
698,"Large Language Models (LLMs) are vulnerable to jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires a time-consuming search for adversarial prompts, whereas automatic adversarial prompt generation often leads to semantically meaningless attacks that do not scale well. In this paper, we present a novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds. AdvPrompter, which is trained using an alternating optimization algorithm, generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show highly competitive results on the AdvBench and HarmBench datasets, that also transfer to closed-source black-box LLMs. We also show that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.",,"Anselm Paulus, Arman Zharmagambetov, Chuan Guo, Brandon Amos, Yuandong Tian",2025-06-02T18:59:01Z,AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs,AdvPrompter: Schnelle adaptive Adversarial Prompting für LLMs,AdvPrompter: 快速适应性抗反转录病毒刺激LLMs,http://arxiv.org/abs/2404.16873v2
699,"Recent studies on interpreting the hidden states of speech models have shown their ability to capture speaker-specific features, including gender. Does this finding also hold for speech translation (ST) models? If so, what are the implications for the speaker's gender assignment in translation? We address these questions from an interpretability perspective, using probing methods to assess gender encoding across diverse ST models. Results on three language directions (English-French/Italian/Spanish) indicate that while traditional encoder-decoder models capture gender information, newer architectures -- integrating a speech encoder with a machine translation system via adapters -- do not. We also demonstrate that low gender encoding capabilities result in systems' tendency toward a masculine default, a translation bias that is more pronounced in newer architectures.",,"Dennis Fucci, Marco Gaido, Matteo Negri, Luisa Bentivogli, Andre Martins, Giuseppe Attanasio",2025-06-02T18:58:41Z,Different Speech Translation Models Encode and Translate Speaker Gender   Differently,Verschiedene Sprachübersetzungsmodelle Kodieren und Übersetzen Sprecher Geschlecht unterschiedlich,不同的发言翻译模式 不同的语音翻译模式 编码和翻译 不同性别的演讲者,http://arxiv.org/abs/2506.02172v1
700,"This research addresses the issue of missing structured data in dental records by extracting diagnostic information from unstructured text. The updated periodontology classification system's complexity has increased incomplete or missing structured diagnoses. To tackle this, we use advanced AI and NLP methods, leveraging GPT-4 to generate synthetic notes for fine-tuning a RoBERTa model. This significantly enhances the model's ability to understand medical and dental language. We evaluated the model using 120 randomly selected clinical notes from two datasets, demonstrating its improved diagnostic extraction accuracy. The results showed high accuracy in diagnosing periodontal status, stage, and grade, with Site 1 scoring 0.99 and Site 2 scoring 0.98. In the subtype category, Site 2 achieved perfect scores, outperforming Site 1. This method enhances extraction accuracy and broadens its use across dental contexts. The study underscores AI and NLP's transformative impact on healthcare delivery and management. Integrating AI and NLP technologies enhances documentation and simplifies administrative tasks by precisely extracting complex clinical information. This approach effectively addresses challenges in dental diagnostics. Using synthetic training data from LLMs optimizes the training process, improving accuracy and efficiency in identifying periodontal diagnoses from clinical notes. This innovative method holds promise for broader healthcare applications, potentially improving patient care quality.",,"Yao-Shun Chuang, Chun-Teh Lee, Oluwabunmi Tokede, Guo-Hao Lin, Ryan Brandon, Trung Duong Tran, Xiaoqian Jiang, Muhammad F. Walji",2025-06-02T18:52:22Z,Cross-Institutional Dental EHR Entity Extraction via Generative AI and   Synthetic Notes,institutsübergreifende Dental-EHR-Entitätsextraktion über Generative KI und Synthetische Anmerkungen,通过生成性人工智能和合成笔记提取的跨机构间牙科,http://arxiv.org/abs/2407.21050v3
701,"This research aims to develop a dynamic and scalable framework to facilitate harmonization of Common Data Elements (CDEs) across heterogeneous biomedical datasets by addressing challenges such as semantic heterogeneity, structural variability, and context dependence to streamline integration, enhance interoperability, and accelerate scientific discovery. Our methodology leverages Large Language Models (LLMs) for context-aware text embeddings that convert CDEs into dense vectors capturing semantic relationships and patterns. These embeddings are clustered using Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) to group semantically similar CDEs. The framework incorporates four key steps: (1) LLM-based text embedding to mathematically represent semantic context, (2) unsupervised clustering of embeddings via HDBSCAN, (3) automated labeling using LLM summarization, and (4) supervised learning to train a classifier assigning new or unclustered CDEs to labeled clusters. Evaluated on the NIH NLM CDE Repository with over 24,000 CDEs, the system identified 118 meaningful clusters at an optimized minimum cluster size of 20. The classifier achieved 90.46 percent overall accuracy, performing best in larger categories. External validation against Gravity Projects Social Determinants of Health domains showed strong agreement (Adjusted Rand Index 0.52, Normalized Mutual Information 0.78), indicating that embeddings effectively capture cluster characteristics. This adaptable and scalable approach offers a practical solution to CDE harmonization, improving selection efficiency and supporting ongoing data interoperability.",,"Madan Krishnamurthy, Daniel Korn, Melissa A Haendel, Christopher J Mungall, Anne E Thessen",2025-06-02T18:43:37Z,A Dynamic Framework for Semantic Grouping of Common Data Elements (CDE)   Using Embeddings and Clustering,Ein dynamisches Framework für die semantische Gruppierung von Common Data Elements (CDE) mit Einbettungen und Clustering,使用嵌入和集群的通用数据要素(CDE)语义组化动态框架,http://arxiv.org/abs/2506.02160v1
702,"Neural transducers (NT) provide an effective framework for speech streaming, demonstrating strong performance in automatic speech recognition (ASR). However, the application of NT to speech translation (ST) remains challenging, as existing approaches struggle with word reordering and performance degradation when jointly modeling ASR and ST, resulting in a gap with attention-based encoder-decoder (AED) models. Existing NT-based ST approaches also suffer from high computational training costs. To address these issues, we propose HENT-SRT (Hierarchical Efficient Neural Transducer for Speech Recognition and Translation), a novel framework that factorizes ASR and translation tasks to better handle reordering. To ensure robust ST while preserving ASR performance, we use self-distillation with CTC consistency regularization. Moreover, we improve computational efficiency by incorporating best practices from ASR transducers, including a down-sampled hierarchical encoder, a stateless predictor, and a pruned transducer loss to reduce training complexity. Finally, we introduce a blank penalty during decoding, reducing deletions and improving translation quality. Our approach is evaluated on three conversational datasets Arabic, Spanish, and Mandarin achieving new state-of-the-art performance among NT models and substantially narrowing the gap with AED-based systems.",,"Amir Hussein, Cihan Xiao, Matthew Wiesner, Dan Povey, Leibny Paola Garcia, Sanjeev Khudanpur",2025-06-02T18:37:50Z,HENT-SRT: Hierarchical Efficient Neural Transducer with   Self-Distillation for Joint Speech Recognition and Translation,HENT-SRT: Hierarchischer effizienter Neuronaltransducer mit Selbstdestillation für gemeinsame Spracherkennung und Übersetzung,HENT-SRT: 具有联合语音识别和翻译自学能力的等级高效神经传感器,http://arxiv.org/abs/2506.02157v1
703,"Recent studies have shown that Large Vision-Language Models (VLMs) tend to neglect image content and over-rely on language-model priors, resulting in errors in visually grounded tasks and hallucinations. We hypothesize that this issue arises because existing VLMs are not explicitly trained to generate texts that are accurately grounded in fine-grained image details. To enhance visual feedback during VLM training, we propose S-VCO (Symmetrical Visual Contrastive Optimization), a novel finetuning objective that steers the model toward capturing important visual details and aligning them with corresponding text tokens. To further facilitate this detailed alignment, we introduce MVC, a paired image-text dataset built by automatically filtering and augmenting visual counterfactual data to challenge the model with hard contrastive cases involving Minimal Visual Contrasts. Experiments show that our method consistently improves VLM performance across diverse benchmarks covering various abilities and domains, achieving up to a 22% reduction in hallucinations, and significant gains in vision-centric and general tasks. Notably, these improvements become increasingly pronounced in benchmarks with higher visual dependency. In short, S-VCO offers a significant enhancement of VLM's visually-dependent task performance while retaining or even improving the model's general abilities. We opensource our code at https://s-vco.github.io/",,"Shengguang Wu, Fan-Yun Sun, Kaiyue Wen, Nick Haber",2025-06-02T18:36:08Z,Symmetrical Visual Contrastive Optimization: Aligning Vision-Language   Models with Minimal Contrastive Images,Symmetrische visuelle Kontrast-Optimierung: Ausrichten von Vision-Sprachenmodellen mit minimalen Kontrastbildern,对称视觉对比优化:将视觉-语言模型与最小对比图像相匹配,http://arxiv.org/abs/2502.13928v2
704,"We introduce a scaling law for fine-tuning large language models (LLMs) under fixed compute budgets that explicitly accounts for data composition. Conventional approaches measure training data solely by total tokens, yet the number of examples and their average token length -- what we term \emph{dataset volume} -- play a decisive role in model performance. Our formulation is tuned following established procedures. Experiments on the BRICC dataset \cite{salavati2024reducing} and subsets of the MMLU dataset \cite{hendrycks2021measuringmassivemultitasklanguage}, evaluated under multiple subsampling strategies, reveal that data composition significantly affects token efficiency. These results motivate refined scaling laws for practical LLM fine-tuning in resource-constrained settings.",,"Ryan Lagasse, Aidan Kierans, Avijit Ghosh, Shiri Dori-Hacohen",2025-06-02T18:33:23Z,A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed   Compute Budgets,Ein Skalierungsgesetz für Token-Effizienz im LLM-Fine-Tuning unter festen Berechnungsbudgets,固定计算预算之下的LLM 微调-提法中提高招工效率法,http://arxiv.org/abs/2505.06150v2
705,"Quality estimation (QE)-the automatic assessment of translation quality-has recently become crucial across several stages of the translation pipeline, from data curation to training and decoding. While QE metrics have been optimized to align with human judgments, whether they encode social biases has been largely overlooked. Biased QE risks favoring certain demographic groups over others, e.g., by exacerbating gaps in visibility and usability. This paper defines and investigates gender bias of QE metrics and discusses its downstream implications for machine translation (MT). Experiments with state-of-the-art QE metrics across multiple domains, datasets, and languages reveal significant bias. When a human entity's gender in the source is undisclosed, masculine-inflected translations score higher than feminine-inflected ones, and gender-neutral translations are penalized. Even when contextual cues disambiguate gender, using context-aware QE metrics leads to more errors in selecting the correct translation inflection for feminine referents than for masculine ones. Moreover, a biased QE metric affects data filtering and quality-aware decoding. Our findings underscore the need for a renewed focus on developing and evaluating QE metrics centered on gender.",,"Emmanouil Zaranis, Giuseppe Attanasio, Sweta Agrawal, André F. T. Martins",2025-06-02T18:32:06Z,Watching the Watchers: Exposing Gender Disparities in Machine   Translation Quality Estimation,Watching the Watchers: Aufdecken von Geschlechterdisparitäten in der Bewertung der maschinellen Übersetzungsqualität,观察观察者:在机器翻译质量估计中发现性别差异,http://arxiv.org/abs/2410.10995v4
706,"Understanding the expressive power of transformers has recently attracted attention, as it offers insights into their abilities and limitations. Many studies analyze unique hard attention transformers, where attention selects a single position that maximizes the attention scores. When multiple positions achieve the maximum score, either the rightmost or the leftmost of those is chosen. In this paper, we highlight the importance of this seeming triviality. Recently, finite-precision transformers with both leftmost- and rightmost-hard attention were shown to be equivalent to Linear Temporal Logic (LTL). We show that this no longer holds with only leftmost-hard attention -- in that case, they correspond to a \emph{strictly weaker} fragment of LTL. Furthermore, we show that models with leftmost-hard attention are equivalent to \emph{soft} attention, suggesting they may better approximate real-world transformers than right-attention models. These findings refine the landscape of transformer expressivity and underscore the role of attention directionality.",,"Selim Jerad, Anej Svete, Jiaoda Li, Ryan Cotterell",2025-06-02T18:30:46Z,Unique Hard Attention: A Tale of Two Sides,Unique Hard Attention: Eine Geschichte von zwei Seiten,独特难受注意:两面的故事,http://arxiv.org/abs/2503.14615v2
707,"Construction grammar posits that children acquire constructions (form-meaning pairings) from the statistics of their environment. Recent work supports this hypothesis by showing sensitivity to constructions in pretrained language models (PLMs), including one recent study (Rozner et al., 2025) demonstrating that constructions shape the PLM's output distribution. However, models under study have generally been trained on developmentally implausible amounts of data, casting doubt on their relevance to human language learning. Here we use Rozner et al.'s methods to evaluate constructional learning in models from the 2024 BabyLM challenge. Our results show that even when trained on developmentally plausible quantities of data, models represent diverse constructions, even hard cases that are superficially indistinguishable. We further find correlational evidence that constructional performance may be functionally relevant: models that better represent constructions perform better on the BabyLM benchmarks.",,"Joshua Rozner, Leonie Weissweiler, Cory Shain",2025-06-02T18:19:38Z,BabyLM's First Constructions: Causal interventions provide a signal of   learning,Erste Konstruktionen von BabyLM: Kausale Eingriffe sind ein Signal des Lernens,BabyLM的第一批建筑:因果干预提供了学习的信号。,http://arxiv.org/abs/2506.02147v1
708,"Humans exhibit remarkable compositional reasoning by integrating knowledge from various sources. For example, if someone learns ( B = f(A) ) from one source and ( C = g(B) ) from another, they can deduce ( C=g(B)=g(f(A)) ) even without encountering ( ABC ) together, showcasing the generalization ability of human intelligence. In this paper, we introduce a synthetic learning task, ""FTCT"" (Fragmented at Training, Chained at Testing), to validate the potential of Transformers in replicating this skill and interpret its inner mechanism. In the training phase, data consist of separated knowledge fragments from an overall causal graph. During testing, Transformers must infer complete causal graph traces by integrating these fragments. Our findings demonstrate that few-shot Chain-of-Thought prompting enables Transformers to perform compositional reasoning on FTCT by revealing correct combinations of fragments, even if such combinations were absent in the training data. Furthermore, the emergence of compositional reasoning ability is strongly correlated with the model complexity and training-testing data similarity. We propose, both theoretically and empirically, that Transformers learn an underlying generalizable program from training, enabling effective compositional reasoning during testing.",,"Yutong Yin, Zhaoran Wang",2025-06-02T18:17:04Z,Are Transformers Able to Reason by Connecting Separated Knowledge in   Training Data?,"Sind Transformer durch die Verbindung getrennter Kenntnisse in Trainingsdaten in der Lage, Vernunft zu erreichen?",将培训数据方面的单独知识连接起来的变换者是否具有理性?,http://arxiv.org/abs/2501.15857v7
709,"Recent advances in reasoning-enhanced Large Language Models such as OpenAI-o1/3 and DeepSeek-R1 have significantly improved performance on complex tasks. However, the quality and transparency of their internal reasoning processes remain underexplored. This work moves beyond the final-answer accuracy and investigates step-by-step reasoning in the medical and mathematical domains by explicitly decomposing the thinking trajectories into two parts: knowledge and reasoning. Specifically, we introduce a fine-grained evaluation framework that judges: (1) the correctness of knowledge used (measured by Knowledge Index (KI)) and (2) the quality of reasoning (measured by Information Gain (InfoGain)). Using this framework, we study R1-distilled and base Qwen models trained with supervised fine-tuning (SFT) and/or reinforcement learning (RL) in the medical and math domains. Three intriguing findings emerge: (1) The general reasoning abilities in R1-distilled models do not transfer effectively to the medical domain through either SFT or RL. (2) SFT raises final-answer accuracy in both domains, but often at the cost of reasoning quality: InfoGain drops by 38.9% on average compared with untrained models; In the medical domain, however, SFT remains crucial because domain knowledge is indispensable. (3) RL enhances medical reasoning by pruning inaccurate or irrelevant knowledge from reasoning paths, thereby improving both reasoning accuracy and knowledge correctness.",,"Juncheng Wu, Sheng Liu, Haoqin Tu, Hang Yu, Xiaoke Huang, James Zou, Cihang Xie, Yuyin Zhou",2025-06-02T18:01:00Z,Knowledge or Reasoning? A Close Look at How LLMs Think Across Domains,"Wissen oder Vernunft? Ein genauer Blick darauf, wie LLMs über Domains hinweg denken",知识或理由? 仔细看看LLLM女士如何看待整个域域,http://arxiv.org/abs/2506.02126v1
710,"Prior methods for controlling image generation are limited in their ability to be taught new tasks. In contrast, vision-language models, or VLMs, can learn tasks in-context and produce the correct outputs for a given input. We propose a dual-process distillation scheme that allows feed-forward image generators to learn new tasks from deliberative VLMs. Our scheme uses a VLM to rate the generated images and backpropagates this gradient to update the weights of the image generator. Our general framework enables a wide variety of new control tasks through the same text-and-image based interface. We showcase a handful of applications of this technique for different types of control signals, such as commonsense inferences and visual prompts. With our method, users can implement multimodal controls for properties such as color palette, line weight, horizon position, and relative depth within a matter of minutes. Project page: https://dual-process.github.io.",,"Grace Luo, Jonathan Granskog, Aleksander Holynski, Trevor Darrell",2025-06-02T17:59:56Z,Dual-Process Image Generation,Dual-Process-Bildgenerierung,双处理图像生成,http://arxiv.org/abs/2506.01955v1
711,"Retrieval-Augmented Generation (RAG) methods have proven highly effective for tasks requiring factual consistency and robust knowledge retrieval. However, large-scale RAG systems consume significant computational resources and are prone to generating hallucinated content from Humans. In this work, we introduce $\texttt{DRAG}$, a novel framework for distilling RAG knowledge from large-scale Language Models (LLMs) into small LMs (SLMs). Our approach leverages evidence- and knowledge graph-based distillation, ensuring that the distilled model retains critical factual knowledge while significantly reducing model size and computational cost. By aligning the smaller model's predictions with a structured knowledge graph and ranked evidence, $\texttt{DRAG}$ effectively mitigates hallucinations and improves factual accuracy. We further present a case demonstrating how our framework mitigates user privacy risks and introduce a corresponding benchmark. Experimental evaluations on multiple benchmarks demonstrate that our method outperforms the prior competitive RAG methods like MiniRAG for SLMs by up to 27.7% using the same models, preserving high-level efficiency and reliability. With $\texttt{DRAG}$, we provide a practical and resource-efficient roadmap to deploying enhanced retrieval and generation capabilities in small-sized LLMs.",,"Jennifer Chen, Aidar Myrzakhan, Yaxin Luo, Hassaan Muhammad Khan, Sondos Mahmoud Bsharat, Zhiqiang Shen",2025-06-02T17:59:51Z,DRAG: Distilling RAG for SLMs from LLMs to Transfer Knowledge and   Mitigate Hallucination via Evidence and Graph-based Distillation,DRAG: Destillation von RAG für SLMs von LLMs zur Übertragung von Wissen und Mitigate-Halluzination über Evidenz- und Graph-basierte Destillation,"DRAG:通过证据和基于图表的蒸馏,为可持续土地管理从LLMMLM到通过证据和基于图的蒸馏转让知识和光化幻觉,为可持续土地管理提炼RAG",http://arxiv.org/abs/2506.01954v1
712,"Powered by a large language model (LLM), a web browsing agent operates web browsers in a human-like manner and offers a highly transparent path toward automating a wide range of everyday tasks. As web agents become increasingly capable and demonstrate proficiency in general browsing tasks, a critical question emerges: Can they go beyond general browsing to robustly handle tasks that are tedious and complex, or chores that humans often avoid doing themselves? In this paper, we introduce WebChoreArena, a new fully reproducible benchmark comprising 532 carefully curated tasks designed to extend the scope of WebArena beyond general browsing to more labor-intensive and tedious tasks. WebChoreArena systematically integrates three key challenges: (i) Massive Memory tasks requiring accurate retrieval of large amounts of information in the observations, (ii) Calculation tasks demanding precise mathematical reasoning, and (iii) Long-Term Memory tasks necessitating long-term memory across multiple webpages. Built on top of the fully reproducible and widely adopted four WebArena simulation environments, WebChoreArena ensures strict reproducibility and enables fair, direct comparisons with the established WebArena benchmark, offering key insights into agent progress. Our experimental results demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7 Sonnet, and Gemini 2.5 Pro, significant improvements in performance are observed on WebChoreArena. These findings suggest that WebChoreArena is well-suited to measure the advancement of state-of-the-art LLMs with greater clarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro, there remains substantial room for improvement compared to WebArena, highlighting the increased challenges posed by WebChoreArena.",,"Atsuyuki Miyai, Zaiying Zhao, Kazuki Egashira, Atsuki Sato, Tatsumi Sunada, Shota Onohara, Hiromasa Yamanishi, Mashiro Toyooka, Kunato Nishina, Ryoma Maeda, Kiyoharu Aizawa, Toshihiko Yamasaki",2025-06-02T17:59:45Z,WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web   Tasks,WebChoreArena: Bewertung von Web-Browsing-Agenten auf realistischen Tedious Web-Aufgaben,Web ChoreArena: 评估网络浏览代理商在现实的网络特大任务中的浏览代理商,http://arxiv.org/abs/2506.01952v1
713,"Although Large Language Models (LLMs) perform well in general fields, they exhibit a confidence distortion problem on multi-choice question-answering (MCQA), particularly as the number of answer choices increases. Specifically, on MCQA with many choices, LLMs suffer from under-confidence in correct predictions and over-confidence in incorrect ones, leading to a substantially degraded performance. To solve this problem, we propose Self-ensemble in this work. Our method splits the choices into several groups and ensembles LLM predictions across these groups to reach a final decision. The advantage of Self-ensemble is its plug-and-play nature, where it can be integrated into existing LLM architecture based on a designed attention mask and positional encoding, without requiring labeled datasets for parameter tuning. Experimental results on three LLMs and datasets demonstrate that Self-ensemble comprehensively addresses the confidence distortion problem of LLMs, outperforming standard inference as well as baseline methods.",,"Zicheng Xu, Guanchu Wang, Guangyao Zheng, Yu-Neng Chuang, Alexander Szalay, Xia Hu, Vladimir Braverman",2025-06-02T17:59:29Z,Self-ensemble: Mitigating Confidence Distortion for Large Language   Models,Self-ensemble: Vertrauensverzerrung für große Sprachmodelle abmildern,减少大语言模式的信心扭曲,http://arxiv.org/abs/2506.01951v1
714,"Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning.",,"Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, Junyang Lin",2025-06-02T17:54:39Z,Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective   Reinforcement Learning for LLM Reasoning,Über die 80/20-Regel hinaus: High-Entropy Minority Tokens treiben effektives Stärkungslernen für LLM-Reasoning voran,在80/20规则之后的规则:为LLM 合理性有效加强学习,http://arxiv.org/abs/2506.01939v1
715,"Effective wastewater and stormwater management is essential for urban sustainability and environmental protection. Extracting structured knowledge from reports and regulations is challenging due to domainspecific terminology and multilingual contexts. This work focuses on domain-specific Named Entity Recognition (NER) as a first step towards effective relation and information extraction to support decision making. A multilingual benchmark is crucial for evaluating these methods. This study develops a French-Italian domain-specific text corpus for wastewater management. It evaluates state-of-the-art NER methods, including LLM-based approaches, to provide a reliable baseline for future strategies and explores automated annotation projection in view of an extension of the corpus to new languages.",,"Franco Alberto Cardillo, Franca Debole, Francesca Frontini, Mitra Aelami, Nanée Chahinian, Serge Conrad",2025-06-02T17:54:16Z,Novel Benchmark for NER in the Wastewater and Stormwater Domain,Neuer Benchmark für NER im Bereich Abwasser und Sturmwasser,废水和暴风水域内净净水新新基准,http://arxiv.org/abs/2506.01938v1
716,"Reward models are used throughout the post-training of language models to capture nuanced signals from preference data and provide a training target for optimization across instruction following, reasoning, safety, and more domains. The community has begun establishing best practices for evaluating reward models, from the development of benchmarks that test capabilities in specific skill areas to others that test agreement with human preferences. At the same time, progress in evaluation has not been mirrored by the effectiveness of reward models in downstream tasks -- simpler direct alignment algorithms are reported to work better in many cases. This paper introduces RewardBench 2, a new multi-skill reward modeling benchmark designed to bring new, challenging data for accuracy-based reward model evaluation -- models score about 20 points on average lower on RewardBench 2 compared to the first RewardBench -- while being highly correlated with downstream performance. Compared to most other benchmarks, RewardBench 2 sources new human prompts instead of existing prompts from downstream evaluations, facilitating more rigorous evaluation practices. In this paper, we describe our benchmark construction process and report how existing models perform on it, while quantifying how performance on the benchmark correlates with downstream use of the models in both inference-time scaling algorithms, like best-of-N sampling, and RLHF training algorithms like proximal policy optimization.",,"Saumya Malik, Valentina Pyatkin, Sander Land, Jacob Morrison, Noah A. Smith, Hannaneh Hajishirzi, Nathan Lambert",2025-06-02T17:54:04Z,RewardBench 2: Advancing Reward Model Evaluation,BelohnungBench 2: Verbesserung der Bewertung des Prämienmodells,评分2:提高评分模式评价,http://arxiv.org/abs/2506.01937v1
717,"Diffusion-based language models offer a compelling alternative to autoregressive (AR) models by enabling parallel and controllable generation. Among this family of models, Masked Diffusion Models (MDMs) achieve the strongest performance but still underperform AR models in perplexity and lack key inference-time efficiency features--most notably, KV caching. In this work, we introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms, enabling smooth interpolation between their perplexities while overcoming their respective limitations. Eso-LMs set a new state of the art on standard language modeling benchmarks. Crucially, we are the **first to introduce KV caching for MDMs** while preserving parallel generation, significantly improving inference efficiency. Combined with an optimized sampling schedule, our method achieves up to **65x** faster inference than standard MDMs and **4x** faster inference than prior semi-autoregressive approaches. We provide the code and model checkpoints on the project page: [http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)",,"Subham Sekhar Sahoo, Zhihan Yang, Yash Akhauri, Johnna Liu, Deepansha Singh, Zhoujun Cheng, Zhengzhong Liu, Eric Xing, John Thickstun, Arash Vahdat",2025-06-02T17:47:27Z,Esoteric Language Models,Esoterische Sprachmodelle,远距离语言模型,http://arxiv.org/abs/2506.01928v1
718,"Vision-language models (VLMs) trained via reinforcement learning with verifiable reward (RLVR) have shown notable progress in scaling test-time compute effectively. In this work, we investigate how synthesized RL data can further improve RLVR. To this end, we propose \textbf{SynthRL}-a scalable and guaranteed pipeline for automatic data scaling in reasoning-oriented RL training. SynthRL comprises three key stages: (1) selecting seed questions with appropriate distribution, (2) augmenting them into more challenging variants while preserving the original answers, and (3) a guaranteed verification stage that ensures near-perfect correctness and difficulty enhancement. Our empirical experiments demonstrate SynthRL's scalability and effectiveness. When applied to the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable, challenging questions from approximately 8K seed samples. Models trained with our synthesized data achieve consistent gains across five out-of-domain visual math reasoning benchmarks, with a significant improvement over baseline models trained on seed data alone. Notably, detailed analysis reveals that the gains are more pronounced on the most challenging evaluation samples, highlighting SynthRL's effectiveness in eliciting deeper and more complex reasoning patterns.",,"Zijian Wu, Jinjie Ni, Xiangyan Liu, Zichen Liu, Hang Yan, Michael Qizhe Shieh",2025-06-02T17:45:16Z,SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis,SynthRL: Skalierung der visuellen Vernunft mit überprüfbarer Datensynthese,合成合成RL:利用可核查数据合成增强视觉理性,http://arxiv.org/abs/2506.02096v1
719,"Chain-of-thought (CoT) reasoning not only enhances large language model performance but also provides critical insights into decision-making processes, marking it as a useful tool for monitoring model intent and planning. By proactively preventing models from acting on CoT indicating misaligned or harmful intent, CoT monitoring can be used to reduce risks associated with deploying models. However, developers may be incentivized to train away the appearance of harmful intent from CoT traces, by either customer preferences or regulatory requirements. Recent works have shown that banning mention of a specific example of reward hacking, which may be done either to make CoT presentable to users or as a naive attempt to prevent the behavior, causes obfuscation of the undesired reasoning traces but the persistence of the undesired behavior. Such obfuscation threatens the reliability of CoT monitoring. However, obfuscation of reasoning can be due to its internalization to latent space computation, or its encoding within the CoT. Here, we provide an extension to these results. First, we show that penalizing the use of specific strings within load-bearing reasoning traces causes models to substitute alternative strings. Crucially, this does not alter the underlying method by which the model performs the task, demonstrating that the model can learn to steganographically encode its reasoning. We further demonstrate that models can generalize an encoding scheme. When the penalized strings belong to an overarching class, the model learns not only to substitute strings seen in training, but also develops a general encoding scheme for all members of the class which it can apply to held-out testing strings.",,"Joey Skaf, Luis Ibanez-Lissen, Robert McCarthy, Connor Watts, Vasil Georgiv, Hannes Whittingham, Lorena Gonzalez-Manzano, David Lindner, Cameron Tice, Edward James Young, Puria Radmard",2025-06-02T17:45:15Z,Large language models can learn and generalize steganographic   chain-of-thought under process supervision,Große Sprachmodelle können Steganographische Gedankenkette unter Prozessüberwachung erlernen und verallgemeinern,大型语言模式可以在程序监督下学习和普及地理学思维链,http://arxiv.org/abs/2506.01926v1
720,"We present SwingArena, a competitive evaluation framework for Large Language Models (LLMs) that closely mirrors real-world software development workflows. Unlike traditional static benchmarks, SwingArena models the collaborative process of software iteration by pairing LLMs as submitters, who generate patches, and reviewers, who create test cases and verify the patches through continuous integration (CI) pipelines. To support these interactive evaluations, we introduce a retrieval-augmented code generation (RACG) module that efficiently handles long-context challenges by providing syntactically and semantically relevant code snippets from large codebases, supporting multiple programming languages (C++, Python, Rust, and Go). This enables the framework to scale across diverse tasks and contexts while respecting token limitations. Our experiments, using over 400 high-quality real-world GitHub issues selected from a pool of 2,300 issues, show that models like GPT-4o excel at aggressive patch generation, whereas DeepSeek and Gemini prioritize correctness in CI validation. SwingArena presents a scalable and extensible methodology for evaluating LLMs in realistic, CI-driven software development settings. More details are available on our project page: swing-bench.github.io",,"Wendong Xu, Jing Xiong, Chenyang Zhao, Qiujiang Chen, Haoran Wang, Hui Shen, Zhongwei Wan, Jianbo Dai, Taiqiang Wu, He Xiao, Chaofan Tao, Z. Morley Mao, Ying Sheng, Zhijiang Guo, Hongxia Yang, Bei Yu, Lingpeng Kong, Quanquan Gu, Ngai Wong",2025-06-02T17:42:36Z,SwingArena: Competitive Programming Arena for Long-context GitHub Issue   Solving,SwingArena: Wettbewerbsfähige Programmierarena für die Lösung der Langkontext-Ausgabe GitHub,SWINGARENA:长期解决吉特布问题竞争性方案拟订竞技场,http://arxiv.org/abs/2505.23932v2
721,"This work introduces RARE (Retrieval-Augmented Reasoning Enhancement), a versatile extension to the mutual reasoning framework (rStar), aimed at enhancing reasoning accuracy and factual integrity across large language models (LLMs) for complex, knowledge-intensive tasks such as commonsense and medical reasoning. RARE incorporates two innovative actions within the Monte Carlo Tree Search (MCTS) framework: A6, which generates search queries based on the initial problem statement, performs information retrieval using those queries, and augments reasoning with the retrieved data to formulate the final answer; and A7, which leverages information retrieval specifically for generated sub-questions and re-answers these sub-questions with the relevant contextual information. Additionally, a Retrieval-Augmented Factuality Scorer is proposed to replace the original discriminator, prioritizing reasoning paths that meet high standards of factuality. Experimental results with LLaMA 3.1 show that RARE enables open-source LLMs to achieve competitive performance with top open-source models like GPT-4 and GPT-4o. This research establishes RARE as a scalable solution for improving LLMs in domains where logical coherence and factual integrity are critical.",,"Hieu Tran, Zonghai Yao, Junda Wang, Yifan Zhang, Zhichao Yang, Hong Yu",2025-06-02T17:40:21Z,RARE: Retrieval-Augmented Reasoning Enhancement for Large Language   Models,RARE: Retrieval-Augmented Reasoning Enhancement für große Sprachmodelle,RARE: 提高大语言模型检索率 - 增加理由,http://arxiv.org/abs/2412.02830v4
722,"This paper addresses critical gaps in Arabic language model evaluation by establishing comprehensive theoretical guidelines and introducing a novel evaluation framework. We first analyze existing Arabic evaluation datasets, identifying significant issues in linguistic accuracy, cultural alignment, and methodological rigor. To address these limitations in LLMs, we present the Arabic Depth Mini Dataset (ADMD), a carefully curated collection of 490 challenging questions spanning ten major domains (42 sub-domains, see Figure 1. Using ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet, Gemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant variations in model performance across different domains, with particular challenges in areas requiring deep cultural understanding and specialized knowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\%, showing relative strength in mathematical theory in Arabic, Arabic language, and islamic domains. This work provides both theoretical foundations and practical insights for improving Arabic language model evaluation, emphasizing the importance of cultural competence alongside technical capabilities.",,"Serry Sibaee, Omer Nacar, Adel Ammar, Yasser Al-Habashi, Abdulrahman Al-Batati, Wadii Boulila",2025-06-02T17:39:50Z,From Guidelines to Practice: A New Paradigm for Arabic Language Model   Evaluation,Von Richtlinien zur Praxis: Ein neues Paradigma für die Bewertung von arabischen Sprachmodellen,《从准则到实践:阿拉伯文模式评价的新范例》,http://arxiv.org/abs/2506.01920v1
723,"Image mass cytometry (IMC) enables high-dimensional spatial profiling by combining mass cytometry's analytical power with spatial distributions of cell phenotypes. Recent studies leverage large language models (LLMs) to extract cell states by translating gene or protein expression into biological context. However, existing single-cell LLMs face two major challenges: (1) Integration of spatial information: they struggle to generalize spatial coordinates and effectively encode spatial context as text, and (2) Treating each cell independently: they overlook cell-cell interactions, limiting their ability to capture biological relationships. To address these limitations, we propose Spatial2Sentence, a novel framework that integrates single-cell expression and spatial information into natural language using a multi-sentence approach. Spatial2Sentence constructs expression similarity and distance matrices, pairing spatially adjacent and expressionally similar cells as positive pairs while using distant and dissimilar cells as negatives. These multi-sentence representations enable LLMs to learn cellular interactions in both expression and spatial contexts. Equipped with multi-task learning, Spatial2Sentence outperforms existing single-cell LLMs on preprocessed IMC datasets, improving cell-type classification by 5.98% and clinical status prediction by 4.18% on the diabetes dataset while enhancing interpretability. The source code can be found here: https://github.com/UNITES-Lab/Spatial2Sentence.",,"Chi-Jane Chen, Yuhang Chen, Sukwon Yun, Natalie Stanley, Tianlong Chen",2025-06-02T17:38:59Z,Spatial Coordinates as a Cell Language: A Multi-Sentence Framework for   Imaging Mass Cytometry Analysis,Raumkoordinaten als Zellsprache: Ein Multi-Sentence-Framework für bildgebende Massenzytometrieanalyse,以空间坐标作为手机语言:成像大规模测距分析的多空间框架,http://arxiv.org/abs/2506.01918v1
724,"Large Language Models (LLMs) have been found to struggle with systematic reasoning. Even on tasks where they appear to perform well, their performance often depends on shortcuts, rather than on genuine reasoning abilities, leading them to collapse on out-of-distribution (OOD) examples. Post-training strategies based on reinforcement learning and chain-of-thought prompting have recently been hailed as a step change. However, little is known about the potential of the resulting ``Large Reasoning Models'' (LRMs) beyond maths and programming-based problem solving, where genuine OOD problems can be sparse. In this paper, we focus on tasks that require systematic relational composition for qualitative spatial and temporal reasoning. The setting allows fine control over problem difficulty to precisely measure OOD generalization. We find that, zero-shot LRMs generally outperform their LLM counterparts in single-path reasoning tasks but struggle in the multi-path setting. Whilst showing comparatively better results, fine-tuned LLMs are also not capable of multi-path generalization. We also provide evidence for the behavioral interpretation for this, i.e., that LRMs are shallow disjunctive reasoners.",,"Irtaza Khalid, Amir Masoud Nourollah, Steven Schockaert",2025-06-02T17:37:39Z,Large Language and Reasoning Models are Shallow Disjunctive Reasoners,Große Sprach- und Vernunftmodelle sind shallow disjunktive Reasoner,大语言和说明理由的模型是浅分解理由,http://arxiv.org/abs/2503.23487v2
725,"Large Language Models (LLMs) have shown great potential in automated story generation, but challenges remain in maintaining long-form coherence and providing users with intuitive and effective control. Retrieval-Augmented Generation (RAG) has proven effective in reducing hallucinations in text generation; however, the use of structured data to support generative storytelling remains underexplored. This paper investigates how knowledge graphs (KGs) can enhance LLM-based storytelling by improving narrative quality and enabling user-driven modifications. We propose a KG-assisted storytelling pipeline and evaluate its effectiveness through a user study with 15 participants. Participants created their own story prompts, generated stories, and edited knowledge graphs to shape their narratives. Through quantitative and qualitative analysis, our findings demonstrate that knowledge graphs significantly enhance story quality in action-oriented and structured narratives within our system settings. Additionally, editing the knowledge graph increases users' sense of control, making storytelling more engaging, interactive, and playful.",,"Zhijun Pan, Antonios Andronis, Eva Hayek, Oscar AP Wilkinson, Ilya Lasy, Annette Parry, Guy Gadney, Tim J. Smith, Mick Grierson",2025-06-02T17:37:17Z,Guiding Generative Storytelling with Knowledge Graphs,Leitende Generative Storytelling mit Wissensgraphen,"带有知识图的 "" 指导产生故事 """,http://arxiv.org/abs/2505.24803v2
726,"Systematic reviews (SRs) are vital for evidence-based practice in high stakes disciplines, such as healthcare, but are often impeded by intensive labors and lengthy processes that can take months to complete. Due to the high demand for domain expertise, existing automatic summarization methods fail to accurately identify relevant studies and generate high-quality summaries. To that end, we introduce InsightAgent, a human-centered interactive AI agent powered by large language models that revolutionize this workflow. InsightAgent partitions a large literature corpus based on semantics and employs a multi-agent design for more focused processing of literature, leading to significant improvement in the quality of generated SRs. InsightAgent also provides intuitive visualizations of the corpus and agent trajectories, allowing users to effortlessly monitor the actions of the agent and provide real-time feedback based on their expertise. Our user studies with 9 medical professionals demonstrate that the visualization and interaction mechanisms can effectively improve the quality of synthesized SRs by 27.2%, reaching 79.7% of human-written quality. At the same time, user satisfaction is improved by 34.4%. With InsightAgent, it only takes a clinician about 1.5 hours, rather than months, to complete a high-quality systematic review.",,"Rui Qiu, Shijie Chen, Yu Su, Po-Yin Yen, Han-Wei Shen",2025-06-02T17:34:14Z,Completing A Systematic Review in Hours instead of Months with   Interactive AI Agents,Eine systematische Überprüfung in Stunden statt Monaten mit interaktiven KI-Agenten abschließen,与互动的AI代理机构在小时而不是月完成系统审查,http://arxiv.org/abs/2504.14822v2
727,"Vision-language models pre-trained on large scale of unlabeled biomedical images and associated reports learn generalizable semantic representations. These multi-modal representations can benefit various downstream tasks in the biomedical domain. Contrastive learning is widely used to pre-train vision-language models for general natural images and associated captions. Despite its popularity, we found biomedical texts have complex and domain-specific semantics that are often neglected by common contrastive methods. To address this issue, we propose a novel method, perturbed report discrimination, for pre-train biomedical vision-language models. First, we curate a set of text perturbation methods that keep the same words, but disrupt the semantic structure of the sentence. Next, we apply different types of perturbation to reports, and use the model to distinguish the original report from the perturbed ones given the associated image. Parallel to this, we enhance the sensitivity of our method to higher level of granularity for both modalities by contrasting attention-weighted image sub-regions and sub-words in the image-text pairs. We conduct extensive experiments on multiple downstream tasks, and our method outperforms strong baseline methods. The results demonstrate that our approach learns more semantic meaningful and robust multi-modal representations.",,"Xinliu Zhong, Kayhan Batmanghelich, Li Sun",2025-06-02T17:23:25Z,Enhancing Biomedical Multi-modal Representation Learning with   Multi-scale Pre-training and Perturbed Report Discrimination,Verbesserung des biomedizinischen multimodalen Repräsentationslernens durch Multi-Skala-Vorausbildung und Störungsbericht Diskriminierung,"增强生物医学多模式代表制学习,同时开展多规模的预培训和无保障报告歧视",http://arxiv.org/abs/2506.01902v1
728,"Large Language Models (LLMs) suffer from hallucinations and outdated knowledge due to their reliance on static training data. Retrieval-Augmented Generation (RAG) mitigates these issues by integrating external dynamic information for improved factual grounding. With advances in multimodal learning, Multimodal RAG extends this approach by incorporating multiple modalities such as text, images, audio, and video to enhance the generated outputs. However, cross-modal alignment and reasoning introduce unique challenges beyond those in unimodal RAG. This survey offers a structured and comprehensive analysis of Multimodal RAG systems, covering datasets, benchmarks, metrics, evaluation, methodologies, and innovations in retrieval, fusion, augmentation, and generation. We review training strategies, robustness enhancements, loss functions, and agent-based approaches, while also exploring the diverse Multimodal RAG scenarios. In addition, we outline open challenges and future directions to guide research in this evolving field. This survey lays the foundation for developing more capable and reliable AI systems that effectively leverage multimodal dynamic external knowledge bases. All resources are publicly available at https://github.com/llm-lab-org/Multimodal-RAG-Survey.",,"Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammadkhani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, Ehsaneddin Asgari",2025-06-02T17:15:08Z,Ask in Any Modality: A Comprehensive Survey on Multimodal   Retrieval-Augmented Generation,Fragen Sie in jeder Modalität: Eine umfassende Umfrage über multimodale Retrieval-Augmented Generation,《任何方式中的询问:关于多式回收-养代人的全面调查》,http://arxiv.org/abs/2502.08826v3
729,"Task-oriented dialogue systems often face difficulties when user utterances seem semantically complete but lack necessary structural information for appropriate system action. This arises because users frequently do not fully understand their own needs, while systems require precise intent definitions. Current LLM-based agents cannot effectively distinguish between linguistically complete and contextually triggerable expressions, lacking frameworks for collaborative intent formation. We present STORM, a framework modeling asymmetric information dynamics through conversations between UserLLM (full internal access) and AgentLLM (observable behavior only). STORM produces annotated corpora capturing expression trajectories and latent cognitive transitions, enabling systematic analysis of collaborative understanding development. Our contributions include: (1) formalizing asymmetric information processing in dialogue systems; (2) modeling intent formation tracking collaborative understanding evolution; and (3) evaluation metrics measuring internal cognitive improvements alongside task performance. Experiments across four language models reveal that moderate uncertainty (40-60%) can outperform complete transparency in certain scenarios, with model-specific patterns suggesting reconsideration of optimal information completeness in human-AI collaboration. These findings contribute to understanding asymmetric reasoning dynamics and inform uncertainty-calibrated dialogue system design.",,"Yaoyao Qian, Jindan Huang, Yuanli Wang, Simon Yu, Kyrie Zhixuan Zhou, Jiayuan Mao, Mingfu Liang, Hanhan Zhou",2025-06-02T17:11:10Z,"WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent   Triggerability in Task-Oriented Dialogue","WENN ZU ACT, WENN ZU WAIT: Modellierung struktureller Trajektorien für Intent Triggerability im aufgabenorientierten Dialog","行动时,等待时:在以任务为方向的对话中为本可触发性模拟结构轨迹",http://arxiv.org/abs/2506.01881v1
730,"Conversations are usually structured by roles -- who is speaking, who's being addressed, and who's listening -- and unfold in threads that break with changes in speaker floor or topical focus. While large language models (LLMs) have shown incredible capabilities in dialogue and reasoning, their ability to understand fine-grained conversational structure, especially in multi-modal, multi-party settings, remains underexplored. To address this gap, we introduce a suite of tasks focused on conversational role attribution (speaker, addressees, side-participants) and conversation threading (utterance linking and clustering), drawing on conversation analysis and sociolinguistics. To support those tasks, we present a human annotated dataset of 4,398 annotations for speakers and reply-to relationship, 5,755 addressees, and 3,142 side-participants.   We evaluate popular audio-visual LLMs and vision-language models on our dataset, and our experimental results suggest that multimodal conversational structure understanding remains challenging. The most performant audio-visual LLM outperforms all vision-language models across all metrics, especially in speaker and addressee recognition. However, its performance drops significantly when conversation participants are anonymized. The number of conversation participants in a clip is the strongest negative predictor of role-attribution performance, while acoustic clarity (measured by pitch and spectral centroid) and detected face coverage yield positive associations. We hope this work lays the groundwork for future evaluation and development of multimodal LLMs that can reason more effectively about conversation structure.",,"Kent K. Chang, Mackenzie Hanh Cramer, Anna Ho, Ti Ti Nguyen, Yilin Yuan, David Bamman",2025-06-02T17:10:18Z,Multimodal Conversation Structure Understanding,Multimodales Verständnis der Gesprächsstruktur,多式联运结构理解,http://arxiv.org/abs/2505.17536v2
731,"Dense retrievers encode texts into embeddings to efficiently retrieve relevant documents from large databases in response to user queries. However, real-world corpora continually evolve, leading to a shift from the original training distribution of the retriever. Without timely updates or retraining, indexing newly emerging documents can degrade retrieval performance for future queries. Thus, identifying when a dense retriever requires an update is critical for maintaining robust retrieval systems. In this paper, we propose a novel task of predicting whether a corpus is out-of-distribution (OOD) relative to a dense retriever before indexing. Addressing this task allows us to proactively manage retriever updates, preventing potential retrieval failures. We introduce GradNormIR, an unsupervised approach that leverages gradient norms to detect OOD corpora effectively. Experiments on the BEIR benchmark demonstrate that GradNormIR enables timely updates of dense retrievers in evolving document collections, significantly enhancing retrieval robustness and efficiency.",,"Dayoon Ko, Jinyoung Kim, Sohyeon Kim, Jinhyuk Kim, Jaehoon Lee, Seonghak Song, Minyoung Lee, Gunhee Kim",2025-06-02T17:06:35Z,When Should Dense Retrievers Be Updated in Evolving Corpora? Detecting   Out-of-Distribution Corpora Using GradNormIR,Wann sollten Dense Retriever in Evolving Corpora aktualisiert werden? Detecting Out-of-Distribution Corpora Verwendung GradNormIR,"何时在不断演变的公司中更新 "" 大量开发 "" ? 利用GradNormIR检测散散外公司",http://arxiv.org/abs/2506.01877v1
732,"Multimodal large language models (MLLMs) have extended the success of large language models (LLMs) to multiple data types, such as image, text and audio, achieving significant performance in various domains, including multimodal translation, visual question answering and content generation. Nonetheless, existing systems are inefficient to train MLLMs due to substantial GPU bubbles caused by the heterogeneous modality models and complex data dependencies in 3D parallelism. This paper proposes Optimus, a distributed MLLM training system that reduces end-to-end MLLM training time. Optimus is based on our principled analysis that scheduling the encoder computation within the LLM bubbles can reduce bubbles in MLLM training. To make scheduling encoder computation possible for all GPUs, Optimus searches the separate parallel plans for encoder and LLM, and adopts a bubble scheduling algorithm to enable exploiting LLM bubbles without breaking the original data dependencies in the MLLM model architecture. We further decompose encoder layer computation into a series of kernels, and analyze the common bubble pattern of 3D parallelism to carefully optimize the sub-millisecond bubble scheduling, minimizing the overall training time. Our experiments in a production cluster show that Optimus accelerates MLLM training by 20.5%-21.3% with ViT-22B and GPT-175B model over 3072 GPUs compared to baselines.",,"Weiqi Feng, Yangrui Chen, Shaoyu Wang, Yanghua Peng, Haibin Lin, Minlan Yu",2025-06-02T17:02:34Z,Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble   Exploitation,Optimus: Beschleunigen von großräumigen Multi-Modal LLM-Trainings durch Bubble Exploitation,顶柱:通过泡沫开采加速大型多式多模式LM培训,http://arxiv.org/abs/2408.03505v2
733,"Omni-modal language models (OLMs) aim to integrate and reason over diverse input modalities--such as text, images, video, and audio--while maintaining strong language capabilities. Despite recent advancements, existing models, especially open-source ones, remain far from true omni-modality, struggling to generalize beyond the specific modality pairs they are trained on or to achieve strong performance when processing multi-modal inputs. We study the effect of extending modality, the dominant technique for training multimodal models, where an off-the-shelf language model is fine-tuned on target-domain and language data. Specifically, we investigate three key questions: (1) Does modality extension compromise core language abilities? (2) Can model merging effectively integrate independently fine-tuned modality-specific models to achieve omni-modality? (3) Does omni-modality extension lead to better knowledge sharing and generalization compared to sequential extension? Through extensive experiments, we analyze these trade-offs and provide insights into the feasibility of achieving true omni-modality using current approaches.",,"Tinghui Zhu, Kai Zhang, Muhao Chen, Yu Su",2025-06-02T17:01:40Z,Is Extending Modality The Right Path Towards Omni-Modality?,Ist die Erweiterung der Modalität der richtige Weg zur Omni-Modalität?,扩展模式是否是通向全方位模式的正确途径?,http://arxiv.org/abs/2506.01872v1
734,"Scaling laws have shaped recent advances in machine learning by enabling predictable scaling of model performance based on model size, computation, and data volume. Concurrently, the rise in computational cost for AI has motivated model compression techniques, notably quantization and sparsification, which have emerged to mitigate the steep computational demands associated with large-scale training and inference. This paper investigates the interplay between scaling laws and compression formats, exploring whether a unified scaling framework can accurately predict model performance when training occurs over various compressed representations, such as sparse, scalar-quantized, sparse-quantized or even vector-quantized formats. Our key contributions include validating a general scaling law formulation and showing that it is applicable both individually but also composably across compression types. Based on this, our main finding is demonstrating both theoretically and empirically that there exists a simple ""capacity"" metric -- based on the representation's ability to fit random Gaussian data -- which can robustly predict parameter efficiency across multiple compressed representations. On the practical side, we extend our formulation to directly compare the accuracy potential of different compressed formats, and to derive better algorithms for training over sparse-quantized formats.",,"Andrei Panferov, Alexandra Volkova, Ionut-Vlad Modoranu, Vage Egiazarian, Mher Safaryan, Dan Alistarh",2025-06-02T16:52:51Z,Unified Scaling Laws for Compressed Representations,Einheitliche Skalierungsgesetze für komprimierte Vertretungen,压缩代表的统一扩大法律,http://arxiv.org/abs/2506.01863v1
735,"Though Large Vision-Language Models (LVLMs) are being actively explored in medicine, their ability to conduct telemedicine consultations combining accurate diagnosis with professional dialogue remains underexplored. In this paper, we present 3MDBench (Medical Multimodal Multi-agent Dialogue Benchmark), an open-source framework for simulating and evaluating LVLM-driven telemedical consultations. 3MDBench simulates patient variability through four temperament-based Patient Agents and an Assessor Agent that jointly evaluate diagnostic accuracy and dialogue quality. It includes 3013 cases across 34 diagnoses drawn from real-world telemedicine interactions, combining textual and image-based data. The experimental study compares diagnostic strategies for popular LVLMs, including GPT-4o-mini, LLaVA-3.2-11B-Vision-Instruct, and Qwen2-VL-7B-Instruct. We demonstrate that multimodal dialogue with internal reasoning improves F1 score by 6.5% over non-dialogue settings, highlighting the importance of context-aware, information-seeking questioning. Moreover, injecting predictions from a diagnostic convolutional network into the LVLM's context boosts F1 by up to 20%. Source code is available at https://anonymous.4open.science/r/3mdbench_acl-0511.",,"Ivan Sviridov, Amina Miftakhova, Artemiy Tereshchenko, Galina Zubkova, Pavel Blinov, Andrey Savchenko",2025-06-02T16:50:59Z,3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark,3MDBench: Medical Multimodal Multi-Agent Dialog Benchmark,3MMDBench:医疗多式联运多机构对话基准,http://arxiv.org/abs/2504.13861v2
736,"Vision Language Models (VLMs) are impressive at visual question answering and image captioning. But they underperform on multi-step visual reasoning -- even compared to LLMs on the same tasks presented in text form -- giving rise to perceptions of modality imbalance or brittleness. Towards a systematic study of such issues, we introduce a synthetic framework for assessing the ability of VLMs to perform algorithmic visual reasoning, comprising three tasks: Table Readout, Grid Navigation, and Visual Analogy. Each has two levels of difficulty, SIMPLE and HARD, and even the SIMPLE versions are difficult for frontier VLMs. We propose strategies for training on the SIMPLE version of tasks that improve performance on the corresponding HARD task, i.e., simple-to-hard (S2H) generalization. This controlled setup, where each task also has an equivalent text-only version, allows a quantification of the modality imbalance and how it is impacted by training strategy. We show that 1) explicit image-to-text conversion is important in promoting S2H generalization on images, by transferring reasoning from text; 2) conversion can be internalized at test time. We also report results of mechanistic study of this phenomenon. We identify measures of gradient alignment that can identify training strategies that promote better S2H generalization. Ablations highlight the importance of chain-of-thought.",,"Simon Park, Abhishek Panigrahi, Yun Cheng, Dingli Yu, Anirudh Goyal, Sanjeev Arora",2025-06-02T16:48:29Z,Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate   Modality Imbalance in VLMs?,Verallgemeinern von SIMPLE zu HARD Visual Reasoning: Können wir Modalitätsungleichgewichte in VLMs ausgleichen?,从SIMPLE推广到高难度视觉理由:我们能否在VLMS中扩大模式平衡?,http://arxiv.org/abs/2501.02669v2
737,"We introduce Conversational Function-Calling Evaluation Through Turn-Level Interactions (CONFETTI), a conversational benchmark1 designed to evaluate the function-calling capabilities and response quality of large language models (LLMs). Current benchmarks lack comprehensive assessment of LLMs in complex conversational scenarios. CONFETTI addresses this gap through 109 human-simulated conversations, comprising 313 user turns and covering 86 APIs. These conversations explicitly target various conversational complexities, such as follow-ups, goal correction and switching, ambiguous and implicit goals. We perform off-policy turn-level evaluation using this benchmark targeting function-calling. Our benchmark also incorporates dialog act annotations to assess agent responses. We evaluate a series of state-of-the-art LLMs and analyze their performance with respect to the number of available APIs, conversation lengths, and chained function calling. Our results reveal that while some models are able to handle long conversations, and leverage more than 20+ APIs successfully, other models struggle with longer context or when increasing the number of APIs. We also report that the performance on chained function-calls is severely limited across the models. Overall, the top performing models on CONFETTI are Nova Pro (40.01%), Claude Sonnet v3.5 (35.46%) and Llama 3.1 405B (33.19%) followed by command-r-plus (31.18%) and Mistral-Large-2407 (30.07%).",,"Tamer Alkhouli, Katerina Margatina, James Gung, Raphael Shu, Claudia Zaghi, Monica Sunkara, Yi Zhang",2025-06-02T16:48:11Z,CONFETTI: Conversational Function-Calling Evaluation Through Turn-Level   Interactions,CONFETTI: Conversational Function-Calling Evaluation durch Turn-Level-Interaktionen,CONFETTI:通过转变层面的相互作用对职能进行相互评价,http://arxiv.org/abs/2506.01859v1
738,"The theoretical code-switching (CS) literature provides numerous pointwise investigations that aim to explain patterns in CS, i.e. why bilinguals switch language in certain positions in a sentence more often than in others. A resulting consensus is that CS can be explained by the syntax of the contributing languages. There is however no large-scale, multi-language, cross-phenomena experiment that tests this claim. When designing such an experiment, we need to make sure that the system that is predicting where bilinguals tend to switch has access only to syntactic information. We provide such an experiment here. Results show that syntax alone is sufficient for an automatic system to distinguish between sentences in minimal pairs of CS, to the same degree as bilingual humans. Furthermore, the learnt syntactic patterns generalise well to unseen language pairs.",,"Igor Sterner, Simone Teufel",2025-06-02T16:32:14Z,Code-Switching and Syntax: A Large-Scale Experiment,Code-Schalten und Syntax: Ein groß angelegtes Experiment,代码开动和语法:大规模实验,http://arxiv.org/abs/2506.01846v1
739,"There is a lack of an evaluation methodology that estimates the extent to which large language models (LLMs) use code-switching (CS) in the same way as bilinguals. Existing methods do not have wide language coverage, fail to account for the diverse range of CS phenomena, or do not scale. We propose an intervention based on minimal pairs of CS. Each minimal pair contains one naturally occurring CS sentence and one minimally manipulated variant. We collect up to 1,000 such pairs each for 11 language pairs. Our human experiments show that, for every language pair, bilinguals consistently prefer the naturally occurring CS sentence. Meanwhile our experiments with current LLMs show that the larger the model, the more consistently it assigns higher probability to the naturally occurring CS sentence than to the variant. In accordance with theoretical claims, the largest probability differences arise in those pairs where the manipulated material consisted of closed-class words.",,"Igor Sterner, Simone Teufel",2025-06-02T16:27:53Z,Minimal Pair-Based Evaluation of Code-Switching,Minimale Pair-basierte Auswertung von Code-Switching,对代码转换的最小对等评价,http://arxiv.org/abs/2506.01840v1
740,"Large language models can generate factually inaccurate content, a problem known as hallucination. Recent works have built upon retrieved-augmented generation to improve factuality through iterative prompting but these methods are limited by the traditional RAG design. To address these challenges, we introduce EWE (Explicit Working Memory), a novel approach that enhances factuality in long-form text generation by integrating a working memory that receives real-time feedback from external resources. The memory is refreshed based on online fact-checking and retrieval feedback, allowing EWE to rectify false claims during the generation process and ensure more accurate and reliable outputs. Our experiments demonstrate that Ewe outperforms strong baselines on four fact-seeking long-form generation datasets, increasing the factuality metric, VeriScore, by 2 to 6 points absolute without sacrificing the helpfulness of the responses. Further analysis reveals that the design of rules for memory updates, configurations of memory units, and the quality of the retrieval datastore are crucial factors for influencing model performance.",,"Mingda Chen, Yang Li, Karthik Padthe, Rulin Shao, Alicia Sun, Luke Zettlemoyer, Gargi Ghosh, Wen-tau Yih",2025-06-02T16:27:09Z,Improving Factuality with Explicit Working Memory,Verbesserung der Faktizität mit explizitem Arbeitsgedächtnis,提高有明确工作记忆的事实质量,http://arxiv.org/abs/2412.18069v3
741,"Citation quality is crucial in information-seeking systems, directly influencing trust and the effectiveness of information access. Current evaluation frameworks, both human and automatic, mainly rely on Natural Language Inference (NLI) to assess binary or ternary supportiveness from cited sources, which we argue is a suboptimal proxy for citation evaluation. In this work we introduce CiteEval, a citation evaluation framework driven by principles focusing on fine-grained citation assessment within a broad context, encompassing not only the cited sources but the full retrieval context, user query, and generated text. Guided by the proposed framework, we construct CiteBench, a multi-domain benchmark with high-quality human annotations on citation quality. To enable efficient evaluation, we further develop CiteEval-Auto, a suite of model-based metrics that exhibit strong correlation with human judgments. Experiments across diverse systems demonstrate CiteEval-Auto's superior ability to capture the multifaceted nature of citations compared to existing metrics, offering a principled and scalable approach to evaluate and improve model-generated citations.",,"Yumo Xu, Peng Qi, Jifan Chen, Kunlun Liu, Rujun Han, Lan Liu, Bonan Min, Vittorio Castelli, Arshit Gupta, Zhiguo Wang",2025-06-02T16:15:34Z,CiteEval: Principle-Driven Citation Evaluation for Source Attribution,CiteEval: Prinzipgestützte Citation-Evaluierung für die Quellenzuweisung,CiteEval: 来源来源原则驱动的引文评价,http://arxiv.org/abs/2506.01829v1
742,"With the recent advances in Artificial Intelligence (AI) and Large Language Models (LLMs), the automation of daily tasks, like automatic writing, is getting more and more attention. Hence, efforts have focused on aligning LLMs with human values, yet humor, particularly professional industrial humor used in workplaces, has been largely neglected. To address this, we develop a dataset of professional humor statements along with features that determine the appropriateness of each statement. Our evaluation of five LLMs shows that LLMs often struggle to judge the appropriateness of humor accurately.",,"Moahmmadamin Shafiei, Hamidreza Saffari",2025-06-02T16:00:50Z,Not All Jokes Land: Evaluating Large Language Models Understanding of   Workplace Humor,Nicht alle Witze Land: Bewertung großer Sprachmodelle Verständnis des Arbeitsplatzes Humor,并非所有的笑话地:评价工作场所幽默性大语言模型理解,http://arxiv.org/abs/2506.01819v1
743,"We present Team BD's submission to the BEA 2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors, under Track 1 (Mistake Identification) and Track 2 (Mistake Location). Both tracks involve three-class classification of tutor responses in educational dialogues - determining if a tutor correctly recognizes a student's mistake (Track 1) and whether the tutor pinpoints the mistake's location (Track 2). Our system is built on MPNet, a Transformer-based language model that combines BERT and XLNet's pre-training advantages. We fine-tuned MPNet on the task data using a class-weighted cross-entropy loss to handle class imbalance, and leveraged grouped cross-validation (10 folds) to maximize the use of limited data while avoiding dialogue overlap between training and validation. We then performed a hard-voting ensemble of the best models from each fold, which improves robustness and generalization by combining multiple classifiers. Our approach achieved strong results on both tracks, with exact-match macro-F1 scores of approximately 0.7110 for Mistake Identification and 0.5543 for Mistake Location on the official test set. We include comprehensive analysis of our system's performance, including confusion matrices and t-SNE visualizations to interpret classifier behavior, as well as a taxonomy of common errors with examples. We hope our ensemble-based approach and findings provide useful insights for designing reliable tutor response evaluation systems in educational dialogue settings.",,"Shadman Rohan, Ishita Sur Apan, Muhtasim Ibteda Shochcho, Md Fahim, Mohammad Ashfaq Ur Rahman, AKM Mahbubur Rahman, Amin Ahsan Ali",2025-06-02T15:57:49Z,BD at BEA 2025 Shared Task: MPNet Ensembles for Pedagogical Mistake   Identification and Localization in AI Tutor Responses,BD bei BEA 2025 Shared Task: MPNet Ensembles für pädagogische Fehlererkennung und Lokalisierung in KI Tutor Responses,BEA 2025 BEA BD 的BD 共同任务:在AI Titor 回应中为教育错误识别和本地化而建立 MPNet 集合,http://arxiv.org/abs/2506.01817v1
744,"Large language models (LLMs) are prone to hallucinations and sensitive to prompt perturbations, often resulting in inconsistent or unreliable generated text. Different methods have been proposed to mitigate such hallucinations and fragility -- one of them being measuring the consistency (the model's confidence in the response, or likelihood of generating a similar response when resampled) of LLM responses. In previous work, measuring consistency often relied on the probability of a response appearing within a pool of resampled responses, or internal states or logits of responses. However, it is not yet clear how well these approaches approximate how humans perceive the consistency of LLM responses. We performed a user study (n=2,976) and found current methods typically do not approximate users' perceptions of LLM consistency very well. We propose a logit-based ensemble method for estimating LLM consistency, and we show that this method matches the performance of the best-performing existing metric in estimating human ratings of LLM consistency. Our results suggest that methods of estimating LLM consistency without human evaluation are sufficiently imperfect that we suggest evaluation with human input be more broadly used.",,"Xiaoyuan Wu, Weiran Lin, Omer Akgul, Lujo Bauer",2025-06-02T15:55:44Z,Estimating LLM Consistency: A User Baseline vs Surrogate Metrics,Schätzung der LLM-Konsistenz: Ein Benutzer Baseline vs Surrogate Metrics,估计LLM 一致性:用户基线与代代用计量,http://arxiv.org/abs/2505.23799v2
745,"Large language models (LLMs) increasingly shape public understanding and civic decisions, yet their ideological neutrality is a growing concern. While existing research has explored various forms of LLM bias, a direct, cross-lingual comparison of models with differing geopolitical alignments-specifically a PRC-system model versus a non-PRC counterpart-has been lacking. This study addresses this gap by systematically evaluating DeepSeek-R1 (PRC-aligned) against ChatGPT o3-mini-high (non-PRC) for Chinese-state propaganda and anti-U.S. sentiment. We developed a novel corpus of 1,200 de-contextualized, reasoning-oriented questions derived from Chinese-language news, presented in Simplified Chinese, Traditional Chinese, and English. Answers from both models (7,200 total) were assessed using a hybrid evaluation pipeline combining rubric-guided GPT-4o scoring with human annotation. Our findings reveal significant model-level and language-dependent biases. DeepSeek-R1 consistently exhibited substantially higher proportions of both propaganda and anti-U.S. bias compared to ChatGPT o3-mini-high, which remained largely free of anti-U.S. sentiment and showed lower propaganda levels. For DeepSeek-R1, Simplified Chinese queries elicited the highest bias rates; these diminished in Traditional Chinese and were nearly absent in English. Notably, DeepSeek-R1 occasionally responded in Simplified Chinese to Traditional Chinese queries and amplified existing PRC-aligned terms in its Chinese answers, demonstrating an ""invisible loudspeaker"" effect. Furthermore, such biases were not confined to overtly political topics but also permeated cultural and lifestyle content, particularly in DeepSeek-R1.",,"PeiHsuan Huang, ZihWei Lin, Simon Imbot, WenCheng Fu, Ethan Tu",2025-06-02T15:54:06Z,Analysis of LLM Bias (Chinese Propaganda & Anti-US Sentiment) in   DeepSeek-R1 vs. ChatGPT o3-mini-high,Analyse von LLM Bias (chinesische Propaganda & Anti-US-Sentiment) in DeepSeek-R1 gegen ChatGPT o3-mini-high,《DeepSeek-R1诉ChatGPT o3-mini-high案LLM Bias(中国宣传反美国情绪)的分析》,http://arxiv.org/abs/2506.01814v1
746,"Is automated hallucination detection possible? In this work, we introduce a theoretical framework to analyze the feasibility of automatically detecting hallucinations produced by large language models (LLMs). Inspired by the classical Gold-Angluin framework for language identification and its recent adaptation to language generation by Kleinberg and Mullainathan, we investigate whether an algorithm, trained on examples drawn from an unknown target language $K$ (selected from a countable collection) and given access to an LLM, can reliably determine whether the LLM's outputs are correct or constitute hallucinations.   First, we establish an equivalence between hallucination detection and the classical task of language identification. We prove that any hallucination detection method can be converted into a language identification method, and conversely, algorithms solving language identification can be adapted for hallucination detection. Given the inherent difficulty of language identification, this implies that hallucination detection is fundamentally impossible for most language collections if the detector is trained using only correct examples from the target language.   Second, we show that the use of expert-labeled feedback, i.e., training the detector with both positive examples (correct statements) and negative examples (explicitly labeled incorrect statements), dramatically changes this conclusion. Under this enriched training regime, automated hallucination detection becomes possible for all countable language collections.   These results highlight the essential role of expert-labeled examples in training hallucination detectors and provide theoretical support for feedback-based methods, such as reinforcement learning with human feedback (RLHF), which have proven critical for reliable LLM deployment.",,"Amin Karbasi, Omar Montasser, John Sous, Grigoris Velegkas",2025-06-02T15:53:18Z,(Im)possibility of Automated Hallucination Detection in Large Language   Models,(Un)Möglichkeit der automatischen Halluzinationserkennung in großen Sprachmodellen,(m) 在大语言模型中自动发现幻觉的可能性,http://arxiv.org/abs/2504.17004v2
747,"In this paper we describe NAVER LABS Europe submission to the instruction-following speech processing short track at IWSLT 2025. We participate in the constrained settings, developing systems that can simultaneously perform ASR, ST, and SQA tasks from English speech input into the following target languages: Chinese, Italian, and German. Our solution leverages two pretrained modules: (1) a speech-to-LLM embedding projector trained using representations from the SeamlessM4T-v2-large speech encoder; and (2) LoRA adapters trained on text data on top of a Llama-3.1-8B-Instruct. These modules are jointly loaded and further instruction-tuned for 1K steps on multilingual and multimodal data to form our final system submitted for evaluation.",,"Beomseok Lee, Marcely Zanon Boito, Laurent Besacier, Ioan Calapodescu",2025-06-02T15:52:57Z,NAVER LABS Europe Submission to the Instruction-following Track,NAVER LABS Europe Unterbreitung in die Anleitung-folgende Spur,欧洲提交指示执行轨道的欧洲划界案,http://arxiv.org/abs/2506.01808v1
748,"Text classification assigns text to predefined categories. Traditional methods struggle with complex structures and long-range dependencies. Deep learning with recurrent neural networks and Transformer models has improved feature extraction and context awareness. However, these models still trade off interpretability, efficiency and contextual range. We propose the Dynamic Bidirectional Elman Attention Network (DBEAN). DBEAN combines bidirectional temporal modeling and self-attention. It dynamically weights critical input segments and preserves computational efficiency.",,"Dong Xu, ZhengLin Lai, MengYao Liao, Xueliang Li, Junkai Ji",2025-06-02T15:47:08Z,A Dual-Directional Context-Aware Test-Time Learning for Text   Classification,Ein Dual-Directional Context-Aware Test-Time Learning für die Textklassifikation,文本分类双调背景-软件-测试-时间学习,http://arxiv.org/abs/2503.15469v4
749,"Data diversity is crucial for the instruction tuning of large language models. Existing studies have explored various diversity-aware data selection methods to construct high-quality datasets and enhance model performance. However, the fundamental problem of precisely defining and measuring data diversity remains underexplored, limiting clear guidance for data engineering. To address this, we systematically analyze 11 existing diversity measurement methods by evaluating their correlation with model performance through extensive fine-tuning experiments. Our results indicate that a reliable diversity measure should properly account for both inter-sample differences and the information density in the sample space. Building on this, we propose NovelSum, a new diversity metric based on sample-level ""novelty."" Experiments on both simulated and real-world data show that NovelSum accurately captures diversity variations and achieves a 0.97 correlation with instruction-tuned model performance, highlighting its value in guiding data engineering practices. With NovelSum as an optimization objective, we further develop a greedy, diversity-oriented data selection strategy that outperforms existing approaches, validating both the effectiveness and practical significance of our metric. The code is available at https://github.com/UmeanNever/NovelSum.",,"Yuming Yang, Yang Nan, Junjie Ye, Shihan Dou, Xiao Wang, Shuo Li, Huijie Lv, Mingqi Wu, Tao Gui, Qi Zhang, Xuanjing Huang",2025-06-02T15:41:05Z,Measuring Data Diversity for Instruction Tuning: A Systematic Analysis   and A Reliable Metric,Messdatenvielfalt für die Instruction Tuning: Eine systematische Analyse und eine zuverlässige Metric,用于教学图示的衡量数据多样性:系统分析和可靠计量,http://arxiv.org/abs/2502.17184v5
750,"Inverse tasks can uncover potential reasoning gaps as Large Language Models (LLMs) scale up. In this work, we explore the redefinition task, in which we assign alternative values to well-known physical constants and units of measure, prompting LLMs to respond accordingly. Our findings show that not only does model performance degrade with scale, but its false confidence also rises. Moreover, while factors such as prompting strategies or response formatting are influential, they do not preclude LLMs from anchoring to memorized values.",,"Elena Stringli, Maria Lymperaiou, Giorgos Filandrianos, Athanasios Voulodimos, Giorgos Stamou",2025-06-02T15:40:35Z,Pitfalls of Scale: Investigating the Inverse Task of Redefinition in   Large Language Models,Pitfalls of Scale: Untersuchung der inversen Aufgabe der Neudefinition in großen Sprachmodellen,缩放空隙:调查大语言模式重新定义的逆向任务,http://arxiv.org/abs/2502.12821v2
751,"While Retrieval-Augmented Generation (RAG) has emerged as an effective approach for addressing the knowledge outdating problem in Large Language Models (LLMs), it still faces a critical challenge: the prevalence of outdated information in knowledge bases. Current research primarily focuses on incorporating up-to-date information, yet the impact of outdated information coexisting in retrieval sources remains inadequately addressed. To bridge this gap, we introduce HoH, the first benchmark specifically designed to evaluate the impact of outdated information on RAG. Our benchmark leverages token-level diff algorithms combined with LLM pipelines to efficiently create a large-scale QA dataset that accurately captures the evolution of temporal knowledge in real-world facts. Through comprehensive experiments, we reveal that outdated information significantly degrades RAG performance in two critical ways: (1) it substantially reduces response accuracy by distracting models from correct information, and (2) it can mislead models into generating potentially harmful outputs, even when current information is available. Current RAG approaches struggle with both retrieval and generation aspects when handling outdated information. These findings highlight the urgent need for innovative solutions to address the temporal challenges in RAG. Our code and data are available at: https://github.com/0russwest0/HoH.",,"Jie Ouyang, Tingyue Pan, Mingyue Cheng, Ruiran Yan, Yucong Luo, Jiaying Lin, Qi Liu",2025-06-02T15:39:49Z,HoH: A Dynamic Benchmark for Evaluating the Impact of Outdated   Information on Retrieval-Augmented Generation,HoH: Ein dynamischer Benchmark zur Bewertung der Auswirkungen veralteter Informationen auf die retrieval-augmentierte Generation,HoH:评估过时信息对回源一代人的影响的动态基准,http://arxiv.org/abs/2503.04800v2
752,"While large language models (LLMs) have shown promise in translating extremely low-resource languages using resources like dictionaries, the effectiveness of grammar books remains debated. This paper investigates the role of grammar books in translating extremely low-resource languages by decomposing it into two key steps: grammar rule retrieval and application. To facilitate the study, we introduce ZhuangRules, a modularized dataset of grammar rules and their corresponding test sentences. Our analysis reveals that rule retrieval constitutes a primary bottleneck in grammar-based translation. Moreover, although LLMs can apply simple rules for translation when explicitly provided, they encounter difficulties in handling more complex rules. To address these challenges, we propose representing grammar rules as code functions, considering their similarities in structure and the benefit of code in facilitating LLM reasoning. Our experiments show that using code rules significantly boosts both rule retrieval and application, ultimately resulting in a 13.1% BLEU improvement in translation.",,"Chen Zhang, Jiuheng Lin, Xiao Liu, Zekai Zhang, Yansong Feng",2025-06-02T15:36:37Z,Read it in Two Steps: Translating Extremely Low-Resource Languages with   Code-Augmented Grammar Books,Lesen Sie es in zwei Schritten: Übersetzen extrem ressourcenarmer Sprachen mit Code-Augmented Grammar Books,分两步读读:将极低资源语言转换为代码缩写语法书本。,http://arxiv.org/abs/2506.01796v1
753,"Currently, nearly all evaluations of foundation models focus on objective metrics, emphasizing quiz performance to define model capabilities. While this model-centric approach enables rapid performance assessment, it fails to reflect authentic human experiences. To address this gap, we propose a Human-Centric subjective Evaluation (HCE) framework, focusing on three core dimensions: problem-solving ability, information quality, and interaction experience. Through experiments involving Deepseek R1, OpenAI o3 mini, Grok 3, and Gemini 2.5, we conduct over 540 participant-driven evaluations, where humans and models collaborate on open-ended research tasks, yielding a comprehensive subjective dataset. This dataset captures diverse user feedback across multiple disciplines, revealing distinct model strengths and adaptability. Our findings highlight Grok 3's superior performance, followed by Deepseek R1 and Gemini 2.5, with OpenAI o3 mini lagging behind. By offering a novel framework and a rich dataset, this study not only enhances subjective evaluation methodologies but also lays the foundation for standardized, automated assessments, advancing LLM development for research and practical scenarios. Our dataset link is https://github.com/yijinguo/Human-Centric-Evaluation.",,"Yijin Guo, Kaiyuan Ji, Xiaorong Zhu, Junying Wang, Farong Wen, Chunyi Li, Zicheng Zhang, Guangtao Zhai",2025-06-02T15:33:29Z,Human-Centric Evaluation for Foundation Models,Human-Centric Evaluation für Stiftungsmodelle,基金会模型的人类环境评价,http://arxiv.org/abs/2506.01793v1
754,"While Large Language Models (LLMs) excel at many natural language processing tasks, they often suffer from factual inaccuracies in knowledge-intensive scenarios. Integrating external knowledge resources, particularly knowledge graphs (KGs), provides a transparent and updatable foundation for more reliable reasoning. Knowledge Base Question Answering (KBQA), which queries and reasons over KGs, is central to this effort, especially for complex, multi-hop queries. However, multi-hop reasoning poses two key challenges: (1)~maintaining coherent reasoning paths, and (2)~avoiding prematurely discarding critical multi-hop connections. To address these issues, we introduce iQUEST, a question-guided KBQA framework that iteratively decomposes complex queries into simpler sub-questions, ensuring a structured and focused reasoning trajectory. Additionally, we integrate a Graph Neural Network (GNN) to look ahead and incorporate 2-hop neighbor information at each reasoning step. This dual approach strengthens the reasoning process, enabling the model to explore viable paths more effectively. Detailed experiments demonstrate the consistent improvement delivered by iQUEST across four benchmark datasets and four LLMs.",,"Shuai Wang, Yinan Yu",2025-06-02T15:30:02Z,iQUEST: An Iterative Question-Guided Framework for Knowledge Base   Question Answering,iQUEST: Ein iteratives Frage-Framework für die Beantwortung von Fragen in der Wissensdatenbank,i. 知识基础问题解答的动态问题指导框架,http://arxiv.org/abs/2506.01784v1
755,"Recent improvement in large language model performance have, in all likelihood, been accompanied by improvement in how well they can approximate the distribution of their training data. In this work, we explore the following question: which properties of text domains do LLMs faithfully approximate, and how well do they do so? Applying observational approaches familiar from corpus linguistics, we prompt a commonly used, opensource LLM to regenerate text from two domains of permissively licensed English text which are often contained in LLM training data -- Wikipedia and news text. This regeneration paradigm allows us to investigate whether LLMs can faithfully match the original human text domains in a fairly semantically-controlled setting. We investigate varying levels of syntactic abstraction, from more simple properties like sentence length, and article readability, to more complex and higher order properties such as dependency tag distribution, parse depth, and parse complexity. We find that the majority of the regenerated distributions show a shifted mean, a lower standard deviation, and a reduction of the long tail, as compared to the human originals.",,"Da Ju, Hagen Blix, Adina Williams",2025-06-02T15:27:28Z,Domain Regeneration: How well do LLMs match syntactic properties of text   domains?,Domain Regeneration: Wie gut stimmen LLMs mit syntaktischen Eigenschaften von Textdomänen überein?,域再生: LLMs 如何匹配文本域的合成特性 ?,http://arxiv.org/abs/2505.07784v2
756,"Motivated by the surge of large language models, there has been a push to formally characterize the symbolic abilities intrinsic to the transformer architecture. A programming language, called RASP, has been proposed, which can be directly compiled into transformer weights to implement these algorithms. However, the tasks that can be implemented in RASP are often uncommon to learn from natural unsupervised data, showing a mismatch between theoretical capabilities of the transformer architecture, and the practical learnability of these capabilities from unsupervised data. We propose tracr-injection, a method that allows us to distill algorithms written in RASP directly into a pre-trained language model. We showcase our method by injecting 3 different algorithms into a language model. We show how our method creates an interpretable subspace within the model's residual stream, which can be decoded into the variables present in the code of the RASP algorithm. Additionally, we found that the proposed method can improve out-of-distribution performance compared to our baseline, indicating that indeed a more symbolic mechanism is taking place in the inner workings of the model. We release the code used to run our experiments.",,"Tomás Vergara-Browne, Álvaro Soto",2025-06-02T15:23:49Z,Tracr-Injection: Distilling Algorithms into Pre-trained Language Models,Tracr-Injektion: Destillieren von Algorithmen in vortrainierte Sprachmodelle,Tracr-注射:将算法提炼成预培训语言模型,http://arxiv.org/abs/2505.10719v3
757,"Kwak'wala is an Indigenous language spoken in British Columbia, with a rich legacy of published documentation spanning more than a century, and an active community of speakers, teachers, and learners engaged in language revitalization. Over 11 volumes of the earliest texts created during the collaboration between Franz Boas and George Hunt have been scanned but remain unreadable by machines. Complete digitization through optical character recognition has the potential to facilitate transliteration into modern orthographies and the creation of other language technologies. In this paper, we apply the latest OCR techniques to a series of Kwak'wala texts only accessible as images, and discuss the challenges and unique adaptations necessary to make such technologies work for these real-world texts. Building on previous methods, we propose using a mix of off-the-shelf OCR methods, language identification, and masking to effectively isolate Kwak'wala text, along with post-correction models, to produce a final high-quality transcription.",,"Milind Agarwal, Daisy Rosenblum, Antonios Anastasopoulos",2025-06-02T15:20:09Z,Developing a Mixed-Methods Pipeline for Community-Oriented Digitization   of Kwak'wala Legacy Texts,Entwicklung einer Mixed-Methods-Pipeline für die gemeinschaftsorientierte Digitalisierung von Kwak'wala Legacy Texts,为Kwak'wala遗留文字的社区数字化开发混合方法管道,http://arxiv.org/abs/2506.01775v1
758,"The impressive multimodal capabilities demonstrated by OpenAI's GPT-4 have generated significant interest in the development of Multimodal Large Language Models (MLLMs). Visual instruction tuning of MLLMs with machine-generated instruction-following data has shown to enhance zero-shot capabilities across various tasks. However, there has been limited exploration into controlling the quality of the instruction data.Current methodologies for data selection in MLLMs often rely on single, unreliable scores or use downstream tasks for selection, which is time-consuming and can lead to potential overfitting on the chosen evaluation datasets. To mitigate these limitations, we propose a novel data selection methodology that utilizes image-text correlation and model perplexity to evaluate and select data of varying quality. This approach leverages the distinct distribution of these two attributes, mapping data quality into a two-dimensional space that allows for the selection of data based on their location within this distribution. By utilizing this space, we can analyze the impact of task type settings, used as prompts, on data quality. Additionally, this space can be used to construct multi-stage subsets of varying quality to facilitate curriculum learning. Our research includes comprehensive experiments conducted on various datasets. The results emphasize substantial enhancements in five commonly assessed capabilities compared to using the complete dataset. Our codes, data, and models are publicly available at: https://anonymous.4open.science/r/EHIT-31B4",,"Biao Wu, Ling Chen",2025-06-02T15:14:22Z,Curriculum Learning with Quality-Driven Data Selection,Curriculum-Lernen mit qualitativ hochwertiger Datenauswahl,高质量数据选择学习,http://arxiv.org/abs/2407.00102v2
759,"Text-to-image generation models have recently achieved astonishing results in image quality, flexibility, and text alignment, and are consequently employed in a fast-growing number of applications. Through improvements in multilingual abilities, a larger community now has access to this technology. However, our results show that multilingual models suffer from significant gender biases just as monolingual models do. Furthermore, the natural expectation that multilingual models will provide similar results across languages does not hold up. Instead, there are important differences between languages. We propose a novel benchmark, MAGBIG, intended to foster research on gender bias in multilingual models. We use MAGBIG to investigate the effect of multilingualism on gender bias in T2I models. To this end, we construct multilingual prompts requesting portraits of people with a certain occupation or trait. Our results show that not only do models exhibit strong gender biases but they also behave differently across languages. Furthermore, we investigate prompt engineering strategies, such as indirect, neutral formulations, to mitigate these biases. Unfortunately, these approaches have limited success and result in worse text-to-image alignment. Consequently, we call for more research into diverse representations across languages in image generators, as well as into steerability to address biased model behavior.",,"Felix Friedrich, Katharina Hämmerl, Patrick Schramowski, Manuel Brack, Jindrich Libovicky, Kristian Kersting, Alexander Fraser",2025-06-02T15:04:03Z,Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and   Prompt Engineering May Not Help You,Mehrsprachige Text-zu-Bild-Generation vergrössert Geschlechterstereotypen und Prompt-Engineering kann Ihnen nicht helfen,"多语言文字到图像一代会放大性别陈规定型观念和迅速工程,但无济于事。",http://arxiv.org/abs/2401.16092v4
760,"The advancement of Large Language Models (LLMs) has spurred significant interest in Role-Playing Agents (RPAs) for applications such as emotional companionship and virtual interaction. However, recent RPAs are often built on explicit dialogue data, lacking deep, human-like internal thought processes, resulting in superficial knowledge and style expression. While Large Reasoning Models (LRMs) can be employed to simulate character thought, their direct application is hindered by attention diversion (i.e., RPAs forget their role) and style drift (i.e., overly formal and rigid reasoning rather than character-consistent reasoning). To address these challenges, this paper introduces a novel Role-Aware Reasoning (RAR) method, which consists of two important stages: Role Identity Activation (RIA) and Reasoning Style Optimization (RSO). RIA explicitly guides the model with character profiles during reasoning to counteract attention diversion, and then RSO aligns reasoning style with the character and scene via LRM distillation to mitigate style drift. Extensive experiments demonstrate that the proposed RAR significantly enhances the performance of RPAs by effectively addressing attention diversion and style drift.",,"Yihong Tang, Kehai Chen, Muyun Yang, Zhengyu Niu, Jing Li, Tiejun Zhao, Min Zhang",2025-06-02T14:55:04Z,Thinking in Character: Advancing Role-Playing Agents with Role-Aware   Reasoning,In Charakter denken: Rollenspiel-Agenten mit rollenbewusster Vernunft fördern,性格思考:提高角色扮演代理人的作用,http://arxiv.org/abs/2506.01748v1
761,"In face-to-face interaction, we use multiple modalities, including speech and gestures, to communicate information and resolve references to objects. However, how representational co-speech gestures refer to objects remains understudied from a computational perspective. In this work, we address this gap by introducing a multimodal reference resolution task centred on representational gestures, while simultaneously tackling the challenge of learning robust gesture embeddings. We propose a self-supervised pre-training approach to gesture representation learning that grounds body movements in spoken language. Our experiments show that the learned embeddings align with expert annotations and have significant predictive power. Moreover, reference resolution accuracy further improves when (1) using multimodal gesture representations, even when speech is unavailable at inference time, and (2) leveraging dialogue history. Overall, our findings highlight the complementary roles of gesture and speech in reference resolution, offering a step towards more naturalistic models of human-machine interaction.",,"Esam Ghaleb, Bulat Khaertdinov, Aslı Özyürek, Raquel Fernández",2025-06-02T14:52:36Z,I see what you mean: Co-Speech Gestures for Reference Resolution in   Multimodal Dialogue,"Ich verstehe, was Sie meinen: Co-Speech Gestures for Reference Resolution in Multimodal Dialogue","我理解你的意思:在多模式对话中,用共同语音手势解决参考问题",http://arxiv.org/abs/2503.00071v2
762,"Large Language Models (LLMs) exhibit impressive performance on complex reasoning tasks, yet they frequently fail on basic numerical problems, producing incorrect outputs. Inspired by Benford's Law -- a statistical pattern where lower digits occur more frequently as leading digits -- we hypothesize that the long-tailed digit distributions in web-collected corpora may be learned by LLMs during pretraining, leading to biased numerical generation. To investigate the hypothesis, we first examine whether digits frequencies in pretraining corpus (OLMo2) follows Benford's law. We then construct an evaluation benchmark with uniformly distributed ground-truth digits across seven numerical reasoning tasks. Our evaluation results demonstrate that leading open-source LLMs show a consistent pattern of digit bias that resembles Benford's law. Through logit-lens tracing and neuron-level dissection, we identify that this bias arises predominantly from a small subset of highly digit-selective feed-forward network (FFN) neurons in the deeper layers. Finally, we demonstrate that pruning these neurons mitigates imbalanced overgeneration and partially corrects erroneous outputs, providing causal evidence that fine-grained pretraining digit bias can propagate into model behavior. Our findings reveal a fundamental connection between corpus-level statistics and symbolic failure modes in LLMs, offering a new lens for diagnosing and mitigating hallucinations in numerical tasks.",,"Jiandong Shao, Yao Lu, Jianfei Yang",2025-06-02T14:44:30Z,Benford's Curse: Tracing Digit Bias to Numerical Hallucination in LLMs,Benfords Fluch: Digit Bias in LLMs zur numerischen Halluzination verfolgen,"Benford的诅咒:追踪Digit Bias, 以LLM号",http://arxiv.org/abs/2506.01734v1
763,"Large Language Models (LLMs) are pre-trained on large amounts of data from different sources and domains. These data most often contain trillions of tokens with large portions of copyrighted or proprietary content, which hinders the usage of such models under AI legislation. This raises the need for truly open pre-training data that is compliant with the data security regulations. In this paper, we introduce Common Corpus, the largest open dataset for language model pre-training. The data assembled in Common Corpus are either uncopyrighted or under permissible licenses and amount to about two trillion tokens. The dataset contains a wide variety of languages, ranging from the main European languages to low-resource ones rarely present in pre-training datasets; in addition, it includes a large portion of code data. The diversity of data sources in terms of covered domains and time periods opens up the paths for both research and entrepreneurial needs in diverse areas of knowledge. In this technical report, we present the detailed provenance of data assembling and the details of dataset filtering and curation. Being already used by such industry leaders as Anthropic and multiple LLM training projects, we believe that Common Corpus will become a critical infrastructure for open science research in LLMs.",,"Pierre-Carl Langlais, Carlos Rosas Hinostroza, Mattia Nee, Catherine Arnett, Pavel Chizhov, Eliot Krzystof Jones, Irène Girard, David Mach, Anastasia Stasenko, Ivan P. Yamshchikov",2025-06-02T14:43:15Z,Common Corpus: The Largest Collection of Ethical Data for LLM   Pre-Training,Common Corpus: Die größte Sammlung ethischer Daten für LLM Pre-Training,共同单位:为LLM培训前收集的道德数据最多,http://arxiv.org/abs/2506.01732v1
764,"This paper presents an AI-generated review of Vision-Language-Action (VLA) models, summarizing key methodologies, findings, and future directions. The content is produced using large language models (LLMs) and is intended only for demonstration purposes. This work does not represent original research, but highlights how AI can help automate literature reviews. As AI-generated content becomes more prevalent, ensuring accuracy, reliability, and proper synthesis remains a challenge. Future research will focus on developing a structured framework for AI-assisted literature reviews, exploring techniques to enhance citation accuracy, source credibility, and contextual understanding. By examining the potential and limitations of LLM in academic writing, this study aims to contribute to the broader discussion of integrating AI into research workflows. This work serves as a preliminary step toward establishing systematic approaches for leveraging AI in literature review generation, making academic knowledge synthesis more efficient and scalable.",,"Adilzhan Adilkhanov, Amir Yelenov, Assylkhan Seitzhanov, Ayan Mazhitov, Azamat Abdikarimov, Danissa Sandykbayeva, Daryn Kenzhebek, Dinmukhammed Mukashev, Ilyas Umurbekov, Jabrail Chumakov, Kamila Spanova, Karina Burunchina, Madina Yergibay, Margulan Issa, Moldir Zabirova, Nurdaulet Zhuzbay, Nurlan Kabdyshev, Nurlan Zhaniyar, Rasul Yermagambet, Rustam Chibar, Saltanat Seitzhan, Soibkhon Khajikhanov, Tasbolat Taunyazov, Temirlan Galimzhanov, Temirlan Kaiyrbay, Tleukhan Mussin, Togzhan Syrymova, Valeriya Kostyukova, Yerkebulan Massalim, Yermakhan Kassym, Zerde Nurbayeva, Zhanat Kappassov",2025-06-02T14:38:08Z,Survey on Vision-Language-Action Models,Umfrage zu Vision-Language-Action-Modellen,展望-语言-行动模式调查,http://arxiv.org/abs/2502.06851v3
765,"In-context learning (ICL) enables large language models (LLMs) to perform downstream tasks through advanced prompting and high-quality demonstrations. However, traditional ICL paradigms encounter significant limitations in complex reasoning tasks, stemming primarily from their dependence on example quality and absence of explicit reasoning guidance. To address these challenges, we introduce HiAR-ICL, a **Hi**gh-level **A**utomated **R**easoning paradigm in **ICL** that shifts focus from specific examples to abstract reasoning patterns, thereby extending the conventional concept of ""context"" in ICL. Our approach begins by defining five atomic reasoning actions, upon which we employ Monte Carlo Tree Search to systematically construct high-level reasoning patterns. During inference, HiAR-ICL dynamically selects appropriate reasoning patterns based on problem attributes, providing explicit guidance for the model's reasoning process. Experiments demonstrate HiAR-ICL's effectiveness and efficiency: utilizing only 200 prior samples with Qwen2.5-7B-Instruct, our method achieves 80.6% accuracy on MATH and 62.5% on AMC, exceeding GPT-4o's 77.2% and 57.5%. Our approach enhances performance across models of varying sizes while generalizing effectively across domains. Further analysis reveals that HiAR-ICL can also serve as a plug-and-play inference method compatible with post-training techniques like GRPO. Code and data are available at https://github.com/jinyangwu/HiARICL.",,"Jinyang Wu, Mingkuan Feng, Shuai Zhang, Feihu Che, Zengqi Wen, Chonghua Liao, Jianhua Tao",2025-06-02T14:26:19Z,Beyond Examples: High-level Automated Reasoning Paradigm in In-Context   Learning via MCTS,Beyond examples: High-level Automated Reasoning Paradigm im In-Context Learning via MCTS,更多实例:通过MCTS学习的内文学习中高级自动说明理由的范例,http://arxiv.org/abs/2411.18478v2
766,"Large language models are quickly becoming the foundation for intelligent agents that are capable of using tools. However, training such agents is challenging because it requires human creation and annotation of a diverse set of tasks, tools, and evaluation criteria. In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself. The agent first plays the role of challenger and generates a task after interacting with the given tools. The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. The agent then takes an executor role and trains on those tasks with reinforcement learning using the evaluation feedback as a reward. Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.",,"Yifei Zhou, Sergey Levine, Jason Weston, Xian Li, Sainbayar Sukhbaatar",2025-06-02T14:23:33Z,Self-Challenging Language Model Agents,Selbstbeherrschende Sprachmodell-Agenten,自我挑战语言示范语言代理,http://arxiv.org/abs/2506.01716v1
767,"We introduce GrammaMT, a grammatically-aware prompting approach for machine translation that uses Interlinear Glossed Text (IGT), a common form of linguistic description providing morphological and lexical annotations for source sentences. GrammaMT proposes three prompting strategies: gloss-shot, chain-gloss and model-gloss. All are training-free, requiring only a few examples that involve minimal effort to collect, and making them well-suited for low-resource setups. Experiments show that GrammaMT enhances translation performance on open-source instruction-tuned LLMs for various low- to high-resource languages across three benchmarks: (1) the largest IGT corpus, (2) the challenging 2023 SIGMORPHON Shared Task data over endangered languages, and (3) even in an out-of-domain setting with FLORES. Moreover, ablation studies reveal that leveraging gloss resources could substantially boost MT performance (by over 17 BLEU points) if LLMs accurately generate or access input sentence glosses.",,"Rita Ramos, Everlyn Asiko Chimoto, Maartje ter Hoeve, Natalie Schluter",2025-06-02T14:23:25Z,GrammaMT: Improving Machine Translation with Grammar-Informed In-Context   Learning,GrammaMT: Verbesserung der maschinellen Übersetzung mit grammar-informiertem In-Context-Lernen,GrammaMT:用文法不完善的文法学习改进机器翻译,http://arxiv.org/abs/2410.18702v2
768,"Multimodal large language models (MLLMs) have shown promising capabilities in reasoning tasks, yet still struggle with complex problems requiring explicit self-reflection and self-correction, especially compared to their unimodal text-based counterparts. Existing reflection methods are simplistic and struggle to generate meaningful and instructive feedback, as the reasoning ability and knowledge limits of pre-trained models are largely fixed during initial training. To overcome these challenges, we propose Multimodal Self-Reflection enhanced reasoning with Group Relative Policy Optimization (SRPO), a two-stage reflection-aware reinforcement learning (RL) framework explicitly designed to enhance multimodal LLM reasoning. In the first stage, we construct a high-quality, reflection-focused dataset under the guidance of an advanced MLLM, which generates reflections based on initial responses to help the policy model learn both reasoning and self-reflection. In the second stage, we introduce a novel reward mechanism within the GRPO framework that encourages concise and cognitively meaningful reflection while avoiding redundancy. Extensive experiments across multiple multimodal reasoning benchmarks, including MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B and Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms state-of-the-art models, achieving notable improvements in both reasoning accuracy and reflection quality.",,"Zhongwei Wan, Zhihao Dou, Che Liu, Yu Zhang, Dongfei Cui, Qinjian Zhao, Hui Shen, Jing Xiong, Yi Xin, Yifan Jiang, Yangfan He, Mi Zhang, Shen Yan",2025-06-02T14:21:44Z,SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware   Reinforcement Learning,SRPO: Verbesserung der multimodalen LLM-Reasoning durch Reflection-Aware-Verstärkung,"SRPO: 通过反射-软件强化学习,加强多式LLM",http://arxiv.org/abs/2506.01713v1
769,"Table reasoning, encompassing tasks such as table question answering, fact verification, and text-to-SQL, requires precise understanding of structured tabular data, coupled with numerical computation and code manipulation for effective inference. Supervised fine-tuning (SFT) approaches have achieved notable success but often struggle with generalization and robustness due to biases inherent in imitative learning. We introduce Reasoning-Table, the first application of reinforcement learning (RL) to table reasoning, achieving state-of-the-art performance. Through rigorous data preprocessing, reward design, and tailored training strategies, our method leverages simple rule-based outcome rewards to outperform SFT across multiple benchmarks. Unified training across diverse tasks enables Reasoning-Table to emerge as a robust table reasoning large language model, surpassing larger proprietary models like Claude-3.7-Sonnet by 4.0% on table reasoning benchmarks. The approach also achieves excellent performance on text-to-SQL tasks, reaching 68.3% performance on the BIRD dev dataset with a 7B model. Further experiments demonstrate that Reasoning-Table enhances the model's generalization capabilities and robustness.",,"Fangyu Lei, Jinxiang Meng, Yiming Huang, Tinghong Chen, Yun Zhang, Shizhu He, Jun Zhao, Kang Liu",2025-06-02T14:18:09Z,Reasoning-Table: Exploring Reinforcement Learning for Table Reasoning,Reasoning-Table: Erforschendes Erlernen von Verstärkungslernen für die Tabellenveranlagung,理由表:探索为表格理由而加强学习,http://arxiv.org/abs/2506.01710v1
770,"We investigate fairness dynamics during Large Language Model (LLM) training to enable the diagnoses of biases and mitigations through training interventions like early stopping; we find that biases can emerge suddenly and do not always follow common performance metrics. We introduce two new metrics to evaluate fairness dynamics holistically during model pre-training: Average Rank and Jensen-Shannon Divergence by Parts. These metrics provide insights into the Pythia models' progression of biases in gender prediction of occupations on the WinoBias dataset. By monitoring these dynamics, we find that (1) Pythia-6.9b is biased towards men; it becomes more performant and confident predicting ""male"" than ""female"" during training, (2) via early-stopping, Pythia-6.9b can exchange 1.7% accuracy on LAMBADA for a 92.5% increase in fairness, and (3) larger models can exhibit more bias; Pythia-6.9b makes more assumptions about gender than Pythia-160m, even when a subject's gender is not specified.",,"Krishna Patel, Nivedha Sivakumar, Barry-John Theobald, Luca Zappella, Nicholas Apostoloff",2025-06-02T14:15:37Z,Fairness Dynamics During Training,Fairness-Dynamik während des Trainings,培训期间的公平动态,http://arxiv.org/abs/2506.01709v1
771,"Theory-of-Mind (ToM), the ability to infer others' perceptions and mental states, is fundamental to human interaction but remains challenging for Large Language Models (LLMs). While existing ToM reasoning methods show promise with reasoning via perceptual perspective-taking, they often rely excessively on off-the-shelf LLMs, reducing their efficiency and limiting their applicability to high-order ToM reasoning. To address these issues, we present EnigmaToM, a novel neuro-symbolic framework that enhances ToM reasoning by integrating a Neural Knowledge Base of entity states (Enigma) for (1) a psychology-inspired iterative masking mechanism that facilitates accurate perspective-taking and (2) knowledge injection that elicits key entity information. Enigma generates structured knowledge of entity states to build spatial scene graphs for belief tracking across various ToM orders and enrich events with fine-grained entity state details. Experimental results on ToMi, HiToM, and FANToM benchmarks show that EnigmaToM significantly improves ToM reasoning across LLMs of varying sizes, particularly excelling in high-order reasoning scenarios.",,"Hainiu Xu, Siya Qi, Jiazheng Li, Yuxiang Zhou, Jinhua Du, Caroline Catmur, Yulan He",2025-06-02T14:15:13Z,EnigmaToM: Improve LLMs' Theory-of-Mind Reasoning Capabilities with   Neural Knowledge Base of Entity States,"EnigmaToM: Verbesserung der Fähigkeit von LLMs, mit neuraler Wissensbasis von Entitätsstaaten die Theorie der Mind-of-Mind-Reasoning-Fähigkeiten zu verbessern",Enigma ToM:提高实体国神经知识库中LLMs的理学理论理性能力,http://arxiv.org/abs/2503.03340v2
772,"We propose a new method for estimating how much a model ``knows'' about a datapoint and use it to measure the capacity of modern language models. Prior studies of language model memorization have struggled to disentangle memorization from generalization. We formally separate memorization into two components: \textit{unintended memorization}, the information a model contains about a specific dataset, and \textit{generalization}, the information a model contains about the true data-generation process. When we completely eliminate generalization, we can compute the total memorization, which provides an estimate of model capacity: our measurements estimate that GPT-style models have a capacity of approximately 3.6 bits per parameter. We train language models on datasets of increasing size and observe that models memorize until their capacity fills, at which point ``grokking'' begins, and unintended memorization decreases as models begin to generalize. We train hundreds of transformer language models ranging from $500K$ to $1.5B$ parameters and produce a series of scaling laws relating model capacity and data size to membership inference.",,"John X. Morris, Chawin Sitawarin, Chuan Guo, Narine Kokhlikyan, G. Edward Suh, Alexander M. Rush, Kamalika Chaudhuri, Saeed Mahloujifar",2025-06-02T14:13:41Z,How much do language models memorize?,Wie viel merken sich Sprachmodelle?,语言模型背书多少?,http://arxiv.org/abs/2505.24832v2
773,"To address the challenge of information overload from massive web contents, recommender systems are widely applied to retrieve and present personalized results for users. However, recommendation tasks are inherently constrained to filtering existing items and lack the ability to generate novel concepts, limiting their capacity to fully satisfy user demands and preferences. In this paper, we propose a new paradigm that goes beyond content filtering and selecting: directly generating personalized items in a multimodal form, such as images, tailored to individual users. To accomplish this, we leverage any-to-any Large Multimodal Models (LMMs) and train them in both supervised fine-tuning and online reinforcement learning strategy to equip them with the ability to yield tailored next items for users. Experiments on two benchmark datasets and user study confirm the efficacy of the proposed method. Notably, the generated images not only align well with users' historical preferences but also exhibit relevance to their potential future interests.",,"Jiongnan Liu, Zhicheng Dou, Ning Hu, Chenyan Xiong",2025-06-02T14:10:08Z,"Generate, Not Recommend: Personalized Multimodal Content Generation","Generieren, nicht empfehlen: Personalisierte multimodale Content-Generierung","生成,不建议:个性化多式内容生成",http://arxiv.org/abs/2506.01704v1
774,"The large language models (LLMs) are able to generate high-quality texts in multiple languages. Such texts are often not recognizable by humans as generated, and therefore present a potential of LLMs for misuse (e.g., plagiarism, spams, disinformation spreading). An automated detection is able to assist humans to indicate the machine-generated texts; however, its robustness to out-of-distribution data is still challenging. This notebook describes our mdok approach in robust detection, based on fine-tuning smaller LLMs for text classification. It is applied to both subtasks of Voight-Kampff Generative AI Detection 2025, providing remarkable performance in binary detection as well as in multiclass (1st rank) classification of various cases of human-AI collaboration.",,Dominik Macko,2025-06-02T14:07:32Z,mdok of KInIT: Robustly Fine-tuned LLM for Binary and Multiclass   AI-Generated Text Detection,mdok von KInIT: Robust fein abgestimmtes LLM für Binär- und Mehrklassen-KI-generierte Texterkennung,KInIT 的 mdok: 用于二进制和多级 AI 光导文本检测的精密微调LLMLM,http://arxiv.org/abs/2506.01702v1
775,"Knowledge Editing (KE) has gained increasing attention, yet current KE tasks remain relatively simple. Under current evaluation frameworks, many editing methods achieve exceptionally high scores, sometimes nearing perfection. However, few studies integrate KE into real-world application scenarios (e.g., recent interest in LLM-as-agent). To support our analysis, we introduce a novel script-based benchmark -- ScEdit (Script-based Knowledge Editing Benchmark) -- which encompasses both counterfactual and temporal edits. We integrate token-level and text-level evaluation methods, comprehensively analyzing existing KE techniques. The benchmark extends traditional fact-based (""What""-type question) evaluation to action-based (""How""-type question) evaluation. We observe that all KE methods exhibit a drop in performance on established metrics and face challenges on text-level metrics, indicating a challenging task. Our benchmark is available at https://github.com/asdfo123/ScEdit.",,"Xinye Li, Zunwen Zheng, Qian Zhang, Dekai Zhuang, Jiabao Kang, Liyan Xu, Qingbin Liu, Xi Chen, Zhiying Tu, Dianhui Chu, Dianbo Sui",2025-06-02T14:05:59Z,ScEdit: Script-based Assessment of Knowledge Editing,ScEdit: Script-basierte Bewertung von Wissensbearbeitung,ScEdit: 基于脚本的知识编辑评估,http://arxiv.org/abs/2505.23291v2
776,"Chain-of-thought (CoT) prompting has achieved remarkable success in natural language processing (NLP). However, its vast potential remains largely unexplored for graphs. This raises an interesting question: How can we design CoT prompting for graphs to guide graph models to learn step by step? On one hand, unlike natural languages, graphs are non-linear and characterized by complex topological structures. On the other hand, many graphs lack textual data, making it difficult to formulate language-based CoT prompting. In this work, we propose the first CoT prompt learning framework for text-free graphs, GCoT. Specifically, we decompose the adaptation process for each downstream task into a series of inference steps, with each step consisting of prompt-based inference, ``thought'' generation, and thought-conditioned prompt learning. While the steps mimic CoT prompting in NLP, the exact mechanism differs significantly. Specifically, at each step, an input graph, along with a prompt, is first fed into a pre-trained graph encoder for prompt-based inference. We then aggregate the hidden layers of the encoder to construct a ``thought'', which captures the working state of each node in the current step. Conditioned on this thought, we learn a prompt specific to each node based on the current state. These prompts are fed into the next inference step, repeating the cycle. To evaluate and analyze the effectiveness of GCoT, we conduct comprehensive experiments on eight public datasets, which demonstrate the advantage of our approach.",,"Xingtong Yu, Chang Zhou, Zhongwei Kuai, Xinming Zhang, Yuan Fang",2025-06-02T14:02:08Z,GCoT: Chain-of-Thought Prompt Learning for Graphs,GCoT: Chain-of-Thought-Prompt-Lernen für Graphen,GGCT: 图表研究链快速学习,http://arxiv.org/abs/2502.08092v2
777,"Affective Computing (AC) is essential in bridging the gap between human emotional experiences and machine understanding. Traditionally, AC tasks in natural language processing (NLP) have been approached through pipeline architectures, which often suffer from structure rigidity that leads to inefficiencies and limited adaptability. The advent of Large Language Models (LLMs) has revolutionized this field by offering a unified approach to affective understanding and generation tasks, enhancing the potential for dynamic, real-time interactions. However, LLMs face cognitive limitations in affective reasoning, such as misinterpreting cultural nuances or contextual emotions, and hallucination problems in decision-making. To address these challenges, recent research advocates for LLM-based collaboration systems that emphasize interactions among specialized models and LLMs, mimicking human-like affective intelligence through the synergy of emotional and rational thinking that aligns with Dual Process Theory in psychology. This survey aims to provide a comprehensive overview of LLM-based collaboration systems in AC, exploring from structured collaborations to autonomous collaborations. Specifically, it includes: (1) A systematic review of existing methods, focusing on collaboration strategies, mechanisms, key functions, and applications; (2) Experimental comparisons of collaboration strategies across representative tasks in affective understanding and generation; (3) An analysis highlighting the potential of these systems to enhance robustness and adaptability in complex affective reasoning; (4) A discussion of key challenges and future research directions to further advance the field. This work is the first to systematically explore collaborative intelligence with LLMs in AC, paving the way for more powerful applications that approach human-like social intelligence.",,"Wenna Lai, Haoran Xie, Guandong Xu, Qing Li, S. Joe Qin",2025-06-02T14:00:54Z,When LLMs Team Up: The Emergence of Collaborative Affective Computing,Wenn LLMs sich zusammentun: Das Aufkommen des kollaborativen Affektiven Computing,当LLLM 团队组建时:协作性女性计算机的出现,http://arxiv.org/abs/2506.01698v1
778,"Querying generative AI models, e.g., large language models (LLMs), has become a prevalent method for information acquisition. However, existing query-answer datasets primarily focus on textual responses, making it challenging to address complex user queries that require visual demonstrations or explanations for better understanding. To bridge this gap, we construct a benchmark, RealVideoQuest, designed to evaluate the abilities of text-to-video (T2V) models in answering real-world, visually grounded queries. It identifies 7.5K real user queries with video response intents from Chatbot-Arena and builds 4.5K high-quality query-video pairs through a multistage video retrieval and refinement process. We further develop a multi-angle evaluation system to assess the quality of generated video answers. Experiments indicate that current T2V models struggle with effectively addressing real user queries, pointing to key challenges and future research opportunities in multimodal AI.",,"Shuting Wang, Yunqi Liu, Zixin Yang, Ning Hu, Zhicheng Dou, Chenyan Xiong",2025-06-02T13:52:21Z,Respond Beyond Language: A Benchmark for Video Generation in Response to   Realistic User Intents,Respond Beyond Language: Ein Benchmark für die Videogenerierung in Reaktion auf realistische Benutzer-Intents,回应超越语言:针对现实用户意图的视频制作基准,http://arxiv.org/abs/2506.01689v1
779,"Subword-level understanding is integral to numerous tasks, including understanding multi-digit numbers, spelling mistakes, abbreviations, rhyming, and wordplay. Despite this, current large language models (LLMs) still often struggle with seemingly simple subword-level tasks like How many 'r's in 'strawberry'?. A key factor behind these failures is tokenization which obscures the fine-grained structure of words. Current alternatives, such as character-level and dropout tokenization methods, significantly increase computational costs and provide inconsistent improvements. In this paper we revisit tokenization and introduce StochasTok, a simple, efficient stochastic tokenization scheme that randomly splits tokens during training, allowing LLMs to 'see' their internal structure. Our experiments show that pretraining with StochasTok substantially improves LLMs' downstream performance across multiple subword-level language games, including character counting, substring identification, and math tasks. Furthermore, StochasTok's simplicity allows seamless integration at any stage of the training pipeline; and we demonstrate that post-training with StochasTok can instill improved subword understanding into existing pretrained models, thus avoiding costly pretraining from scratch. These dramatic improvements achieved with a minimal change suggest StochasTok holds exciting potential when applied to larger, more capable models. Code open-sourced at: https://github.com/anyasims/stochastok.",,"Anya Sims, Thom Foster, Klara Kaleb, Tuan-Duy H. Nguyen, Joseph Lee, Jakob N. Foerster, Yee Whye Teh, Cong Lu",2025-06-02T13:51:11Z,StochasTok: Improving Fine-Grained Subword Understanding in LLMs,StochasTok: Verbesserung des feinkörnigen Unterwortverständnisses in LLMs,StochasTok:改进在LLM中精美的子字理解,http://arxiv.org/abs/2506.01687v1
780,"Training SER models in natural, spontaneous speech is especially challenging due to the subtle expression of emotions and the unpredictable nature of real-world audio. In this paper, we present a robust system for the INTERSPEECH 2025 Speech Emotion Recognition in Naturalistic Conditions Challenge, focusing on categorical emotion recognition. Our method combines state-of-the-art audio models with text features enriched by prosodic and spectral cues. In particular, we investigate the effectiveness of Fundamental Frequency (F0) quantization and the use of a pretrained audio tagging model. We also employ an ensemble model to improve robustness. On the official test set, our system achieved a Macro F1-score of 39.79% (42.20% on validation). Our results underscore the potential of these methods, and analysis of fusion techniques confirmed the effectiveness of Graph Attention Networks. Our source code is publicly available.",,"Alef Iury Siqueira Ferreira, Lucas Rafael Gris, Alexandre Ferro Filho, Lucas Ólives, Daniel Ribeiro, Luiz Fernando, Fernanda Lustosa, Rodrigo Tanaka, Frederico Santos de Oliveira, Arlindo Galvão Filho",2025-06-02T13:46:02Z,Enhancing Speech Emotion Recognition with Graph-Based Multimodal Fusion   and Prosodic Features for the Speech Emotion Recognition in Naturalistic   Conditions Challenge at Interspeech 2025,Verbesserung der Sprachemotionserkennung mit graphisch basierter multimodaler Fusion und prosodischen Funktionen für die Sprachemotionserkennung unter naturalistischen Bedingungen Herausforderung bei Interspeech 2025,"2025年Interspeech Interspeech会议在自然条件挑战中言论情感承认方面,用图表式多模式融合和实验特征增强对言论情绪的承认",http://arxiv.org/abs/2506.02088v1
781,"Despite substantial research efforts evaluating how well large language models~(LLMs) handle global cultural diversity, the mechanisms behind their cultural knowledge acquisition, particularly in multilingual settings, remain unclear. We study this question by investigating how cultural knowledge transfers across languages during language adaptation of LLMs. We introduce an interpretable framework for studying this transfer, ensuring training data transparency and controlling transfer effects. Through a study of four non-Anglophonic cultures, we observe bidirectional cultural transfer between English and other high-resource languages, while low-resource languages primarily transfer knowledge to English with limited reverse flow. To explain this asymmetric phenomenon, we propose a frequency-based hypothesis: cultural knowledge appearing more frequently in the pretraining data transfers more easily, which is supported by empirical analysis of the training corpora.",,"Chen Zhang, Zhiyuan Liao, Yansong Feng",2025-06-02T13:45:09Z,Cross-Lingual Transfer of Cultural Knowledge: An Asymmetric Phenomenon,Cross-Lingual Transfer of Cultural Knowledge: Ein asymmetrisches Phänomen,文化知识的横向横向横向转让:非对称现象,http://arxiv.org/abs/2506.01675v1
782,"Generative recommendation is an emerging paradigm that leverages the extensive knowledge of large language models by formulating recommendations into a text-to-text generation task. However, existing studies face two key limitations in (i) incorporating implicit item relationships and (ii) utilizing rich yet lengthy item information. To address these challenges, we propose a Generative Recommender via semantic-Aware Multi-granular late fusion (GRAM), introducing two synergistic innovations. First, we design semantic-to-lexical translation to encode implicit hierarchical and collaborative item relationships into the vocabulary space of LLMs. Second, we present multi-granular late fusion to integrate rich semantics efficiently with minimal information loss. It employs separate encoders for multi-granular prompts, delaying the fusion until the decoding stage. Experiments on four benchmark datasets show that GRAM outperforms eight state-of-the-art generative recommendation models, achieving significant improvements of 11.5-16.0% in Recall@5 and 5.3-13.6% in NDCG@5. The source code is available at https://github.com/skleee/GRAM.",,"Sunkyung Lee, Minjin Choi, Eunseong Choi, Hye-young Kim, Jongwuk Lee",2025-06-02T13:42:46Z,GRAM: Generative Recommendation via Semantic-aware Multi-granular Late   Fusion,GRAM: Generative Empfehlung über semantisch-bewusste Multi-Granular Late Fusion,GRAM: 通过语义觉悟多谱状延迟熔化生成建议,http://arxiv.org/abs/2506.01673v1
783,"Modern Slavery Acts mandate that corporations disclose their efforts to combat modern slavery, aiming to enhance transparency and strengthen practices for its eradication. However, verifying these statements remains challenging due to their complex, diversified language and the sheer number of statements that must be reviewed. The development of NLP tools to assist in this task is also difficult due to a scarcity of annotated data. Furthermore, as modern slavery transparency legislation has been introduced in several countries, the generalizability of such tools across legal jurisdictions must be studied. To address these challenges, we work with domain experts to make two key contributions. First, we present AIMS.uk and AIMS.ca, newly annotated datasets from the UK and Canada to enable cross-jurisdictional evaluation. Second, we introduce AIMSCheck, an end-to-end framework for compliance validation. AIMSCheck decomposes the compliance assessment task into three levels, enhancing interpretability and practical applicability. Our experiments show that models trained on an Australian dataset generalize well across UK and Canadian jurisdictions, demonstrating the potential for broader application in compliance monitoring. We release the benchmark datasets and AIMSCheck to the public to advance AI-adoption in compliance assessment and drive further research in this field.",,"Adriana Eufrosina Bora, Akshatha Arodi, Duoyi Zhang, Jordan Bannister, Mirko Bronzi, Arsene Fansi Tchango, Md Abul Bashar, Richi Nayak, Kerrie Mengersen",2025-06-02T13:40:59Z,AIMSCheck: Leveraging LLMs for AI-Assisted Review of Modern Slavery   Statements Across Jurisdictions,AIMSCheck: LLMs für KI-Assistente Überprüfung moderner Sklaverei-Erklärungen über die Zuständigkeit hinweg nutzen,AI协助审查跨越管辖范围的现代奴隶制声明,http://arxiv.org/abs/2506.01671v1
784,"We introduce ESGenius, a comprehensive benchmark for evaluating and enhancing the proficiency of Large Language Models (LLMs) in Environmental, Social and Governance (ESG) and sustainability-focused question answering. ESGenius comprises two key components: (i) ESGenius-QA, a collection of 1 136 multiple-choice questions generated by LLMs and rigorously validated by domain experts, covering a broad range of ESG pillars and sustainability topics. Each question is systematically linked to its corresponding source text, enabling transparent evaluation and supporting retrieval-augmented generation (RAG) methods; and (ii) ESGenius-Corpus, a meticulously curated repository of 231 foundational frameworks, standards, reports and recommendation documents from seven authoritative sources. Moreover, to fully assess the capabilities and adaptation potential of the model, we implement a rigorous two-stage evaluation protocol -- Zero-Shot and RAG. Extensive experiments across 50 LLMs (ranging from 0.5 B to 671 B parameters) demonstrate that state-of-the-art models achieve only moderate performance in zero-shot settings, with accuracies typically around 55--70\%, highlighting ESGenius's challenging nature for LLMs in interdisciplinary contexts. However, models employing RAG show significant performance improvements, particularly for smaller models. For example, ""DeepSeek-R1-Distill-Qwen-14B"" improves from 63.82\% (zero-shot) to 80.46\% with RAG. These results underscore the necessity of grounding responses in authoritative sources for enhanced ESG understanding. To the best of our knowledge, ESGenius is the first benchmark curated for LLMs and the relevant enhancement technologies that focuses on ESG and sustainability topics.",,"Chaoyue He, Xin Zhou, Yi Wu, Xinjia Yu, Yan Zhang, Lei Zhang, Di Wang, Shengfei Lyu, Hong Xu, Xiaoqiao Wang, Wei Liu, Chunyan Miao",2025-06-02T13:19:09Z,"ESGenius: Benchmarking LLMs on Environmental, Social, and Governance   (ESG) and Sustainability Knowledge","ESGenius: Benchmarking LLMs zu Umwelt-, Sozial- und Governance (ESG) und Nachhaltigkeitswissen",ESGenius: 环境、社会和治理以及可持续性知识方面确定基准的LLMs,http://arxiv.org/abs/2506.01646v1
785,"The rapid increase in the parameter counts of Large Language Models (LLMs), reaching billions or even trillions, presents significant challenges for their practical deployment, particularly in resource-constrained environments. To ease this issue, we propose PIP (Perturbation-based Iterative Pruning), a novel double-view structured pruning method to optimize LLMs, which combines information from two different views: the unperturbed view and the perturbed view. With the calculation of gradient differences, PIP iteratively prunes those that struggle to distinguish between these two views. Our experiments show that PIP reduces the parameter count by approximately 20% while retaining over 85% of the original model's accuracy across varied benchmarks. In some cases, the performance of the pruned model is within 5% of the unpruned version, demonstrating PIP's ability to preserve key aspects of model effectiveness. Moreover, PIP consistently outperforms existing state-of-the-art (SOTA) structured pruning methods, establishing it as a leading technique for optimizing LLMs in environments with constrained resources.",,"Yi Cao, Wei-Jie Xu, Yucheng Shen, Weijie Shi, Chi-Min Chan, Jianfeng Qu, Jiajie Xu",2025-06-02T13:12:41Z,PIP: Perturbation-based Iterative Pruning for Large Language Models,PIP: Perturbationsbasiertes Iteratives Pruning für große Sprachmodelle,PIP: 用于大语言模型的基于扰扰动的迭代保护,http://arxiv.org/abs/2501.15278v2
786,"Large language models (LLMs) excel in high-resource languages but struggle with low-resource languages (LRLs), particularly those spoken by minority communities in China, such as Tibetan, Uyghur, Kazakh, and Mongolian. To systematically track the progress in these languages, we introduce MiLiC-Eval, a benchmark designed for minority languages in China, featuring 24K instances across 9 tasks. MiLiC-Eval focuses on underrepresented writing systems. Its parallelism between tasks and languages can provide a faithful and fine-grained assessment of linguistic and problem-solving skills. Our evaluation reveals that open-source LLMs perform poorly on syntax-intensive tasks and multi-script languages. We further demonstrate how MiLiC-Eval can help advance LRL research in handling diverse writing systems and understanding the process of language adaptation.",,"Chen Zhang, Mingxu Tao, Zhiyuan Liao, Yansong Feng",2025-06-02T13:06:53Z,MiLiC-Eval: Benchmarking Multilingual LLMs for China's Minority   Languages,MiLiC-Eval: Benchmarking mehrsprachiger LLMs für Chinas Minderheitensprachen,MILIC-Eval:中国少数民族语言多语种LLM标准基准,http://arxiv.org/abs/2503.01150v2
787,"Multilingual language models (MLLMs) have demonstrated remarkable abilities to transfer knowledge across languages, despite being trained without explicit cross-lingual supervision. We analyze the parameter spaces of three MLLMs to study how their representations evolve during pre-training, observing patterns consistent with compression: models initially form language-specific representations, which gradually converge into cross-lingual abstractions as training progresses. Through probing experiments, we observe a clear transition from uniform language identification capabilities across layers to more specialized layer functions. For deeper analysis, we focus on neurons that encode distinct semantic concepts. By tracing their development during pre-training, we show how they gradually align across languages. Notably, we identify specific neurons that emerge as increasingly reliable predictors for the same concepts across languages.",,"Frederick Riemenschneider, Anette Frank",2025-06-02T13:06:30Z,Cross-Lingual Generalization and Compression: From Language-Specific to   Shared Neurons,Cross-Lingual Generalization und Compression: Von sprachspezifischen zu geteilten Neuronen,交叉语言一般化和压缩:从语言特定到共享中枢,http://arxiv.org/abs/2506.01629v1
788,"Fake news on social media is a widespread and serious problem in today's society. Existing fake news detection methods focus on finding clues from Long text content, such as original news articles and user comments. This paper solves the problem of fake news detection in more realistic scenarios. Only source shot-text tweet and its retweet users are provided without user comments. We develop a novel neural network based model, \textbf{M}ulti-\textbf{V}iew \textbf{A}ttention \textbf{N}etworks (MVAN) to detect fake news and provide explanations on social media. The MVAN model includes text semantic attention and propagation structure attention, which ensures that our model can capture information and clues both of source tweet content and propagation structure. In addition, the two attention mechanisms in the model can find key clue words in fake news texts and suspicious users in the propagation structure. We conduct experiments on two real-world datasets, and the results demonstrate that MVAN can significantly outperform state-of-the-art methods by 2.5\% in accuracy on average, and produce a reasonable explanation.",,"Shiwen Ni, Jiawen Li, Hung-Yu Kao",2025-06-02T13:05:23Z,MVAN: Multi-View Attention Networks for Fake News Detection on Social   Media,MVAN: Multi-View Aufmerksamkeitsnetzwerke für Fake News Detection auf Social Media,MVAN:社会媒体假新闻探测多视网,http://arxiv.org/abs/2506.01627v1
789,"Efficient deployment of large audio-language models for speech translation remains challenging due to their significant computational requirements. In this paper, we address this challenge through our system submissions to the ""Model Compression"" track at the International Conference on Spoken Language Translation (IWSLT 2025). We experiment with a combination of approaches including iterative layer pruning based on layer importance evaluation, low-rank adaptation with 4-bit quantization (QLoRA), and knowledge distillation. In our experiments, we use Qwen2-Audio-7B-Instruct for speech translation into German and Chinese. Our pruned (student) models achieve up to a 50% reduction in both model parameters and storage footprint, while retaining 97-100% of the translation quality of the in-domain (teacher) models.",,Yasmin Moslem,2025-06-02T12:59:54Z,Efficient Speech Translation through Model Compression and Knowledge   Distillation,Effiziente Sprachübersetzung durch Modellkompression und Wissensdestillation,通过模型压缩和知识蒸馏高效语音翻译,http://arxiv.org/abs/2505.20237v2
790,"Pre-trained language models such as BERT have been proved to be powerful in many natural language processing tasks. But in some text classification applications such as emotion recognition and sentiment analysis, BERT may not lead to satisfactory performance. This often happens in applications where keywords play critical roles in the prediction of class labels. Our investigation found that the root cause of the problem is that the context-based BERT embedding of the keywords may not be discriminative enough to produce discriminative text representation for classification. Motivated by this finding, we develop a method to enhance word embeddings using domain-specific lexical knowledge. The knowledge-based embedding enhancement model projects the BERT embedding into a new space where within-class similarity and between-class difference are maximized. To implement the knowledge-based word embedding enhancement model, we also develop a knowledge acquisition algorithm for automatically collecting lexical knowledge from online open sources. Experiment results on three classification tasks, including sentiment analysis, emotion recognition and question answering, have shown the effectiveness of our proposed word embedding enhancing model. The codes and datasets are in https://github.com/MidiyaZhu/KVWEFFER.",,"Zixiao Zhu, Kezhi Mao",2025-06-02T12:59:41Z,Domain Lexical Knowledge-based Word Embedding Learning for Text   Classification under Small Data,Domäne Lexikales Wissen-basiertes Wort-Embedding-Lernen für Textklassifikation unter kleinen Daten,用于小数据下文本分类的基于知识的词嵌入学习,http://arxiv.org/abs/2506.01621v1
791,"Autoregressive transformers exhibit adaptive learning through in-context learning (ICL), which begs the question of how. Prior works have shown that transformers represent the ICL tasks as vectors in their representations. In this paper, we leverage the encoding-decoding framework to study how transformers form task vectors during pretraining and how their task encoding quality predicts ICL task performance. On synthetic ICL tasks, we analyze the training dynamics of a small transformer and report the coupled emergence of task encoding and decoding. As the model learns to encode different latent tasks (e.g., ""Finding the first noun in a sentence."") into distinct, separable representations, it concurrently builds conditional decoding algorithms and improves its ICL performance. We validate this phenomenon across pretrained models of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B) and over the course of pretraining in OLMo-7B. Further, we demonstrate that the quality of task encoding inferred from representations predicts ICL performance, and that, surprisingly, finetuning the earlier layers can improve the task encoding and performance more than finetuning the latter layers. Our empirical insights shed light into better understanding the success and failure modes of large language models via their representations.",,"Seungwook Han, Jinyeop Song, Jeff Gore, Pulkit Agrawal",2025-06-02T12:55:12Z,Emergence and Effectiveness of Task Vectors in In-Context Learning: An   Encoder Decoder Perspective,Entstehung und Wirksamkeit von Task-Vektoren im In-Context-Lernen: Eine Encoder-Decoder-Perspektive,内容内学习中任务矢量的出现和有效性:编码器编码器编码器拆解器的视角,http://arxiv.org/abs/2412.12276v3
792,"This paper describes our system submission to the International Conference on Spoken Language Translation (IWSLT 2025), low-resource languages track, namely for Bemba-to-English speech translation. We built cascaded speech translation systems based on Whisper and NLLB-200, and employed data augmentation techniques, such as back-translation. We investigate the effect of using synthetic data and discuss our experimental setup.",,"Muhammad Hazim Al Farouq, Aman Kassahun Wassie, Yasmin Moslem",2025-06-02T12:55:06Z,Bemba Speech Translation: Exploring a Low-Resource African Language,Bemba Speech Translation: Erforschen einer ressourcenarmen afrikanischen Sprache,本巴语言翻译:探索非洲低资源语言,http://arxiv.org/abs/2505.02518v3
793,"We introduce a set of training-free ABX-style discrimination tasks to evaluate how multilingual language models represent language identity (form) and semantic content (meaning). Inspired from speech processing, these zero-shot tasks measure whether minimal differences in representation can be reliably detected. This offers a flexible and interpretable alternative to probing. Applied to XLM-R (Conneau et al, 2020) across pretraining checkpoints and layers, we find that language discrimination declines over training and becomes concentrated in lower layers, while meaning discrimination strengthens over time and stabilizes in deeper layers. We then explore probing tasks, showing some alignment between our metrics and linguistic learning performance. Our results position ABX tasks as a lightweight framework for analyzing the structure of multilingual representations.",,"Maureen de Seyssel, Jie Chi, Skyler Seto, Maartje ter Hoeve, Masha Fedzechkina, Natalie Schluter",2025-06-02T12:51:26Z,Discriminating Form and Meaning in Multilingual Models with Minimal-Pair   ABX Tasks,Form und Bedeutung von Mehrsprachigen Modellen mit Minimal-Pair ABX-Aufgaben diskriminieren,多语种模式中与最小配平ABX任务多语种模式中的差异形式和含义,http://arxiv.org/abs/2505.17747v2
794,"User information needs are often highly diverse and varied. A key challenge in current research is how to achieve controllable multi-objective generation while enabling rapid adaptation to accommodate diverse user demands during test time. Existing solutions, such as Rewarded Soup, focus on merging language models individually tuned on single objectives. While easy to implement and widely used, these approaches face limitations in achieving optimal performance due to their disregard for the impacts of competing objectives on model tuning. To address this issue, we propose Bone Soup, a novel model merging approach that first seeks a series of backbone models by considering the impacts of multiple objectives and then makes the soup (i.e., merge the backbone models). Specifically, Bone Soup begins by training multiple backbone models for different objectives using multi-objective reinforcement learning. Each backbone model is guided by a combination of backbone reward signals. To ensure that these models are optimal for the Pareto front, the backbone rewards are crafted by combining standard reward functions into basis vectors, which can then be modified through a rule-based construction method. Bone Soup leverages a symmetric circulant matrix mapping to generate the merging coefficients, which are used to merge the backbone models according to user preferences. Extensive experimental results demonstrate that Bone Soup exhibits strong controllability and Pareto optimality in controllable multi-objective generation, providing a more effective and efficient approach to addressing diverse user needs at test time.",,"Guofu Xie, Xiao Zhang, Ting Yao, Yunsheng Shi",2025-06-02T12:51:19Z,Bone Soups: A Seek-and-Soup Model Merging Approach for Controllable   Multi-Objective Generation,Knochensuppen: Ein Such-und-Soup-Modell Merging-Ansatz für kontrollierbare Multi-Objektive Generation,Bone Soups:可控多目标一代的寻找和救济合并模式办法,http://arxiv.org/abs/2502.10762v2
795,"The rapid advancement of large models, driven by their exceptional abilities in learning and generalization through large-scale pre-training, has reshaped the landscape of Artificial Intelligence (AI). These models are now foundational to a wide range of applications, including conversational AI, recommendation systems, autonomous driving, content generation, medical diagnostics, and scientific discovery. However, their widespread deployment also exposes them to significant safety risks, raising concerns about robustness, reliability, and ethical implications. This survey provides a systematic review of current safety research on large models, covering Vision Foundation Models (VFMs), Large Language Models (LLMs), Vision-Language Pre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models (DMs), and large-model-based Agents. Our contributions are summarized as follows: (1) We present a comprehensive taxonomy of safety threats to these models, including adversarial attacks, data poisoning, backdoor attacks, jailbreak and prompt injection attacks, energy-latency attacks, data and model extraction attacks, and emerging agent-specific threats. (2) We review defense strategies proposed for each type of attacks if available and summarize the commonly used datasets and benchmarks for safety research. (3) Building on this, we identify and discuss the open challenges in large model safety, emphasizing the need for comprehensive safety evaluations, scalable and effective defense mechanisms, and sustainable data practices. More importantly, we highlight the necessity of collective efforts from the research community and international collaboration. Our work can serve as a useful reference for researchers and practitioners, fostering the ongoing development of comprehensive defense systems and platforms to safeguard AI models.",,"Xingjun Ma, Yifeng Gao, Yixu Wang, Ruofan Wang, Xin Wang, Ye Sun, Yifan Ding, Hengyuan Xu, Yunhao Chen, Yunhan Zhao, Hanxun Huang, Yige Li, Jiaming Zhang, Xiang Zheng, Yang Bai, Zuxuan Wu, Xipeng Qiu, Jingfeng Zhang, Yiming Li, Xudong Han, Haonan Li, Jun Sun, Cong Wang, Jindong Gu, Baoyuan Wu, Siheng Chen, Tianwei Zhang, Yang Liu, Mingming Gong, Tongliang Liu, Shirui Pan, Cihang Xie, Tianyu Pang, Yinpeng Dong, Ruoxi Jia, Yang Zhang, Shiqing Ma, Xiangyu Zhang, Neil Gong, Chaowei Xiao, Sarah Erfani, Tim Baldwin, Bo Li, Masashi Sugiyama, Dacheng Tao, James Bailey, Yu-Gang Jiang",2025-06-02T12:47:50Z,Safety at Scale: A Comprehensive Survey of Large Model Safety,Sicherheit im Maßstab: Eine umfassende Untersuchung der großen Modellsicherheit,规模安全规模安全:大型安全模型综合调查,http://arxiv.org/abs/2502.05206v4
796,"Audio deepfakes are acquiring an unprecedented level of realism with advanced AI. While current research focuses on discerning real speech from spoofed speech, tracing the source system is equally crucial. This work proposes a novel audio source tracing system combining deep metric multi-class N-pair loss with Real Emphasis and Fake Dispersion framework, a Conformer classification network, and ensemble score-embedding fusion. The N-pair loss improves discriminative ability, while Real Emphasis and Fake Dispersion enhance robustness by focusing on differentiating real and fake speech patterns. The Conformer network captures both global and local dependencies in the audio signal, crucial for source tracing. The proposed ensemble score-embedding fusion shows an optimal trade-off between in-domain and out-of-domain source tracing scenarios. We evaluate our method using Frechet Distance and standard metrics, demonstrating superior performance in source tracing over the baseline system.",,"Ajinkya Kulkarni, Sandipana Dowerah, Tanel Alumae, Mathew Magimai. -Doss",2025-06-02T12:42:09Z,Unveiling Audio Deepfake Origins: A Deep Metric learning And Conformer   Network Approach With Ensemble Fusion,Enthüllen von Audio Deepfake Origins: Ein tiefes Metrisches Lernen und Konformer-Netzwerkansatz mit Ensemble Fusion,"无法解锁的音频深假起源:深度学习和连接网络方法,与集合融合",http://arxiv.org/abs/2506.02085v1
797,"Word sense analysis is an essential analysis work for interpreting the linguistic and social backgrounds. The word sense change detection is a task of identifying and interpreting shifts in word meanings over time. This paper proposes MMD-Sense-Analysis, a novel approach that leverages Maximum Mean Discrepancy (MMD) to select semantically meaningful variables and quantify changes across time periods. This method enables both the identification of words undergoing sense shifts and the explanation of their evolution over multiple historical periods. To my knowledge, this is the first application of MMD to word sense change detection. Empirical assessment results demonstrate the effectiveness of the proposed approach.",,Kensuke Mitsuzawa,2025-06-02T12:40:46Z,MMD-Sense-Analysis: Word Sense Detection Leveraging Maximum Mean   Discrepancy,MMD-Sense-Analyse: Word Sense Detection Leveraging Maximale mittlere Diskrepanz,MMD-传感器分析: Word Sensense 探测利用最大平均值差异,http://arxiv.org/abs/2506.01602v1
798,"Visual Question Answer (VQA) poses the problem of answering a natural language question about a visual context. Bangla, despite being a widely spoken language, is considered low-resource in the realm of VQA due to the lack of proper benchmarks, challenging models known to be performant in other languages. Furthermore, existing Bangla VQA datasets offer little regional relevance and are largely adapted from their foreign counterparts. To address these challenges, we introduce a large-scale Bangla VQA dataset, ChitroJera, totaling over 15k samples from diverse and locally relevant data sources. We assess the performance of text encoders, image encoders, multimodal models, and our novel dual-encoder models. The experiments reveal that the pre-trained dual-encoders outperform other models of their scale. We also evaluate the performance of current large vision language models (LVLMs) using prompt-based techniques, achieving the overall best performance. Given the underdeveloped state of existing datasets, we envision ChitroJera expanding the scope of Vision-Language tasks in Bangla.",,"Deeparghya Dutta Barua, Md Sakib Ul Rahman Sourove, Md Fahim, Fabiha Haider, Fariha Tanjim Shifat, Md Tasmim Rahman Adib, Anam Borhan Uddin, Md Farhan Ishmam, Md Farhad Alam",2025-06-02T12:38:12Z,ChitroJera: A Regionally Relevant Visual Question Answering Dataset for   Bangla,ChitroJera: Ein regional relevanter Visual Question Antwortdatensatz für Bangla,ChitroJera:孟加拉国与区域有关的视觉问题解答数据集,http://arxiv.org/abs/2410.14991v2
799,"Large Language Models (LLMs) excel in zero-shot and few-shot tasks, but achieving similar performance with encoder-only models like BERT and RoBERTa has been challenging due to their architecture. However, encoders offer advantages such as lower computational and memory costs. Recent work adapts them for zero-shot generalization using Statement Tuning, which reformulates tasks into finite templates. We extend this approach to multilingual NLP, exploring whether encoders can achieve zero-shot cross-lingual generalization and serve as efficient alternatives to memory-intensive LLMs for low-resource languages. Our results show that state-of-the-art encoder models generalize well across languages, rivaling multilingual LLMs while being more efficient. We also analyze multilingual Statement Tuning dataset design, efficiency gains, and language-specific generalization, contributing to more inclusive and resource-efficient NLP models. We release our code and models.",,"Ahmed Elshabrawy, Thanh-Nhi Nguyen, Yeeun Kang, Lihan Feng, Annant Jain, Faadil Abdullah Shaikh, Jonibek Mansurov, Mohamed Fazli Mohamed Imam, Jesus-German Ortiz-Barajas, Rendi Chevi, Alham Fikri Aji",2025-06-02T12:28:03Z,Statement-Tuning Enables Efficient Cross-lingual Generalization in   Encoder-only Models,Statement-Tuning ermöglicht effiziente Cross-lingual Generalization in Encoder-only-Modellen,报表 - 允许在只使用编码器的模型中实现高效率的跨语言通用化,http://arxiv.org/abs/2506.01592v1
800,"The rapid expansion of social media platforms has significantly increased the dissemination of forged content and misinformation, making the detection of fake news a critical area of research. Although fact-checking efforts predominantly focus on English-language news, there is a noticeable gap in resources and strategies to detect news in regional languages, such as Urdu. Advanced Fake News Detection (FND) techniques rely heavily on large, accurately labeled datasets. However, FND in under-resourced languages like Urdu faces substantial challenges due to the scarcity of extensive corpora and the lack of validated lexical resources. Current Urdu fake news datasets are often domain-specific and inaccessible to the public. They also lack human verification, relying mainly on unverified English-to-Urdu translations, which compromises their reliability in practical applications. This study highlights the necessity of developing reliable, expert-verified, and domain-independent Urdu-enhanced FND datasets to improve fake news detection in Urdu and other resource-constrained languages. This paper presents the first benchmark large FND dataset for Urdu news, which is publicly available for validation and deep analysis. We also evaluate this dataset using multiple state-of-the-art pre-trained large language models (LLMs), such as XLNet, mBERT, XLM-RoBERTa, RoBERTa, DistilBERT, and DeBERTa. Additionally, we propose a unified LLM model that outperforms the others with different embedding and feature extraction techniques. The performance of these models is compared based on accuracy, F1 score, precision, recall, and human judgment for vetting the sample results of news.",,"Muhammad Islam, Javed Ali Khan, Mohammed Abaker, Ali Daud, Azeem Irshad",2025-06-02T12:19:28Z,Unified Large Language Models for Misinformation Detection in   Low-Resource Linguistic Settings,Unified Large Language Models für Fehlinformationserkennung in ressourcenarmen Spracheinstellungen,用于在低资源语言环境中发现错误信息的统一大语言模式,http://arxiv.org/abs/2506.01587v1
801,"This work presents the details and findings of the first mentorship in speech translation (SpeechT), which took place in December 2024 and January 2025. To fulfil the mentorship requirements, the participants engaged in key activities, including data preparation, modelling, and advanced research. The participants explored data augmentation techniques and compared end-to-end and cascaded speech translation systems. The projects covered various languages other than English, including Arabic, Bengali, Galician, Indonesian, Japanese, and Spanish.",,"Yasmin Moslem, Juan Julián Cea Morán, Mariano Gonzalez-Gomez, Muhammad Hazim Al Farouq, Farah Abdou, Satarupa Deb",2025-06-02T12:11:35Z,SpeechT: Findings of the First Mentorship in Speech Translation,SpeechT: Ergebnisse der ersten Mentorschaft in der Sprachübersetzung,演讲者:第一次演讲者演讲者翻译指导会的调查结果,http://arxiv.org/abs/2502.12050v3
802,"Large language model performance can be improved in a large number of ways. Many such techniques, like fine-tuning or advanced tool usage, are time-intensive and expensive. Although prompt engineering is significantly cheaper and often works for simpler tasks, it remains unclear whether prompt engineering suffices for more complex domains like forecasting. Here we show that small prompt modifications rarely boost forecasting accuracy beyond a minimal baseline. In our first study, we tested 38 prompts across Claude 3.5 Sonnet, Claude 3.5 Haiku, GPT-4o, and Llama 3.1 405B. In our second, we introduced compound prompts and prompts from external sources, also including the reasoning models o1 and o1-mini. Our results show that most prompts lead to negligible gains, although references to base rates yield slight benefits. Surprisingly, some strategies showed strong negative effects on accuracy: especially encouraging the model to engage in Bayesian reasoning. These results suggest that, in the context of complex tasks like forecasting, basic prompt refinements alone offer limited gains, implying that more robust or specialized techniques may be required for substantial performance improvements in AI forecasting.",,"Philipp Schoenegger, Cameron R. Jones, Philip E. Tetlock, Barbara Mellers",2025-06-02T12:07:11Z,Prompt Engineering Large Language Models' Forecasting Capabilities,Prompt Engineering Large Language Models' Forecasting Capabilities,快速工程大语言模型预测能力,http://arxiv.org/abs/2506.01578v1
803,"Test-time compute is emerging as a new paradigm for enhancing language models' complex multi-step reasoning capabilities, as demonstrated by the success of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit reasoning in test-time compute, implicit reasoning is more inference-efficient, requiring fewer generated tokens. However, why does the advanced reasoning capability fail to emerge in the implicit reasoning style? In this work, we train GPT-2 from scratch on a curated multi-step mathematical reasoning dataset and conduct analytical experiments to investigate how language models perform implicit reasoning in multi-step tasks. Our findings reveal: 1) Language models can perform step-by-step reasoning and achieve high accuracy in both in-domain and out-of-domain tests via implicit reasoning. However, this capability only emerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning abilities emerging from training on unfixed-pattern data tend to overfit a specific pattern and fail to generalize further. Notably, this limitation is also observed in state-of-the-art large language models. These findings suggest that language models acquire implicit reasoning through shortcut learning, enabling strong performance on tasks with similar patterns while lacking generalization.",,"Tianhe Lin, Jian Xie, Siyu Yuan, Deqing Yang",2025-06-02T12:06:31Z,Implicit Reasoning in Transformers is Reasoning through Shortcuts,Implizite Vernunft in Transformatoren ist Vernunft durch Abkürzungen,变换器中的隐含理由来自快捷键,http://arxiv.org/abs/2503.07604v3
804,"Large Language Models (LLMs) have exhibited exceptional performance across a spectrum of natural language processing tasks. However, their substantial sizes pose considerable challenges, particularly in computational demands and inference speed, due to their quadratic complexity. In this work, we have identified a key pattern: certain seemingly meaningless separator tokens (i.e., punctuations) contribute disproportionately to attention scores compared to semantically meaningful tokens. This observation suggests that information of the segments between these separator tokens can be effectively condensed into the separator tokens themselves without significant information loss. Guided by this insight, we introduce SepLLM, a plug-and-play framework that accelerates inference by compressing these segments and eliminating redundant tokens. Additionally, we implement efficient kernels for training acceleration. Experimental results across training-free, training-from-scratch, and post-training settings demonstrate SepLLM's effectiveness. Notably, using the Llama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the GSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in streaming settings, SepLLM effectively processes sequences of up to 4 million tokens or more while maintaining consistent language modeling capabilities.",,"Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xiaozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li, Weiyang Liu, Chao Huang",2025-06-02T11:46:43Z,SepLLM: Accelerate Large Language Models by Compressing One Segment into   One Separator,"SepLLM: Beschleunigen Sie große Sprachmodelle, indem Sie ein Segment zu einem Separator komprimieren",SELLM: 通过将单段压缩成一个分隔符加速大语言模式,http://arxiv.org/abs/2412.12094v6
805,"The rapid growth of social media platforms has raised significant concerns regarding online content toxicity. When Large Language Models (LLMs) are used for toxicity detection, two key challenges emerge: 1) the absence of domain-specific toxic knowledge leads to false negatives; 2) the excessive sensitivity of LLMs to toxic speech results in false positives, limiting freedom of speech. To address these issues, we propose a novel method called MetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance hatred and toxicity detection. First, we construct a comprehensive meta-toxic knowledge graph by utilizing LLMs to extract toxic information through a three-step pipeline, with toxic benchmark datasets serving as corpora. Second, we query the graph via retrieval and ranking processes to supplement accurate, relevant toxic knowledge. Extensive experiments and in-depth case studies across multiple datasets demonstrate that our MetaTox significantly decreases the false positive rate while boosting overall toxicity detection performance. Our code is available at https://github.com/YiboZhao624/MetaTox.",,"Yibo Zhao, Jiapeng Zhu, Can Xu, Yao Liu, Xiang Li",2025-06-02T11:45:29Z,Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic   Knowledge Graph,Verbesserung der LLM-basierten Haß- und Toxizitätserkennung mit Meta-Toxic Knowledge Graph,"加强基于LLM的仇恨和毒性检测,并配有超毒性知识图",http://arxiv.org/abs/2412.15268v4
806,"Culture is a rich and dynamic domain that evolves across both geography and time. However, existing studies on cultural understanding with vision-language models (VLMs) primarily emphasize geographic diversity, often overlooking the critical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a novel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning ancient Chinese dynasties, serves as a representative cultural heritage that reflects the profound temporal aspects of Chinese culture while remaining highly popular in Chinese contemporary society. Hanfu-Bench comprises two core tasks: cultural visual understanding and cultural image transcreation.The former task examines temporal-cultural feature recognition based on single- or multi-image inputs through multiple-choice visual question answering, while the latter focuses on transforming traditional attire into modern designs through cultural element inheritance and modern context adaptation. Our evaluation shows that closed VLMs perform comparably to non-experts on visual cutural understanding but fall short by 10\% to human experts, while open VLMs lags further behind non-experts. For the transcreation task, multi-faceted human evaluation indicates that the best-performing model achieves a success rate of only 42\%. Our benchmark provides an essential testbed, revealing significant challenges in this new direction of temporal cultural understanding and creative adaptation.",,"Li Zhou, Lutong Yu, Dongchu Xie, Shaohuan Cheng, Wenyan Li, Haizhou Li",2025-06-02T11:43:46Z,Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural   Understanding and Transcreation,Hanfu-Bench: Ein multimodaler Benchmark für interkulturelles Verständnis und Transkreation,Hanfu-Bunch:跨时文化理解和交流的多模式基准,http://arxiv.org/abs/2506.01565v1
807,"Transformers, as the fundamental deep learning architecture, have demonstrated great capability in reasoning. This paper studies the generalizable first-order logical reasoning ability of transformers with their parameterized knowledge and how to improve it. Transformers' capability of first-order reasoning is further captured by whether they can conduct first-order logical entailment, which is quantitatively measured by their performance in answering knowledge graph queries. We establish the connections between (1) two types of distribution shifts studied in out-of-distribution generalization and (2) unseen knowledge and query settings discussed in the task of knowledge graph query answering, which makes it possible to characterize the fine-grained generalizability. Results on our comprehensive dataset showed that transformers outperform previous methods designed particularly for this task and provided detailed empirical evidence about the impact of the input query syntax, token embedding, and transformer architectures on the reasoning capability of transformers. Interestingly, our results revealed the mismatch of positional encoding and other design choices of transformer architectures in previous practices. Motivated by this, we propose TEGA, a logic-aware architecture that significantly improves the performance in generalizable first-order logical entailment.",,"Tianshi Zheng, Jiazheng Wang, Zihao Wang, Jiaxin Bai, Hang Yin, Zheye Deng, Yangqiu Song, Jianxin Li",2025-06-02T11:36:26Z,Enhancing Transformers for Generalizable First-Order Logical Entailment,Erweiterung der Transformer für generalisierbare Logical Entailment erster Ordnung,增强通用一级一级逻辑元件的变压器,http://arxiv.org/abs/2501.00759v2
808,"There is an increasing trend towards evaluating NLP models with LLMs instead of human judgments, raising questions about the validity of these evaluations, as well as their reproducibility in the case of proprietary models. We provide JUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations covering a broad range of evaluated properties and types of data, and comprehensively evaluate 11 current LLMs, covering both open-weight and proprietary models, for their ability to replicate the annotations. Our evaluations show substantial variance across models and datasets. Models are reliable evaluators on some tasks, but overall display substantial variability depending on the property being evaluated, the expertise level of the human judges, and whether the language is human or model-generated. We conclude that LLMs should be carefully validated against human judgments before being used as evaluators.",,"Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, André F. T. Martins, Philipp Mondorf, Vera Neplenbroek, Sandro Pezzelle, Barbara Plank, David Schlangen, Alessandro Suglia, Aditya K Surikuchi, Ece Takmaz, Alberto Testoni",2025-06-02T11:31:19Z,LLMs instead of Human Judges? A Large Scale Empirical Study across 20   NLP Evaluation Tasks,LLMs statt menschlicher Richter? Eine groß angelegte empirische Studie über 20 NLP-Evaluierungsaufgaben,* 在20个国家劳工规划评价任务中进行大规模经验研究,http://arxiv.org/abs/2406.18403v3
809,"Large language models (LLMs) have been well-researched in various long-context tasks. However, the scarcity of long-context summarization datasets hinders progress in this area. To address this, we introduce CNNSum, a multi-scale long-context summarization benchmark based on Chinese novels, featuring human-driven annotations across four subsets totaling 695 samples, with lengths ranging from 16k to 128k. We benchmark numerous LLMs and conduct detailed human assessments to summarize abnormal output types. Furthermore, we extensively explore how to improve long-context summarization. In our study: (1) Advanced LLMs may generate much subjective commentary, leading to vague summaries. (2) Currently, long-context summarization mainly relies on memory ability. The advantages of Large LLMs are hard to utilize, thus small LLMs are more cost-effective. (3) Different prompt types paired with various version models may cause large performance gaps. In further fine-tuning, these can be mitigated, and the Base version models perform better. (4) LLMs with RoPE-base scaled exhibit strong extrapolation potential; using short-context data can significantly improve long-context summarization performance. However, further applying other interpolation methods requires careful selection. (5) CNNSum provides more reliable evaluation results than other benchmarks. We release CNNSum to advance future research.(https://github.com/CxsGhost/CNNSum)",,"Lingxiao Wei, He Yan, Xiangju Lu, Junmin Zhu, Jun Wang, Wei Zhang",2025-06-02T11:31:07Z,CNNSum: Exploring Long-Context Summarization with Large Language Models   in Chinese Novels,CNNSum: Lang-Kontext-Zusammenfassung mit großen Sprachmodellen in chinesischen Romanen,CNNSum:探索中文小说中与大语言模型的长文本摘要,http://arxiv.org/abs/2412.02819v5
810,"Lexicon or dictionary generation across domains has the potential for societal impact, as it can potentially enhance information accessibility for a diverse user base while preserving language identity. Prior work in the field primarily focuses on bilingual lexical induction, which deals with word alignments using mapping or corpora-based approaches. However, these approaches do not cater to domain-specific lexicon generation that consists of domain-specific terminology. This task becomes particularly important in specialized medical, engineering, and other technical domains, owing to the highly infrequent usage of the terms and scarcity of data involving domain-specific terms especially for low/mid-resource languages. In this paper, we propose a new model to generate dictionary words for $6$ Indian languages in the multi-domain setting. Our model consists of domain-specific and domain-generic layers that encode information, and these layers are invoked via a learnable routing technique. We also release a new benchmark dataset consisting of >75K translation pairs across 6 Indian languages spanning 8 diverse domains.We conduct both zero-shot and few-shot experiments across multiple domains to show the efficacy of our proposed model in generalizing to unseen domains and unseen languages. Additionally, we also perform a post-hoc human evaluation on unseen languages. The source code and dataset is present at https://github.com/Atulkmrsingh/lexgen.",,"Ayush Maheshwari, Atul Kumar Singh, Karthika NJ, Krishnakant Bhatt, Preethi Jyothi, Ganesh Ramakrishnan",2025-06-02T11:31:05Z,LexGen: Domain-aware Multilingual Lexicon Generation,LexGen: Domain-aware Mehrsprachige Lexikon-Generation,LexGen:多语种多语种词汇代,http://arxiv.org/abs/2405.11200v3
811,"Building Vision-Language Navigation (VLN) agents which can navigate following natural language instructions is a long-standing goal in human-robot interaction applications. Recent studies have revealed the potential of training open-source Large Language Models (LLMs) to unleash LLMs' reasoning ability for improving navigation, and simultaneously mitigate the domain gap between LLMs' training corpus and the VLN task. However, these approaches primarily adopt direct input-output mapping paradigms, causing the mapping learning difficult and the navigational decisions unexplainable. Chain-of-Thought (CoT) training is a promising way to improve both navigational decision accuracy and interpretability, while the complexity of the navigation task makes the perfect CoT labels unavailable and may lead to overfitting through pure CoT supervised fine-tuning. In this paper, we propose a novel sElf-improving embodied reasoning framework for boosting LLM-based vision-language Navigation, dubbed EvolveNav. Our EvolveNav consists of two stages: (1) Formalized CoT Supervised Fine-Tuning, where we train the model with formalized CoT labels to both activate the model's navigational reasoning capabilities and increase the reasoning speed; (2) Self-Reflective Post-Training, where the model is iteratively trained with its own reasoning outputs as self-enriched CoT labels to enhance the supervision diversity. A self-reflective auxiliary task is also introduced to encourage learning correct reasoning patterns by contrasting with wrong ones. Experimental results on the popular VLN benchmarks demonstrate the superiority of EvolveNav over previous LLM-based VLN approaches. Code is available at https://github.com/expectorlin/EvolveNav.",,"Bingqian Lin, Yunshuang Nie, Khun Loun Zai, Ziming Wei, Mingfei Han, Rongtao Xu, Minzhe Niu, Jianhua Han, Liang Lin, Cewu Lu, Xiaodan Liang",2025-06-02T11:28:32Z,EvolveNav: Self-Improving Embodied Reasoning for LLM-Based   Vision-Language Navigation,EvolveNav: Selbstverbessernde körpereigene Begründung für LLM-basierte Vision-Language-Navigation,EvolveNav:基于LLM的愿景-语言导航自我改善自足理由,http://arxiv.org/abs/2506.01551v1
812,"Retrieval-Augmented Generation (RAG) systems show remarkable potential as question answering tools in the K-12 Education domain, where knowledge is typically queried within the restricted scope of authoritative textbooks. However, discrepancies between these textbooks and the parametric knowledge inherent in Large Language Models (LLMs) can undermine the effectiveness of RAG systems. To systematically investigate RAG system robustness against such knowledge discrepancies, we introduce KnowShiftQA. This novel question answering dataset simulates these discrepancies by applying deliberate hypothetical knowledge updates to both answers and source documents, reflecting how textbook knowledge can shift. KnowShiftQA comprises 3,005 questions across five subjects, designed with a comprehensive question typology focusing on context utilization and knowledge integration. Our extensive experiments on retrieval and question answering performance reveal that most RAG systems suffer a substantial performance drop when faced with these knowledge discrepancies. Furthermore, questions requiring the integration of contextual (textbook) knowledge with parametric (LLM) knowledge pose a significant challenge to current LLMs.",,"Tianshi Zheng, Weihan Li, Jiaxin Bai, Weiqi Wang, Yangqiu Song",2025-06-02T11:22:49Z,KnowShiftQA: How Robust are RAG Systems when Textbook Knowledge Shifts   in K-12 Education?,"KnowShiftQA: Wie robust sind RAG-Systeme, wenn Textbook Knowledge Shifts in K-12 Education?",K-12教育中教科书知识转移时RAG系统如何强大?,http://arxiv.org/abs/2412.08985v2
813,"Recent generative large language models (LLMs) show remarkable performance in non-English languages, but when prompted in those languages they tend to express higher harmful social biases and toxicity levels. Prior work has shown that finetuning on specialized datasets can mitigate this behavior, and doing so in English can transfer to other languages. In this work, we investigate the impact of different finetuning methods on the model's bias and toxicity, but also on its ability to produce fluent and diverse text. We reduce biases by finetuning on curated non-harmful text, but find only direct preference optimization to be effective for mitigating toxicity. The mitigation caused by applying these methods in English also transfers to non-English languages. We find evidence that the extent to which transfer takes place can be predicted by the amount of data in a given language present in the model's pretraining data. However, this transfer of bias and toxicity mitigation often comes at the expense of decreased language generation ability in non-English languages, highlighting the importance of developing language-specific bias and toxicity mitigation methods.",,"Vera Neplenbroek, Arianna Bisazza, Raquel Fernández",2025-06-02T11:03:39Z,Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual   LLMs: An Extensive Investigation,Cross-Lingual Transfer von Debiasing und Entgiftung in mehrsprachigen LLMs: Eine umfassende Untersuchung,多语种LLMM:广泛调查,http://arxiv.org/abs/2412.14050v4
814,"High-quality instruction data is crucial for developing large language models (LLMs), yet existing approaches struggle to effectively control instruction complexity. We present TAG-INSTRUCT, a novel framework that enhances instruction complexity through structured semantic compression and controlled difficulty augmentation. Unlike previous prompt-based methods operating on raw text, TAG-INSTRUCT compresses instructions into a compact tag space and systematically enhances complexity through RL-guided tag expansion. Through extensive experiments, we show that TAG-INSTRUCT outperforms existing instruction complexity augmentation approaches. Our analysis reveals that operating in tag space provides superior controllability and stability across different instruction synthesis frameworks.",,"He Zhu, Zhiwen Ruan, Junyou Su, Xingwei He, Yun Chen, Wenjia Zhang, Guanhua Chen",2025-06-02T11:00:28Z,TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through   Structure-based Augmentation,TAG-INSTRUCT: Controlled Instruction Complexity Enhancement durch strukturbasierte Augmentation,TAG-INSTRSUCT:通过基于结构的增强增强控制性教学复杂度,http://arxiv.org/abs/2505.18557v2
815,"This study explores current limitations of learned image captioning evaluation metrics, specifically the lack of granular assessments for errors within captions, and the reliance on single-point quality estimates without considering uncertainty. To address the limitations, we propose a simple yet effective strategy for generating and calibrating distributions of CLIPScore values. Leveraging a model-agnostic conformal risk control framework, we calibrate CLIPScore values for task-specific control variables, tackling the aforementioned limitations. Experimental results demonstrate that using conformal risk control, over score distributions produced with simple methods such as input masking, can achieve competitive performance compared to more complex approaches. Our method effectively detects erroneous words, while providing formal guarantees aligned with desired risk levels. It also improves the correlation between uncertainty estimations and prediction errors, thus enhancing the overall reliability of caption evaluation metrics.",,"Gonçalo Gomes, Bruno Martins, Chrysoula Zerva",2025-06-02T10:58:26Z,A Conformal Risk Control Framework for Granular Word Assessment and   Uncertainty Calibration of CLIPScore Quality Estimates,Ein konformer Rahmen für die Risikokontrolle für Granular Word Assessment und Uncertainty Calibration von CLIPScore Qualitätsschätzungen,CLIPS核心质量估计的颗粒词评估和不确定性校准的常规风险控制框架,http://arxiv.org/abs/2504.01225v2
816,"Sparse Autoencoders (SAEs) aim to decompose the activation space of large language models (LLMs) into human-interpretable latent directions or features. As we increase the number of features in the SAE, hierarchical features tend to split into finer features (""math"" may split into ""algebra"", ""geometry"", etc.), a phenomenon referred to as feature splitting. However, we show that sparse decomposition and splitting of hierarchical features is not robust. Specifically, we show that seemingly monosemantic features fail to fire where they should, and instead get ""absorbed"" into their children features. We coin this phenomenon feature absorption, and show that it is caused by optimizing for sparsity in SAEs whenever the underlying features form a hierarchy. We introduce a metric to detect absorption in SAEs, and validate our findings empirically on hundreds of LLM SAEs. Our investigation suggests that varying SAE sizes or sparsity is insufficient to solve this issue. We discuss the implications of feature absorption in SAEs and some potential approaches to solve the fundamental theoretical issues before SAEs can be used for interpreting LLMs robustly and at scale.",,"David Chanin, James Wilken-Smith, Tomáš Dulka, Hardik Bhatnagar, Satvik Golechha, Joseph Bloom",2025-06-02T10:58:16Z,A is for Absorption: Studying Feature Splitting and Absorption in Sparse   Autoencoders,A ist für Absorption: Studieren Feature Splitting und Absorption in Sparse Autoencoder,A 用于吸收:研究在粗态自动编码器中的地物分割和吸收,http://arxiv.org/abs/2409.14507v5
817,"Cross-lingual vocabulary transfer plays a promising role in adapting pre-trained language models to new languages, including low-resource languages. Existing approaches that utilize monolingual or parallel corpora face challenges when applied to languages with limited resources. In this work, we propose a simple yet effective vocabulary transfer method that utilizes bilingual dictionaries, which are available for many languages, thanks to descriptive linguists. Our proposed method leverages a property of BPE tokenizers where removing a subword from the vocabulary causes a fallback to shorter subwords. The embeddings of target subwords are estimated iteratively by progressively removing them from the tokenizer. The experimental results show that our approach outperforms existing methods for low-resource languages, demonstrating the effectiveness of a dictionary-based approach for cross-lingual vocabulary transfer.",,"Haruki Sakajo, Yusuke Ide, Justin Vasselli, Yusuke Sakai, Yingtao Tian, Hidetaka Kamigaito, Taro Watanabe",2025-06-02T10:52:52Z,Dictionaries to the Rescue: Cross-Lingual Vocabulary Transfer for   Low-Resource Languages Using Bilingual Dictionaries,Wörterbücher zur Rettung: Cross-Lingual Vocabulary Transfer für ressourcenarme Sprachen mit zweisprachigen Wörterbüchern,救援词典:使用双语词典的低资源语言跨语言词汇传输,http://arxiv.org/abs/2506.01535v1
818,"With the continued proliferation of Large Language Model (LLM) based chatbots, there is a growing demand for generating responses that are not only linguistically fluent but also consistently aligned with persona-specific traits in conversations. However, existing role-play and persona-based chat approaches rely heavily on static role descriptions, coarse-grained signal space, and low-quality synthetic data, which fail to capture dynamic fine-grained details in human-like chat. Human-like chat requires modeling subtle latent traits, such as emotional tone, situational awareness, and evolving personality, which are difficult to predefine and cannot be easily learned from synthetic or distillation-based data. To address these limitations, we propose a Verbal Variational Auto-Encoding (V-VAE) framework, containing a variational auto-encoding module and fine-grained control space which dynamically adapts dialogue behaviour based on fine-grained, interpretable latent variables across talking style, interaction patterns, and personal attributes. We also construct a high-quality dataset, HumanChatData, and benchmark HumanChatBench to address the scarcity of high-quality data in the human-like domain. Experiments show that LLMs based on V-VAE consistently outperform standard baselines on HumanChatBench and DialogBench, which further demonstrates the effectiveness of V-VAE and HumanChatData.",,"Qi Lin, Weikai Xu, Lisi Chen, Bin Dai",2025-06-02T10:38:02Z,V-VAE: A Variational Auto Encoding Framework Towards Fine-Grained   Control over Human-Like Chat,V-VAE: Ein abwechslungsreiches Auto-Encoding-Framework zur feinkörnigen Kontrolle über den menschlichen Chat,V-VAE: 对人类聊天进行精密控制的变化式自动编码框架,http://arxiv.org/abs/2506.01524v1
819,"Online form filling is a common yet labor-intensive task involving extensive keyboard and mouse interactions. Despite the long-standing vision of automating this process with ""one click"", existing tools remain largely rule-based and lack generalizable, generative capabilities. Recent advances in Multimodal Large Language Models (MLLMs) have enabled promising agents for GUI-related tasks in general-purpose scenarios. However, they struggle with the unique challenges of form filling, such as flexible layouts and the difficulty of aligning textual instructions with on-screen fields. To bridge this gap, we formally define the form-filling task and propose FormFactory, an interactive benchmarking suite comprising a web-based interface, backend evaluation module, and carefully constructed dataset. Our benchmark covers diverse real-world scenarios, incorporates various field formats, and simulates high-fidelity form interactions. We conduct a comprehensive evaluation of state-of-the-art MLLMs and observe that no model surpasses 5% accuracy, underscoring the inherent difficulty of the task. These findings also reveal significant limitations in current models' visual layout reasoning and field-value alignment abilities. We hope our benchmark can serve as a stepping stone for further research into robust, practical form-filling agents.",,"Bobo Li, Yuheng Wang, Hao Fei, Juncheng Li, Wei Ji, Mong-Li Lee, Wynne Hsu",2025-06-02T10:34:57Z,FormFactory: An Interactive Benchmarking Suite for Multimodal   Form-Filling Agents,FormFactory: Eine interaktive Benchmarking-Suite für multimodale Formular-Filling-Agenten,形式方式:多种形式填充剂互动基准制定套件,http://arxiv.org/abs/2506.01520v1
820,"We investigate whether the success of a zero-shot Chain-of-Thought (CoT) process can be predicted before completion. We discover that a probing classifier, based on LLM representations, performs well \emph{even before a single token is generated}, suggesting that crucial information about the reasoning process is already present in the initial steps representations. In contrast, a strong BERT-based baseline, which relies solely on the generated tokens, performs worse, likely because it depends on shallow linguistic cues rather than deeper reasoning dynamics. Surprisingly, using later reasoning steps does not always improve classification. When additional context is unhelpful, earlier representations resemble later ones more, suggesting LLMs encode key information early. This implies reasoning can often stop early without loss. To test this, we conduct early stopping experiments, showing that truncating CoT reasoning still improves performance over not using CoT at all, though a gap remains compared to full reasoning. However, approaches like supervised learning or reinforcement learning designed to shorten CoT chains could leverage our classifier's guidance to identify when early stopping is effective. Our findings provide insights that may support such methods, helping to optimize CoT's efficiency while preserving its benefits.",,"Anum Afzal, Florian Matthes, Gal Chechik, Yftah Ziser",2025-06-02T10:26:59Z,Knowing Before Saying: LLM Representations Encode Information About   Chain-of-Thought Success Before Completion,"Wissen, bevor man sagt: LLM-Repräsentanzen Kodieren Sie Informationen über den Erfolg der Kette des Gedankens vor der Fertigstellung",在完成之前知道之前说:LLM 演示文法编码关于努力成功链的信息,http://arxiv.org/abs/2505.24362v2
821,"Rational speakers are supposed to know what they know and what they do not know, and to generate expressions matching the strength of evidence. In contrast, it is still a challenge for current large language models to generate corresponding utterances based on the assessment of facts and confidence in an uncertain real-world environment. While it has recently become popular to estimate and calibrate confidence of LLMs with verbalized uncertainty, what is lacking is a careful examination of the linguistic knowledge of uncertainty encoded in the latent space of LLMs. In this paper, we draw on typological frameworks of epistemic expressions to evaluate LLMs' knowledge of epistemic modality, using controlled stories. Our experiments show that the performance of LLMs in generating epistemic expressions is limited and not robust, and hence the expressions of uncertainty generated by LLMs are not always reliable. To build uncertainty-aware LLMs, it is necessary to enrich semantic knowledge of epistemic modality in LLMs.",,"Meng Li, Michael Vrazitulis, David Schlangen",2025-06-02T10:19:42Z,"Representations of Fact, Fiction and Forecast in Large Language Models:   Epistemics and Attitudes","Darstellungen von Fakten, Fiktionen und Prognosen in großen Sprachmodellen: Epistemik und Haltungen",《大语言模型中事实、小听和预测的表述:流行和态度》,http://arxiv.org/abs/2506.01512v1
822,"We introduce LinearVC, a simple voice conversion method that sheds light on the structure of self-supervised representations. First, we show that simple linear transformations of self-supervised features effectively convert voices. Next, we probe the geometry of the feature space by constraining the set of allowed transformations. We find that just rotating the features is sufficient for high-quality voice conversion. This suggests that content information is embedded in a low-dimensional subspace which can be linearly transformed to produce a target voice. To validate this hypothesis, we finally propose a method that explicitly factorizes content and speaker information using singular value decomposition; the resulting linear projection with a rank of just 100 gives competitive conversion results. Our work has implications for both practical voice conversion and a broader understanding of self-supervised speech representations. Samples and code: https://www.kamperh.com/linearvc/.",,"Herman Kamper, Benjamin van Niekerk, Julian Zaïdi, Marc-André Carbonneau",2025-06-02T10:18:02Z,LinearVC: Linear transformations of self-supervised features through the   lens of voice conversion,LinearVC: Lineare Transformationen von selbstüberwachten Funktionen durch die Linse der Sprachumwandlung,线性VC:通过语音转换透镜进行自我监督特征的线性转换,http://arxiv.org/abs/2506.01510v1
823,"Standard benchmarks fixate on how well large language model (LLM) agents perform in finance, yet say little about whether they are safe to deploy. We argue that accuracy metrics and return-based scores provide an illusion of reliability, overlooking vulnerabilities such as hallucinated facts, stale data, and adversarial prompt manipulation. We take a firm position: financial LLM agents should be evaluated first and foremost on their risk profile, not on their point-estimate performance. Drawing on risk-engineering principles, we outline a three-level agenda: model, workflow, and system, for stress-testing LLM agents under realistic failure modes. To illustrate why this shift is urgent, we audit six API-based and open-weights LLM agents on three high-impact tasks and uncover hidden weaknesses that conventional benchmarks miss. We conclude with actionable recommendations for researchers, practitioners, and regulators: audit risk-aware metrics in future studies, publish stress scenarios alongside datasets, and treat ``safety budget'' as a primary success criterion. Only by redefining what ``good'' looks like can the community responsibly advance AI-driven finance.",,"Zichen Chen, Jiaao Chen, Jianda Chen, Misha Sra",2025-06-02T10:13:24Z,Standard Benchmarks Fail -- Auditing LLM Agents in Finance Must   Prioritize Risk,Standard-Benchmarks-Fehler -- Prüfung von LLM-Agenten in der Finanzierung muss das Risiko priorisieren,标准基准失败 -- -- 财务部门审计LLM代理商,http://arxiv.org/abs/2502.15865v2
824,"In this paper, we propose Selection and Pooling with Large Language Models (SPILL), an intuitive and domain-adaptive method for intent clustering without fine-tuning. Existing embeddings-based clustering methods rely on a few labeled examples or unsupervised fine-tuning to optimize results for each new dataset, which makes them less generalizable to multiple datasets. Our goal is to make these existing embedders more generalizable to new domain datasets without further fine-tuning. Inspired by our theoretical derivation and simulation results on the effectiveness of sampling and pooling techniques, we view the clustering task as a small-scale selection problem. A good solution to this problem is associated with better clustering performance. Accordingly, we propose a two-stage approach: First, for each utterance (referred to as the seed), we derive its embedding using an existing embedder. Then, we apply a distance metric to select a pool of candidates close to the seed. Because the embedder is not optimized for new datasets, in the second stage, we use an LLM to further select utterances from these candidates that share the same intent as the seed. Finally, we pool these selected candidates with the seed to derive a refined embedding for the seed. We found that our method generally outperforms directly using an embedder, and it achieves comparable results to other state-of-the-art studies, even those that use much larger models and require fine-tuning, showing its strength and efficiency. Our results indicate that our method enables existing embedders to be further improved without additional fine-tuning, making them more adaptable to new domain datasets. Additionally, viewing the clustering task as a small-scale selection problem gives the potential of using LLMs to customize clustering tasks according to the user's goals.",,"I-Fan Lin, Faegheh Hasibi, Suzan Verberne",2025-06-02T10:04:32Z,SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling   with Large Language Models,SPILL: Domain-Adaptive Intent Clustering basierend auf Auswahl und Pooling mit großen Sprachmodellen,SPIL:基于选择和与大语言模式合并的域-适应性意图集群,http://arxiv.org/abs/2503.15351v2
825,"Ensuring that Large Language Models (LLMs) align with mainstream human values and ethical norms is crucial for the safe and sustainable development of AI. Current value evaluation and alignment are constrained by Western cultural bias and incomplete domestic frameworks reliant on non-native rules; furthermore, the lack of scalable, rule-driven scenario generation methods makes evaluations costly and inadequate across diverse cultural contexts. To address these challenges, we propose a hierarchical value framework grounded in core Chinese values, encompassing three main dimensions, 12 core values, and 50 derived values. Based on this framework, we construct a large-scale Chinese Values Corpus (CVC) containing over 250,000 value rules enhanced and expanded through human annotation. Experimental results show that CVC-guided scenarios outperform direct generation ones in value boundaries and content diversity. In the evaluation across six sensitive themes (e.g., surrogacy, suicide), seven mainstream LLMs preferred CVC-generated options in over 70.5% of cases, while five Chinese human annotators showed an 87.5% alignment with CVC, confirming its universality, cultural relevance, and strong alignment with Chinese values. Additionally, we construct 400,000 rule-based moral dilemma scenarios that objectively capture nuanced distinctions in conflicting value prioritization across 17 LLMs. Our work establishes a culturally-adaptive benchmarking framework for comprehensive value evaluation and alignment, representing Chinese characteristics. All data are available at https://huggingface.co/datasets/Beijing-AISI/CVC, and the code is available at https://github.com/Beijing-AISI/CVC.",,"Ping Wu, Guobin Shen, Dongcheng Zhao, Yuwei Wang, Yiting Dong, Yu Shi, Enmeng Lu, Feifei Zhao, Yi Zeng",2025-06-02T09:56:59Z,CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of   Large Language Models,CVC: Eine groß angelegte chinesische Wertregel Corpus zur Wertausrichtung großer Sprachmodelle,CVC: 大型中文大语言模式价值调整大型中国价值规则公司,http://arxiv.org/abs/2506.01495v1
826,"Existing Medical Large Vision-Language Models (Med-LVLMs), encapsulating extensive medical knowledge, demonstrate excellent capabilities in understanding medical images. However, there remain challenges in visual localization in medical images, which is crucial for abnormality detection and interpretation. To address these issues, we propose a novel UMed-LVLM designed to unveil medical abnormalities. Specifically, we collect a Medical Abnormalities Unveiling (MAU) dataset and propose a two-stage training method for UMed-LVLM training. To collect MAU dataset, we propose a prompt method utilizing the GPT-4V to generate diagnoses based on identified abnormal areas in medical images. Moreover, the two-stage training method includes Abnormal-Aware Instruction Tuning and Abnormal-Aware Rewarding, comprising Relevance Reward, Abnormal Localization Reward and Vision Relevance Reward. Experimental results demonstrate that our UMed-LVLM significantly outperforms existing Med-LVLMs in identifying and understanding medical abnormalities, achieving a 58% improvement over the baseline. In addition, this work shows that enhancing the abnormality detection capabilities of Med-LVLMs significantly improves their understanding of medical images and generalization capability.",,"Yucheng Zhou, Lingran Song, Jianbing Shen",2025-06-02T09:56:36Z,Improving Medical Large Vision-Language Models with Abnormal-Aware   Feedback,Medizinische Modelle mit großer Vision-Sprache mit abnormal-aware Feedback verbessern,改进具有异常-软件反馈的大型医疗视觉-语言模型,http://arxiv.org/abs/2501.01377v2
827,"Humans naturally interpret numbers non-literally, effortlessly combining context, world knowledge, and speaker intent. We investigate whether large language models (LLMs) interpret numbers similarly, focusing on hyperbole and pragmatic halo effects. Through systematic comparison with human data and computational models of pragmatic reasoning, we find that LLMs diverge from human interpretation in striking ways. By decomposing pragmatic reasoning into testable components, grounded in the Rational Speech Act framework, we pinpoint where LLM processing diverges from human cognition -- not in prior knowledge, but in reasoning with it. This insight leads us to develop a targeted solution -- chain-of-thought prompting inspired by an RSA model makes LLMs' interpretations more human-like. Our work demonstrates how computational cognitive models can both diagnose AI-human differences and guide development of more human-like language understanding capabilities.",,"Polina Tsvilodub, Kanishk Gandhi, Haoran Zhao, Jan-Philipp Fränken, Michael Franke, Noah D. Goodman",2025-06-02T09:56:25Z,Non-literal Understanding of Number Words by Language Models,Nicht-literales Verständnis von Zahlenwörtern nach Sprachmodellen,按语言模式对数字字的不识字理解,http://arxiv.org/abs/2502.06204v2
828,"In this paper, we propose the first multilingual study on definition modeling. We use monolingual dictionary data for four new languages (Spanish, French, Portuguese, and German) and perform an in-depth empirical study to test the performance of pre-trained multilingual language models on definition modeling of monosemic words when finetuned on this data. Furthermore, we use a zero-shot approach to test the multilingual capabilities of two popular chat-based Large Language Models (LLMs) in the task. Results show that multilingual language models can perform on-pair with English but cannot leverage potential cross-lingual synergies, with LLMs generally offering better performance overall. A comprehensive human evaluation of the LLM-generated definition highlights the zero and few-shot capabilities of these models in this new task, also showing their shortcomings. Finally, we show that performance on our task via BERTScore strongly correlates to the performance on multilingual LLM benchmarks, suggesting that our task offers a viable compute-constrained, stable and natural alternative to these.",,"Edison Marrese-Taylor, Erica K. Shimomoto, Alfredo Solano, Enrique Reid",2025-06-02T09:48:37Z,Multilingual Definition Modeling,Mehrsprachige Definitionsmodellierung,多语种定义模型,http://arxiv.org/abs/2506.01489v1
829,"Cross-document Event Coreference Resolution (CD-ECR) is a fundamental task in natural language processing (NLP) that seeks to determine whether event mentions across multiple documents refer to the same real-world occurrence. However, current CD-ECR approaches predominantly rely on trigger features within input mention pairs, which induce spurious correlations between surface-level lexical features and coreference relationships, impairing the overall performance of the models. To address this issue, we propose a novel cross-document event coreference resolution method based on Argument-Centric Causal Intervention (ACCI). Specifically, we construct a structural causal graph to uncover confounding dependencies between lexical triggers and coreference labels, and introduce backdoor-adjusted interventions to isolate the true causal effect of argument semantics. To further mitigate spurious correlations, ACCI integrates a counterfactual reasoning module that quantifies the causal influence of trigger word perturbations, and an argument-aware enhancement module to promote greater sensitivity to semantically grounded information. In contrast to prior methods that depend on costly data augmentation or heuristic-based filtering, ACCI enables effective debiasing in a unified end-to-end framework without altering the underlying training procedure. Extensive experiments demonstrate that ACCI achieves CoNLL F1 of 88.4% on ECB+ and 85.2% on GVC, achieving state-of-the-art performance. The implementation and materials are available at https://github.com/era211/ACCI.",,"Long Yao, Wenzhong Yang, Yabo Yin, Fuyuan Wei, Hongzhen Lv, Jiaren Peng, Liejun Wang, Xiaoming Tao",2025-06-02T09:46:59Z,Argument-Centric Causal Intervention Method for Mitigating Bias in   Cross-Document Event Coreference Resolution,Argument-Centric Causal Intervention Methode zur Bekämpfung von Bias in Cross-Document-Event-Koreferenzlösung,跨文件活动共同参考决议中减轻偏见的 理由-中心原因干预方法,http://arxiv.org/abs/2506.01488v1
830,"Detoxification, the task of rewriting harmful language into non-toxic text, has become increasingly important amid the growing prevalence of toxic content online. However, high-quality parallel datasets for detoxification, especially for hate speech, remain scarce due to the cost and sensitivity of human annotation. In this paper, we propose a novel LLM-in-the-loop pipeline leveraging GPT-4o-mini for automated detoxification. We first replicate the ParaDetox pipeline by replacing human annotators with an LLM and show that the LLM performs comparably to human annotation. Building on this, we construct PARADEHATE, a large-scale parallel dataset specifically for hatespeech detoxification. We release PARADEHATE as a benchmark of over 8K hate/non-hate text pairs and evaluate a wide range of baseline methods. Experimental results show that models such as BART, fine-tuned on PARADEHATE, achieve better performance in style accuracy, content preservation, and fluency, demonstrating the effectiveness of LLM-generated detoxification text as a scalable alternative to human annotation.",,"Shuzhou Yuan, Ercong Nie, Lukas Kouba, Ashish Yashwanth Kangen, Helmut Schmid, Hinrich Schutze, Michael Farber",2025-06-02T09:45:05Z,LLM in the Loop: Creating the PARADEHATE Dataset for Hate Speech   Detoxification,LLM in the Loop: Erstellen des PARADEHATE-Datensatzes für die Hate Speech Entgiftung,圈圈中的LLM:建立PARADEHATE 仇恨言论解毒数据集,http://arxiv.org/abs/2506.01484v1
831,"We propose OmniCaptioner, a versatile visual captioning framework for generating fine-grained textual descriptions across a wide variety of visual domains. Unlike prior methods limited to specific image types (e.g., natural images or geometric visuals), our framework provides a unified solution for captioning natural images, visual text (e.g., posters, UIs, textbooks), and structured visuals (e.g., documents, tables, charts). By converting low-level pixel information into semantically rich textual representations, our framework bridges the gap between visual and textual modalities. Our results highlight three key advantages: (i) Enhanced Visual Reasoning with LLMs, where long-context captions of visual modalities empower LLMs, particularly the DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii) Improved Image Generation, where detailed captions improve tasks like text-to-image generation and image transformation; and (iii) Efficient Supervised Fine-Tuning (SFT), which enables faster convergence with less data. We believe the versatility and adaptability of OmniCaptioner can offer a new perspective for bridging the gap between language and visual modalities.",,"Yiting Lu, Jiakang Yuan, Zhen Li, Shitian Zhao, Qi Qin, Xinyue Li, Le Zhuo, Licheng Wen, Dongyang Liu, Yuewen Cao, Xiangchao Yan, Xin Li, Tianshuo Peng, Shufei Zhang, Botian Shi, Tao Chen, Zhibo Chen, Lei Bai, Peng Gao, Bo Zhang",2025-06-02T09:38:26Z,OmniCaptioner: One Captioner to Rule Them All,"OmniCaptioner: Ein Captioner, der sie alle beherrscht",统称: 将所有规则统归一个标题,http://arxiv.org/abs/2504.07089v3
832,"Understanding the interaction between different drugs (drug-drug interaction or DDI) is critical for ensuring patient safety and optimizing therapeutic outcomes. Existing DDI datasets primarily focus on textual information, overlooking multimodal data that reflect complex drug mechanisms. In this paper, we (1) introduce MUDI, a large-scale Multimodal biomedical dataset for Understanding pharmacodynamic Drug-drug Interactions, and (2) benchmark learning methods to study it. In brief, MUDI provides a comprehensive multimodal representation of drugs by combining pharmacological text, chemical formulas, molecular structure graphs, and images across 310,532 annotated drug pairs labeled as Synergism, Antagonism, or New Effect. Crucially, to effectively evaluate machine-learning based generalization, MUDI consists of unseen drug pairs in the test set. We evaluate benchmark models using both late fusion voting and intermediate fusion strategies. All data, annotations, evaluation scripts, and baselines are released under an open research license.",,"Tung-Lam Ngo, Ba-Hoang Tran, Duy-Cat Can, Trung-Hieu Do, Oliver Y. Chén, Hoang-Quynh Le",2025-06-02T09:36:08Z,MUDI: A Multimodal Biomedical Dataset for Understanding Pharmacodynamic   Drug-Drug Interactions,MUDI: Ein multimodaler biomedizinischer Datensatz zum Verständnis pharmakodynamischer Arzneimittel-Drogen-Interaktionen,MUDI:用于了解制药动力药物-药物-药物相互作用的多模式生物医学数据集,http://arxiv.org/abs/2506.01478v1
833,"Large Language Model (LLM) agents have demonstrated impressive capabilities in handling complex interactive problems. Existing LLM agents mainly generate natural language plans to guide reasoning, which is verbose and inefficient. NL plans are also tailored to specific tasks and restrict agents' ability to generalize across similar tasks. To this end, we explore pseudocode-style plans (P-code Plan) to capture the structural logic of reasoning. We find that P-code Plan empowers LLM agents with stronger generalization ability and more efficiency. Inspired by this finding, we propose a pseudocode-style Planning Guided Preference Optimization method called PGPO for effective agent learning. With two planning-oriented rewards, PGPO further enhances LLM agents' ability to generate high-quality P-code Plans and subsequent reasoning. Experiments show that PGPO achieves superior performance on representative agent benchmarks and outperforms the current leading baselines. Analyses reveal the advantage of PGPO in reducing action errors and omissions during reasoning.",,"Zouying Cao, Runze Wang, Yifei Yang, Xinbei Ma, Xiaoyong Zhu, Bo Zheng, Hai Zhao",2025-06-02T09:35:07Z,PGPO: Enhancing Agent Reasoning via Pseudocode-style Planning Guided   Preference Optimization,PGPO: Erweitern der Agent-Reasoning über Pseudocode-Stil Planung Guided Preference Optimierung,"PGPO:通过 "" 优杜code-规划 "" 型式规划加强代理理由说明",http://arxiv.org/abs/2506.01475v1
834,"Computational models of pragmatic language use have traditionally relied on hand-specified sets of utterances and meanings, limiting their applicability to real-world language use. We propose a neuro-symbolic framework that enhances probabilistic cognitive models by integrating LLM-based modules to propose and evaluate key components in natural language, eliminating the need for manual specification. Through a classic case study of pragmatic question-answering, we systematically examine various approaches to incorporating neural modules into the cognitive model -- from evaluating utilities and literal semantics to generating alternative utterances and goals. We find that hybrid models can match or exceed the performance of traditional probabilistic models in predicting human answer patterns. However, the success of the neuro-symbolic model depends critically on how LLMs are integrated: while they are particularly effective for proposing alternatives and transforming abstract goals into utilities, they face challenges with truth-conditional semantic evaluation. This work charts a path toward more flexible and scalable models of pragmatic language use while illuminating crucial design considerations for balancing neural and symbolic components.",,"Polina Tsvilodub, Robert D. Hawkins, Michael Franke",2025-06-02T09:34:37Z,Integrating Neural and Symbolic Components in a Model of Pragmatic   Question-Answering,Integration neuraler und symbolischer Komponenten in ein Modell der Pragmatischen Fragestellung,将神经元和符号元件纳入实用问题问答模式,http://arxiv.org/abs/2506.01474v1
835,"With the rapid emergence of novel capabilities in Large Language Models (LLMs), the need for rigorous multilingual and multicultural benchmarks that are integrated has become more pronounced. Though existing LLM benchmarks are capable of evaluating specific capabilities of LLMs in English as well as in various mid- to low-resource languages, including those in the Southeast Asian (SEA) region, a comprehensive and culturally representative evaluation suite for the SEA languages has not been developed thus far. Here, we present SEA-HELM, a holistic linguistic and cultural LLM evaluation suite that emphasises SEA languages, comprising five core pillars: (1) NLP Classics, (2) LLM-specifics, (3) SEA Linguistics, (4) SEA Culture, (5) Safety. SEA-HELM currently supports Filipino, Indonesian, Tamil, Thai, and Vietnamese. We also introduce the SEA-HELM leaderboard, which allows users to understand models' multilingual and multicultural performance in a systematic and user-friendly manner. We make the SEA-HELM evaluation code publicly available.",,"Yosephine Susanto, Adithya Venkatadri Hulagadri, Jann Railey Montalan, Jian Gang Ngui, Xian Bin Yong, Weiqi Leong, Hamsawardhini Rengarajan, Peerat Limkonchotiwat, Yifan Mai, William Chandra Tjhi",2025-06-02T09:23:18Z,SEA-HELM: Southeast Asian Holistic Evaluation of Language Models,SEA-HELM: Südostasiennische ganzheitliche Bewertung von Sprachmodellen,SEA-HELM:东南亚语文模式综合评价,http://arxiv.org/abs/2502.14301v2
836,"This paper describes the language identification and multilingual speech recognition system developed at Tallinn University of Technology for the Interspeech 2025 ML-SUPERB 2.0 Challenge. A hybrid language identification system is used, consisting of a pretrained language embedding model and a light-weight speech recognition model with a shared encoder across languages and language-specific bigram language models. For speech recognition, three models are used, where only a single model is applied for each language, depending on the training data availability and performance on held-out data. The model set consists of a finetuned version of SeamlessM4T, MMS-1B-all with custom language adapters and MMS-zeroshot. The system obtained the top overall score in the challenge.",,"Tanel Alumäe, Artem Fedorchenko",2025-06-02T09:16:09Z,TalTech Systems for the Interspeech 2025 ML-SUPERB 2.0 Challenge,TalTech Systeme für die Interspeech 2025 ML-SUPERB 2.0 Challenge,2025 ML-SUPERB 2025 ML-SUPERB 2025 Interspeech 的Taltech系统 2.0 挑战,http://arxiv.org/abs/2506.01458v1
837,"Decomposing weight matrices into quantization and low-rank components ($\mathbf{W} \approx \mathbf{Q} + \mathbf{L}\mathbf{R}$) is a widely used technique for compressing large language models (LLMs). Existing joint optimization methods iteratively alternate between quantization and low-rank approximation. However, these methods tend to prioritize one component at the expense of the other, resulting in suboptimal decompositions that fail to leverage each component's unique strengths. In this work, we introduce Outlier-Driven Low-Rank Initialization (ODLRI), which assigns low-rank components the specific role of capturing activation-sensitive weights. This structured decomposition mitigates outliers' negative impact on quantization, enabling more effective balance between quantization and low-rank approximation. Experiments on Llama2 (7B, 13B, 70B), Llama3-8B, and Mistral-7B demonstrate that incorporating ODLRI into the joint optimization framework consistently reduces activation-aware error, minimizes quantization scale, and improves perplexity and zero-shot accuracy in low-bit settings.",,"Yoonjun Cho, Soeun Kim, Dongjae Jeon, Kyelim Lee, Beomsoo Lee, Albert No",2025-06-02T09:15:13Z,Assigning Distinct Roles to Quantized and Low-Rank Matrices Toward   Optimal Weight Decomposition,Zuweisen von unterscheidenden Rollen auf Quantisierte und Low-Rank-Matrizen zur optimalen Gewichtszersetzung,向实现最佳体重分解的量化和低 Rank 矩阵分配不同作用,http://arxiv.org/abs/2506.02077v1
838,"The application of rule-based reinforcement learning (RL) to multimodal large language models (MLLMs) introduces unique challenges and potential deviations from findings in text-only domains, particularly for perception-heavy tasks. This paper provides a comprehensive study of rule-based visual RL, using jigsaw puzzles as a structured experimental framework. Jigsaw puzzles offer inherent ground truth, adjustable difficulty, and demand complex decision-making, making them ideal for this study. Our research reveals several key findings: \textit{Firstly,} we find that MLLMs, initially performing near to random guessing on the simplest jigsaw puzzles, achieve near-perfect accuracy and generalize to complex, unseen configurations through fine-tuning. \textit{Secondly,} training on jigsaw puzzles can induce generalization to other visual tasks, with effectiveness tied to specific task configurations. \textit{Thirdly,} MLLMs can learn and generalize with or without explicit reasoning, though open-source models often favor direct answering. Consequently, even when trained for step-by-step reasoning, they can ignore the thinking process in deriving the final answer. \textit{Fourthly,} we observe that complex reasoning patterns appear to be pre-existing rather than emergent, with their frequency increasing alongside training and task difficulty. \textit{Finally,} our results demonstrate that RL exhibits more effective generalization than Supervised Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL optimization. Although these observations are based on jigsaw puzzles and may vary across other visual tasks, this research contributes a valuable piece of jigsaw to the larger puzzle of collective understanding rule-based visual RL and its potential in multimodal learning. The code is available at: https://github.com/zifuwanggg/Jigsaw-R1.",,"Zifu Wang, Junyi Zhu, Bo Tang, Zhiyu Li, Feiyu Xiong, Jiaqian Yu, Matthew B. Blaschko",2025-06-02T09:10:33Z,Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with   Jigsaw Puzzles,Jigsaw-R1: Eine Studie über regelbasiertes Visuelles Verstärkungslernen mit Puzzle-Puzzles,Jigsaw-R1:用Jigsaw谜语进行基于规则的视觉强化学习研究,http://arxiv.org/abs/2505.23590v2
839,"While large language models demonstrate remarkable capabilities at task-specific applications through fine-tuning, extending these benefits across diverse languages is essential for broad accessibility. However, effective cross-lingual transfer is hindered by LLM performance gaps across languages and the scarcity of fine-tuning data in many languages. Through analysis of LLM internal representations from over 1,000+ language pairs, we discover that middle layers exhibit the strongest potential for cross-lingual alignment. Building on this finding, we propose a middle-layer alignment objective integrated into task-specific training. Our experiments on slot filling, machine translation, and structured text generation show consistent improvements in cross-lingual transfer, especially to lower-resource languages. The method is robust to the choice of alignment languages and generalizes to languages unseen during alignment. Furthermore, we show that separately trained alignment modules can be merged with existing task-specific modules, improving cross-lingual capabilities without full re-training. Our code is publicly available (https://github.com/dannigt/mid-align).",,"Danni Liu, Jan Niehues",2025-06-02T09:09:36Z,Middle-Layer Representation Alignment for Cross-Lingual Transfer in   Fine-Tuned LLMs,Mittelschicht-Repräsentanz für Cross-Lingual-Transfer in feinverstellten LLMs,"微调LMLM中途代表代表人数协调,以利跨语言转移",http://arxiv.org/abs/2502.14830v3
840,"This work introduces a novel framework for evaluating LLMs' capacity to balance instruction-following with critical reasoning when presented with multiple-choice questions containing no valid answers. Through systematic evaluation across arithmetic, domain-specific knowledge, and high-stakes medical decision tasks, we demonstrate that post-training aligned models often default to selecting invalid options, while base models exhibit improved refusal capabilities that scale with model size. Our analysis reveals that alignment techniques, though intended to enhance helpfulness, can inadvertently impair models' reflective judgment--the ability to override default behaviors when faced with invalid options. We additionally conduct a parallel human study showing similar instruction-following biases, with implications for how these biases may propagate through human feedback datasets used in alignment. We provide extensive ablation studies examining the impact of model size, training techniques, and prompt engineering. Our findings highlight fundamental tensions between alignment optimization and preservation of critical reasoning capabilities, with important implications for developing more robust AI systems for real-world deployment.",,"Gracjan Góral, Emilia Wiśnios, Piotr Sankowski, Paweł Budzianowski",2025-06-02T09:08:56Z,"Wait, that's not an option: LLMs Robustness with Incorrect   Multiple-Choice Options","Warten Sie, das ist keine Option: LLMs Robustheit mit falschen Multiple-Choice-Optionen","等待, 这不是一个选项 : LLMs 强力与不正确的多选择选项",http://arxiv.org/abs/2409.00113v3
841,"Extracting useful signals or pattern to support important business decisions for example analyzing investment product traction and discovering customer preference, risk monitoring etc. from unstructured text is a challenging task. Capturing interaction of entities or concepts and association mining is a crucial component in text mining, enabling information extraction and reasoning over and knowledge discovery from text. Furthermore, it can be used to enrich or filter knowledge graphs to guide exploration processes, descriptive analytics and uncover hidden stories in the text. In this paper, we introduce a domain independent pipeline i.e., generalized framework to enable document filtering, entity extraction using various sources (or techniques) as plug-ins and association mining to build any text mining business use-case and quantitatively define a scoring metric for ranking purpose. The proposed framework has three major components a) Document filtering: filtering documents/text of interest from massive amount of texts b) Configurable entity extraction pipeline: include entity extraction techniques i.e., i) DBpedia Spotlight, ii) Spacy NER, iii) Custom Entity Matcher, iv) Phrase extraction (or dictionary) based c) Association Relationship Mining: To generates co-occurrence graph to analyse potential relationships among entities, concepts. Further, co-occurrence count based frequency statistics provide a holistic window to observe association trends or buzz rate in specific business context. The paper demonstrates the usage of framework as fundamental building box in two financial use-cases namely brand product discovery and vendor risk monitoring. We aim that such framework will remove duplicated effort, minimize the development effort, and encourage reusability and rapid prototyping in association mining business applications for institutions.",,"Anshika Rawal, Abhijeet Kumar, Mridul Mishra",2025-06-02T09:08:38Z,Building Entity Association Mining Framework for Knowledge Discovery,Gebäude Entity Association Mining Framework for Knowledge Discovery,建设实体协会知识发现采矿框架,http://arxiv.org/abs/2506.01451v1
842,"This paper reports on the development of a large-scale speech recognition model, Whale. Similar to models such as Whisper and OWSM, Whale leverages both a large model size and a diverse, extensive dataset. Whale's architecture integrates w2v-BERT self-supervised model, an encoder-decoder backbone built on E-Branchformer, and a joint CTC-attention decoding strategy. The training corpus comprises varied speech data, of not only public corpora but also in-house data, thereby enhancing the model's robustness to different speaking styles and acoustic conditions. Through evaluations on multiple benchmarks, Whale achieved comparable performance to existing models. In particular, it achieves a word error rate of 2.4% on the Librispeech test-clean set and a character error rate of 3.4% on the CSJ eval3 set, outperforming Whisper large-v3 and OWSM v3.1.",,"Yosuke Kashiwagi, Hayato Futami, Emiru Tsunoo, Satoshi Asakawa",2025-06-02T08:52:50Z,Whale: Large-Scale multilingual ASR model with w2v-BERT and   E-Branchformer with large speech data,Wal: Mehrsprachiges ASR-Modell mit w2v-BERT und E-Branchformer mit großen Sprachdaten,鲸:具有 w2v-BERT 和具有大量语音数据的E-Branchrender 的大型多语言ASR模型,http://arxiv.org/abs/2506.01439v1
843,"Prompt-based text embedding models, which generate task-specific embeddings upon receiving tailored prompts, have recently demonstrated remarkable performance. However, their resulting embeddings often have thousands of dimensions, leading to high storage costs and increased computational costs of embedding-based operations. In this paper, we investigate how post-hoc dimensionality reduction applied to the embeddings affects the performance of various tasks that leverage these embeddings, specifically classification, clustering, retrieval, and semantic textual similarity (STS) tasks. Our experiments show that even a naive dimensionality reduction, which keeps only the first 25% of the dimensions of the embeddings, results in a very slight performance degradation, indicating that these embeddings are highly redundant. Notably, for classification and clustering, even when embeddings are reduced to less than 0.5% of the original dimensionality the performance degradation is very small. To quantitatively analyze this redundancy, we perform an analysis based on the intrinsic dimensionality and isotropy of the embeddings. Our analysis reveals that embeddings for classification and clustering, which are considered to have very high dimensional redundancy, exhibit lower intrinsic dimensionality and less isotropy compared with those for retrieval and STS.",,"Hayato Tsukagoshi, Ryohei Sasano",2025-06-02T08:50:38Z,"Redundancy, Isotropy, and Intrinsic Dimensionality of Prompt-based Text   Embeddings","Redundanz, Isotropie und Intrinsische Dimensionalität von Prompt-basierten Text-Embeddings",即时基于文本嵌入的冗余、异粒化和内在多维性,http://arxiv.org/abs/2506.01435v1
844,"The recent surge in high-quality open-source Generative AI text models (colloquially: LLMs), as well as efficient finetuning techniques, have opened the possibility of creating high-quality personalized models that generate text attuned to a specific individual's needs and are capable of credibly imitating their writing style by refining an open-source model with that person's own data. The technology to create such models is accessible to private individuals, and training and running such models can be done cheaply on consumer-grade hardware. While these advancements are a huge gain for usability and privacy, this position paper argues that the practical feasibility of impersonating specific individuals also introduces novel safety risks. For instance, this technology enables the creation of phishing emails or fraudulent social media accounts, based on small amounts of publicly available text, or by the individuals themselves to escape AI text detection. We further argue that these risks are complementary to - and distinct from - the much-discussed risks of other impersonation attacks such as image, voice, or video deepfakes, and are not adequately addressed by the larger research community, or the current generation of open- and closed-source models.",,"Eugenia Iofinova, Andrej Jovanovic, Dan Alistarh",2025-06-02T08:49:20Z,Position: It's Time to Act on the Risk of Efficient Personalized Text   Generation,"Position: Es ist an der Zeit, über das Risiko einer effizienten Personalisierten Textgenerierung zu handeln",立场:现在是就高效个性化文本生成风险采取行动的时候了,http://arxiv.org/abs/2502.06560v2
845,"Integrating structured graph data with rich textual information from nodes poses a significant challenge, particularly for heterophilic node classification. Current approaches often struggle with computational costs or effective fusion of disparate modalities. We propose \textbf{Graph Masked Language Model (GMLM)}, a novel architecture efficiently combining Graph Neural Networks (GNNs) with Pre-trained Language Models (PLMs). GMLM introduces three key innovations: (i) a \textbf{dynamic active node selection} strategy for scalable PLM text processing; (ii) a GNN-specific \textbf{contrastive pretraining stage} using soft masking with a learnable graph \texttt{[MASK]} token for robust structural representations; and (iii) a \textbf{dedicated fusion module} integrating RGCN-based GNN embeddings with PLM (GTE-Small \& DistilBERT) embeddings. Extensive experiments on heterophilic benchmarks (Cornell, Wisconsin, Texas) demonstrate GMLM's superiority. Notably, GMLM(DistilBERT) achieves significant performance gains, improving accuracy by over \textbf{4.7\%} on Cornell and over \textbf{2.0\%} on Texas compared to the previous best-performing baselines. This work underscores the benefits of targeted PLM engagement and modality-specific pretraining for improved, efficient learning on text-rich graphs.",,"Aarush Sinha, OM Kumar CU",2025-06-02T08:42:48Z,GMLM: Bridging Graph Neural Networks and Language Models for   Heterophilic Node Classification,GMLM: Überbrückung von Graph Neuronalen Netzwerken und Sprachmodellen für heterophile Knotenklassifikation,GMLM: 血氧哲学节点分类的架接图形神经网络和语言模型,http://arxiv.org/abs/2503.05763v3
846,"Recent research on explainable recommendation generally frames the task as a standard text generation problem, and evaluates models simply based on the textual similarity between the predicted and ground-truth explanations. However, this approach fails to consider one crucial aspect of the systems: whether their outputs accurately reflect the users' (post-purchase) sentiments, i.e., whether and why they would like and/or dislike the recommended items. To shed light on this issue, we introduce new datasets and evaluation methods that focus on the users' sentiments. Specifically, we construct the datasets by explicitly extracting users' positive and negative opinions from their post-purchase reviews using an LLM, and propose to evaluate systems based on whether the generated explanations 1) align well with the users' sentiments, and 2) accurately identify both positive and negative opinions of users on the target items. We benchmark several recent models on our datasets and demonstrate that achieving strong performance on existing metrics does not ensure that the generated explanations align well with the users' sentiments. Lastly, we find that existing models can provide more sentiment-aware explanations when the users' (predicted) ratings for the target items are directly fed into the models as input. The datasets and benchmark implementation are available at: https://github.com/jchanxtarov/sent_xrec.",,"Ryotaro Shimizu, Takashi Wada, Yu Wang, Johannes Kruse, Sean O'Brien, Sai HtaungKham, Linxin Song, Yuya Yoshikawa, Yuki Saito, Fugee Tsung, Masayuki Goto, Julian McAuley",2025-06-02T08:41:09Z,Disentangling Likes and Dislikes in Personalized Generative Explainable   Recommendation,Entwirren von Vorlieben und Abneigungen in personalisierter generativer erklärbarer Empfehlung,在个性化可解释建议中拆分同异异,http://arxiv.org/abs/2410.13248v2
847,"Large language models (LLMs) are increasingly used in sensitive domains, where their ability to infer personal data from seemingly benign text poses emerging privacy risks. While recent LLM-based anonymization methods help mitigate such risks, they often rely on proprietary models (e.g., GPT-4), raising concerns about cost and the potential exposure of sensitive data to untrusted external systems. To address this, we introduce SElf-refining Anonymization with Language model (SEAL), a novel distillation framework for training small language models (SLMs) to perform effective anonymization without relying on external costly models at inference time. We leverage adversarial interactions between an LLM anonymizer and an inference model to collect trajectories of anonymized texts and inferred attributes, which are used to distill anonymization, adversarial inference, and utility evaluation capabilities into SLMs via supervised fine-tuning and preference learning. The resulting models learn to both anonymize text and critique their outputs, enabling iterative improvement of anonymization quality via self-refinement. Experiments on SynthPAI, a dataset of synthetic personal profiles and text comments, demonstrate that SLMs trained with SEAL achieve substantial improvements in anonymization capabilities. Notably, 8B models attain a privacy-utility trade-off comparable to that of the GPT-4 anonymizer and, with self-refinement, even surpass it in terms of privacy. These results show the effectiveness of our adversarial distillation framework in training SLMs as efficient anonymizers. To facilitate further research, we release the full dataset used in our experiments.",,"Kyuyoung Kim, Hyunjun Jeon, Jinwoo Shin",2025-06-02T08:21:27Z,Self-Refining Language Model Anonymizers via Adversarial Distillation,Selbstrefinierende Sprachmodellanonymisatoren über Adversarial Destillation,通过对流蒸馏自精语言示范匿名器,http://arxiv.org/abs/2506.01420v1
848,"We introduce UniversalCEFR, a large-scale multilingual multidimensional dataset of texts annotated according to the CEFR (Common European Framework of Reference) scale in 13 languages. To enable open research in both automated readability and language proficiency assessment, UniversalCEFR comprises 505,807 CEFR-labeled texts curated from educational and learner-oriented resources, standardized into a unified data format to support consistent processing, analysis, and modeling across tasks and languages. To demonstrate its utility, we conduct benchmark experiments using three modelling paradigms: a) linguistic feature-based classification, b) fine-tuning pre-trained LLMs, and c) descriptor-based prompting of instruction-tuned LLMs. Our results further support using linguistic features and fine-tuning pretrained models in multilingual CEFR level assessment. Overall, UniversalCEFR aims to establish best practices in data distribution in language proficiency research by standardising dataset formats and promoting their accessibility to the global research community.",,"Joseph Marvin Imperial, Abdullah Barayan, Regina Stodden, Rodrigo Wilkens, Ricardo Munoz Sanchez, Lingyun Gao, Melissa Torgbi, Dawn Knight, Gail Forey, Reka R. Jablonkai, Ekaterina Kochmar, Robert Reynolds, Eugenio Ribeiro, Horacio Saggion, Elena Volodina, Sowmya Vajjala, Thomas Francois, Fernando Alva-Manchego, Harish Tayyar Madabushi",2025-06-02T08:21:16Z,UniversalCEFR: Enabling Open Multilingual Research on Language   Proficiency Assessment,UniversalCEFR: Open Multilingual Research on Language Proficiency Assessment ermöglichen,通用:对语言能力评估进行开放的多种语言研究,http://arxiv.org/abs/2506.01419v1
849,"Critical text assessment is at the core of many expert activities, such as fact-checking, peer review, and essay grading. Yet, existing work treats critical text assessment as a black box problem, limiting interpretability and human-AI collaboration. To close this gap, we introduce Structured Reasoning In Critical Text Assessment (STRICTA), a novel specification framework to model text assessment as an explicit, step-wise reasoning process. STRICTA breaks down the assessment into a graph of interconnected reasoning steps drawing on causality theory (Pearl, 1995). This graph is populated based on expert interaction data and used to study the assessment process and facilitate human-AI collaboration. We formally define STRICTA and apply it in a study on biomedical paper assessment, resulting in a dataset of over 4000 reasoning steps from roughly 40 biomedical experts on more than 20 papers. We use this dataset to empirically study expert reasoning in critical text assessment, and investigate if LLMs are able to imitate and support experts within these workflows. The resulting tools and datasets pave the way for studying collaborative expert-AI reasoning in text assessment, in peer review and beyond.",,"Nils Dycke, Matej Zečević, Ilia Kuznetsov, Beatrix Suess, Kristian Kersting, Iryna Gurevych",2025-06-02T08:18:21Z,STRICTA: Structured Reasoning in Critical Text Assessment for Peer   Review and Beyond,STRICTA: Strukturierte Begründung in kritischer Textbewertung für Peer Review und darüber hinaus,STRITRTA:同行审议及以后关键文本评估中结构合理性,http://arxiv.org/abs/2409.05367v2
850,"We propose EdiText, a controllable text editing method that modifies the reference text to desired attributes at various scales. We integrate an SDEdit-based editing technique that allows for broad adjustments in the degree of text editing. Additionally, we introduce a novel fine-level editing method based on self-conditioning, which allows subtle control of reference text. While being capable of editing on its own, this fine-grained method, integrated with the SDEdit approach, enables EdiText to make precise adjustments within the desired range. EdiText demonstrates its controllability to robustly adjust reference text at a broad range of levels across various tasks, including toxicity control and sentiment control.",,"Che Hyun Lee, Heeseung Kim, Jiheum Yeom, Sungroh Yoon",2025-06-02T08:12:36Z,EdiText: Controllable Coarse-to-Fine Text Editing with Diffusion   Language Models,EdiText: Kontrollierbare Coarse-to-Fine Textbearbeitung mit Diffusions-Sprachenmodellen,"EdiText: 可控 Coarse 到 Fine 文本编辑, 配有传播语言模型",http://arxiv.org/abs/2502.19765v2
851,"Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data are available at https://github.com/yuleiqin/RAIF.",,"Yulei Qin, Gang Li, Zongyi Li, Zihan Xu, Yuchen Shi, Zhekai Lin, Xiao Cui, Ke Li, Xing Sun",2025-06-02T08:11:44Z,Incentivizing Reasoning for Advanced Instruction-Following of Large   Language Models,Anreize für eine fortgeschrittene Instruktions-Folge von großen Sprachmodellen,为采用大语言模式的高级指示提供激励理由,http://arxiv.org/abs/2506.01413v1
852,"The limited reasoning capabilities of small language models (SLMs) cast doubt on their suitability for tasks demanding deep, multi-step logical deduction. This paper introduces a framework called Small Reasons, Large Hints (SMART), which selectively augments SLM reasoning with targeted guidance from large language models (LLMs). Inspired by the concept of cognitive scaffolding, SMART employs a score-based evaluation to identify uncertain reasoning steps and injects corrective LLM-generated reasoning only when necessary. By framing structured reasoning as an optimal policy search, our approach steers the reasoning trajectory toward correct solutions without exhaustive sampling. Our experiments on mathematical reasoning datasets demonstrate that targeted external scaffolding significantly improves performance, paving the way for collaborative use of both SLM and LLM to tackle complex reasoning tasks that are currently unsolvable by SLMs alone.",,"Yujin Kim, Euiin Yi, Minu Kim, Se-Young Yun, Taehyeon Kim",2025-06-02T08:10:54Z,Guiding Reasoning in Small Language Models with LLM Assistance,Leitende Vernunft in kleinen Sprachmodellen mit LLM Assistance,由LLM协助的小型语言模式指导原则,http://arxiv.org/abs/2504.09923v2
853,"We study word learning in subword and character language models with the psycholinguistic lexical decision task. While subword LMs struggle to discern words and non-words with high accuracy, character LMs solve this task easily and consistently. Only when supplied with further contexts do subword LMs perform similarly to character models. Additionally, when looking at word-level and syntactic learning trajectories, we find that both processes are separable in character LMs. Word learning happens before syntactic learning, whereas both occur simultaneously in subword LMs. This raises questions about the adequacy of subword LMs for modeling language acquisition and positions character LMs as a viable alternative to study processes below the syntactic level.",,"Bastian Bunzeck, Sina Zarrieß",2025-06-02T08:05:04Z,"Subword models struggle with word learning, but surprisal hides it","Subword-Modelle kämpfen mit Wort lernen, aber Surprisal versteckt es","子字模型与文字学习搏斗, 但伪善隐藏它",http://arxiv.org/abs/2502.12835v2
854,"This study provides the first comprehensive comparison of New York Times-style text generated by six large language models against real, human-authored NYT writing. The comparison is based on a formal syntactic theory. We use Head-driven Phrase Structure Grammar (HPSG) to analyze the grammatical structure of the texts. We then investigate and illustrate the differences in the distributions of HPSG grammar types, revealing systematic distinctions between human and LLM-generated writing. These findings contribute to a deeper understanding of the syntactic behavior of LLMs as well as humans, within the NYT genre.",,"Olga Zamaraeva, Dan Flickinger, Francis Bond, Carlos Gómez-Rodríguez",2025-06-02T08:04:34Z,Comparing LLM-generated and human-authored news text using formal   syntactic theory,Vergleich von LLM-generierten und von Menschen verfassten Nachrichtentexten mittels formaler syntaktischer Theorie,使用正式的合成理论比较LLM产生的和人写的新闻文本,http://arxiv.org/abs/2506.01407v1
855,"The popularity of automatic speech-to-speech translation for human conversations is growing, but the quality varies significantly depending on the language pair. In a context of community interpreting for low-resource languages, namely Turkish and Pashto to/from French, we collected fine-tuning and testing data, and compared systems using several automatic metrics (BLEU, COMET, and BLASER) and human assessments. The pipelines included automatic speech recognition, machine translation, and speech synthesis, with local models and cloud-based commercial ones. Some components have been fine-tuned on our data. We evaluated over 60 pipelines and determined the best one for each direction. We also found that the ranks of components are generally independent of the rest of the pipeline.",,"Andrei Popescu-Belis, Alexis Allemann, Teo Ferrari, Gopal Krishnamani",2025-06-02T08:02:44Z,Speech-to-Speech Translation Pipelines for Conversations in Low-Resource   Languages,Speech-to-Speech-Übersetzungspipelines für Gespräche in ressourcenarmen Sprachen,用于低资源语言对话的语音对语音翻译管道,http://arxiv.org/abs/2506.01406v1
856,"Data augmentation is an essential technique in natural language processing (NLP) for enriching training datasets by generating diverse samples. This process is crucial for improving the robustness and generalization capabilities of NLP models. However, a significant challenge remains: \textit{Insufficient Attention to Sample Distribution Diversity}. Most existing methods focus on increasing the sample numbers while neglecting the sample distribution diversity, which can lead to model overfitting. In response, we explore data augmentation's impact on dataset diversity and propose a \textbf{\underline{D}}iversity-\textbf{\underline{o}}riented data \textbf{\underline{Aug}}mentation framework (\textbf{DoAug}). % \(\mathscr{DoAug}\) Specifically, we utilize a diversity-oriented fine-tuning approach to train an LLM as a diverse paraphraser, which is capable of augmenting textual datasets by generating diversified paraphrases. Then, we apply the LLM paraphraser to a selected coreset of highly informative samples and integrate the paraphrases with the original data to create a more diverse augmented dataset. Finally, we conduct extensive experiments on 12 real-world textual datasets. The results show that our fine-tuned LLM augmenter improves diversity while preserving label consistency, thereby enhancing the robustness and performance of downstream tasks. Specifically, it achieves an average performance gain of \(10.52\%\), surpassing the runner-up baseline with more than three percentage points.",,"Zaitian Wang, Jinghan Zhang, Xinhao Zhang, Kunpeng Liu, Pengfei Wang, Yuanchun Zhou",2025-06-02T07:51:20Z,Diversity-oriented Data Augmentation with Large Language Models,Diversity-orientierte Daten Augmentation mit großen Sprachmodellen,具有大语言模式的多样化型数据扩增,http://arxiv.org/abs/2502.11671v2
857,"Scientific communication is receiving increasing attention in natural language processing, especially to help researches access, summarize, and generate content. One emerging application in this area is Speech-to-Abstract Generation (SAG), which aims to automatically generate abstracts from recorded scientific presentations. SAG enables researchers to efficiently engage with conference talks, but progress has been limited by a lack of large-scale datasets. To address this gap, we introduce NUTSHELL, a novel multimodal dataset of *ACL conference talks paired with their corresponding abstracts. We establish strong baselines for SAG and evaluate the quality of generated abstracts using both automatic metrics and human judgments. Our results highlight the challenges of SAG and demonstrate the benefits of training on NUTSHELL. By releasing NUTSHELL under an open license (CC-BY 4.0), we aim to advance research in SAG and foster the development of improved models and evaluation methods.",,"Maike Züfle, Sara Papi, Beatrice Savoldi, Marco Gaido, Luisa Bentivogli, Jan Niehues",2025-06-02T07:51:11Z,NUTSHELL: A Dataset for Abstract Generation from Scientific Talks,NUTSHELL: Ein Datensatz für die abstrakte Generation aus wissenschaftlichen Vorträgen,NUTSHELL:科学会谈摘要生成数据集,http://arxiv.org/abs/2502.16942v2
858,"Child literacy is a strong predictor of life outcomes at the subsequent stages of an individual's life. This points to a need for targeted interventions in vulnerable low and middle income populations to help bridge the gap between literacy levels in these regions and high income ones. In this effort, reading assessments provide an important tool to measure the effectiveness of these programs and AI can be a reliable and economical tool to support educators with this task. Developing accurate automatic reading assessment systems for child speech in low-resource languages poses significant challenges due to limited data and the unique acoustic properties of children's voices. This study focuses on Xhosa, a language spoken in South Africa, to advance child speech recognition capabilities. We present a novel dataset composed of child speech samples in Xhosa. The dataset is available upon request and contains ten words and letters, which are part of the Early Grade Reading Assessment (EGRA) system. Each recording is labeled with an online and cost-effective approach by multiple markers and a subsample is validated by an independent EGRA reviewer. This dataset is evaluated with three fine-tuned state-of-the-art end-to-end models: wav2vec 2.0, HuBERT, and Whisper. The results indicate that the performance of these models can be significantly influenced by the amount and balancing of the available training data, which is fundamental for cost-effective large dataset collection. Furthermore, our experiments indicate that the wav2vec 2.0 performance is improved by training on multiple classes at a time, even when the number of available samples is constrained.",,"Sergio Chevtchenko, Nikhil Navas, Rafaella Vale, Franco Ubaudi, Sipumelele Lucwaba, Cally Ardington, Soheil Afshar, Mark Antoniou, Saeed Afshar",2025-06-02T07:47:01Z,An End-to-End Approach for Child Reading Assessment in the Xhosa   Language,Ein End-to-End-Ansatz für Kinderlesebewertung in der Xhosa-Sprache,Xhosa语言儿童阅读评估的端至端办法,http://arxiv.org/abs/2505.17371v2
859,"Multilingual NMT is a viable solution for translating low-resource languages (LRLs) when data from high-resource languages (HRLs) from the same language family is available. However, the training schedule, i.e. the order of presentation of languages, has an impact on the quality of such systems. Here, in a many-to-one translation setting, we propose to apply two algorithms that use reinforcement learning to optimize the training schedule of NMT: (1) Teacher-Student Curriculum Learning and (2) Deep Q Network. The former uses an exponentially smoothed estimate of the returns of each action based on the loss on monolingual or multilingual development subsets, while the latter estimates rewards using an additional neural network trained from the history of actions selected in different states of the system, together with the rewards received. On a 8-to-1 translation dataset with LRLs and HRLs, our second method improves BLEU and COMET scores with respect to both random selection of monolingual batches and shuffled multilingual batches, by adjusting the number of presentations of LRL vs. HRL batches.",,"Alexis Allemann, Àlex R. Atrio, Andrei Popescu-Belis",2025-06-02T07:35:16Z,Optimizing the Training Schedule of Multilingual NMT using Reinforcement   Learning,Optimierung des Trainingsplans für mehrsprachiges NMT mit Verstärkungslernen,"利用强化学习,优化多语种国家管理培训时间表",http://arxiv.org/abs/2410.06118v2
860,"The recent progress of large language model agents has opened new possibilities for automating tasks through graphical user interfaces (GUIs), especially in mobile environments where intelligent interaction can greatly enhance usability. However, practical deployment of such agents remains constrained by several key challenges. Existing training data is often noisy and lack semantic diversity, which hinders the learning of precise grounding and planning. Models trained purely by imitation tend to overfit to seen interface patterns and fail to generalize in unfamiliar scenarios. Moreover, most prior work focuses on English interfaces while overlooks the growing diversity of non-English applications such as those in the Chinese mobile ecosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent built for robust and efficient on-device GUI interaction. Our training pipeline includes grounding-aware pre-training to enhance perception, supervised fine-tuning on high-quality Chinese and English trajectories to imitate human-like actions, and reinforcement fine-tuning with GRPO to improve reasoning capability. We also introduce a compact action space that reduces output length and supports low-latency execution on mobile devices. AgentCPM-GUI achieves state-of-the-art performance on five public benchmarks and a new Chinese GUI benchmark called CAGUI, reaching $96.9\%$ Type-Match and $91.3\%$ Exact-Match. To facilitate reproducibility and further research, we publicly release all code, model checkpoint, and evaluation data.",,"Zhong Zhang, Yaxi Lu, Yikun Fu, Yupeng Huo, Shenzhi Yang, Yesai Wu, Han Si, Xin Cong, Haotian Chen, Yankai Lin, Jie Xie, Wei Zhou, Wang Xu, Yuanheng Zhang, Zhou Su, Zhongwu Zhai, Xiaoming Liu, Yudong Mei, Jianming Xu, Hongyan Tian, Chongyi Wang, Chi Chen, Yuan Yao, Zhiyuan Liu, Maosong Sun",2025-06-02T07:30:29Z,AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning,AgentCPM-GUI: Mobile-Use-Agenten mit Verstärkungs-Fine-Tuning bauen,Agent CPM-GUI: 制造具有加固精度的移动用途制剂,http://arxiv.org/abs/2506.01391v1
861,"Hubness, the tendency for a few points to be among the nearest neighbours of a disproportionate number of other points, commonly arises when applying standard distance measures to high-dimensional data, often negatively impacting distance-based analysis. As autoregressive large language models (LLMs) operate on high-dimensional representations, we ask whether they are also affected by hubness. We first prove that the only large-scale representation comparison operation performed by LLMs, namely that between context and unembedding vectors to determine continuation probabilities, is not characterized by the concentration of distances phenomenon that typically causes the appearance of nuisance hubness. We then empirically show that this comparison still leads to a high degree of hubness, but the hubs in this case do not constitute a disturbance. They are rather the result of context-modulated frequent tokens often appearing in the pool of likely candidates for next token prediction. However, when other distances are used to compare LLM representations, we do not have the same theoretical guarantees, and, indeed, we see nuisance hubs appear. There are two main takeaways. First, hubness, while omnipresent in high-dimensional spaces, is not a negative property that needs to be mitigated when LLMs are being used for next token prediction. Second, when comparing representations from LLMs using Euclidean or cosine distance, there is a high risk of nuisance hubs and practitioners should use mitigation techniques if relevant.",,"Beatrix M. G. Nielsen, Iuri Macocco, Marco Baroni",2025-06-02T07:26:26Z,Prediction hubs are context-informed frequent tokens in LLMs,Vorhersagezentren sind kontextinformierte häufige Token in LLMs,预测中枢是LLMM中根据实际情况经常发送的信号,http://arxiv.org/abs/2502.10201v2
862,"Automated radiology report generation from chest X-ray (CXR) images has the potential to improve clinical efficiency and reduce radiologists' workload. However, most datasets, including the publicly available MIMIC-CXR and CheXpert Plus, consist entirely of free-form reports, which are inherently variable and unstructured. This variability poses challenges for both generation and evaluation: existing models struggle to produce consistent, clinically meaningful reports, and standard evaluation metrics fail to capture the nuances of radiological interpretation. To address this, we introduce Structured Radiology Report Generation (SRRG), a new task that reformulates free-text radiology reports into a standardized format, ensuring clarity, consistency, and structured clinical reporting. We create a novel dataset by restructuring reports using large language models (LLMs) following strict structured reporting desiderata. Additionally, we introduce SRR-BERT, a fine-grained disease classification model trained on 55 labels, enabling more precise and clinically informed evaluation of structured reports. To assess report quality, we propose F1-SRR-BERT, a metric that leverages SRR-BERT's hierarchical disease taxonomy to bridge the gap between free-text variability and structured clinical reporting. We validate our dataset through a reader study conducted by five board-certified radiologists and extensive benchmarking experiments.",,"Jean-Benoit Delbrouck, Justin Xu, Johannes Moll, Alois Thomas, Zhihong Chen, Sophie Ostmeier, Asfandyar Azhar, Kelvin Zhenghao Li, Andrew Johnston, Christian Bluethgen, Eduardo Reis, Mohamed Muneer, Maya Varma, Curtis Langlotz",2025-06-02T07:21:17Z,Automated Structured Radiology Report Generation,Automatisierte strukturierte Radiologie-Berichtserstellung,自动结构放射报告生成,http://arxiv.org/abs/2505.24223v2
863,"Prompting-based conversational query reformulation has emerged as a powerful approach for conversational search, refining ambiguous user queries into standalone search queries. Best-of-N reformulation over the generated candidates via prompting shows impressive potential scaling capability. However, both the previous tuning methods (training time) and adaptation approaches (test time) can not fully unleash their benefits. In this paper, we propose AdaRewriter, a novel framework for query reformulation using an outcome-supervised reward model via test-time adaptation. By training a lightweight reward model with contrastive ranking loss, AdaRewriter selects the most promising reformulation during inference. Notably, it can operate effectively in black-box systems, including commercial LLM APIs. Experiments on five conversational search datasets show that AdaRewriter significantly outperforms the existing methods across most settings, demonstrating the potential of test-time adaptation for conversational query reformulation.",,"Yilong Lai, Jialong Wu, Zhenglin Wang, Deyu Zhou",2025-06-02T07:18:26Z,AdaRewriter: Unleashing the Power of Prompting-based Conversational   Query Reformulation via Test-Time Adaptation,AdaRewriter: Entfesseln der Macht der prompting-basierten Conversational Query Reformulation durch Test-Time Adaption,"Ada Recrecripter:通过测试时间的适应,释放基于快速对话询问重新调整的能力",http://arxiv.org/abs/2506.01381v1
864,"Recent efforts in LLM alignment have focused on constructing large-scale preference datasets via human or Artificial Intelligence (AI) annotators. However, such approaches rely on instance-wise supervision, incurring substantial annotation cost and limited interpretability. In this paper, we propose ZEBRA - a model behavior-wise zero-annotation framework that constructs preference data by leveraging model behavior knowledge derived from benchmark performances. ZEBRA binarizes response pairs by evaluating the quality and similarity of their origin models, entirely bypassing instance-level annotation. This allows scalable, controllable, and cost-effective alignment data generation. Empirical results show that ZEBRA achieves alignment performance comparable to instance-supervised methods, despite requiring no manual or model-based labeling.",,"Jeesu Jung, Chanjun Park, Sangkeun Jung",2025-06-02T07:16:11Z,ZEBRA: Leveraging Model-Behavioral Knowledge for Zero-Annotation   Preference Dataset Construction,ZEBRA: Nutzung von Modell-Verhaltenswissen für Null-Annotations-Präferenzdatensatz-Konstruktion,ZEBRA:利用示范行为知识促进零说明优先数据集建设,http://arxiv.org/abs/2502.18744v3
865,"The emergence of Artificial Intelligence (AI) Scientist represents a paradigm shift in scientific discovery, with large language models (LLMs) taking the lead as the primary executor in the entire scientific workflow from idea generation to experiment implementation. Recent AI Scientist studies demonstrate sufficient capabilities for independent scientific discovery, with the generated research reports gaining acceptance at the ICLR 2025 workshop and ACL 2025, arguing that a human-level AI Scientist, capable of uncovering phenomena previously unknown to humans, may be imminent. Despite this substantial progress, AI Scientist has yet to produce a groundbreaking achievement in the domain of computer science on par with automated scientific tools. Based on extensive quantitative evidence from existing benchmarks in complex engineering tasks and a systematic evaluation assess 28 research papers generated by five advanced AI Scientist systems, we argue that \textbf{the fundamental bottleneck for AI Scientists lies in their capability to execute the requisite verification procedures.} Current AI Scientist systems lack the execution capabilities needed to execute rigorous experiments and produce high-quality scientific papers. To better illustrate the root cause of this \textbf{implementation gap}, we provide an in-depth discussion on the fundamental limitations of AI Scientist. This position paper aims to call for the participants in the community to bridge the implementation gap.",,"Minjun Zhu, Qiujie Xie, Yixuan Weng, Jian Wu, Zhen Lin, Linyi Yang, Yue Zhang",2025-06-02T06:59:10Z,AI Scientists Fail Without Strong Implementation Capability,KI-Wissenschaftler scheitern ohne starke Umsetzungsfähigkeit,AI 缺乏强有力的执行能力的科学家失败,http://arxiv.org/abs/2506.01372v1
866,"Active Learning (AL) allows models to learn interactively from user feedback. This paper introduces a counterfactual data augmentation approach to AL, particularly addressing the selection of datapoints for user querying, a pivotal concern in enhancing data efficiency. Our approach is inspired by Variation Theory, a theory of human concept learning that emphasizes the essential features of a concept by focusing on what stays the same and what changes. Instead of just querying with existing datapoints, our approach synthesizes artificial datapoints that highlight potential key similarities and differences among labels using a neuro-symbolic pipeline combining large language models (LLMs) and rule-based models. Through an experiment in the example domain of text classification, we show that our approach achieves significantly higher performance when there are fewer annotated data. As the annotated training data gets larger the impact of the generated data starts to diminish showing its capability to address the cold start problem in AL. This research sheds light on integrating theories of human learning into the optimization of AL.",,"Simret Araya Gebreegziabher, Kuangshi Ai, Zheng Zhang, Elena L. Glassman, Toby Jia-Jun Li",2025-06-02T06:56:42Z,Leveraging Variation Theory in Counterfactual Data Augmentation for   Optimized Active Learning,Leveraging Variation Theory in Counterfactual Data Augmentation for Optimized Active Learning,在优化积极学习中利用反事实数据增量利用变异理论,http://arxiv.org/abs/2408.03819v2
867,"Large language models (LLMs) have become pervasive in our everyday life. Yet, a fundamental obstacle prevents their use in many critical applications: their propensity to generate fluent, human-quality content that is not grounded in reality. The detection of such hallucinations is thus of the highest importance. In this work, we propose a new method to flag hallucinated content, MMD-Flagger. It relies on Maximum Mean Discrepancy (MMD), a non-parametric distance between distributions. On a high-level perspective, MMD-Flagger tracks the MMD between the generated documents and documents generated with various temperature parameters. We show empirically that inspecting the shape of this trajectory is sufficient to detect most hallucinations. This novel method is benchmarked on two machine translation datasets, on which it outperforms natural competitors.",,"Kensuke Mitsuzawa, Damien Garreau",2025-06-02T06:50:58Z,MMD-Flagger: Leveraging Maximum Mean Discrepancy to Detect   Hallucinations,MMD-Flagger: Hebelwirkung der maximalen mittleren Diskrepanz zur Erkennung von Halluzinationen,MMD-Flagger:利用最大平均值差异来探测幻觉,http://arxiv.org/abs/2506.01367v1
868,"Voice Activity Detection (VAD) plays a key role in speech processing, often utilizing hand-crafted or neural features. This study examines the effectiveness of Mel-Frequency Cepstral Coefficients (MFCCs) and pre-trained model (PTM) features, including wav2vec 2.0, HuBERT, WavLM, UniSpeech, MMS, and Whisper. We propose FusionVAD, a unified framework that combines both feature types using three fusion strategies: concatenation, addition, and cross-attention (CA). Experimental results reveal that simple fusion techniques, particularly addition, outperform CA in both accuracy and efficiency. Fusion-based models consistently surpass single-feature models, highlighting the complementary nature of MFCCs and PTM features. Notably, our best-performing fusion model exceeds the state-of-the-art Pyannote across multiple datasets, achieving an absolute average improvement of 2.04%. These results confirm that simple feature fusion enhances VAD robustness while maintaining computational efficiency.",,"Kumud Tripathi, Chowdam Venkata Kumar, Pankaj Wasnik",2025-06-02T06:47:42Z,Attention Is Not Always the Answer: Optimizing Voice Activity Detection   with Simple Feature Fusion,Aufmerksamkeit ist nicht immer die Antwort: Optimierung der Sprachaktivitätserkennung mit einfacher Feature Fusion,"关注并非总是答案: 优化语音活动探测, 使用简单功能融合 。",http://arxiv.org/abs/2506.01365v1
869,"Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models are fully open-sourced.",,"Maohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory Wornell, Subhro Das, David Cox, Chuang Gan",2025-06-02T06:42:17Z,Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM   Reasoning via Autoregressive Search,Satori: Verstärktes Lernen mit Chain-of-Action-Thought verbessert LLM-Reasoning durch autoregressive Suche,"教程:通过自动递减搜索,加强学习,通过行动链-探索加强LLM",http://arxiv.org/abs/2502.02508v2
870,"Understanding pragmatics-the use of language in context-is crucial for developing NLP systems capable of interpreting nuanced language use. Despite recent advances in language technologies, including large language models, evaluating their ability to handle pragmatic phenomena such as implicatures and references remains challenging. To advance pragmatic abilities in models, it is essential to understand current evaluation trends and identify existing limitations. In this survey, we provide a comprehensive review of resources designed for evaluating pragmatic capabilities in NLP, categorizing datasets by the pragmatics phenomena they address. We analyze task designs, data collection methods, evaluation approaches, and their relevance to real-world applications. By examining these resources in the context of modern language models, we highlight emerging trends, challenges, and gaps in existing benchmarks. Our survey aims to clarify the landscape of pragmatic evaluation and guide the development of more comprehensive and targeted benchmarks, ultimately contributing to more nuanced and context-aware NLP models.",,"Bolei Ma, Yuting Li, Wei Zhou, Ziwei Gong, Yang Janet Liu, Katja Jasinskaja, Annemarie Friedrich, Julia Hirschberg, Frauke Kreuter, Barbara Plank",2025-06-02T06:40:17Z,"Pragmatics in the Era of Large Language Models: A Survey on Datasets,   Evaluation, Opportunities and Challenges","Pragmatics in the Era of Large Language Models: Eine Umfrage zu Datensätzen, Evaluation, Chancen und Herausforderungen",《大语言模式时代中的实用模型:关于数据集、评价、机遇和挑战的调查》,http://arxiv.org/abs/2502.12378v2
871,"In recent research, large language models (LLMs) have been increasingly used to investigate public opinions. This study investigates the algorithmic fidelity of LLMs, i.e., the ability to replicate the socio-cultural context and nuanced opinions of human participants. Using open-ended survey data from the German Longitudinal Election Studies (GLES), we prompt different LLMs to generate synthetic public opinions reflective of German subpopulations by incorporating demographic features into the persona prompts. Our results show that Llama performs better than other LLMs at representing subpopulations, particularly when there is lower opinion diversity within those groups. Our findings further reveal that the LLM performs better for supporters of left-leaning parties like The Greens and The Left compared to other parties, and matches the least with the right-party AfD. Additionally, the inclusion or exclusion of specific variables in the prompts can significantly impact the models' predictions. These findings underscore the importance of aligning LLMs to more effectively model diverse public opinions while minimizing political biases and enhancing robustness in representativeness.",,"Bolei Ma, Berk Yoztyurk, Anna-Carolina Haensch, Xinpeng Wang, Markus Herklotz, Frauke Kreuter, Barbara Plank, Matthias Assenmacher",2025-06-02T06:24:38Z,Algorithmic Fidelity of Large Language Models in Generating Synthetic   German Public Opinions: A Case Study,Algorithmische Fidelität großer Sprachmodelle bei der Generierung synthetischer deutscher öffentlicher Meinungen: Eine Fallstudie,产生合成德国公共舆论中大语言模型的说服力:案例研究,http://arxiv.org/abs/2412.13169v2
872,"Generating psychological counseling responses with language models relies heavily on high-quality datasets. Crowdsourced data collection methods require strict worker training, and data from real-world counseling environments may raise privacy and ethical concerns. While recent studies have explored using large language models (LLMs) to augment psychological counseling dialogue datasets, the resulting data often suffers from limited diversity and authenticity. To address these limitations, this study adopts a role-playing approach where trained counselors simulate counselor-client interactions, ensuring high-quality dialogues while mitigating privacy risks. Using this method, we construct KokoroChat, a Japanese psychological counseling dialogue dataset comprising 6,589 long-form dialogues, each accompanied by comprehensive client feedback. Experimental results demonstrate that fine-tuning open-source LLMs with KokoroChat improves both the quality of generated counseling responses and the automatic evaluation of counseling dialogues. The KokoroChat dataset is available at https://github.com/UEC-InabaLab/KokoroChat.",,"Zhiyang Qi, Takumasa Kaneko, Keiko Takamizo, Mariko Ukiyo, Michimasa Inaba",2025-06-02T06:20:53Z,KokoroChat: A Japanese Psychological Counseling Dialogue Dataset   Collected via Role-Playing by Trained Counselors,"KokoroChat: Ein japanischer Psychologischer Beratungsdialog Datensatz, der durch Rollenspiel von ausgebildeten Beratern gesammelt wurde","KokoroChat:日本心理咨询对话数据集,由经过培训的辅导员通过角色扮演收集",http://arxiv.org/abs/2506.01357v1
873,"Reinforcement learning with verifiable rewards (RLVR) is a promising approach for training language models (LMs) on reasoning tasks that elicit emergent long chains of thought (CoTs). Unlike supervised learning, it updates the model using both correct and incorrect samples via policy gradients. To better understand its mechanism, we decompose the learning signal into reinforcing correct responses and penalizing incorrect ones, referred to as Positive and Negative Sample Reinforcement (PSR and NSR), respectively. We train Qwen2.5-Math-7B and Qwen3-4B on a mathematical reasoning dataset and uncover a surprising result: training with only negative samples -- without reinforcing correct responses -- can be highly effective: it consistently improves performance over the base model across the entire Pass@$k$ spectrum ($k$ up to $256$), often matching or surpassing PPO and GRPO. In contrast, reinforcing only correct responses improves Pass@$1$ but degrades performance at higher $k$, due to reduced diversity. These inference-scaling trends highlight that solely penalizing incorrect responses may contribute more to performance than previously recognized. Through gradient analysis, we show that NSR works by suppressing incorrect generations and redistributing probability mass toward other plausible candidates, guided by the model's prior beliefs. It refines the model's existing knowledge rather than introducing entirely new behaviors. Building on this insight, we propose a simple variant of the RL objective that upweights NSR, and show that it consistently improves overall Pass@$k$ performance on MATH, AIME 2025, and AMC23. Our code is available at https://github.com/TianHongZXY/RLVR-Decomposed.",,"Xinyu Zhu, Mengzhou Xia, Zhepei Wei, Wei-Lin Chen, Danqi Chen, Yu Meng",2025-06-02T06:10:54Z,The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning,Die überraschende Wirksamkeit negativer Verstärkung bei der LLM-Vernunft,LLM 理由方面的负强化的惊人效果,http://arxiv.org/abs/2506.01347v1
874,"In this work, we investigate the Meta PL unsupervised domain adaptation framework for Automatic Speech Recognition (ASR). We introduce a Multi-Stage Domain Adaptation pipeline (MSDA), a sample-efficient, two-stage adaptation approach that integrates self-supervised learning with semi-supervised techniques. MSDA is designed to enhance the robustness and generalization of ASR models, making them more adaptable to diverse conditions. It is particularly effective for low-resource languages like Greek and in weakly supervised scenarios where labeled data is scarce or noisy. Through extensive experiments, we demonstrate that Meta PL can be applied effectively to ASR tasks, achieving state-of-the-art results, significantly outperforming state-of-the-art methods, and providing more robust solutions for unsupervised domain adaptation in ASR. Our ablations highlight the necessity of utilizing a cascading approach when combining self-supervision with self-training.",,"Dimitrios Damianos, Georgios Paraskevopoulos, Alexandros Potamianos",2025-06-02T06:10:21Z,MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised   Domain Adaptation in ASR,MSDA: Kombination von Pseudo-Etikettierung und Selbstüberwachung für unüberwachte Domain-Anpassung in ASR,MSDA:将ASR中无人监督的域适应的优多标签和自控合并,http://arxiv.org/abs/2505.24656v2
875,"Flowcharts are a critical tool for visualizing decision-making processes. However, their non-linear structure and complex visual-textual relationships make it challenging to interpret them using LLMs, as vision-language models frequently hallucinate nonexistent connections and decision paths when analyzing these diagrams. This leads to compromised reliability for automated flowchart processing in critical domains such as logistics, health, and engineering. We introduce the task of Fine-grained Flowchart Attribution, which traces specific components grounding a flowchart referring LLM response. Flowchart Attribution ensures the verifiability of LLM predictions and improves explainability by linking generated responses to the flowchart's structure. We propose FlowPathAgent, a neurosymbolic agent that performs fine-grained post hoc attribution through graph-based reasoning. It first segments the flowchart, then converts it into a structured symbolic graph, and then employs an agentic approach to dynamically interact with the graph, to generate attribution paths. Additionally, we present FlowExplainBench, a novel benchmark for evaluating flowchart attributions across diverse styles, domains, and question types. Experimental results show that FlowPathAgent mitigates visual hallucinations in LLM answers over flowchart QA, outperforming strong baselines by 10-14% on our proposed FlowExplainBench dataset.",,"Manan Suri, Puneet Mathur, Nedim Lipka, Franck Dernoncourt, Ryan A. Rossi, Vivek Gupta, Dinesh Manocha",2025-06-02T06:02:41Z,Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic   Agents,Folgen Sie dem Fluss: Feinkörnige Flussdiagrammzuweisung mit neurosymbolischen Wirkstoffen,"顺流而下:精细的花卉图,配有神经阳性剂",http://arxiv.org/abs/2506.01344v1
876,"Systematic reviews are comprehensive literature reviews that address highly focused research questions and represent the highest form of evidence in medicine. A critical step in this process is the development of complex Boolean queries to retrieve relevant literature. Given the difficulty of manually constructing these queries, recent efforts have explored Large Language Models (LLMs) to assist in their formulation. One of the first studies,Wang et al., investigated ChatGPT for this task, followed by Staudinger et al., which evaluated multiple LLMs in a reproducibility study. However, the latter overlooked several key aspects of the original work, including (i) validation of generated queries, (ii) output formatting constraints, and (iii) selection of examples for chain-of-thought (Guided) prompting. As a result, its findings diverged significantly from the original study. In this work, we systematically reproduce both studies while addressing these overlooked factors. Our results show that query effectiveness varies significantly across models and prompt designs, with guided query formulation benefiting from well-chosen seed studies. Overall, prompt design and model selection are key drivers of successful query formulation. Our findings provide a clearer understanding of LLMs' potential in Boolean query generation and highlight the importance of model- and prompt-specific optimisations. The complex nature of systematic reviews adds to challenges in both developing and reproducing methods but also highlights the importance of reproducibility studies in this domain.",,"Shuai Wang, Harrisen Scells, Bevan Koopman, Guido Zuccon",2025-06-02T05:57:53Z,Reassessing Large Language Model Boolean Query Generation for Systematic   Reviews,Neubewertung des Large Language Model Boolesche Abfrage-Generierung für systematische Bewertungen,重新评估用于系统审查的大型语言模拟布尔生成查询模型,http://arxiv.org/abs/2505.07155v2
877,"Large language models (LLMs) have demonstrated remarkable capabilities in various domains, including radiology report generation. Previous approaches have attempted to utilize multimodal LLMs for this task, enhancing their performance through the integration of domain-specific knowledge retrieval. However, these approaches often overlook the knowledge already embedded within the LLMs, leading to redundant information integration. To address this limitation, we propose Radar, a framework for enhancing radiology report generation with supplementary knowledge injection. Radar improves report generation by systematically leveraging both the internal knowledge of an LLM and externally retrieved information. Specifically, it first extracts the model's acquired knowledge that aligns with expert image-based classification outputs. It then retrieves relevant supplementary knowledge to further enrich this information. Finally, by aggregating both sources, Radar generates more accurate and informative radiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU X-ray demonstrate that our model outperforms state-of-the-art LLMs in both language quality and clinical accuracy.",,"Wenjun Hou, Yi Cheng, Kaishuai Xu, Heng Li, Yan Hu, Wenjie Li, Jiang Liu",2025-06-02T05:56:06Z,RADAR: Enhancing Radiology Report Generation with Supplementary   Knowledge Injection,RADAR: Radiologie-Berichtserstellung durch zusätzliche Wissensinjektion verbessern,雷达:用补充知识注射加强放射学报告的编制,http://arxiv.org/abs/2505.14318v2
878,"Despite impressive advances in large language models (LLMs), existing benchmarks often focus on single-turn or single-step tasks, failing to capture the kind of iterative reasoning required in real-world settings. To address this limitation, we introduce TurnBench, a novel benchmark that evaluates multi-turn, multi-step reasoning through an interactive code-breaking task inspired by a ""Turing Machine Board Game."" In each episode, a model must uncover hidden logical or arithmetic rules by making sequential guesses, receiving structured feedback, and integrating clues across multiple rounds. This dynamic setup requires models to reason over time, adapt based on past information, and maintain consistency across steps-capabilities underexplored in current benchmarks. TurnBench includes two modes: Classic, which tests standard reasoning, and Nightmare, which introduces increased complexity and requires robust inferential chains. To support fine-grained analysis, we provide ground-truth annotations for intermediate reasoning steps. Our evaluation of state-of-the-art LLMs reveals significant gaps: the best model achieves 81.5% accuracy in Classic mode, but performance drops to 17.8% in Nightmare mode. In contrast, human participants achieve 100% in both, underscoring the challenge TurnBench poses to current models. By incorporating feedback loops and hiding task rules, TurnBench reduces contamination risks and provides a rigorous testbed for diagnosing and advancing multi-step, multi-turn reasoning in LLMs.",,"Yiran Zhang, Mo Wang, Xiaoyang Li, Kaixuan Ren, Chencheng Zhu, Usman Naseem",2025-06-02T05:47:50Z,"TurnBench-MS: A Benchmark for Evaluating Multi-Turn, Multi-Step   Reasoning in Large Language Models","TurnBench-MS: Ein Benchmark für die Bewertung von Multi-Turn, Multi-Step-Reasoning in großen Sprachmodellen",环转基准-MS:大语言模型多发、多标准原因评估基准,http://arxiv.org/abs/2506.01341v1
879,"Vision-language generative reward models (VL-GenRMs) play a crucial role in aligning and evaluating multimodal AI systems, yet their own evaluation remains under-explored. Current assessment methods primarily rely on AI-annotated preference labels from traditional VL tasks, which can introduce biases and often fail to effectively challenge state-of-the-art models. To address these limitations, we introduce VL-RewardBench, a comprehensive benchmark spanning general multimodal queries, visual hallucination detection, and complex reasoning tasks. Through our AI-assisted annotation pipeline that combines sample selection with human verification, we curate 1,250 high-quality examples specifically designed to probe VL-GenRMs limitations. Comprehensive evaluation across 16 leading large vision-language models demonstrates VL-RewardBench's effectiveness as a challenging testbed, where even GPT-4o achieves only 65.4% accuracy, and state-of-the-art open-source models such as Qwen2-VL-72B, struggle to surpass random-guessing. Importantly, performance on VL-RewardBench strongly correlates (Pearson's r $>$ 0.9) with MMMU-Pro accuracy using Best-of-N sampling with VL-GenRMs. Analysis experiments uncover three critical insights for improving VL-GenRMs: (i) models predominantly fail at basic visual perception tasks rather than reasoning tasks; (ii) inference-time scaling benefits vary dramatically by model capacity; and (iii) training VL-GenRMs to learn to judge substantially boosts judgment capability (+14.7% accuracy for a 7B VL-GenRM). We believe VL-RewardBench along with the experimental insights will become a valuable resource for advancing VL-GenRMs.",,"Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, Lingpeng Kong, Qi Liu",2025-06-02T05:46:18Z,VL-RewardBench: A Challenging Benchmark for Vision-Language Generative   Reward Models,VL-RewardBench: Ein herausfordernder Benchmark für Vision-Language Generative Reward Models,VL-RondBrench:愿景-语言生成奖励模式的质疑基准,http://arxiv.org/abs/2411.17451v2
880,"The emergence of ChatGPT marked a transformative milestone for Artificial Intelligence (AI), showcasing the remarkable potential of Large Language Models (LLMs) to generate human-like text. This wave of innovation has revolutionized how we interact with technology, seamlessly integrating LLMs into everyday tasks such as vacation planning, email drafting, and content creation. While English-speaking users have significantly benefited from these advancements, the Arabic world faces distinct challenges in developing Arabic-specific LLMs. Arabic, one of the languages spoken most widely around the world, serves more than 422 million native speakers in 27 countries and is deeply rooted in a rich linguistic and cultural heritage. Developing Arabic LLMs (ALLMs) presents an unparalleled opportunity to bridge technological gaps and empower communities. The journey of ALLMs has been both fascinating and complex, evolving from rudimentary text processing systems to sophisticated AI-driven models. This article explores the trajectory of ALLMs, from their inception to the present day, highlighting the efforts to evaluate these models through benchmarks and public leaderboards. We also discuss the challenges and opportunities that ALLMs present for the Arab world.",,"Shahad Al-Khalifa, Nadir Durrani, Hend Al-Khalifa, Firoj Alam",2025-06-02T05:45:19Z,The Landscape of Arabic Large Language Models (ALLMs): A New Era for   Arabic Language Technology,Die Landschaft der arabischen großen Sprachmodelle (ALLMs): Eine neue Ära für die arabische Sprachtechnologie,阿拉伯语大语言模型的景观:阿拉伯语技术的新时代,http://arxiv.org/abs/2506.01340v1
881,"Concept Bottleneck Models (CBMs) decompose image classification into a process governed by interpretable, human-readable concepts. Recent advances in CBMs have used Large Language Models (LLMs) to generate candidate concepts. However, a critical question remains: What is the optimal number of concepts to use? Current concept banks suffer from redundancy or insufficient coverage. To address this issue, we introduce a dynamic, agent-based approach that adjusts the concept bank in response to environmental feedback, optimizing the number of concepts for sufficiency yet concise coverage. Moreover, we propose Conditional Concept Bottleneck Models (CoCoBMs) to overcome the limitations in traditional CBMs' concept scoring mechanisms. It enhances the accuracy of assessing each concept's contribution to classification tasks and feature an editable matrix that allows LLMs to correct concept scores that conflict with their internal knowledge. Our evaluations across 6 datasets show that our method not only improves classification accuracy by 6% but also enhances interpretability assessments by 30%.",,"Yiwen Jiang, Deval Mehta, Wei Feng, Zongyuan Ge",2025-06-02T05:25:52Z,Enhancing Interpretable Image Classification Through LLM Agents and   Conditional Concept Bottleneck Models,Verbesserung der interpretierbaren Bildklassifizierung durch LLM-Agenten und bedingte Konzept-Flaschenhalsmodelle,通过LLLM代理和有条件概念瓶颈模型加强可解释图像分类,http://arxiv.org/abs/2506.01334v1
882,"Recent advances in Large Language Models (LLMs) have enabled multi-agent systems that simulate real-world interactions with near-human reasoning. While previous studies have extensively examined biases related to protected attributes such as race, the emergence and propagation of biases on socially contentious issues in multi-agent LLM interactions remain underexplored. This study explores how LLM agents shape public opinion through debates on five contentious topics. By simulating over 2,500 debates, we analyze how initially neutral agents, assigned a centrist disposition, adopt specific stances over time. Statistical analyses reveal significant group conformity mirroring human behavior; LLM agents tend to align with numerically dominant groups or more intelligent agents, exerting a greater influence. These findings underscore the crucial role of agent intelligence in shaping discourse and highlight the risks of bias amplification in online interactions. Our results emphasize the need for policy measures that promote diversity and transparency in LLM-generated discussions to mitigate the risks of bias propagation within anonymous online environments.",,"Min Choi, Keonwoo Kim, Sungwon Chae, Sangyeob Baek",2025-06-02T05:22:29Z,An Empirical Study of Group Conformity in Multi-Agent Systems,Eine empirische Studie der Gruppenkonformität in Multi-Agent-Systemen,关于多机构机构系统小组合规性的经验研究,http://arxiv.org/abs/2506.01332v1
883,"Psychological support hotlines are critical for crisis intervention but face significant challenges due to rising demand. Large language models (LLMs) could support crisis assessments, yet their capabilities in emotionally sensitive contexts remain unclear. We introduce PsyCrisisBench, a benchmark of 540 annotated transcripts from the Hangzhou Psychological Assistance Hotline, assessing four tasks: mood status recognition, suicidal ideation detection, suicide plan identification, and risk assessment. We evaluated 64 LLMs across 15 families (e.g., GPT, Claude, Gemini, Llama, Qwen, DeepSeek) using zero-shot, few-shot, and fine-tuning paradigms. Performance was measured by F1-score, with statistical comparisons via Welch's t-tests. LLMs performed strongly on suicidal ideation detection (F1=0.880), suicide plan identification (F1=0.779), and risk assessment (F1=0.907), improved with few-shot and fine-tuning. Mood status recognition was more challenging (max F1=0.709), likely due to lost vocal cues and ambiguity. A fine-tuned 1.5B-parameter model (Qwen2.5-1.5B) surpassed larger models on mood and suicidal ideation. Open-source models like QwQ-32B performed comparably to closed-source on most tasks (p>0.3), though closed models retained an edge in mood detection (p=0.007). Performance scaled with size up to a point; quantization (AWQ) reduced GPU memory by 70% with minimal F1 degradation. LLMs show substantial promise in structured psychological crisis assessments, especially with fine-tuning. Mood recognition remains limited due to contextual complexity. The narrowing gap between open- and closed-source models, combined with efficient quantization, suggests feasible integration. PsyCrisisBench offers a robust evaluation framework to guide model development and ethical deployment in mental health.",,"Guifeng Deng, Shuyin Rao, Tianyu Lin, Anlu Dai, Pan Wang, Junyi Xie, Haidong Song, Ke Zhao, Dongwu Xu, Zhengdong Cheng, Tao Li, Haiteng Jiang",2025-06-02T05:18:24Z,Evaluating Large Language Models in Crisis Detection: A Real-World   Benchmark from Psychological Support Hotlines,Bewertung großer Sprachmodelle in der Krisenerkennung: Ein echter Benchmark von Psychologischen Support-Hotlines,评价危机探测中的大语言模式:心理支持热线上的现实世界基准,http://arxiv.org/abs/2506.01329v1
884,"This paper introduces PhoAudiobook, a newly curated dataset comprising 941 hours of high-quality audio for Vietnamese text-to-speech. Using PhoAudiobook, we conduct experiments on three leading zero-shot TTS models: VALL-E, VoiceCraft, and XTTS-V2. Our findings demonstrate that PhoAudiobook consistently enhances model performance across various metrics. Moreover, VALL-E and VoiceCraft exhibit superior performance in synthesizing short sentences, highlighting their robustness in handling diverse linguistic contexts. We publicly release PhoAudiobook to facilitate further research and development in Vietnamese text-to-speech.",,"Thi Vu, Linh The Nguyen, Dat Quoc Nguyen",2025-06-02T05:07:06Z,Zero-Shot Text-to-Speech for Vietnamese,Zero-Shot Text-to-Speech für Vietnamesen,越南语零热文本到语音,http://arxiv.org/abs/2506.01322v1
885,"While feed-forward neurons in pre-trained language models (PLMs) can encode knowledge, past research targeted a small subset of neurons that heavily influence outputs. This leaves the broader role of neuron activations unclear, limiting progress in areas like knowledge editing. We uncover a global linear relationship between neuron activations and outputs using neuron interventions on a knowledge probing dataset. The gradient of this linear relationship, which we call the neuron empirical gradient (NEG), captures how changes in activations affect predictions. To compute NEG efficiently, we propose NeurGrad, enabling large-scale analysis of neuron behavior in PLMs. We also show that NEG effectively captures language skills across diverse prompts through skill neuron probing. Experiments on MCEval8k, a multi-genre multiple-choice knowledge benchmark, support NEG's ability to represent model knowledge. Further analysis highlights the key properties of NEG-based skill representation: efficiency, robustness, flexibility, and interdependency. The code and data are released.",,"Xin Zhao, Zehui Jiang, Naoki Yoshinaga",2025-06-02T05:01:12Z,Neuron Empirical Gradient: Discovering and Quantifying Neurons Global   Linear Controllability,Neuron Empirischer Gradient: Neuronen entdecken und quantifizieren Globale Lineare Kontrollierbarkeit,中子经验进化:发现和量化中子全球线性控制,http://arxiv.org/abs/2412.18053v3
886,"Mitigating positional bias of language models (LMs) for listwise inputs is a well-known and important problem (e.g., lost-in-the-middle). While zero-shot order-invariant LMs have been proposed to solve this issue, their success on practical listwise problems has been limited. In this work, as a first contribution, we identify and overcome two limitations to make zero-shot invariant LMs more practical: (1) training and inference distribution mismatch arising from modifying positional ID assignments to enforce invariance, and (2) failure to adapt to mixture of order-invariant and sensitive inputs in practical listwise problems. Then, to overcome these issues we propose (1) RoToR, a zero-shot invariant LM for genuinely order-invariant inputs with minimal modifications of positional IDs, and (2) Selective Routing, an adaptive framework that handles both order-invariant and order-sensitive inputs in listwise tasks. On the Lost in the middle (LitM), Knowledge Graph QA (KGQA), and MMLU benchmarks, we show that RoToR with Selective Routing can effectively handle practical listwise input tasks in a zero-shot manner (https://github.com/soyoung97/RoToR)",,"Soyoung Yoon, Dongha Ahn, Youngwon Lee, Minkyu Jung, HyungJoo Jang, Seung-won Hwang",2025-06-02T04:54:00Z,RoToR: Towards More Reliable Responses for Order-Invariant Inputs,RoToR: Auf dem Weg zu zuverlässigeren Reaktionen für auftragsunabhängige Eingaben,RoToR:争取更可靠地应对有秩序和有秩序者投入,http://arxiv.org/abs/2502.08662v3
887,"Language models (LMs) require robust episodic grounding-the capacity to learn from and apply past experiences-to excel at physical planning tasks. Current episodic grounding approaches struggle with scalability and integration, limiting their effectiveness, especially for medium-sized LMs (7B parameters). While larger LMs (70-405B parameters) possess superior hierarchical representations and extensive pre-trained knowledge, they encounter a fundamental scale paradox: despite their advanced abstraction capabilities, they lack efficient mechanisms to leverage experience streams. We propose a scalable weak-to-strong episodic learning framework that effectively transfers episodic behaviors from smaller to larger LMs. This framework integrates Monte Carlo tree search for structured experience collection with a novel distillation method, preserving the inherent LM capabilities while embedding episodic memory. Experiments demonstrate our method surpasses state-of-the-art proprietary LMs by 3.45% across diverse planning and question-answering tasks. Layer-wise probing further indicates significant improvements in task alignment, especially within deeper LM layers, highlighting stable generalization even for previously unseen scenarios with increased planning complexity-conditions where baseline methods degrade markedly.",,"Chunhui Zhang, Sirui, Wang, Zhongyu Ouyang, Xiangchi Yuan, Soroush Vosoughi",2025-06-02T04:52:19Z,Growing Through Experience: Scaling Episodic Grounding in Language   Models,Durch Erfahrung wachsen: episodische Erdung in Sprachmodellen skalieren,通过不断积累的经验:在语言模式中逐步扩大理论依据,http://arxiv.org/abs/2506.01312v1
888,"A recent rise in online content expressing concerns with public health initiatives has contributed to already stalled uptake of preemptive measures globally. Future public health efforts must attempt to understand such content, what concerns it may raise among readers, and how to effectively respond to it. To this end, we present ConcernScope, a platform that uses a teacher-student framework for knowledge transfer between large language models and light-weight classifiers to quickly and effectively identify the health concerns raised in a text corpus. The platform allows uploading massive files directly, automatically scraping specific URLs, and direct text editing. ConcernScope is built on top of a taxonomy of public health concerns. Intended for public health officials, we demonstrate several applications of this platform: guided data exploration to find useful examples of common concerns found in online community datasets, identification of trends in concerns through an example time series analysis of 186,000 samples, and finding trends in topic frequency before and after significant events.",,"Christopher Li, Rickard Stureborg, Bhuwan Dhingra, Jun Yang",2025-06-02T04:36:13Z,A Platform for Investigating Public Health Content with Efficient   Concern Classification,Eine Plattform für die Untersuchung von Gesundheitsinhalten mit effizienter Klassifizierung von Belangen,具有高效关注分类的公共卫生内容调查平台,http://arxiv.org/abs/2506.01308v1
889,"Medical benchmarks are indispensable for evaluating the capabilities of language models in healthcare for non-English-speaking communities,therefore help ensuring the quality of real-life applications. However, not every community has sufficient resources and standardized methods to effectively build and design such benchmark, and available non-English medical data is normally fragmented and difficult to verify. We developed an approach to tackle this problem and applied it to create the first Vietnamese medical question benchmark, featuring 14,000 multiple-choice questions across 34 medical specialties. Our benchmark was constructed using various verifiable sources, including carefully curated medical exams and clinical records, and eventually annotated by medical experts. The benchmark includes four difficulty levels, ranging from foundational biological knowledge commonly found in textbooks to typical clinical case studies that require advanced reasoning. This design enables assessment of both the breadth and depth of language models' medical understanding in the target language thanks to its extensive coverage and in-depth subject-specific expertise. We release the benchmark in three parts: a sample public set (4k questions), a full public set (10k questions), and a private set (2k questions) used for leaderboard evaluation. Each set contains all medical subfields and difficulty levels. Our approach is scalable to other languages, and we open-source our data construction pipeline to support the development of future multilingual benchmarks in the medical domain.",,"Thong Nguyen, Duc Nguyen, Minh Dang, Thai Dao, Long Nguyen, Quan H. Nguyen, Dat Nguyen, Kien Tran, Minh Tran",2025-06-02T04:32:15Z,VM14K: First Vietnamese Medical Benchmark,VM14K: Erster vietnamesischer medizinischer Benchmark,VM14K:第一个越南医疗基准,http://arxiv.org/abs/2506.01305v1
890,"Theory-of-Mind (ToM) enables humans to infer mental states-such as beliefs, desires, and intentions-forming the foundation of social cognition. However, existing computational ToM methods rely on structured workflows with ToM-specific priors or deep model fine-tuning, which struggle with scalability in multimodal environments and fail to generalize as task complexity increases. To address these limitations, we propose a scalable Bayesian ToM planner that decomposes ToM reasoning into stepwise Bayesian updates. Our framework introduces weak-to-strong control, allowing smaller language models (LMs) to specialize in ToM-specific likelihood estimation and transfer their reasoning behaviors to larger LMs (7B to 405B) for integration with social and world knowledge. This synergistic approach aligns large-model inference of human mental states with Bayesian principles. Extensive experiments show that our method achieves a 4.6% accuracy improvement over state-of-the-art techniques on multimodal ToM benchmarks, including challenging unseen scenarios, thereby establishing a new standard for modeling human mental states in complex environments.",,"Chunhui Zhang, Zhongyu Ouyang, Kwonjoon Lee, Nakul Agarwal, Sean Dae Houlihan, Soroush Vosoughi, Shao-Yuan Lo",2025-06-02T04:23:45Z,Overcoming Multi-step Complexity in Multimodal Theory-of-Mind Reasoning:   A Scalable Bayesian Planner,Mehrstufige Komplexität in der multimodalen Theorie der Vernunft überwinden: Ein skalierbarer Bayesian Planner,克服多式联运理论理由方面的多步复杂问题:一个可伸缩的贝耶斯规划员,http://arxiv.org/abs/2506.01301v1
891,"While large language models (LLMs) achieve near-perfect scores on medical licensing exams, these evaluations inadequately reflect the complexity and diversity of real-world clinical practice. We introduce MedHELM, an extensible evaluation framework for assessing LLM performance for medical tasks with three key contributions. First, a clinician-validated taxonomy spanning 5 categories, 22 subcategories, and 121 tasks developed with 29 clinicians. Second, a comprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly formulated) providing complete coverage of all categories and subcategories in the taxonomy. Third, a systematic comparison of LLMs with improved evaluation methods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9 frontier LLMs, using the 35 benchmarks, revealed significant performance variation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64% win-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved comparable results at 40% lower estimated computational cost. On a normalized accuracy scale (0-1), most models performed strongly in Clinical Note Generation (0.73-0.85) and Patient Communication & Education (0.78-0.83), moderately in Medical Research Assistance (0.65-0.75), and generally lower in Clinical Decision Support (0.56-0.72) and Administration & Workflow (0.53-0.63). Our LLM-jury evaluation method achieved good agreement with clinician ratings (ICC = 0.47), surpassing both average clinician-clinician agreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and BERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top models at lower estimated cost. These findings highlight the importance of real-world, task-specific evaluation for medical use of LLMs and provides an open source framework to enable this.",,"Suhana Bedi, Hejie Cui, Miguel Fuentes, Alyssa Unell, Michael Wornow, Juan M. Banda, Nikesh Kotecha, Timothy Keyes, Yifan Mai, Mert Oez, Hao Qiu, Shrey Jain, Leonardo Schettini, Mehr Kashyap, Jason Alan Fries, Akshay Swaminathan, Philip Chung, Fateme Nateghi, Asad Aali, Ashwin Nayak, Shivam Vedak, Sneha S. Jain, Birju Patel, Oluseyi Fayanju, Shreya Shah, Ethan Goh, Dong-han Yao, Brian Soetikno, Eduardo Reis, Sergios Gatidis, Vasu Divi, Robson Capasso, Rachna Saralkar, Chia-Chun Chiang, Jenelle Jindal, Tho Pham, Faraz Ghoddusi, Steven Lin, Albert S. Chiou, Christy Hong, Mohana Roy, Michael F. Gensheimer, Hinesh Patel, Kevin Schulman, Dev Dash, Danton Char, Lance Downing, Francois Grolleau, Kameron Black, Bethel Mieso, Aydin Zahedivash, Wen-wai Yim, Harshita Sharma, Tony Lee, Hannah Kirsch, Jennifer Lee, Nerissa Ambers, Carlene Lugtu, Aditya Sharma, Bilal Mawji, Alex Alekseyev, Vicky Zhou, Vikas Kakkar, Jarrod Helzer, Anurang Revri, Yair Bannett, Roxana Daneshjou, Jonathan Chen, Emily Alsentzer, Keith Morse, Nirmal Ravi, Nima Aghaeepour, Vanessa Kennedy, Akshay Chaudhari, Thomas Wang, Sanmi Koyejo, Matthew P. Lungren, Eric Horvitz, Percy Liang, Mike Pfeffer, Nigam H. Shah",2025-06-02T04:19:10Z,MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks,MedHELM: Ganzheitliche Bewertung großer Sprachmodelle für medizinische Aufgaben,MedHELM: 对医疗任务大语言模式的综合评价,http://arxiv.org/abs/2505.23802v2
892,"Multi-modal large language models (MLLMs) incorporate heterogeneous modalities into LLMs, enabling a comprehensive understanding of diverse scenarios and objects. Despite the proliferation of evaluation benchmarks and leaderboards for MLLMs, they predominantly overlook the critical capacity of MLLMs to comprehend world knowledge with structured abstractions that appear in visual form. To address this gap, we propose a novel evaluation paradigm and devise M3STR, an innovative benchmark grounded in the Multi-Modal Map for STRuctured understanding. This benchmark leverages multi-modal knowledge graphs to synthesize images encapsulating subgraph architectures enriched with multi-modal entities. M3STR necessitates that MLLMs not only recognize the multi-modal entities within the visual inputs but also decipher intricate relational topologies among them. We delineate the benchmark's statistical profiles and automated construction pipeline, accompanied by an extensive empirical analysis of 26 state-of-the-art MLLMs. Our findings reveal persistent deficiencies in processing abstractive visual information with structured knowledge, thereby charting a pivotal trajectory for advancing MLLMs' holistic reasoning capacities. Our code and data are released at https://github.com/zjukg/M3STR",,"Yichi Zhang, Zhuo Chen, Lingbing Guo, Yajing Xu, Min Zhang, Wen Zhang, Huajun Chen",2025-06-02T04:00:35Z,Abstractive Visual Understanding of Multi-modal Structured Knowledge: A   New Perspective for MLLM Evaluation,Abstraktes visuelles Verständnis multimodaler strukturierter Kenntnisse: Eine neue Perspektive für die MLLM-Evaluierung,对多模式结构知识的抽象直观理解:MLLLM评价的新视角,http://arxiv.org/abs/2506.01293v1
893,"Failure attribution in LLM multi-agent systems-identifying the agent and step responsible for task failures-provides crucial clues for systems debugging but remains underexplored and labor-intensive. In this paper, we propose and formulate a new research area: automated failure attribution for LLM multi-agent systems. To support this initiative, we introduce the Who&When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps. Using the Who&When, we develop and evaluate three automated failure attribution methods, summarizing their corresponding pros and cons. The best method achieves 53.5% accuracy in identifying failure-responsible agents but only 14.2% in pinpointing failure steps, with some methods performing below random. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to achieve practical usability. These results highlight the task's complexity and the need for further research in this area. Code and dataset are available at https://github.com/mingyin1/Agents_Failure_Attribution",,"Shaokun Zhang, Ming Yin, Jieyu Zhang, Jiale Liu, Zhiguang Han, Jingyang Zhang, Beibin Li, Chi Wang, Huazheng Wang, Yiran Chen, Qingyun Wu",2025-06-02T03:25:55Z,Which Agent Causes Task Failures and When? On Automated Failure   Attribution of LLM Multi-Agent Systems,Welcher Agent verursacht Aufgabenausfälle und wann? Über automatisierte Fehlerzuweisung von LLM-Multiagentensystemen,哪些代理原因任务失败和何时发生?关于LLM多机构系统自动失败归属,http://arxiv.org/abs/2505.00212v3
894,"Large Language Models have achieved remarkable success in natural language processing tasks, with Reinforcement Learning playing a key role in adapting them to specific applications. However, obtaining ground truth answers for training LLMs in mathematical problem-solving is often challenging, costly, and sometimes unfeasible. This research delves into the utilization of format and length as surrogate signals to train LLMs for mathematical problem-solving, bypassing the need for traditional ground truth answers. Our study shows that a reward function centered on format correctness alone can yield performance improvements comparable to the standard GRPO algorithm in early phases. Recognizing the limitations of format-only rewards in the later phases, we incorporate length-based rewards. The resulting GRPO approach, leveraging format-length surrogate signals, not only matches but surpasses the performance of the standard GRPO algorithm relying on ground truth answers in certain scenarios, achieving 40.0% accuracy on AIME2024 with a 7B base model. Through systematic exploration and experimentation, this research not only offers a practical solution for training LLMs to solve mathematical problems and reducing the dependence on extensive ground truth data collection, but also reveals the essence of why our label-free approach succeeds: the powerful base model is like an excellent student who has already mastered mathematical and logical reasoning skills, but performs poorly on the test paper, it simply needs to develop good answering habits to achieve outstanding results in exams, to unlock the capabilities it already possesses.",,"Rihui Xin, Han Liu, Zecheng Wang, Yupeng Zhang, Dianbo Sui, Xiaolin Hu, Bingning Wang",2025-06-02T03:24:21Z,Surrogate Signals from Format and Length: Reinforcement Learning for   Solving Mathematical Problems without Ground Truth Answers,Surrogate Signale aus Format und Länge: Verstärkungslernen zur Lösung mathematischer Probleme ohne Grundwahrheitsantworten,格式和长度的代用信号:为解决没有事实答案的数学问题进行强化学习,http://arxiv.org/abs/2505.19439v3
895,"Universal information extraction (UIE) primarily employs an extractive generation approach with large language models (LLMs), typically outputting structured information based on predefined schemas such as JSON or tables. UIE suffers from a lack of adaptability when selecting between predefined schemas and on-the-fly schema generation within the in-context learning paradigm, especially when there are numerous schemas to choose from. In this paper, we propose a unified adaptive text-to-structure generation framework, called Schema as Parameterized Tools (SPT), which reimagines the tool-calling capability of LLMs by treating predefined schemas as parameterized tools for tool selection and parameter filling. Specifically, our SPT method can be applied to unify closed, open, and on-demand IE tasks by adopting Schema Retrieval by fetching the relevant schemas from a predefined pool, Schema Filling by extracting information and filling slots as with tool parameters, or Schema Generation by synthesizing new schemas with uncovered cases. Experiments show that the SPT method can handle four distinct IE tasks adaptively, delivering robust schema retrieval and selection performance. SPT also achieves comparable extraction performance to LoRA baselines and current leading UIE systems with significantly fewer trainable parameters.",,"Sheng Liang, Yongyue Zhang, Yaxiong Wu, Ruiming Tang, Yong Liu",2025-06-02T03:12:44Z,Schema as Parameterized Tools for Universal Information Extraction,Schema als parameterisierte Werkzeuge für die universelle Informationsgewinnung,"作为通用信息采掘的参数工具的 "" Schemma "" 系统",http://arxiv.org/abs/2506.01276v1
896,"We introduce MentalChat16K, an English benchmark dataset combining a synthetic mental health counseling dataset and a dataset of anonymized transcripts from interventions between Behavioral Health Coaches and Caregivers of patients in palliative or hospice care. Covering a diverse range of conditions like depression, anxiety, and grief, this curated dataset is designed to facilitate the development and evaluation of large language models for conversational mental health assistance. By providing a high-quality resource tailored to this critical domain, MentalChat16K aims to advance research on empathetic, personalized AI solutions to improve access to mental health support services. The dataset prioritizes patient privacy, ethical considerations, and responsible data usage. MentalChat16K presents a valuable opportunity for the research community to innovate AI technologies that can positively impact mental well-being. The dataset is available at https://huggingface.co/datasets/ShenLab/MentalChat16K and the code and documentation are hosted on GitHub at https://github.com/ChiaPatricia/MentalChat16K.",,"Jia Xu, Tianyi Wei, Bojian Hou, Patryk Orzechowski, Shu Yang, Ruochen Jin, Rachael Paulbeck, Joost Wagenaar, George Demiris, Li Shen",2025-06-02T02:53:02Z,MentalChat16K: A Benchmark Dataset for Conversational Mental Health   Assistance,MentalChat16K: Ein Benchmark-Datensatz für die psychische Gesundheitsunterstützung,MindMindChat16K:交流心理健康援助的基准数据集,http://arxiv.org/abs/2503.13509v2
897,"Large language models (LLMs) have demonstrated impressive performance on natural language tasks, but their decision-making processes remain largely opaque. Existing explanation methods either suffer from limited faithfulness to the model's reasoning or produce explanations that humans find difficult to understand. To address these challenges, we propose \textbf{ProtoSurE}, a novel prototype-based surrogate framework that provides faithful and human-understandable explanations for LLMs. ProtoSurE trains an interpretable-by-design surrogate model that aligns with the target LLM while utilizing sentence-level prototypes as human-understandable concepts. Extensive experiments show that ProtoSurE consistently outperforms SOTA explanation methods across diverse LLMs and datasets. Importantly, ProtoSurE demonstrates strong data efficiency, requiring relatively few training examples to achieve good performance, making it practical for real-world applications.",,"Bowen Wei, Mehrdad Fazli, Ziwei Zhu",2025-06-02T02:47:57Z,Learning to Explain: Prototype-Based Surrogate Models for LLM   Classification,Erklären lernen: Prototypenbasierte Surrogate-Modelle für die LLM-Klassifikation,学习解释:LLM分类原型代用模型,http://arxiv.org/abs/2505.18970v2
898,"While the ability of language models to elicit facts has been widely investigated, how they handle temporally changing facts remains underexplored. We discover Temporal Heads, specific attention heads that primarily handle temporal knowledge, through circuit analysis. We confirm that these heads are present across multiple models, though their specific locations may vary, and their responses differ depending on the type of knowledge and its corresponding years. Disabling these heads degrades the model's ability to recall time-specific knowledge while maintaining its general capabilities without compromising time-invariant and question-answering performances. Moreover, the heads are activated not only numeric conditions (""In 2004"") but also textual aliases (""In the year ...""), indicating that they encode a temporal dimension beyond simple numerical representation. Furthermore, we expand the potential of our findings by demonstrating how temporal knowledge can be edited by adjusting the values of these heads.",,"Yein Park, Chanwoong Yoon, Jungwoo Park, Minbyul Jeong, Jaewoo Kang",2025-06-02T02:47:44Z,Does Time Have Its Place? Temporal Heads: Where Language Models Recall   Time-specific Information,Hat die Zeit ihren Platz? Temporal Heads: Wo Sprachmodelle zeitspezifische Informationen abrufen,时间是否在它的位置? 时尚头领:语言模式回忆具体时间信息的位置,http://arxiv.org/abs/2502.14258v2
899,"Existing approaches for Large language model (LLM) detoxification generally rely on training on large-scale non-toxic or human-annotated preference data, designing prompts to instruct the LLM to generate safe content, or modifying the model parameters to remove toxic information, which are computationally expensive, lack robustness, and often compromise LLMs' fluency and contextual understanding. In this paper, we propose a simple yet effective approach for LLM detoxification, which leverages a compact, pre-trained calibration model that guides the detoxification process of a target LLM via a lightweight intervention in its generation pipeline. By learning a detoxified embedding space from non-toxic data, the calibration model effectively steers the LLM away from generating harmful content. This approach only requires a one-time training of the calibration model that is able to be seamlessly applied to multiple LLMs without compromising fluency or contextual understanding. Experiment results on the benchmark dataset demonstrate that our approach reduces toxicity while maintaining reasonable content expression.",,"Yuanhe Tian, Mingjie Deng, Guoqing Jin, Yan Song",2025-06-02T02:36:32Z,Detoxification of Large Language Models through Output-layer Fusion with   a Calibration Model,Entgiftung von großen Sprachmodellen durch Output-Layer Fusion mit einem Kalibriermodell,"通过使用校准模型的输出层融合,使大语言模型解毒",http://arxiv.org/abs/2506.01266v1
900,"In-context learning (ICL) is an important yet not fully understood ability of pre-trained large language models (LLMs). It can greatly enhance task performance using a few examples, termed demonstrations, without fine-tuning. Although effective in question answering, ICL often underperforms in long-form generation tasks such as summarization. Under appropriately realistic assumptions, we empirically and theoretically show that ICL demonstrations alone are insufficient to teach LLMs the task language and format distributions for generation. We argue for explicit exposure to the task distributions and hypothesize that defining them by prompting enhances model performance. To this end, we present LongGuide, which efficiently generates two parallel streams of guidelines capturing task language and format properties: (i) Metric Guidelines (MGs) that instruct models to optimize self-evaluated metrics; and (ii) Output Constraint Guidelines (OCGs) that constrain generation at both token and sentence levels. LongGuide automatically selects the best combination of guidelines, improving both strong open- and closed-source LLMs by over 5% in both zero- and few-shot settings. We show that LongGuide is generalizable, learnable by weak models to enhance strong ones, and integrates synergistically with automatic prompt optimizers.",,"Do Xuan Long, Duong Ngoc Yen, Do Xuan Trong, Luu Anh Tuan, Kenji Kawaguchi, Shafiq Joty, Min-Yen Kan, Nancy F. Chen",2025-06-02T02:35:24Z,Beyond In-Context Learning: Aligning Long-form Generation of Large   Language Models via Task-Inherent Attribute Guidelines,Beyond In-Context Learning: Ausrichtung der Langformgenerierung von großen Sprachmodellen über Task-Inhärent Attribute Guidelines,超越内文学习:通过任务内在属性准则调整长式大语言模型的长式生成,http://arxiv.org/abs/2506.01265v1
901,"AI-augmented data processing systems (DPSs) integrate large language models (LLMs) into query pipelines, allowing powerful semantic operations on structured and unstructured data. However, the reliability (a.k.a. trust) of these systems is fundamentally challenged by the potential for LLMs to produce errors, limiting their adoption in critical domains. To help address this reliability bottleneck, we introduce semantic integrity constraints (SICs) -- a declarative abstraction for specifying and enforcing correctness conditions over LLM outputs in semantic queries. SICs generalize traditional database integrity constraints to semantic settings, supporting common types of constraints, such as grounding, soundness, and exclusion, with both proactive and reactive enforcement strategies.   We argue that SICs provide a foundation for building reliable and auditable AI-augmented data systems. Specifically, we present a system design for integrating SICs into query planning and runtime execution and discuss its realization in AI-augmented DPSs. To guide and evaluate the vision, we outline several design goals -- covering criteria around expressiveness, runtime semantics, integration, performance, and enterprise-scale applicability -- and discuss how our framework addresses each, along with open research challenges.",,"Alexander W. Lee, Justin Chan, Michael Fu, Nicolas Kim, Akshay Mehta, Deepti Raghavan, Ugur Cetintemel",2025-06-02T02:31:38Z,Semantic Integrity Constraints: Declarative Guardrails for AI-Augmented   Data Processing Systems,Semantische Integritätsbeschränkungen: Deklarative Guardrails für KI-Augmentierte Datenverarbeitungssysteme,语义完整性制约:AI-增强的数据处理系统的声明保护栏,http://arxiv.org/abs/2503.00600v2
902,"Despite recent advances in end-to-end speech recognition methods, the output tends to be biased to the training data's vocabulary, resulting in inaccurate recognition of proper nouns and other unknown terms. To address this issue, we propose a method to improve recognition accuracy of such rare words in CTC-based models without additional training or text-to-speech systems. Specifically, keyword spotting is performed using acoustic features of intermediate layers during inference, and a bias is applied to the subsequent layers of the acoustic model for detected keywords. For keyword detection, we adopt a wildcard CTC that is both fast and tolerant of ambiguous matches, allowing flexible handling of words that are difficult to match strictly. Since this method does not require retraining of existing models, it can be easily applied to even large-scale models. In experiments on Japanese speech recognition, the proposed method achieved a 29% improvement in the F1 score for unknown words.",,"Yu Nakagome, Michael Hentschel",2025-06-02T02:30:26Z,WCTC-Biasing: Retraining-free Contextual Biasing ASR with Wildcard   CTC-based Keyword Spotting and Inter-layer Biasing,WCTC-Biasing: Retraining-free Contextual Biasing ASR mit Wildcard CTC-basiertem Keyword Spotting und inter-layer Biasing,"WCTC-相竞:与基于狂卡的CTC反恐委员会关键词 "" 点斑 "" 和 "" 跨层相竞 "" 重新培训",http://arxiv.org/abs/2506.01263v1
903,"Recent Large Language Models (LLMs) have demonstrated impressive translation performance without requiring fine-tuning on additional parallel corpora. However, they still face significant challenges in certain scenarios, particularly when translating low-resource languages. A common approach to address this issue is to provide external knowledge, such as few-shot examples, to assist LLMs in translating specific source sentences. However, this method is fundamentally limited by the quality or quantity of relevant sources, which cannot always be guaranteed. To reduce LLMs' reliance on external sources, we propose BridG MT, a method that combines Sentence Bridging, which generates a sequence of sentences as a bridge that gradually transition from easy-to-translate to more difficult, and Gradual MT, which sequentially translates these sentences using earlier translations as few-shot examples for subsequent ones. Experiments conducted on four LLMs across seven languages demonstrate that our method effectively enhances translation performance, even outperforming translation methods that rely on a large number of few-shot examples.",,"Seung-Woo Choi, Ga-Hyun Yoo, Jay-Yoon Lee",2025-06-02T02:28:27Z,BridG MT: Enhancing LLMs' Machine Translation Capabilities with Sentence   Bridging and Gradual MT,BridG MT: Verbesserung der maschinellen Übersetzungsfähigkeiten von LLMs durch Sentence Bridging und Gradual MT,"BridG MT: 提高LLMS的机器翻译能力,与判决衔接和逐步过渡",http://arxiv.org/abs/2410.11693v3
904,"Personalized AI assistants, a hallmark of the human-like capabilities of Large Language Models (LLMs), are a challenging application that intertwines multiple problems in LLM research. Despite the growing interest in the development of personalized assistants, the lack of an open-source conversational dataset tailored for personalization remains a significant obstacle for researchers in the field. To address this research gap, we introduce HiCUPID, a new benchmark to probe and unleash the potential of LLMs to deliver personalized responses. Alongside a conversational dataset, HiCUPID provides a Llama-3.2-based automated evaluation model whose assessment closely mirrors human preferences. We release our dataset, evaluation model, and code at https://github.com/12kimih/HiCUPID.",,"Jisoo Mok, Ik-hwan Kim, Sangkwon Park, Sungroh Yoon",2025-06-02T02:25:46Z,"Exploring the Potential of LLMs as Personalized Assistants: Dataset,   Evaluation, and Analysis","Erforschung des Potenzials von LLMs als Personalisierte Assistenten: Datensatz, Auswertung und Analyse",探索LLMM作为个人化助理的潜力:数据集、评价和分析,http://arxiv.org/abs/2506.01262v1
905,"DeepSeek-R1 is a cutting-edge open-source large language model (LLM) developed by DeepSeek, showcasing advanced reasoning capabilities through a hybrid architecture that integrates mixture of experts (MoE), chain of thought (CoT) reasoning, and reinforcement learning. Released under the permissive MIT license, DeepSeek-R1 offers a transparent and cost-effective alternative to proprietary models like GPT-4o and Claude-3 Opus; it excels in structured problem-solving domains such as mathematics, healthcare diagnostics, code generation, and pharmaceutical research. The model demonstrates competitive performance on benchmarks like the United States Medical Licensing Examination (USMLE) and American Invitational Mathematics Examination (AIME), with strong results in pediatric and ophthalmologic clinical decision support tasks. Its architecture enables efficient inference while preserving reasoning depth, making it suitable for deployment in resource-constrained settings. However, DeepSeek-R1 also exhibits increased vulnerability to bias, misinformation, adversarial manipulation, and safety failures - especially in multilingual and ethically sensitive contexts. This survey highlights the model's strengths, including interpretability, scalability, and adaptability, alongside its limitations in general language fluency and safety alignment. Future research priorities include improving bias mitigation, natural language comprehension, domain-specific validation, and regulatory compliance. Overall, DeepSeek-R1 represents a major advance in open, scalable AI, underscoring the need for collaborative governance to ensure responsible and equitable deployment.",,"Jiancheng Ye, Sophie Bronstein, Jiarui Hai, Malak Abu Hashish",2025-06-02T02:17:04Z,"DeepSeek in Healthcare: A Survey of Capabilities, Risks, and Clinical   Applications of Open-Source Large Language Models","DeepSeek in Healthcare: Eine Übersicht über Fähigkeiten, Risiken und klinische Anwendungen von Open-Source großen Sprachmodellen",《保健的深探索:开放源大语言模型的能力、风险和临床应用调查》,http://arxiv.org/abs/2506.01257v1
906,"Forced alignment is a common tool to align audio with orthographic and phonetic transcriptions. Most forced alignment tools provide only a single estimate of a boundary. The present project introduces a method of deriving confidence intervals for these boundaries using a neural network ensemble technique. Ten different segment classifier neural networks were previously trained, and the alignment process is repeated with each model. The alignment ensemble is then used to place the boundary at the median of the boundaries in the ensemble, and 97.85% confidence intervals are constructed using order statistics. On the Buckeye and TIMIT corpora, the ensemble boundaries show a slight improvement over using just a single model. The confidence intervals are incorporated into Praat TextGrids using a point tier, and they are also output as a table for researchers to analyze separately as diagnostics or to incorporate uncertainty into their analyses.",,Matthew C. Kelley,2025-06-02T02:12:28Z,Confidence intervals for forced alignment boundaries using model   ensembles,Vertrauensintervalle für erzwungene Ausrichtungsgrenzen mit Modellensembles,使用模型组合的强制调整边界信任间隔,http://arxiv.org/abs/2506.01256v1
907,"FastText has established itself as a fundamental algorithm for learning word representations, demonstrating exceptional capability in handling out-of-vocabulary words through character-level n-gram embeddings. However, its hash-based bucketing mechanism introduces critical limitations for large-scale industrial deployment: hash collisions cause semantic drift, and memory requirements become prohibitively expensive when dealing with real-world vocabularies containing millions of terms. This paper presents a comprehensive memory optimization framework that fundamentally reimagines FastText's memory management through the integration of double-array trie (DA-trie) structures and mark-compact garbage collection principles. Our approach leverages the linguistic insight that n-grams sharing common prefixes or suffixes exhibit highly correlated embeddings due to co-occurrence patterns in natural language. By systematically identifying and merging semantically similar embeddings based on structural relationships, we achieve compression ratios of 4:1 to 10:1 while maintaining near-perfect embedding quality. The algorithm consists of four sophisticated phases: prefix trie construction with embedding mapping, prefix-based similarity compression, suffix-based similarity compression, and mark-compact memory reorganization. Comprehensive experiments on a 30-million Chinese vocabulary dataset demonstrate memory reduction from over 100GB to approximately 30GB with negligible performance degradation. Our industrial deployment results show significant cost reduction, faster loading times, and improved model reliability through the elimination of hash collision artifacts. Code and experimental implementations are available at: https://github.com/initial-d/me_fasttext",,Yimin Du,2025-06-02T02:11:22Z,Memory-Efficient FastText: A Comprehensive Approach Using Double-Array   Trie Structures and Mark-Compact Memory Management,Memory-Efficient FastText: Ein umfassender Ansatz mit Doppel-Array-Trie-Strukturen und Mark-Compact-Speicherverwaltung,内存效率高的快速快图:使用双向三边结构的综合办法和标记-合同内存管理,http://arxiv.org/abs/2506.01254v1
908,"Recent decoding methods improve the factuality of large language models (LLMs) by refining how the next token is selected during generation. These methods typically operate at the token level, leveraging internal representations to suppress superficial patterns. Nevertheless, LLMs remain prone to hallucinations, especially over longer contexts. In this paper, we propose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy that actively decides when to apply contrasting layers during generation. By casting decoding as a sequential decision-making problem, ActLCD employs a reinforcement learning policy guided by a reward-aware classifier to optimize factuality beyond the token level. Our experiments demonstrate that ActLCD surpasses state-of-the-art methods across five benchmarks, showcasing its effectiveness in mitigating hallucinations in diverse generation scenarios.",,"Hongxiang Zhang, Hao Chen, Muhao Chen, Tianyi Zhang",2025-06-02T02:11:11Z,Active Layer-Contrastive Decoding Reduces Hallucination in Large   Language Model Generation,Aktives Layer-Kontrastives Decodieren reduziert Halluzination bei der Generierung von Großsprachenmodellen,大型语言模式生成中活性多语言解层解码减少幻觉,http://arxiv.org/abs/2505.23657v2
909,"Knowing which latent conditions lead to a particular outcome is useful for critically examining claims made about complex event outcomes. Identifying implied conditions and examining their influence on an outcome is challenging. We handle this by combining and augmenting annotations from two existing datasets consisting of goals and states, and explore the influence of conditions through our research questions and Condition-based Reasoning tasks. We examine open and closed LLMs of varying sizes and intent-alignment on our reasoning tasks and find that conditions are useful when not all context is available. Models differ widely in their ability to generate and identify outcome-variant conditions which affects their performance on outcome validation when conditions are used to replace missing context. Larger models like GPT-4o, are more cautious in such less constrained situations.",,"Sai Vallurupalli, Francis Ferraro",2025-06-02T02:09:20Z,CoRE: Condition-based Reasoning for Identifying Outcome Variance in   Complex Events,CoRE: Condition-based Reasoning for Identifying Outcome Variance in komplexen Veranstaltungen,CORE: 查明复杂事件成果差异的基于条件的理由,http://arxiv.org/abs/2506.01253v1
910,"LLM inference for enterprise applications, such as summarization, RAG, and code-generation, typically observe much longer prompt than generations, leading to high prefill cost and response latency. We present SwiftKV, a novel model transformation and distillation procedure targeted at reducing the prefill compute (in FLOPs) of prompt tokens while preserving high generation quality. First, SwiftKV prefills later layers' KV cache using an earlier layer's output, allowing prompt tokens to skip those later layers. Second, SwiftKV employs a lightweight knowledge-preserving distillation procedure that can adapt existing LLMs with minimal accuracy impact. Third, SwiftKV can naturally incorporate KV cache compression to improve inference performance in low-memory scenarios. Our comprehensive experiments show that SwiftKV can effectively reduce prefill computation by 25-50% across several LLM families while incurring minimum quality degradation. In the end-to-end inference serving, SwiftKV realizes up to 2x higher aggregate throughput and 60% lower time per output token. It can achieve a staggering 560 TFlops/GPU of normalized inference throughput, which translates to 16K tokens/s for Llama-3.1-70B. SwiftKV is open-sourced at https://github.com/snowflakedb/arctictraining.",,"Aurick Qiao, Zhewei Yao, Samyam Rajbhandari, Yuxiong He",2025-06-02T02:08:06Z,SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving   Model Transformation,SwiftKV: Schnelle präfilloptimierte Schlussfolgerung mit wissenserhaltender Modelltransformation,SwiftKV: 与知识保护模式转型有关的快速预填-优化推断,http://arxiv.org/abs/2410.03960v3
911,"This study explores the potential of using acoustic features of segmental speech sounds to detect deepfake audio. These features are highly interpretable because of their close relationship with human articulatory processes and are expected to be more difficult for deepfake models to replicate. The results demonstrate that certain segmental features commonly used in forensic voice comparison (FVC) are effective in identifying deep-fakes, whereas some global features provide little value. These findings underscore the need to approach audio deepfake detection using methods that are distinct from those employed in traditional FVC, and offer a new perspective on leveraging segmental features for this purpose.",,"Tianle Yang, Chengzhe Sun, Siwei Lyu, Phil Rose",2025-06-02T02:02:04Z,Forensic deepfake audio detection using segmental speech features,Forensische Deepfake-Audioerkennung mit segmentalen Sprachfunktionen,利用部分语言特征进行法证深假声波探测,http://arxiv.org/abs/2505.13847v2
912,"Traditional Chinese Medicine (TCM) is a holistic medical system with millennia of accumulated clinical experience, playing a vital role in global healthcare-particularly across East Asia. However, the implicit reasoning, diverse textual forms, and lack of standardization in TCM pose major challenges for computational modeling and evaluation. Large Language Models (LLMs) have demonstrated remarkable potential in processing natural language across diverse domains, including general medicine. Yet, their systematic evaluation in the TCM domain remains underdeveloped. Existing benchmarks either focus narrowly on factual question answering or lack domain-specific tasks and clinical realism. To fill this gap, we introduce MTCMB-a Multi-Task Benchmark for Evaluating LLMs on TCM Knowledge, Reasoning, and Safety. Developed in collaboration with certified TCM experts, MTCMB comprises 12 sub-datasets spanning five major categories: knowledge QA, language understanding, diagnostic reasoning, prescription generation, and safety evaluation. The benchmark integrates real-world case records, national licensing exams, and classical texts, providing an authentic and comprehensive testbed for TCM-capable models. Preliminary results indicate that current LLMs perform well on foundational knowledge but fall short in clinical reasoning, prescription planning, and safety compliance. These findings highlight the urgent need for domain-aligned benchmarks like MTCMB to guide the development of more competent and trustworthy medical AI systems. All datasets, code, and evaluation tools are publicly available at: https://github.com/Wayyuanyuan/MTCMB.",,"Shufeng Kong, Xingru Yang, Yuanyuan Wei, Zijie Wang, Hao Tang, Jiuqi Qin, Shuting Lan, Yingheng Wang, Junwen Bai, Zhuangbin Chen, Zibin Zheng, Caihua Liu, Hao Liang",2025-06-02T02:01:40Z,"MTCMB: A Multi-Task Benchmark Framework for Evaluating LLMs on   Knowledge, Reasoning, and Safety in Traditional Chinese Medicine","MTCMB: Ein Multi-Task-Benchmark-Framework zur Bewertung von LLMs über Wissen, Vernunft und Sicherheit in der traditionellen chinesischen Medizin",MTCMM: 中国传统医学知识、合理性、安全、知识、合理性、安全等优待评估多任务基准框架,http://arxiv.org/abs/2506.01252v1
913,"This paper introduces ExpertLongBench, an expert-level benchmark containing 11 tasks from 9 domains that reflect realistic expert workflows and applications. Beyond question answering, the application-driven tasks in ExpertLongBench demand long-form outputs that can exceed 5,000 tokens and strict adherence to domain-specific requirements. Notably, each task in ExpertLongBench includes a rubric, designed or validated by domain experts, to specify task requirements and guide output evaluation. Furthermore, we propose CLEAR, an evaluation framework that supports accurate evaluation of long-form model outputs in our benchmark. To achieve fine-grained, expert-aligned evaluation, CLEAR derives checklists from both model outputs and references by extracting information corresponding to items in the task-specific rubric. Checklist items for model outputs are then compared with corresponding items for reference outputs to assess their correctness, enabling grounded evaluation. We benchmark 11 large language models (LLMs) and analyze components in CLEAR, showing that (1) existing LLMs, with the top performer achieving only a 26.8% F1 score, require significant improvement for expert-level tasks; (2) models can generate content corresponding to the required aspects, though often not accurately; and (3) accurate checklist extraction and comparison in CLEAR can be achieved by open-weight models for more scalable and low-cost usage.",,"Jie Ruan, Inderjeet Nair, Shuyang Cao, Amy Liu, Sheza Munir, Micah Pollens-Dempsey, Tiffany Chiang, Lucy Kates, Nicholas David, Sihan Chen, Ruxin Yang, Yuqian Yang, Jasmine Gump, Tessa Bialek, Vivek Sankaran, Margo Schlanger, Lu Wang",2025-06-02T01:39:02Z,ExpertLongBench: Benchmarking Language Models on Expert-Level Long-Form   Generation Tasks with Structured Checklists,ExpertLongBench: Benchmarking-Sprachmodelle auf Expertenebene Langform-Erstellungsaufgaben mit strukturierten Checklisten,专家关系:专家级长期世代任务的语言模式基准与结构化核对清单,http://arxiv.org/abs/2506.01241v1
914,"We introduce the $\underline{Ko}rean \underline{G}rammar \underline{E}valuation Bench\underline{M}ark (KoGEM)$, designed to assess the linguistic competence of LLMs and humans in Korean. KoGEM consists of 1.5k multiple-choice QA pairs covering five main categories and 16 subcategories. The zero-shot evaluation of 27 LLMs of various sizes and types reveals that while LLMs perform remarkably well on straightforward tasks requiring primarily definitional knowledge, they struggle with tasks that demand the integration of real-world experiential knowledge, such as phonological rules and pronunciation. Furthermore, our in-depth analysis suggests that incorporating such experiential knowledge could enhance the linguistic competence of LLMs. With KoGEM, we not only highlight the limitations of current LLMs in linguistic competence but also uncover hidden facets of LLMs in linguistic competence, paving the way for enhancing comprehensive language understanding. Our code and dataset are available at: https://github.com/SungHo3268/KoGEM.",,"SungHo Kim, Nayeon Kim, Taehee Jeon, SangKeun Lee",2025-06-02T01:27:46Z,Polishing Every Facet of the GEM: Testing Linguistic Competence of LLMs   and Humans in Korean,Polieren jeder Seite des GEM: Linguistische Kompetenz von LLMs und Menschen auf Koreanisch testen,"波兰语 "" GEM的方方面面 "" :用朝鲜语测试LLMs和人的语言能力",http://arxiv.org/abs/2506.01237v1
915,"Topic modeling is a fundamental task in natural language processing, allowing the discovery of latent thematic structures in text corpora. While Large Language Models (LLMs) have demonstrated promising capabilities in topic discovery, their direct application to topic modeling suffers from issues such as incomplete topic coverage, misalignment of topics, and inefficiency. To address these limitations, we propose LLM-ITL, a novel LLM-in-the-loop framework that integrates LLMs with Neural Topic Models (NTMs). In LLM-ITL, global topics and document representations are learned through the NTM. Meanwhile, an LLM refines these topics using an Optimal Transport (OT)-based alignment objective, where the refinement is dynamically adjusted based on the LLM's confidence in suggesting topical words for each set of input words. With the flexibility of being integrated into many existing NTMs, the proposed approach enhances the interpretability of topics while preserving the efficiency of NTMs in learning topics and document representations. Extensive experiments demonstrate that LLM-ITL helps NTMs significantly improve their topic interpretability while maintaining the quality of document representation. Our code and datasets are available at https://github.com/Xiaohao-Yang/LLM-ITL",,"Xiaohao Yang, He Zhao, Weijie Xu, Yuanyuan Qi, Jueqing Lu, Dinh Phung, Lan Du",2025-06-02T01:21:35Z,Neural Topic Modeling with Large Language Models in the Loop,Neurale Themenmodellierung mit großen Sprachmodellen in der Schleife,环形大语言模型的神经专题建模,http://arxiv.org/abs/2411.08534v3
916,"Retrieval-augmented generation (RAG) based large language models (LLMs) are widely used in finance for their excellent performance on knowledge-intensive tasks. However, standardized documents (e.g., SEC filing) share similar formats such as repetitive boilerplate texts, and similar table structures. This similarity forces traditional RAG methods to misidentify near-duplicate text, leading to duplicate retrieval that undermines accuracy and completeness. To address these issues, we propose the Hierarchical Retrieval with Evidence Curation (HiREC) framework. Our approach first performs hierarchical retrieval to reduce confusion among similar texts. It first retrieve related documents and then selects the most relevant passages from the documents. The evidence curation process removes irrelevant passages. When necessary, it automatically generates complementary queries to collect missing information. To evaluate our approach, we construct and release a Large-scale Open-domain Financial (LOFin) question answering benchmark that includes 145,897 SEC documents and 1,595 question-answer pairs. Our code and data are available at https://github.com/deep-over/LOFin-bench-HiREC.",,"Jaeyoung Choe, Jihoon Kim, Woohwan Jung",2025-06-02T01:12:15Z,Hierarchical Retrieval with Evidence Curation for Open-Domain Financial   Question Answering on Standardized Documents,Hierarchische Retrieval mit Evidenz-Kuration für Open-Domain-Finanzfrage-Antworten auf standardisierte Dokumente,标准化文件开放域财务问题证据说明的梯级检索,http://arxiv.org/abs/2505.20368v2
917,"Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning and enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE",,"Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, Zhenyu Chen",2025-06-02T00:44:09Z,Token-Budget-Aware LLM Reasoning,Token-Budget-Bewusstsein LLM-Vernunft,Tok- 预算- 软件软件LLM 理由,http://arxiv.org/abs/2412.18547v5
918,"Mental-health stigma remains a pervasive social problem that hampers treatment-seeking and recovery. Existing resources for training neural models to finely classify such stigma are limited, relying primarily on social-media or synthetic data without theoretical underpinnings. To remedy this gap, we present an expert-annotated, theory-informed corpus of human-chatbot interviews, comprising 4,141 snippets from 684 participants with documented socio-cultural backgrounds. Our experiments benchmark state-of-the-art neural models and empirically unpack the challenges of stigma detection. This dataset can facilitate research on computationally detecting, neutralizing, and counteracting mental-health stigma. Our corpus is openly available at https://github.com/HanMeng2004/Mental-Health-Stigma-Interview-Corpus.",,"Han Meng, Yancan Chen, Yunan Li, Yitian Yang, Jungup Lee, Renwen Zhang, Yi-Chieh Lee",2025-06-01T23:55:38Z,"What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated   Interview Corpus for Demystifying Mental-Health Stigma","Was ist Stigma zugeschrieben? Ein theoriegeprägtes, sachverständiges Interview Corpus for Demystifying Mental-Health Stigma",""" 污名化 "" 的属性是什么? "" 消除精神卫生污名化的理论性、专家附加说明的访谈公司 "" 。",http://arxiv.org/abs/2505.12727v2
919,"As large language models increasingly gain popularity in real-world applications, processing extremely long contexts, often exceeding the model's pre-trained context limits, has emerged as a critical challenge. While existing approaches to efficient long-context processing show promise, recurrent compression-based methods struggle with information preservation, whereas random access approaches require substantial memory resources. We introduce REFORM, a novel inference framework that efficiently handles long contexts through a two-phase approach. First, it incrementally processes input chunks while maintaining a compressed KV cache, constructs cross-layer context embeddings, and utilizes early exit strategy for improved efficiency. Second, it identifies and gathers essential tokens via similarity matching and selectively recomputes the KV cache. Compared to baselines, REFORM achieves over 50% and 27% performance gains on RULER and BABILong respectively at 1M context length. It also outperforms baselines on Infinite-Bench and MM-NIAH, demonstrating flexibility across diverse tasks and domains. Additionally, REFORM reduces inference time by 30% and peak memory usage by 5%, achieving both efficiency and superior performance.",,"Woomin Song, Sai Muralidhar Jayanthi, Srikanth Ronanki, Kanthashree Mysore Sathyendra, Jinwoo Shin, Aram Galstyan, Shubham Katiyar, Sravan Babu Bodapati",2025-06-01T23:49:14Z,"Compress, Gather, and Recompute: REFORMing Long-Context Processing in   Transformers","Komprimieren, Sammeln und Rekomprimieren: REFORMieren der Langkontextverarbeitung in Transformern",压缩、集中和重组:改造变换器中的长期文本处理,http://arxiv.org/abs/2506.01215v1
920,"We study the problem of fine-tuning a language model (LM) for a target task by optimally using the information from $n$ auxiliary tasks. This problem has broad applications in NLP, such as targeted instruction tuning and data selection in chain-of-thought fine-tuning. The key challenge of this problem is that not all auxiliary tasks are beneficial in improving the performance of the target task. Thus, selecting the right subset of auxiliary tasks is crucial. Conventional subset selection methods, such as forward and backward stepwise selection, are unsuitable for LM fine-tuning because they require repeated training on subsets of auxiliary tasks. This paper introduces a new algorithm for estimating model fine-tuning performance without requiring repeated training. Our algorithm first performs multitask training using data from all tasks to obtain a meta initialization. Then, we approximate the model fine-tuning loss of a subset using functional values and gradients from the meta initialization. Empirically, we find that this gradient-based approximation holds with remarkable accuracy for twelve transformer-based LMs. Thus, we can now estimate fine-tuning performances on CPUs within a few seconds. Finally, we fine-tune the pretrained base model once on the selected subset of tasks. We conduct extensive experiments to validate this approach, delivering a speedup of $30\times$ over conventional subset selection while incurring only $1\%$ error of the true fine-tuning performances. In downstream evaluations involving both instruction tuning and chain-of-thought fine-tuning, this loss-based selection approach improves over prior gradient or representation similarity-based methods for subset selection by up to $3.8\%$.",,"Dongyue Li, Ziniu Zhang, Lu Wang, Hongyang R. Zhang",2025-06-01T23:17:27Z,Scalable Fine-tuning from Multiple Data Sources: A First-Order   Approximation Approach,Skalierbare Feinabstimmung aus mehreren Datenquellen: Ein Annäherungsansatz erster Ordnung,从多数据来源进行可缩放的微调:一阶近似法,http://arxiv.org/abs/2409.19458v3
921,"Natural Language Processing (NLP) is an established and dynamic field. Despite this, what constitutes NLP research remains debated. In this work, we address the question by quantitatively examining NLP research papers. We propose a taxonomy of research contributions and introduce NLPContributions, a dataset of nearly $2k$ NLP research paper abstracts, carefully annotated to identify scientific contributions and classify their types according to this taxonomy. We also introduce a novel task of automatically identifying contribution statements and classifying their types from research papers. We present experimental results for this task and apply our model to $\sim$$29k$ NLP research papers to analyze their contributions, aiding in the understanding of the nature of NLP research. We show that NLP research has taken a winding path -- with the focus on language and human-centric studies being prominent in the 1970s and 80s, tapering off in the 1990s and 2000s, and starting to rise again since the late 2010s. Alongside this revival, we observe a steady rise in dataset and methodological contributions since the 1990s, such that today, on average, individual NLP papers contribute in more ways than ever before. Our dataset and analyses offer a powerful lens for tracing research trends and offer potential for generating informed, data-driven literature surveys.",,"Aniket Pramanick, Yufang Hou, Saif M. Mohammad, Iryna Gurevych",2025-06-01T23:12:08Z,The Nature of NLP: Analyzing Contributions in NLP Papers,Die Natur von NLP: Analysieren von Beiträgen in NLP-Papieren,NLP的性质:分析对NLP文件的贡献,http://arxiv.org/abs/2409.19505v2
922,"As large language models (LLMs) scale, model compression is crucial for edge deployment and accessibility. Weight-only quantization reduces model size but suffers from performance degradation at lower bit widths. Moreover, standard finetuning is incompatible with quantized models, and alternative methods often fall short of full finetuning. In this paper, we propose ClusComp, a simple yet effective compression paradigm that clusters weight matrices into codebooks and finetunes them block-by-block. ClusComp (1) achieves superior performance in 2-4 bit quantization, (2) pushes compression to 1-bit while outperforming ultra-low-bit methods with minimal finetuning, and (3) enables efficient finetuning, even surpassing existing quantization-based approaches and rivaling full FP16 finetuning. Notably, ClusComp supports compression and finetuning of 70B LLMs on a single A6000-48GB GPU.",,"Baohao Liao, Christian Herold, Seyyed Hadi Hashemi, Stefan Vasilev, Shahram Khadivi, Christof Monz",2025-06-01T23:03:12Z,ClusComp: A Simple Paradigm for Model Compression and Efficient   Finetuning,ClusComp: Ein einfaches Paradigma für Modellkompression und effiziente Feinsteuerung,ClusComp: 用于模型压缩和高效微调的简单示例,http://arxiv.org/abs/2503.13089v2
923,"Speculative decoding has emerged as a promising approach to accelerating large language model (LLM) generation using a fast drafter while maintaining alignment with the target model's distribution. However, existing approaches face a trade-off: external drafters offer flexibility but can suffer from slower drafting, while self-speculation methods use drafters tailored to the target model but require re-training. In this paper, we introduce novel drafters based on Mamba, a state-of-the-art state space model (SSM), as a solution that combines the best aspects of both approaches. By leveraging the linear structure of SSMs, our approach avoids the quadratic complexity inherent in traditional Transformer-based methods, enabling faster drafting and lower memory usage while maintaining the flexibility to work across different target models. We further enhance efficiency with a novel test-time tree search algorithm for generating high-quality draft candidates. Our empirical evaluation demonstrates that Mamba-based drafters not only outperform existing external drafting methods but are also comparable to state-of-the-art self-speculation approaches while using less memory and maintaining their cross-model adaptability.",,"Daewon Choi, Seunghyuk Oh, Saket Dingliwal, Jihoon Tack, Kyuyoung Kim, Woomin Song, Seojin Kim, Insu Han, Jinwoo Shin, Aram Galstyan, Shubham Katiyar, Sravan Babu Bodapati",2025-06-01T22:52:47Z,Mamba Drafters for Speculative Decoding,Mamba-Drawler für spekulative Dekodierung,Mamba 用于投机性代号的 Mamba 起草器,http://arxiv.org/abs/2506.01206v1
924,"Detecting ambiguity is important for language understanding, including uncertainty estimation, humour detection, and processing garden path sentences. We assess language models' sensitivity to ambiguity by introducing an adversarial ambiguity dataset that includes syntactic, lexical, and phonological ambiguities along with adversarial variations (e.g., word-order changes, synonym replacements, and random-based alterations). Our findings show that direct prompting fails to robustly identify ambiguity, while linear probes trained on model representations can decode ambiguity with high accuracy, sometimes exceeding 90\%. Our results offer insights into the prompting paradigm and how language models encode ambiguity at different layers. We release both our code and data: https://github.com/coastalcph/lm_ambiguity.",,"Antonia Karamolegkou, Oliver Eberle, Phillip Rust, Carina Kauf, Anders Søgaard",2025-06-01T22:50:06Z,Trick or Neat: Adversarial Ambiguity and Language Model Evaluation,Trick or Neat: Widersprüchliche Ambiguität und Sprachmodellbewertung,Trick or Neat: 反向模糊和语言模式评价,http://arxiv.org/abs/2506.01205v1
925,"Retrieval-Augmented Generation (RAG) leverages large language models (LLMs) combined with external contexts to enhance the accuracy and reliability of generated responses. However, reliably attributing generated content to specific context segments, context attribution, remains challenging due to the computationally intensive nature of current methods, which often require extensive fine-tuning or human annotation. In this work, we introduce a novel Jensen-Shannon Divergence driven method to Attribute Response to Context (ARC-JSD), enabling efficient and accurate identification of essential context sentences without additional fine-tuning or surrogate modelling. Evaluations on a wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using instruction-tuned LLMs in different scales demonstrate superior accuracy and significant computational efficiency improvements compared to the previous surrogate-based method. Furthermore, our mechanistic analysis reveals specific attention heads and multilayer perceptron (MLP) layers responsible for context attribution, providing valuable insights into the internal workings of RAG models. Our code is available at https://github.com/ruizheliUOA/ARC_JSD",,"Ruizhe Li, Chen Chen, Yuchen Hu, Yanjun Gao, Xi Wang, Emine Yilmaz",2025-06-01T22:42:30Z,Attributing Response to Context: A Jensen-Shannon Divergence Driven   Mechanistic Study of Context Attribution in Retrieval-Augmented Generation,Zuweisende Reaktion auf den Kontext: Eine Jensen-Shannon-Divergenz angetriebene mechanistische Studie des Kontexts Attribution in retrieval-Augmented Generation,对背景的反应:Jensen-Shannon difficolgence 驱动器对回溯率-养殖一代中环境归属的机械研究,http://arxiv.org/abs/2505.16415v2
926,"This paper proposes a parameter collaborative optimization algorithm for large language models, enhanced with graph spectral analysis. The goal is to improve both fine-tuning efficiency and structural awareness during training. In the proposed method, the parameters of a pre-trained language model are treated as nodes in a graph. A weighted graph is constructed, and Laplacian spectral decomposition is applied to enable frequency-domain modeling and structural representation of the parameter space. Based on this structure, a joint loss function is designed. It combines the task loss with a spectral regularization term to facilitate collaborative updates among parameters. In addition, a spectral filtering mechanism is introduced during the optimization phase. This mechanism adjusts gradients in a structure-aware manner, enhancing the model's training stability and convergence behavior. The method is evaluated on multiple tasks, including traditional fine-tuning comparisons, few-shot generalization tests, and convergence speed analysis. In all settings, the proposed approach demonstrates superior performance. The experimental results confirm that the spectral collaborative optimization framework effectively reduces parameter perturbations and improves fine-tuning quality while preserving overall model performance. This work contributes significantly to the field of artificial intelligence by advancing parameter-efficient training methodologies for large-scale models, reinforcing the importance of structural signal processing in deep learning optimization, and offering a robust, generalizable framework for enhancing language model adaptability and performance.",,"Hanlu Zhang, Yumeng Ma, Shuo Wang, Guiran Liu, Binrong Zhu",2025-06-01T22:38:52Z,Graph-Based Spectral Decomposition for Parameter Coordination in   Language Model Fine-Tuning,Graphenbasierte Spektralzersetzung für die Parameterkoordination im Sprachmodell Feintuning,语言模型微调模型参数协调图形化光谱分解,http://arxiv.org/abs/2504.19583v2
927,"No-resource languages - those with minimal or no digital representation - pose unique challenges for machine translation (MT). Unlike low-resource languages, which rely on limited but existent corpora, no-resource languages often have fewer than 100 sentences available for training. This work explores the problem of no-resource translation through three distinct workflows: fine-tuning of translation-specific models, in-context learning with large language models (LLMs) using chain-of-reasoning prompting, and direct prompting without reasoning. Using Owens Valley Paiute as a case study, we demonstrate that no-resource translation demands fundamentally different approaches from low-resource scenarios, as traditional approaches to machine translation, such as those that work for low-resource languages, fail. Empirical results reveal that, although traditional approaches fail, the in-context learning capabilities of general-purpose large language models enable no-resource language translation that outperforms low-resource translation approaches and rivals human translations (BLEU 0.45-0.6); specifically, chain-of-reasoning prompting outperforms other methods for larger corpora, while direct prompting exhibits advantages in smaller datasets. As these approaches are language-agnostic, they have potential to be generalized to translation tasks from a wide variety of no-resource languages without expert input. These findings establish no-resource translation as a distinct paradigm requiring innovative solutions, providing practical and theoretical insights for language preservation.",,Madhavendra Thakur,2025-06-01T22:33:07Z,Towards Neural No-Resource Language Translation: A Comparative   Evaluation of Approaches,Auf dem Weg zu neuraler No-Resource-Sprachenübersetzung: Eine vergleichende Bewertung von Ansätzen,走向神经非资源语言翻译:方法的比较评价,http://arxiv.org/abs/2412.20584v2
928,"Over the last decades, deep neural networks based-models became the dominant paradigm in machine learning. Further, the use of artificial neural networks in symbolic learning has been seen as increasingly relevant recently. To study the capabilities of neural networks in the symbolic AI domain, researchers have explored the ability of deep neural networks to learn mathematical constructions, such as addition and multiplication, logic inference, such as theorem provers, and even the execution of computer programs. The latter is known to be too complex a task for neural networks. Therefore, the results were not always successful, and often required the introduction of biased elements in the learning process, in addition to restricting the scope of possible programs to be executed. In this work, we will analyze the ability of neural networks to learn how to execute programs as a whole. To do so, we propose a different approach. Instead of using an imperative programming language, with complex structures, we use the Lambda Calculus ({\lambda}-Calculus), a simple, but Turing-Complete mathematical formalism, which serves as the basis for modern functional programming languages and is at the heart of computability theory. We will introduce the use of integrated neural learning and lambda calculi formalization. Finally, we explore execution of a program in {\lambda}-Calculus is based on reductions, we will show that it is enough to learn how to perform these reductions so that we can execute any program. Keywords: Machine Learning, Lambda Calculus, Neurosymbolic AI, Neural Networks, Transformer Model, Sequence-to-Sequence Models, Computational Models",,"João Flach, Alvaro F. Moreira, Luis C. Lamb",2025-06-01T22:21:19Z,Towards a Neural Lambda Calculus: Neurosymbolic AI Applied to the   Foundations of Functional Programming,Auf dem Weg zu einem neuralen Lambda Calculus: Neurosymbolische KI auf die Grundlagen der funktionalen Programmierung angewandt,迈向神经兰巴达计分:适用于功能方案编制基础的神经元体AI,http://arxiv.org/abs/2304.09276v2
929,"Sparse dictionary learning (and, in particular, sparse autoencoders) attempts to learn a set of human-understandable concepts that can explain variation on an abstract space. A basic limitation of this approach is that it neither exploits nor represents the semantic relationships between the learned concepts. In this paper, we introduce a modified SAE architecture that explicitly models a semantic hierarchy of concepts. Application of this architecture to the internal representations of large language models shows both that semantic hierarchy can be learned, and that doing so improves both reconstruction and interpretability. Additionally, the architecture leads to significant improvements in computational efficiency.",,"Mark Muchane, Sean Richardson, Kiho Park, Victor Veitch",2025-06-01T22:20:07Z,Incorporating Hierarchical Semantics in Sparse Autoencoder Architectures,Einschließlich hierarchischer Semantik in Sparse Autoencoder-Architekturen,将分级语义包含在 Sparse 自动编码器架构中,http://arxiv.org/abs/2506.01197v1
930,"Language is often used strategically, particularly in high-stakes, adversarial settings, yet most work on pragmatics and LLMs centers on cooperativity. This leaves a gap in systematic understanding of non-cooperative discourse. To address this, we introduce CoBRA (Cooperation-Breach Response Assessment), along with three interpretable metrics -- Benefit at Turn (BaT), Penalty at Turn (PaT), and Normalized Relative Benefit at Turn (NRBaT) -- to quantify the perceived strategic effects of discourse moves. We also present CHARM, an annotated dataset of real courtroom cross-examinations, to demonstrate the framework's effectiveness. Using these tools, we evaluate a range of LLMs and show that LLMs generally exhibit limited pragmatic understanding of strategic language. While model size shows an increase in performance on our metrics, reasoning ability does not help and largely hurts, introducing overcomplication and internal confusion.",,"Anshun Asher Zheng, Junyi Jessy Li, David I. Beaver",2025-06-01T22:07:20Z,CoBRA: Quantifying Strategic Language Use and LLM Pragmatics,CoBRA: Quantifizierung der strategischen Sprachnutzung und LLM-Pragmatics,CoBRA: 量化战略语言使用和LLM,http://arxiv.org/abs/2506.01195v1
931,"Large Language Models (LLMs) struggle with culturally-specific reasoning tasks, particularly in low-resource languages, hindering their global applicability. Addressing this gap is crucial for equitable AI deployment. We introduce Culturally-Grounded Chain-of-Thought (CG-CoT), a novel prompting strategy that combines dense vector retrieval of cultural context with explicit reasoning sequences. Our extensive experiments on Yoruba proverb interpretation demonstrate that CG-CoT provides significantly higher culturally-aligned accuracy and depth than traditional prompting methods, validated through both automated metrics and LLM-based evaluations. Notably, we uncover stark disparities between token-level translation metrics like BLEU and human-judged cultural relevance, suggesting a rethinking of evaluation approaches for low-resource NLP.",,Madhavendra Thakur,2025-06-01T21:57:02Z,Culturally-Grounded Chain-of-Thought (CG-CoT):Enhancing LLM Performance   on Culturally-Specific Tasks in Low-Resource Languages,CG-CoT (Culturally Grounded Chain of Thought): Verbesserung der LLM-Performance bei kulturell spezifischen Aufgaben in ressourcenarmen Sprachen,CG-Cot:加强低资源语言文化特有任务方面的LLM表现,http://arxiv.org/abs/2506.01190v1
932,"Assessing the programming capabilities of Large Language Models (LLMs) is crucial for their effective use in software engineering. Current evaluations, however, predominantly measure the accuracy of generated code on static benchmarks, neglecting the critical aspect of model robustness during programming tasks. While adversarial attacks offer insights on model robustness, their effectiveness is limited and evaluation could be constrained. Current adversarial attack methods for robustness evaluation yield inconsistent results, struggling to provide a unified evaluation across different LLMs. We introduce EVALOOP, a novel assessment framework that evaluate the robustness from a self-consistency perspective, i.e., leveraging the natural duality inherent in popular software engineering tasks, e.g., code generation and code summarization. EVALOOP initiates a self-contained feedback loop: an LLM generates output (e.g., code) from an input (e.g., natural language specification), and then use the generated output as the input to produce a new output (e.g., summarizes that code into a new specification). EVALOOP repeats the process to assess the effectiveness of EVALOOP in each loop. This cyclical strategy intrinsically evaluates robustness without rely on any external attack setups, providing a unified metric to evaluate LLMs' robustness in programming. We evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found that EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1 performance within ten loops. Intriguingly, robustness does not always align with initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo, despite superior initial code generation compared to DeepSeek-V2, demonstrated lower robustness over repeated evaluation loop.",,"Sen Fang, Weiyuan Ding, Bowen Xu",2025-06-01T21:54:31Z,EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency   Perspective,EVALOOP: Bewertung der Robustheit von LLM in der Programmierung aus einer Perspektive der Selbstkonsistenz,EVALOOP: 从自统一的角度评估方案拟订中的LLM强力,http://arxiv.org/abs/2505.12185v2
933,"Grounded text generation models often produce content that deviates from their source material, requiring user verification to ensure accuracy. Existing attribution methods associate entire sentences with source documents, which can be overwhelming for users seeking to fact-check specific claims. In contrast, existing sub-sentence attribution methods may be more precise but fail to align with users' interests. In light of these limitations, we introduce Localized Attribution Queries (LAQuer), a new task that localizes selected spans of generated output to their corresponding source spans, allowing fine-grained and user-directed attribution. We compare two approaches for the LAQuer task, including prompting large language models (LLMs) and leveraging LLM internal representations. We then explore a modeling framework that extends existing attributed text generation methods to LAQuer. We evaluate this framework across two grounded text generation tasks: Multi-document Summarization (MDS) and Long-form Question Answering (LFQA). Our findings show that LAQuer methods significantly reduce the length of the attributed text. Our contributions include: (1) proposing the LAQuer task to enhance attribution usability, (2) suggesting a modeling framework and benchmarking multiple baselines, and (3) proposing a new evaluation setting to promote future research on localized attribution in content-grounded generation.",,"Eran Hirsch, Aviv Slobodkin, David Wan, Elias Stengel-Eskin, Mohit Bansal, Ido Dagan",2025-06-01T21:46:23Z,LAQuer: Localized Attribution Queries in Content-grounded Generation,LAQuer: Lokalisierte Zuordnungsanfragen in der Content-gegründeten Generation,LLAQuer: 以内容为基础的一代中本地化归属查询,http://arxiv.org/abs/2506.01187v1
934,"Fact-checking is a potentially useful application of Large Language Models (LLMs) to combat the growing dissemination of disinformation. However, the performance of LLMs varies across geographic regions. In this paper, we evaluate the factual accuracy of open and private models across a diverse set of regions and scenarios.   Using a dataset containing 600 fact-checked statements balanced across six global regions we examine three experimental setups of fact-checking a statement: (1) when just the statement is available, (2) when an LLM-based agent with Wikipedia access is utilized, and (3) as a best case scenario when a Retrieval-Augmented Generation (RAG) system provided with the official fact check is employed. Our findings reveal that regardless of the scenario and LLM used, including GPT-4, Claude Sonnet, and LLaMA, statements from the Global North perform substantially better than those from the Global South. Furthermore, this gap is broadened for the more realistic case of a Wikipedia agent-based system, highlighting that overly general knowledge bases have a limited ability to address region-specific nuances. These results underscore the urgent need for better dataset balancing and robust retrieval strategies to enhance LLM fact-checking capabilities, particularly in geographically diverse contexts.",,"Bruno Coelho, Shujaat Mirza, Yuyuan Cui, Christina Pöpper, Damon McCoy",2025-06-01T21:44:34Z,Understanding Inequality of LLM Fact-Checking over Geographic Regions   with Agent and Retrieval models,Ungleichbehandlung der LLM-Faktenprüfung über geographische Regionen mit Agenten- und Retrieval-Modellen,了解LLMLM与代理和检索模式对地理区域进行实况调查的不平等,http://arxiv.org/abs/2503.22877v2
935,"The rapid advancement of Large Multimodal Models (LMMs) for 2D images and videos has motivated extending these models to understand 3D scenes, aiming for human-like visual-spatial intelligence. Nevertheless, achieving deep spatial understanding comparable to human capabilities poses significant challenges in model encoding and data acquisition. Existing methods frequently depend on external depth sensors for geometry capture or utilize off-the-shelf algorithms for pre-constructing 3D maps, thereby limiting their scalability, especially with prevalent monocular video inputs and for time-sensitive applications. In this work, we introduce VLM-3R, a unified framework for Vision-Language Models (VLMs) that incorporates 3D Reconstructive instruction tuning. VLM-3R processes monocular video frames by employing a geometry encoder to derive implicit 3D tokens that represent spatial understanding. Leveraging our Spatial-Visual-View Fusion and over 200K curated 3D reconstructive instruction tuning question-answer (QA) pairs, VLM-3R effectively aligns real-world spatial context with language instructions. This enables monocular 3D spatial assistance and embodied reasoning. To facilitate the evaluation of temporal reasoning, we introduce the Vision-Spatial-Temporal Intelligence benchmark, featuring over 138.6K QA pairs across five distinct tasks focused on evolving spatial relationships. Extensive experiments demonstrate that our model, VLM-3R, not only facilitates robust visual-spatial reasoning but also enables the understanding of temporal 3D context changes, excelling in both accuracy and scalability.",,"Zhiwen Fan, Jian Zhang, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan, Hongyu Xu, Justin Theiss, Tianlong Chen, Jiachen Li, Zhengzhong Tu, Zhangyang Wang, Rakesh Ranjan",2025-06-01T21:20:16Z,VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D   Reconstruction,VLM-3R: Vision-Language-Modelle erweitert mit instruction-aligned 3D reconstruction,VLM-3R:通过指示统一3D重建增强的愿景-语言模型,http://arxiv.org/abs/2505.20279v2
936,"In psycholinguistic modeling, surprisal from larger pre-trained language models has been shown to be a poorer predictor of naturalistic human reading times. However, it has been speculated that this may be due to data leakage that caused language models to see the text stimuli during training. This paper presents two studies to address this concern at scale. The first study reveals relatively little leakage of five naturalistic reading time corpora in two pre-training datasets in terms of length and frequency of token $n$-gram overlap. The second study replicates the negative relationship between language model size and the fit of surprisal to reading times using models trained on 'leakage-free' data that overlaps only minimally with the reading time corpora. Taken together, this suggests that previous results using language models trained on these corpora are not driven by the effects of data leakage.",,"Byung-Doh Oh, Hongao Zhu, William Schuler",2025-06-01T21:12:36Z,The Inverse Scaling Effect of Pre-Trained Language Model Surprisal Is   Not Due to Data Leakage,Der inverse Skalierungseffekt des vortrainierten Sprachmodells Surprisal ist nicht auf Datenverlust zurückzuführen.,培训前语言模式模式的反增缩效应不是由于数据渗漏造成的,http://arxiv.org/abs/2506.01172v1
937,"Word-by-word language model surprisal is often used to model the incremental processing of human readers, which raises questions about how various choices in language modeling influence its predictive power. One factor that has been overlooked in cognitive modeling is the granularity of subword tokens, which explicitly encodes information about word length and frequency, and ultimately influences the quality of vector representations that are learned. This paper presents experiments that manipulate the token granularity and evaluate its impact on the ability of surprisal to account for processing difficulty of naturalistic text and garden-path constructions. Experiments with naturalistic reading times reveal a substantial influence of token granularity on surprisal, with tokens defined by a vocabulary size of 8,000 resulting in surprisal that is most predictive. In contrast, on garden-path constructions, language models trained on coarser-grained tokens generally assigned higher surprisal to critical regions, suggesting a greater sensitivity to garden-path effects than previously reported. Taken together, these results suggest a large role of token granularity on the quality of language model surprisal for cognitive modeling.",,"Byung-Doh Oh, William Schuler",2025-06-01T21:05:19Z,The Impact of Token Granularity on the Predictive Power of Language   Model Surprisal,Der Einfluss von Token Granularity auf die vorausschauende Kraft der Surprisal des Sprachmodells,托肯粒子对语言模型外质的预测力的影响,http://arxiv.org/abs/2412.11940v2
938,"Large Language Models (LLMs) pruning seeks to remove unimportant weights for inference speedup with minimal accuracy impact. However, existing methods often suffer from accuracy degradation without full-model sparsity-aware fine-tuning. This paper presents Wanda++, a novel pruning framework that outperforms the state-of-the-art methods by utilizing decoder-block-level \textbf{regional} gradients. Specifically, Wanda++ improves the pruning score with regional gradients for the first time and proposes an efficient regional optimization method to minimize pruning-induced output discrepancies between the dense and sparse decoder output. Notably, Wanda++ improves perplexity by up to 32\% over Wanda in the language modeling task and generalizes effectively to downstream tasks. Moreover, despite updating weights with regional optimization, Wanda++ remains orthogonal to sparsity-aware fine-tuning, further reducing perplexity with LoRA in great extend. Our approach is lightweight, pruning a 7B LLaMA model in under 10 minutes on a single H100 GPU.",,"Yifan Yang, Kai Zhen, Bhavana Ganesh, Aram Galstyan, Goeric Huybrechts, Markus Müller, Jonas M. Kübler, Rupak Vignesh Swaminathan, Athanasios Mouchtaris, Sravan Babu Bodapati, Nathan Susanj, Zheng Zhang, Jack FitzGerald, Abhishek Kumar",2025-06-01T20:48:29Z,Wanda++: Pruning Large Language Models via Regional Gradients,Wanda++: Beschneiden großer Sprachmodelle über regionale Gradienten,Wanda+++:通过区域渐变来保护大语言模式,http://arxiv.org/abs/2503.04992v4
939,"Language models excel at following instructions but often struggle with the collaborative aspects of conversation that humans naturally employ. This limitation in grounding -- the process by which conversation participants establish mutual understanding -- can lead to outcomes ranging from frustrated users to serious consequences in high-stakes scenarios. To systematically study grounding challenges in human-LLM interactions, we analyze logs from three human-assistant datasets: WildChat, MultiWOZ, and Bing Chat. We develop a taxonomy of grounding acts and build models to annotate and forecast grounding behavior. Our findings reveal significant differences in human-human and human-LLM grounding: LLMs were three times less likely to initiate clarification and sixteen times less likely to provide follow-up requests than humans. Additionally, we find that early grounding failures predict later interaction breakdowns. Building on these insights, we introduce Rifts, a benchmark derived from publicly available LLM interaction data containing situations where LLMs fail to initiate grounding. We note that current frontier models perform poorly on Rifts, highlighting the need to reconsider how we train and prompt LLMs for human interaction. To this end, we develop a preliminary intervention aimed at mitigating grounding failures.",,"Omar Shaikh, Hussein Mozannar, Gagan Bansal, Adam Fourney, Eric Horvitz",2025-06-01T20:44:56Z,Navigating Rifts in Human-LLM Grounding: Study and Benchmark,Navigieren von Rifts in der Mensch-LLM-Erdung: Studie und Benchmark,载人LLLM定地裂谷导航:研究和基准,http://arxiv.org/abs/2503.13975v2
940,"Mispronunciation detection (MD) models are the cornerstones of many language learning applications. Unfortunately, most systems are built for English and other major languages, while low-resourced language varieties, such as Finland Swedish (FS), lack such tools. In this paper, we introduce our MD model for FS, trained on 89 hours of first language (L1) speakers' spontaneous speech and tested on 33 minutes of L2 transcribed read-aloud speech.   We trained a multilingual wav2vec 2.0 model with entropy regularization, followed by temperature scaling and top-k normalization after the inference to better adapt it for MD. The main novelty of our method lies in its simplicity, requiring minimal L2 data. The process is also language-independent, making it suitable for other low-resource languages. Our proposed algorithm allows us to balance Recall (43.2%) and Precision (29.8%), compared with the baseline model's Recall (77.5%) and Precision (17.6%).",,"Nhan Phan, Mikko Kuronen, Maria Kautonen, Riikka Ullakonoja, Anna von Zansen, Yaroslav Getman, Ekaterina Voskoboinik, Tamás Grósz, Mikko Kurimo",2025-06-01T20:28:35Z,Mispronunciation Detection Without L2 Pronunciation Dataset in   Low-Resource Setting: A Case Study in Finland Swedish,Mispronunciation Detection without L2 Pronunciation Dataset in Low-Resource Setting: Eine Fallstudie in Finnland Schwedisch,低资源设置中的低资源条件下没有L2发音数据集的读音失灵检测:芬兰的案例研究,http://arxiv.org/abs/2506.01156v1
941,"Real-world data, such as news articles, social media posts, and chatbot conversations, is inherently dynamic and non-stationary, presenting significant challenges for constructing real-time structured representations through knowledge graphs (KGs). Relation Extraction (RE), a fundamental component of KG creation, often struggles to adapt to evolving data when traditional models rely on static, outdated datasets. Continual Relation Extraction (CRE) methods tackle this issue by incrementally learning new relations while preserving previously acquired knowledge. This study investigates the application of pre-trained language models (PLMs), specifically large language models (LLMs), to CRE, with a focus on leveraging memory replay to address catastrophic forgetting. We evaluate decoder-only models (eg, Mistral-7B and Llama2-7B) and encoder-decoder models (eg, Flan-T5 Base) on the TACRED and FewRel datasets. Task-incremental fine-tuning of LLMs demonstrates superior performance over earlier approaches using encoder-only models like BERT on TACRED, excelling in seen-task accuracy and overall performance (measured by whole and average accuracy), particularly with the Mistral and Flan-T5 models. Results on FewRel are similarly promising, achieving second place in whole and average accuracy metrics. This work underscores critical factors in knowledge transfer, language model architecture, and KG completeness, advancing CRE with LLMs and memory replay for dynamic, real-time relation extraction.",,"Sefika Efeoglu, Adrian Paschke, Sonja Schimmler",2025-06-01T20:19:20Z,Post-Training Language Models for Continual Relation Extraction,Post-Training-Sprachmodelle für die kontinuierliche Beziehungsextraktion,培训后持续关系采掘语言模式,http://arxiv.org/abs/2504.05214v2
942,"Recent reasoning models, such as OpenAI's O1 series, have demonstrated exceptional performance on complex reasoning tasks and revealed new test-time scaling laws. Inspired by this, many people have been studying how to train models to achieve effective self-evaluation and self-correction to further enable the scaling paradigm. However, less studied is how to efficiently scale test-time compute from a fixed model, and this remains a challenge. In this paper, we address this challenge by focusing on enhancing the quality of self-reflection data generation for complex problem-solving at test time, which can also subsequently improve the training of next-generation large language models (LLMs). Specifically, we explore how systematically triggering a model's self-correction mechanisms can improve performance on challenging reasoning tasks. To this end, we propose a novel iterative deepening sampling algorithm framework designed to enhance self-correction and generate higher-quality samples. Through extensive experiments on Math500 and AIME benchmarks, we demonstrate that our method achieves a higher success rate on difficult tasks and provide detailed ablation studies to analyze its effectiveness across diverse settings.",,"Weizhe Chen, Sven Koenig, Bistra Dilkina",2025-06-01T20:14:16Z,Iterative Deepening Sampling as Efficient Test-Time Scaling,Iterative Vertiefung der Probenahme als effizientes Testzeitskalieren,高效试验时间缩放式的迭代深化采样,http://arxiv.org/abs/2502.05449v2
943,"Large Language Models (LLMs) have shown remarkable capabilities, yet ensuring their outputs conform to strict structural or grammatical constraints remains challenging, which is critical in function calls and domain-specific language (DSL) generation. Constrained decoding with context-free grammar is a flexible approach to guarantee LLMs' adherence to a specific format by dynamically building a token logits mask. However, creating this mask requires checking the validity of all tokens in the LLM vocabulary at every decoding step, which often incurs significant overheads in existing constrained decoding engines. To address this challenge, we propose $\textbf{ZapFormat}$, a novel $\textbf{dynamic pruning}$ strategy based on the Earley algorithm that identifies and eliminates invalid or redundant Earley states in real-time, significantly reducing memory occupation of the Earley algorithm's states. This further enables us to use a state cache to speed up structured generations on a large number of queries. We implemented ZapFormat in a new constrained decoding engine called Formatron which also incorporates existing optimizations. Through comprehensive experiments on structured generation tasks, including JSON generation, JSON Schema validation, and semantic parsing, we demonstrate that Formatron not only $\textbf{consistently maintains}$ high-precision compliant outputs but also achieves $\textbf{significant improvements}$ in inference speed up to 2x compared to state-of-the-art implementations. More importantly, Formatron is generally applicable across various LLM architectures. We release Formatron as open source at https://github.com/Dan-wanna-M/formatron.",,"Xintong Sun, Chi Wei, Minghao Tian, Shiwen Ni",2025-06-01T20:05:30Z,Earley-Driven Dynamic Pruning for Efficient Structured Decoding,Earley-Driven Dynamic Pruning für effiziente strukturierte Dekodierung,Earre-Driven 高效结构化解码的动态保护,http://arxiv.org/abs/2506.01151v1
944,"System-generated logs are typically converted into categorical log templates through parsing. These templates are crucial for generating actionable insights in various downstream tasks. However, existing parsers often fail to capture fine-grained template details, leading to suboptimal accuracy and reduced utility in downstream tasks requiring precise pattern identification. We propose a character-level log parser utilizing a novel neural architecture that aggregates character embeddings. Our approach estimates a sequence of binary-coded decimals to achieve highly granular log templates extraction. Our low-resource character-level parser, tested on revised Loghub-2k and a manually annotated industrial dataset, matches LLM-based parsers in accuracy while outperforming semantic parsers in efficiency.",,"Prerak Srivastava, Giulio Corallo, Sergey Rybalko",2025-06-01T20:00:00Z,A Word is Worth 4-bit: Efficient Log Parsing with Binary Coded Decimal   Recognition,Ein Wort ist 4 Bit wert: Effizientes Log Parsing mit Binärcoded Dezimalerkennung,A Word 值 4 位元: 具有二进制编码十进制识别的高效日志分割,http://arxiv.org/abs/2506.01147v1
945,"English-centric large language models (LLMs) often show strong multilingual capabilities. However, their multilingual performance remains unclear and is under-evaluated for many other languages. Most benchmarks for multilinguality focus on classic NLP tasks or cover a minimal number of languages. We introduce MEXA, a method for assessing the multilingual capabilities of pre-trained English-centric LLMs using parallel sentences, which are available for more languages than existing downstream tasks. MEXA leverages that English-centric LLMs use English as a pivot language in their intermediate layers. MEXA computes the alignment between English and non-English languages using parallel sentences to evaluate the transfer of language understanding from English to other languages. This alignment can be used to estimate model performance in different languages. We conduct controlled experiments using various parallel datasets (FLORES-200 and Bible), models (Llama family, Gemma family, Mistral, and OLMo), and established downstream tasks (Belebele, m-MMLU, and m-ARC). We explore different methods to compute embeddings in decoder-only models. Our results show that MEXA, in its default settings, achieves an average Pearson correlation of 0.90 between its predicted scores and actual task performance across languages. This suggests that MEXA is a reliable method for estimating the multilingual capabilities of English-centric LLMs, providing a clearer understanding of their multilingual potential and the inner workings of LLMs. Leaderboard: https://cis-lmu-mexa.hf.space, Code: https://github.com/cisnlp/MEXA.",,"Amir Hossein Kargaran, Ali Modarressi, Nafiseh Nikeghbal, Jana Diesner, François Yvon, Hinrich Schütze",2025-06-01T19:44:36Z,MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual   Alignment,MEXA: Mehrsprachige Auswertung von Englisch-Zentrischen LLMs über Cross-Lingual Alignment,MEXA:通过跨语言协调对英语-英语LMLMs进行多语种评价,http://arxiv.org/abs/2410.05873v2
946,"The emergence of large language models (LLMs) has demonstrated that systems trained solely on text can acquire extensive world knowledge, develop reasoning capabilities, and internalize abstract semantic concepts--showcasing properties that can be associated with general intelligence. This raises an intriguing question: Do such concepts emerge in models trained on other modalities, such as speech? Furthermore, when models are trained jointly on multiple modalities: Do they develop a richer, more structured semantic understanding? To explore this, we analyze the conceptual structures learned by speech and textual models both individually and jointly. We employ Latent Concept Analysis, an unsupervised method for uncovering and interpreting latent representations in neural networks, to examine how semantic abstractions form across modalities. For reproducibility we made scripts and other resources available to the community.",,"Asım Ersoy, Basel Mousi, Shammur Chowdhury, Firoj Alam, Fahim Dalvi, Nadir Durrani",2025-06-01T19:33:21Z,From Words to Waves: Analyzing Concept Formation in Speech and   Text-Based Foundation Models,Von Worten zu Wellen: Analysieren der Konzeptbildung in sprachlichen und textbasierten Grundmodellen,从文字到浪潮:分析演讲和基于文本的基金会模型中的概念形成,http://arxiv.org/abs/2506.01133v1
947,"The autoregressive decoding for text generation in large language models (LLMs), while widely used, is inherently suboptimal due to the lack of a built-in mechanism to perform refinement and/or correction of the generated content. In this paper, we consider optimality in terms of the joint probability over the generated response, when jointly considering all tokens at the same time. We theoretically characterize the potential deviation of the autoregressively generated response from its globally optimal counterpart that is of the same length. Our analysis suggests that we need to be cautious when noticeable uncertainty arises during text generation, which may signal the sub-optimality of the generation history. To address the pitfall of autoregressive decoding for text generation, we propose an approach that incorporates a sliding reflection window and a pausing criterion, such that refinement and generation can be carried out interchangeably as the decoding proceeds. Our selective refinement framework strikes a balance between efficiency and optimality, and our extensive experimental results demonstrate the effectiveness of our approach.",,"Zeyu Tang, Zhenhao Chen, Xiangchen Song, Loka Li, Yunlong Deng, Yifan Shen, Guangyi Chen, Peter Spirtes, Kun Zhang",2025-06-01T19:23:43Z,Reflection-Window Decoding: Text Generation with Selective Refinement,Reflexions-Fenster-Dekodierung: Textgenerierung mit selektiver Veredelung,反射窗口解码:有选择性精炼的文本生成,http://arxiv.org/abs/2502.03678v3
948,"The Transformer architecture is central to the success of modern Large Language Models (LLMs), in part due to its surprising ability to perform a wide range of algorithmic tasks -- including mathematical reasoning, memorization, and retrieval -- using only gradient-based training on next-token prediction. While the core component of a Transformer is the self-attention mechanism, we question how much, and which aspects, of the performance gains can be attributed to it. To this end, we compare standard Transformers to variants in which either the multi-layer perceptron (MLP) layers or the attention projectors (queries and keys) are frozen at initialization. To further isolate the contribution of attention, we introduce MixiT -- the Mixing Transformer -- a simplified, principled model in which the attention coefficients are entirely random and fixed at initialization, eliminating any input-dependent computation or learning in attention. Surprisingly, we find that MixiT matches the performance of fully trained Transformers on various algorithmic tasks, especially those involving basic arithmetic or focusing heavily on memorization. For retrieval-based tasks, we observe that having input-dependent attention coefficients is consistently beneficial, while MixiT underperforms. We attribute this failure to its inability to form specialized circuits such as induction heads -- a specific circuit known to be crucial for learning and exploiting repeating patterns in input sequences. Even more interestingly, we find that attention with frozen key and query projectors is not only able to form induction heads, but can also perform competitively on language modeling. Our results underscore the importance of architectural heterogeneity, where distinct components contribute complementary inductive biases crucial for solving different classes of tasks.",,"Yihe Dong, Lorenzo Noci, Mikhail Khodak, Mufan Li",2025-06-01T18:42:39Z,"Attention Retrieves, MLP Memorizes: Disentangling Trainable Components   in the Transformer","Aufmerksamkeit ruft, MLP-Erinnerungen: Entwirren von trainierbaren Komponenten im Transformer","注意检索, MLP 记忆: 变换器中拆分可训练部件",http://arxiv.org/abs/2506.01115v1
949,"Recently, long chain of thought (LCoT), Large Language Models (LLMs), have taken the machine learning world by storm with their breathtaking reasoning capabilities. However, are the abstract reasoning abilities of these models general enough for problems of practical importance? Unlike past work, which has focused mainly on math, coding, and data wrangling, we focus on a historical linguistics-inspired inductive reasoning problem, formulated as Programming by Examples. We develop a fully automated pipeline for dynamically generating a benchmark for this task with controllable difficulty in order to tackle scalability and contamination issues to which many reasoning benchmarks are subject. Using our pipeline, we generate a test set with nearly 1k instances that is challenging for all state-of-the-art reasoning LLMs, with the best model (Claude-3.7-Sonnet) achieving a mere 54% pass rate, demonstrating that LCoT LLMs still struggle with a class or reasoning that is ubiquitous in historical linguistics as well as many other domains.",,"Atharva Naik, Darsh Agrawal, Manav Kapadnis, Yuwei An, Yash Mathur, Carolyn Rose, David Mortensen",2025-06-01T18:35:39Z,PBEBench: A Multi-Step Programming by Examples Reasoning Benchmark   inspired by Historical Linguistics,"PBEBench: Ein mehrstufiges Programmieren nach Beispielen, inspiriert von historischer Linguistik",PBEBench:根据历史语言推导的多层次方案拟定工作,http://arxiv.org/abs/2505.23126v2
950,"The pervasive deployment of large language models (LLMs) in conversational AI systems has revolutionized information access, yet their propensity for generating factually unsupported or hallucinated responses remains a critical impediment to trustworthiness and widespread adoption. This paper introduces Reinforced Unanswerability Learning (RUL), a novel hybrid training paradigm designed to imbue LLMs with the intrinsic capability to accurately detect unanswerable questions and generate reliably appropriate responses. Unlike conventional approaches that rely on external classifiers or simple prompting, RUL integrates a discriminative unanswerability prediction head with the LLM's generative core, guided by a multi-stage learning strategy. This includes supervised fine-tuning on a novel, richly annotated dataset, Enhanced-CAsT-Answerability (ECA), which features hierarchical answerability labels and ground-truth refusal responses. Crucially, RUL incorporates a subsequent reinforcement learning with human feedback (RLHF) phase to refine the nuance, helpfulness, and informativeness of refusal responses. Extensive experiments demonstrate RUL's superior performance, achieving significantly higher accuracy in unanswerability detection across sentence, paragraph, and ranking levels, and substantially increasing the generation of appropriate refusals for unanswerable queries, alongside strong performance on answerable questions. Human evaluations further corroborate RUL's effectiveness, highlighting a marked improvement in perceived helpfulness and trustworthiness, ultimately paving the way for more reliable and user-centric conversational AI.",,"Steven Robinson, Antonio Carlos Rivera",2025-06-01T17:59:27Z,Contextual Candor: Enhancing LLM Trustworthiness Through Hierarchical   Unanswerability Detection,Kontextuelle Candor: Verbesserung der LLM-Treuhandschaft durch hierarchische Unbeantwortbarkeitserkennung,通过等级式无法回答的探测提高LLM的可信赖性,http://arxiv.org/abs/2506.01104v1
951,"We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that enhances multimodal image-text retrieval while preserving the original model's strong zero-shot classification capabilities. CLIP models are typically constrained by fixed image resolutions and limited context, which can hinder their effectiveness in retrieval tasks that require fine-grained cross-modal understanding. DCLIP addresses these challenges through a meta teacher-student distillation framework, where a cross-modal transformer teacher is fine-tuned to produce enriched embeddings via bidirectional cross-attention between YOLO-extracted image regions and corresponding textual spans. These semantically and spatially aligned global representations guide the training of a lightweight student model using a hybrid loss that combines contrastive learning and cosine similarity objectives. Despite being trained on only ~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a fraction of CLIP's original dataset-DCLIP significantly improves image-text retrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's zero-shot classification performance. These results demonstrate that DCLIP effectively mitigates the trade-off between task specialization and generalization, offering a resource-efficient, domain-adaptive, and detail-sensitive solution for advanced vision-language tasks. Code available at https://anonymous.4open.science/r/DCLIP-B772/README.md.",,"Daniel Csizmadia, Andrei Codreanu, Victor Sim, Vighnesh Prabhu, Michael Lu, Kevin Zhu, Sean O'Brien, Vasu Sharma",2025-06-01T17:56:32Z,Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal   Transformer Distillation,Destill CLIP (DCLIP): Bild-Text-Retrieval durch Cross-Modal Transformer-Destillation verbessern,蒸馏 CLIP (DCLIP): 通过跨模式变异器蒸馏加强图像- 文本回收,http://arxiv.org/abs/2505.21549v3
952,"The conversational capabilities of Large Language Models (LLMs) suggest that they may be able to perform as automated talk therapists. It is crucial to know if these systems would be effective and adhere to known standards. We present a counsellor chatbot that focuses on motivating tobacco smokers to quit smoking. It uses a state-of-the-art LLM and a widely applied therapeutic approach called Motivational Interviewing (MI), and was evolved in collaboration with clinician-scientists with expertise in MI. We also describe and validate an automated assessment of both the chatbot's adherence to MI and client responses. The chatbot was tested on 106 participants, and their confidence that they could succeed in quitting smoking was measured before the conversation and one week later. Participants' confidence increased by an average of 1.7 on a 0-10 scale. The automated assessment of the chatbot showed adherence to MI standards in 98% of utterances, higher than human counsellors. The chatbot scored well on a participant-reported metric of perceived empathy but lower than typical human counsellors. Furthermore, participants' language indicated a good level of motivation to change, a key goal in MI. These results suggest that the automation of talk therapy with a modern LLM has promise.",,"Zafarullah Mahmood, Soliman Ali, Jiading Zhu, Mohamed Abdelwahab, Michelle Yu Collins, Sihan Chen, Yi Cheng Zhao, Jodi Wolff, Osnat Melamed, Nadia Minian, Marta Maslej, Carolynne Cooper, Matt Ratto, Peter Selby, Jonathan Rose",2025-06-01T17:47:38Z,A Fully Generative Motivational Interviewing Counsellor Chatbot for   Moving Smokers Towards the Decision to Quit,Ein voll generativer Motivationsgespräch Berater Chatbot für den Umzug Raucher auf dem Weg zu der Entscheidung zu beenden,全面创造动机的访谈参赞Chatbot 移动吸烟者争取决定退出,http://arxiv.org/abs/2505.17362v3
953,"Large Language Models (LLMs) have demonstrated impressive performances in tasks related to coreference resolution. However, previous studies mostly assessed LLM performance on coreference resolution with nouns and third person pronouns. This study evaluates LLM performance on coreference resolution with indexical like I, you, here and tomorrow, which come with unique challenges due to their linguistic properties. We present the first study examining how LLMs interpret indexicals in English, releasing the English Indexical Dataset with 1600 multiple-choice questions. We evaluate pioneering LLMs, including GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro, and DeepSeek V3. Our results reveal that LLMs exhibit an impressive performance with some indexicals (I), while struggling with others (you, here, tomorrow), and that syntactic cues (e.g. quotation) contribute to LLM performance with some indexicals, while they reduce performance with others. Code and data are available at: https://github.com/metehanoguzz/LLMs-Indexicals-English.",,"Metehan Oguz, Yavuz Bakman, Duygu Nur Yaldiz",2025-06-01T17:21:49Z,Un-considering Contextual Information: Assessing LLMs' Understanding of   Indexical Elements,Unüberlegte Kontextinformationen: Bewertung des Verständnisses von indexischen Elementen durch LLMs,不考虑背景信息:评估LLM女士对索引要素的理解,http://arxiv.org/abs/2506.01089v1
954,"Background: Deception detection through analysing language is a promising avenue using both human judgments and automated machine learning judgments. For both forms of credibility assessment, automated adversarial attacks that rewrite deceptive statements to appear truthful pose a serious threat. Methods: We used a dataset of 243 truthful and 262 fabricated autobiographical stories in a deception detection task for humans and machine learning models. A large language model was tasked to rewrite deceptive statements so that they appear truthful. In Study 1, humans who made a deception judgment or used the detailedness heuristic and two machine learning models (a fine-tuned language model and a simple n-gram model) judged original or adversarial modifications of deceptive statements. In Study 2, we manipulated the target alignment of the modifications, i.e. tailoring the attack to whether the statements would be assessed by humans or computer models. Results: When adversarial modifications were aligned with their target, human (d=-0.07 and d=-0.04) and machine judgments (51% accuracy) dropped to the chance level. When the attack was not aligned with the target, both human heuristics judgments (d=0.30 and d=0.36) and machine learning predictions (63-78%) were significantly better than chance. Conclusions: Easily accessible language models can effectively help anyone fake deception detection efforts both by humans and machine learning models. Robustness against adversarial modifications for humans and machines depends on that target alignment. We close with suggestions on advancing deception research with adversarial attack designs and techniques.",,"Bennett Kleinberg, Riccardo Loconte, Bruno Verschuere",2025-06-01T17:16:18Z,Effective faking of verbal deception detection with target-aligned   adversarial attacks,Effektive Fälschung der verbalen Täuschungserkennung mit zielorientierten konversarischen Angriffen,以与目标对准的对抗性攻击有效地假冒口头欺骗检测,http://arxiv.org/abs/2501.05962v2
955,"Analogical reasoning is a unique ability of humans to address unfamiliar challenges by transferring strategies from relevant past experiences. One key finding in psychology is that compared with irrelevant past experiences, recalling relevant ones can help humans better handle new tasks. Coincidentally, the NLP community has also recently found that self-generating relevant examples in the context can help large language models (LLMs) better solve a given problem than hand-crafted prompts. However, it is yet not clear whether relevance is the key factor eliciting such capability, i.e., can LLMs benefit more from self-generated relevant examples than irrelevant ones? In this work, we systematically explore whether LLMs can truly perform analogical reasoning on a diverse set of reasoning tasks. With extensive experiments and analysis, we show that self-generated random examples can surprisingly achieve comparable or even better performance on certain tasks, e.g., 4% performance boost on GSM8K with random biological examples. We find that the accuracy of self-generated examples is the key factor and subsequently design two novel methods with improved performance and significantly reduced inference costs. Overall, we aim to advance a deeper understanding of LLM analogical reasoning and hope this work stimulates further research in the design of self-generated contexts.",,"Chengwei Qin, Wenhan Xia, Tan Wang, Fangkai Jiao, Yuchen Hu, Bosheng Ding, Ruirui Chen, Shafiq Joty",2025-06-01T17:07:35Z,Relevant or Random: Can LLMs Truly Perform Analogical Reasoning?,Relevante oder zufällige: Können LLMs wirklich analogische Vernunft anwenden?,相关或随机:LLLMs能否真正执行分析理由?,http://arxiv.org/abs/2404.12728v3
956,"Tokenization efficiency plays a critical role in the performance and cost of large language models (LLMs), yet most models rely on static tokenizers optimized for general-purpose corpora. These tokenizers' fixed vocabularies often fail to adapt to domain- or language-specific inputs, leading to longer token sequences and higher computational costs. We introduce zip2zip, a framework that enables LLMs to dynamically adjust token vocabulary at inference time, allowing for fewer generated tokens and thus faster inference. zip2zip consists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch (LZW) compression that incrementally compresses tokens into reusable ""hypertokens"" on the fly; (2) an embedding layer that computes embeddings for newly formed hypertokens at runtime; and (3) a causal language modeling variant that trains the model to operate on hypertokenized, compressed sequences. We show that an existing LLM can be zip2zip-fied in 10 GPU-hours via parameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to use hypertokens at inference time, reducing input and output sequence length by 20-60\%, with significant improvements in inference latency.",,"Saibo Geng, Nathan Ranchin, Yunzhen yao, Maxime Peyrard, Chris Wendler, Michael Gastpar, Robert West",2025-06-01T17:03:02Z,zip2zip: Inference-Time Adaptive Vocabularies for Language Models via   Token Compression,zip2zip: Inferenz-Time Adaptive Vokabeln für Sprachmodelle über Token Compression,Liz2zip: 通过 Token 压缩对语言模型进行推论- 时间适应性词汇表,http://arxiv.org/abs/2506.01084v1
957,"Large language models (LLMs) have shown impressive few-shot generalization on many tasks via in-context learning (ICL). Despite their success in showing such emergent abilities, the scale and complexity of larger models also lead to unprecedentedly high computational demands and deployment challenges. In reaction, researchers explore transferring the powerful capabilities of larger models to more efficient and compact models by typically aligning the output of smaller (student) models with that of larger (teacher) models. Existing methods either train student models on the generated outputs of teacher models or imitate their token-level probability distributions. However, these distillation methods pay little to no attention to the input, which also plays a crucial role in ICL. Based on the finding that the performance of ICL is highly sensitive to the selection of demonstration examples, we propose Bidirectional Alignment (BiAlign) to fully leverage the models' preferences for ICL examples to improve the ICL abilities of student models. Specifically, we introduce the alignment of input preferences between student and teacher models by incorporating a novel ranking loss, in addition to aligning the token-level output distribution. With extensive experiments and analysis, we demonstrate that BiAlign can consistently outperform existing baselines on a variety of tasks involving language understanding, reasoning, and coding.",,"Chengwei Qin, Wenhan Xia, Fangkai Jiao, Chen Chen, Yuchen Hu, Bosheng Ding, Ruirui Chen, Shafiq Joty",2025-06-01T17:02:02Z,Beyond Output Matching: Bidirectional Alignment for Enhanced In-Context   Learning,Jenseits von Output Matching: Bidirektionale Ausrichtung für verbessertes In-Context-Lernen,超越输出匹配:加强内文学习的双向对齐,http://arxiv.org/abs/2312.17055v3
958,"Large Language Models (LLMs) have shown remarkable advancements in specialized fields such as finance, law, and medicine. However, in cybersecurity, we have noticed a lack of open-source datasets, with a particular lack of high-quality cybersecurity pretraining corpora, even though much research indicates that LLMs acquire their knowledge during pretraining. To address this, we present a comprehensive suite of datasets covering all major training stages, including pretraining, instruction fine-tuning, and reasoning distillation with cybersecurity-specific self-reflection data. Extensive ablation studies demonstrate their effectiveness on public cybersecurity benchmarks. In particular, continual pre-training on our dataset yields a 15.88% improvement in the aggregate score, while reasoning distillation leads to a 10% gain in security certification (CISSP). We will release all datasets and trained cybersecurity LLMs under the ODC-BY and MIT licenses to encourage further research in the community. For access to all datasets and model weights, please refer to https://huggingface.co/collections/trendmicro-ailab/primus-67b1fd27052b802b4af9d243.",,"Yao-Ching Yu, Tsun-Han Chiang, Cheng-Wei Tsai, Chien-Ming Huang, Wen-Kwang Tsao",2025-06-01T16:28:01Z,Primus: A Pioneering Collection of Open-Source Datasets for   Cybersecurity LLM Training,Primus: Eine bahnbrechende Sammlung von Open-Source-Datensätzen für Cybersecurity LLM Training,Primus:网络安全LLM培训开放源数据集先导集,http://arxiv.org/abs/2502.11191v2
959,"Several studies have explored the mechanisms of large language models (LLMs) in coding tasks, but most have focused on programming languages (PLs) in a monolingual setting. In this paper, we investigate the relationship between multiple PLs and English in the concept space of LLMs. We perform a few-shot translation task on 21 PL pairs using two Llama-based models. By decoding the embeddings of intermediate layers during this task, we observe that the concept space is closer to English (including PL keywords) and assigns high probabilities to English tokens in the second half of the intermediate layers. We analyze neuron activations for 11 PLs and English, finding that while language-specific neurons are primarily concentrated in the bottom layers, those exclusive to each PL tend to appear in the top layers. For PLs that are highly aligned with multiple other PLs, identifying language-specific neurons is not feasible. These PLs also tend to have a larger keyword set than other PLs and are closer to the model's concept space regardless of the input/output PL in the translation task. Our findings provide insights into how LLMs internally represent PLs, revealing structural patterns in the model's concept space. Code is available at https://github.com/cisnlp/code-specific-neurons.",,"Amir Hossein Kargaran, Yihong Liu, François Yvon, Hinrich Schütze",2025-06-01T16:24:13Z,How Programming Concepts and Neurons Are Shared in Code Language Models,Wie Programmierkonzepte und Neuronen in Code Language Models geteilt werden,如何在代码语言模式中共享编程概念和新内容,http://arxiv.org/abs/2506.01074v1
960,"Finetuning language models (LMs) is crucial for adapting the models to downstream data and tasks. However, full finetuning is usually costly. Existing work, such as parameter-efficient finetuning (PEFT), often focuses on \textit{how to finetune} but neglects the issue of \textit{where to finetune}. As a pioneering work on reducing the cost of backpropagation (at the layer level) by answering where to finetune, we conduct a semantic analysis of the LM inference process. We first propose using transition traces of the latent representation to compute deviations (or loss). Then, using a derived formula of scaling law, we estimate the gain of each layer in reducing deviation (or loss). Further, we narrow down the scope for finetuning, and also, study the cost-benefit balance of LM finetuning. We perform extensive experiments across well-known LMs and datasets. The results show that our approach is effective and efficient, and outperforms the existing baselines. Our approach is orthogonal to other techniques for improving finetuning efficiency, such as PEFT methods, offering practical values on LM finetuning.",,"Jian Gu, Aldeida Aleti, Chunyang Chen, Hongyu Zhang",2025-06-01T16:10:59Z,A Semantic-Aware Layer-Freezing Approach to Computation-Efficient   Fine-Tuning of Language Models,Ein Semantisch-Bewusst-Layer-Freezing-Ansatz zur Berechnung-Effizienter Feintuning von Sprachmodellen,语言模型计算-有效精精美精美设计方法的语义-警示-冻结图层的方法,http://arxiv.org/abs/2406.11753v3
961,"We introduce SealQA, a new challenge benchmark for evaluating SEarch-Augmented Language models on fact-seeking questions where web search yields conflicting, noisy, or unhelpful results. SealQA comes in three flavors: (1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and reasoning capabilities, with Seal-0 focusing on the most challenging questions where chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3) LongSeal, which extends SealQA to test long-context, multi-document reasoning in ""needle-in-a-haystack"" settings. Our evaluation reveals critical limitations in current models: Even frontier LLMs perform poorly across all SealQA flavors. On Seal-0, frontier agentic models equipped with tools like o3 and o4-mini achieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning efforts. We find that advanced reasoning models such as DeepSeek-R1-671B and o3-mini are highly vulnerable to noisy search results. Notably, increasing test-time compute does not yield reliable gains across o3-mini, o4-mini, and o3, with performance often plateauing or even declining early. Additionally, while recent models are less affected by the ""lost-in-the-middle"" issue, they still fail to reliably identify relevant documents in LongSeal when faced with numerous distractors. To facilitate future work, we release SealQA at huggingface.co/datasets/vtllms/sealqa.",,"Thinh Pham, Nguyen Nguyen, Pratibha Zunjare, Weiyuan Chen, Yu-Min Tseng, Tu Vu",2025-06-01T16:04:34Z,SealQA: Raising the Bar for Reasoning in Search-Augmented Language   Models,SealQA: Anhebung der Messlatte für die Vernunft in Search-Augmented Language Models,SealQA: 提高搜索增强语言模式中的原因栏,http://arxiv.org/abs/2506.01062v1
962,"The integration of large language model (LLM) and data management (DATA) is rapidly redefining both domains. In this survey, we comprehensively review the bidirectional relationships. On the one hand, DATA4LLM, spanning large-scale data processing, storage, and serving, feeds LLMs with high quality, diversity, and timeliness of data required for stages like pre-training, post-training, retrieval-augmented generation, and agentic workflows: (i) Data processing for LLMs includes scalable acquisition, deduplication, filtering, selection, domain mixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on efficient data and model formats, distributed and heterogeneous storage hierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data serving for LLMs tackles challenges in RAG (e.g., knowledge post-processing), LLM inference (e.g., prompt compression, data provenance), and training strategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA, LLMs are emerging as general-purpose engines for data management. We review recent advances in (i) data manipulation, including automatic data cleaning, integration, discovery; (ii) data analysis, covering reasoning over structured, semi-structured, and unstructured data, and (iii) system optimization (e.g., configuration tuning, query rewriting, anomaly diagnosis), powered by LLM techniques like retrieval-augmented prompting, task-specialized fine-tuning, and multi-agent collaboration.",,"Xuanhe Zhou, Junxuan He, Wei Zhou, Haodong Chen, Zirui Tang, Haoyu Zhao, Xin Tong, Guoliang Li, Youmin Chen, Jun Zhou, Zhaojun Sun, Binyuan Hui, Shuo Wang, Conghui He, Zhiyuan Liu, Jingren Zhou, Fan Wu",2025-06-01T16:00:34Z,A Survey of LLM $\times$ DATA,Eine Umfrage über LLM $\times$ DATEN,对LLLM 美元-美元-美元-美元-数据数据的调查,http://arxiv.org/abs/2505.18458v3
963,"Many challenging reasoning tasks require not just rapid, intuitive responses, but a more deliberate, multi-step approach. Recent progress in large language models (LLMs) highlights an important shift from the ""System 1"" way of quick reactions to the ""System 2"" style of reflection-and-correction problem solving. However, current benchmarks heavily rely on the final-answer accuracy, leaving much of a model's intermediate reasoning steps unexamined. This fails to assess the model's ability to reflect and rectify mistakes within the reasoning process. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmark for fine-grained evaluation of LLMs' reasoning capabilities. Each puzzle can be decomposed into atomic steps, making it ideal for rigorous validation of intermediate correctness. Building on this, we introduce two tasks: state checking, and state transition, for a comprehensive evaluation of how models assess the current situation and plan the next move. To support broader research, we also provide a puzzle training set aimed at enhancing performance on general mathematical tasks. We show that models trained on our state checking and transition data demonstrate gains in math reasoning by up to 5.1% on GSM8K.",,"Guizhen Chen, Weiwen Xu, Hao Zhang, Hou Pong Chan, Chaoqun Liu, Lidong Bing, Deli Zhao, Anh Tuan Luu, Yu Rong",2025-06-01T15:57:10Z,FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through   Reflective Puzzle Solving,FINEREASON: Bewertung und Verbesserung der bewussten Vernunft von LLMs durch Reflektierende Puzzle-Lösung,FINEASON:通过反射谜题解答评估和改进LLLMs的蓄意合理理由,http://arxiv.org/abs/2502.20238v2
964,"Large Language Models (LLMs) have shown remarkable success on a wide range of math and reasoning benchmarks. However, we observe that they often struggle when faced with unreasonable math problems. Instead of recognizing these issues, models frequently proceed as if the problem is well-posed, producing incorrect answers or falling into overthinking and verbose self-correction. To systematically investigate this overlooked vulnerability, we propose the \textbf{Unreasonable Math Problems (UMP)} benchmark, designed to evaluate LLMs' ability to detect and respond to unreasonable math problem statements. Based on extensive experiments covering 19 LLMs, we find that even state-of-the-art general models like GPT-4o achieve only a score of 0.6 on UMP. While reasoning models such as DeepSeek-R1 demonstrate a higher sensitivity to unreasonable inputs, this often comes at the cost of generating overly long and meaningless responses that fail to converge. We further explore prompting and fine-tuning methods, which offer partial improvements but also introduce trade-offs, shedding light on both the potential and limitations of LLMs in this challenging setting.",,"Jingyuan Ma, Damai Dai, Zihang Yuan, Rui li, Weilin Luo, Bin Wang, Qun Liu, Lei Sha, Zhifang Sui",2025-06-01T15:53:04Z,Large Language Models Struggle with Unreasonability in Math Problems,Große Sprachmodelle kämpfen mit Unangemessenheit in Math-Problemen,在数学问题中与不合理性作斗争的大语言模型,http://arxiv.org/abs/2403.19346v6
965,"Speech Emotion Recognition (SER) has seen significant progress with deep learning, yet remains challenging for Low-Resource Languages (LRLs) due to the scarcity of annotated data. In this work, we explore unsupervised learning to improve SER in low-resource settings. Specifically, we investigate contrastive learning (CL) and Bootstrap Your Own Latent (BYOL) as self-supervised approaches to enhance cross-lingual generalization. Our methods achieve notable F1 score improvements of 10.6% in Urdu, 15.2% in German, and 13.9% in Bangla, demonstrating their effectiveness in LRLs. Additionally, we analyze model behavior to provide insights on key factors influencing performance across languages, and also highlighting challenges in low-resource SER. This work provides a foundation for developing more inclusive, explainable, and robust emotion recognition systems for underrepresented languages.",,"Ziwei Gong, Pengyuan Shi, Kaan Donbekci, Lin Ai, Run Chen, David Sasu, Zehui Wu, Julia Hirschberg",2025-06-01T15:49:40Z,Learning More with Less: Self-Supervised Approaches for Low-Resource   Speech Emotion Recognition,Mehr lernen mit weniger: Selbstüberwachte Ansätze für Low-Resource Speech Emotion Recognition,学习更多少一些的学习:自我监督的承认低资源语言情感的方法,http://arxiv.org/abs/2506.02059v1
966,"As the capabilities of large language models (LLMs) continue to expand, aligning these models with human values remains a significant challenge. Recent studies show that reasoning abilities contribute significantly to model safety, while integrating Mixture-of-Experts (MoE) architectures can further enhance alignment. In this work, we address a fundamental question: How to effectively incorporate reasoning abilities and MoE architectures into self-alignment process in LLMs? We propose Mixture of insighTful Experts (MoTE), a novel framework that synergistically combines reasoning chains and expert mixtures to improve self-alignments. From a data perspective, MoTE employs a structured reasoning chain comprising four key stages: Question Analysis, Answer Guidance, Safe Answer, and Safety Checking. This approach enhances safety through multi-step reasoning and proves effective even for smaller and less powerful LLMs (e.g., 7B models). From an architectural perspective, MoTE adopts a multi-LoRA framework with step-level routing, where each expert is dedicated to a specific reasoning step. This design eliminates the need for balance losses, ensures stable training, and supports adaptive inference lengths. Experimental results demonstrate that MoTE significantly improves model safety, jailbreak resistance, and over-refusal capabilities, achieving performance comparable to OpenAI's state-of-the-art o1 model.",,"Zhili Liu, Yunhao Gou, Kai Chen, Lanqing Hong, Jiahui Gao, Fei Mi, Yu Zhang, Zhenguo Li, Xin Jiang, Qun Liu, James T. Kwok",2025-06-01T15:48:57Z,Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and   Expert Mixtures in Self-Alignment,Mixture of insighTful Experts (MoTE): Die Synergie von Gedankenketten und Expertenmischungen in der Selbstjustierung,相亲专家的混合:思想链的协同和自相连接的专家混合,http://arxiv.org/abs/2405.00557v5
967,"Previous benchmarks on prompt injection in large language models (LLMs) have primarily focused on generic tasks and attacks, offering limited insights into more complex threats like data exfiltration. This paper examines how prompt injection can cause tool-calling agents to leak personal data observed during task execution. Using a fictitious banking agent, we develop data flow-based attacks and integrate them into AgentDojo, a recent benchmark for agentic security. To enhance its scope, we also create a richer synthetic dataset of human-AI banking conversations. In 16 user tasks from AgentDojo, LLMs show a 15-50 percentage point drop in utility under attack, with average attack success rates (ASR) around 20 percent; some defenses reduce ASR to zero. Most LLMs, even when successfully tricked by the attack, avoid leaking highly sensitive data like passwords, likely due to safety alignments, but they remain vulnerable to disclosing other personal data. The likelihood of password leakage increases when a password is requested along with one or two additional personal details. In an extended evaluation across 48 tasks, the average ASR is around 15 percent, with no built-in AgentDojo defense fully preventing leakage. Tasks involving data extraction or authorization workflows, which closely resemble the structure of exfiltration attacks, exhibit the highest ASRs, highlighting the interaction between task type, agent performance, and defense efficacy.",,"Meysam Alizadeh, Zeynab Samei, Daria Stetsenko, Fabrizio Gilardi",2025-06-01T15:48:06Z,Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM   Agents During Task Execution,"Einfache Prompt-Injection-Angriffe können persönliche Daten, die von LLM-Agenten während der Task-Execution beobachtet werden, löschen",任务执行期间LLM代理人员观察的个人数据,http://arxiv.org/abs/2506.01055v1
968,"Humblebragging is a phenomenon in which individuals present self-promotional statements under the guise of modesty or complaints. For example, a statement like, ""Ugh, I can't believe I got promoted to lead the entire team. So stressful!"", subtly highlights an achievement while pretending to be complaining. Detecting humblebragging is important for machines to better understand the nuances of human language, especially in tasks like sentiment analysis and intent recognition. However, this topic has not yet been studied in computational linguistics. For the first time, we introduce the task of automatically detecting humblebragging in text. We formalize the task by proposing a 4-tuple definition of humblebragging and evaluate machine learning, deep learning, and large language models (LLMs) on this task, comparing their performance with humans. We also create and release a dataset called HB-24, containing 3,340 humblebrags generated using GPT-4o. Our experiments show that detecting humblebragging is non-trivial, even for humans. Our best model achieves an F1-score of 0.88. This work lays the foundation for further exploration of this nuanced linguistic phenomenon and its integration into broader natural language understanding systems.",,"Sharath Naganna, Saprativa Bhattacharjee, Biplab Banerjee, Pushpak Bhattacharyya",2025-06-01T15:44:14Z,"""My life is miserable, have to sign 500 autographs everyday"": Exposing   Humblebragging, the Brags in Disguise","""Mein Leben ist miserabel, ich muss jeden Tag 500 Autogramme unterschreiben"": Humblebragging, die Brags in Verkleiden","""我的生活悲惨,每天必须签署500个签名"":",http://arxiv.org/abs/2412.20057v2
969,"Attention heads are one of the building blocks of large language models (LLMs). Prior work on investigating their operation mostly focused on analyzing their behavior during inference for specific circuits or tasks. In this work, we seek a comprehensive mapping of the operations they implement in a model. We propose MAPS (Mapping Attention head ParameterS), an efficient framework that infers the functionality of attention heads from their parameters, without any model training or inference. We showcase the utility of MAPS for answering two types of questions: (a) given a predefined operation, mapping how strongly heads across the model implement it, and (b) given an attention head, inferring its salient functionality. Evaluating MAPS on 20 operations across 6 popular LLMs shows its estimations correlate with the head's outputs during inference and are causally linked to the model's predictions. Moreover, its mappings reveal attention heads of certain operations that were overlooked in previous studies, and valuable insights on function universality and architecture biases in LLMs. Next, we present an automatic pipeline and analysis that leverage MAPS to characterize the salient operations of a given head. Our pipeline produces plausible operation descriptions for most heads, as assessed by human judgment, while revealing diverse operations.",,"Amit Elhelo, Mor Geva",2025-06-01T15:37:01Z,Inferring Functionality of Attention Heads from their Parameters,Funktionalität der Aufmerksamkeitsköpfe aus ihren Parametern ableiten,"将 "" 注意主管 "" 的职能从其参数推论为 "" 注意主管 "" 的职能 """,http://arxiv.org/abs/2412.11965v2
970,"Accurate evaluation of large language models (LLMs) is crucial for understanding their capabilities and guiding their development. However, current evaluations often inconsistently reflect the actual capacities of these models. In this paper, we demonstrate that one of many contributing factors to this \textit{evaluation crisis} is the oversight of unseen knowledge -- information encoded by LLMs but not directly observed or not yet observed during evaluations. We introduce KnowSum, a statistical framework designed to provide a more comprehensive assessment by quantifying the unseen knowledge for a class of evaluation tasks. KnowSum estimates the unobserved portion by extrapolating from the appearance frequencies of observed knowledge instances. We demonstrate the effectiveness and utility of KnowSum across three critical applications: estimating total knowledge, evaluating information retrieval effectiveness, and measuring output diversity. Our experiments reveal that a substantial volume of knowledge is omitted when relying solely on observed LLM performance. Importantly, KnowSum yields significantly different comparative rankings for several common LLMs based on their internal knowledge.",,"Xiang Li, Jiayi Xin, Qi Long, Weijie J. Su",2025-06-01T15:32:44Z,Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?,Bewertung der Unsichtbaren Fähigkeiten: Wie viele Theoreme kennen LLMs?,评估看不见的能力:LLM知道多少理论?,http://arxiv.org/abs/2506.02058v1
971,"Automated parsing of scanned documents into richly structured, machine-readable formats remains a critical bottleneck in Document AI, as traditional multi-stage pipelines suffer from error propagation and limited adaptability to diverse layouts. We introduce layoutRL, an end-to-end reinforcement learning framework that trains models to be explicitly layout-aware by optimizing a composite reward of normalized edit distance, paragraph count accuracy, and reading order preservation. Leveraging our newly released dataset, Infinity-Doc-55K, which combines 55K high-fidelity synthetic scanned document parsing data with expert-filtered real-world documents, we instantiate layoutRL in a vision-language-model-based parser called Infinity-Parser. Evaluated on English and Chinese benchmarks for OCR, table and formula extraction, and reading order detection, Infinity-Parser achieves new state-of-the-art performance in both accuracy and structural fidelity, outpacing specialist pipelines and general-purpose vision-language models. We will publicly release our code and dataset to accelerate progress in robust document understanding.",,"Baode Wang, Biao Wu, Weizhen Li, Meng Fang, Yanjie Liang, Zuming Huang, Haozhe Wang, Jun Huang, Ling Chen, Wei Chu, Yuan Qi",2025-06-01T15:19:52Z,Infinity Parser: Layout Aware Reinforcement Learning for Scanned   Document Parsing,Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing,无限剖析器:扫描文档剖析的布局强化学习,http://arxiv.org/abs/2506.03197v1
972,"Evaluating tables qualitatively and quantitatively poses a significant challenge, as standard metrics often overlook subtle structural and content-level discrepancies. To address this, we propose a rubric-based evaluation framework that integrates multi-level structural descriptors with fine-grained contextual signals, enabling more precise and consistent table comparison. Building on this, we introduce TabXEval, an eXhaustive and eXplainable two-phase evaluation framework. TabXEval first aligns reference and predicted tables structurally via TabAlign, then performs semantic and syntactic comparison using TabCompare, offering interpretable and granular feedback. We evaluate TabXEval on TabXBench, a diverse, multi-domain benchmark featuring realistic table perturbations and human annotations. A sensitivity-specificity analysis further demonstrates the robustness and explainability of TabXEval across varied table tasks. Code and data are available at https://coral-lab-asu.github.io/tabxeval/",,"Vihang Pancholi, Jainit Bafna, Tejas Anvekar, Manish Shrivastava, Vivek Gupta",2025-06-01T15:12:03Z,TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table   Evaluation,TabXEval: Warum ist das ein schlechter Tisch? Eine eXhaustive Rubrik für die Tabellenbewertung,TabXEval: 为什么这是一张糟糕的桌子? 用于表格评价的 e Xhaustive Rubric,http://arxiv.org/abs/2505.22176v2
973,"Automatic relationship extraction (RE) from biomedical literature is critical for managing the vast amount of scientific knowledge produced each year. In recent years, utilizing pre-trained language models (PLMs) has become the prevalent approach in RE. Several studies report improved performance when incorporating additional context information while fine-tuning PLMs for RE. However, variations in the PLMs applied, the databases used for augmentation, hyper-parameter optimization, and evaluation methods complicate direct comparisons between studies and raise questions about the generalizability of these findings. Our study addresses this research gap by evaluating PLMs enhanced with contextual information on five datasets spanning four relation scenarios within a consistent evaluation framework. We evaluate three baseline PLMs and first conduct extensive hyperparameter optimization. After selecting the top-performing model, we enhance it with additional data, including textual entity descriptions, relational information from knowledge graphs, and molecular structure encodings. Our findings illustrate the importance of i) the choice of the underlying language model and ii) a comprehensive hyperparameter optimization for achieving strong extraction performance. Although inclusion of context information yield only minor overall improvements, an ablation study reveals substantial benefits for smaller PLMs when such external data was included during fine-tuning.",,"Mario Sänger, Ulf Leser",2025-06-01T15:08:10Z,Knowledge-augmented Pre-trained Language Models for Biomedical Relation   Extraction,Wissensgesteigerte vortrainierte Sprachmodelle für die biomedizinische Beziehungsextraktion,生物医学关系采掘学前语言模式,http://arxiv.org/abs/2505.00814v2
974,"Social media platforms are experiencing a growing presence of AI-Generated Texts (AIGTs). However, the misuse of AIGTs could have profound implications for public opinion, such as spreading misinformation and manipulating narratives. Despite its importance, it remains unclear how prevalent AIGTs are on social media. To address this gap, this paper aims to quantify and monitor the AIGTs on online social media platforms. We first collect a dataset (SM-D) with around 2.4M posts from 3 major social media platforms: Medium, Quora, and Reddit. Then, we construct a diverse dataset (AIGTBench) to train and evaluate AIGT detectors. AIGTBench combines popular open-source datasets and our AIGT datasets generated from social media texts by 12 LLMs, serving as a benchmark for evaluating mainstream detectors. With this setup, we identify the best-performing detector (OSM-Det). We then apply OSM-Det to SM-D to track AIGTs across social media platforms from January 2022 to October 2024, using the AI Attribution Rate (AAR) as the metric. Specifically, Medium and Quora exhibit marked increases in AAR, rising from 1.77% to 37.03% and 2.06% to 38.95%, respectively. In contrast, Reddit shows slower growth, with AAR increasing from 1.31% to 2.45% over the same period. Our further analysis indicates that AIGTs on social media differ from human-written texts across several dimensions, including linguistic patterns, topic distributions, engagement levels, and the follower distribution of authors. We envision our analysis and findings on AIGTs in social media can shed light on future research in this domain.",,"Zhen Sun, Zongmin Zhang, Xinyue Shen, Ziyi Zhang, Yule Liu, Michael Backes, Yang Zhang, Xinlei He",2025-06-01T15:06:21Z,Are We in the AI-Generated Text World Already? Quantifying and   Monitoring AIGT on Social Media,Sind wir schon in der KI-generierten Textwelt? Quantifizierung und Überwachung von AIGT auf Social Media,我们是否已经进入AI-Generation Text World?,http://arxiv.org/abs/2412.18148v3
975,"Quality Estimation (QE) models for Neural Machine Translation (NMT) predict the quality of the hypothesis without having access to the reference. An emerging research direction in NMT involves the use of QE models, which have demonstrated high correlations with human judgment and can enhance translations through Quality-Aware Decoding. Although several approaches have been proposed based on sampling multiple candidate translations and picking the best candidate, none have integrated these models directly into the decoding process. In this paper, we address this by proposing a novel token-level QE model capable of reliably scoring partial translations. We build a uni-directional QE model for this, as decoder models are inherently trained and efficient on partial sequences. We then present a decoding strategy that integrates the QE model for Quality-Aware decoding and demonstrate that the translation quality improves when compared to the N-best list re-ranking with state-of-the-art QE models (up to $1.39$ XCOMET-XXL $\uparrow$). Finally, we show that our approach provides significant benefits in document translation tasks, where the quality of N-best lists is typically suboptimal. Code can be found at https://ai4lt.iar.kit.edu/english/projects\_kontextmt.php",,"Sai Koneru, Matthias Huck, Miriam Exel, Jan Niehues",2025-06-01T15:02:13Z,Quality-Aware Decoding: Unifying Quality Estimation and Decoding,Qualitäts-Bewusst-Dekodierung: Vereinheitlichung der Qualitätsschätzung und -dekodierung,质量软件编码:质量估算和编码不统一,http://arxiv.org/abs/2502.08561v3
976,"Probing large language models (LLMs) has yielded valuable insights into their internal mechanisms by linking neural representations to interpretable semantics. However, how neurons functionally co-activate with each other to give rise to emergent capabilities remains largely unknown, hindering a deeper understanding and safer development of LLMs. In this work, we introduce graph probing, a method for uncovering the functional connectivity topology of LLM neurons and relating it to language generation performance. By analyzing internal neural graphs across diverse LLM families and scales, we discover a universal predictability of next-token prediction performance using only neural topology. This predictability is robust even when retaining just 1% of neuron connections or probing models after only 8 pretraining steps, highlighting the sparsity and early emergence of topological patterns. Further graph matching analysis suggests that, despite significant distinctions in architectures, parameters, and training data, different LLMs develop intricate and consistent neural topological structures that may form the foundation for their language generation abilities. Codes and data for the graph probing toolbox are released at https://github.com/DavyMorgan/llm-graph-probing.",,"Yu Zheng, Yuan Yuan, Yong Li, Paolo Santi",2025-06-01T14:57:03Z,Probing Neural Topology of Large Language Models,Probing Neural Topologie von großen Sprachmodellen,大型语言模型的测测神经构造学,http://arxiv.org/abs/2506.01042v1
977,"The issue-resolving task, where a model generates patches to fix real-world bugs, has emerged as a critical benchmark for evaluating the capabilities of large language models (LLMs). While SWE-bench and its variants have become standard in this domain, they suffer from key limitations: they have not been updated since their initial releases, cover a narrow set of repositories, and depend heavily on manual effort for instance construction and environment setup. These factors hinder scalability and introduce risks of overfitting and data contamination. In this work, we present SWE-bench-Live, a live-updatable benchmark designed to overcome these challenges. Our initial release consists of 1,319 tasks derived from real GitHub issues created since 2024, spanning 93 repositories. Each task is accompanied by a dedicated Docker image to ensure reproducible execution. Central to our benchmark is \method, an automated curation pipeline that streamlines the entire process from instance creation to environment setup, removing manual bottlenecks and enabling scalability and continuous updates. We evaluate a range of state-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a substantial performance gap compared to static benchmarks like SWE-bench, even under controlled evaluation conditions. To better understand this discrepancy, we perform detailed analyses across repository origin, issue recency, and task difficulty. By providing a fresh, diverse, and executable benchmark grounded in live repository activity, SWE-bench-Live facilitates rigorous, contamination-resistant evaluation of LLMs and agents in dynamic, real-world software development settings.",,"Linghao Zhang, Shilin He, Chaoyun Zhang, Yu Kang, Bowen Li, Chengxing Xie, Junhao Wang, Maoquan Wang, Yufan Huang, Shengyu Fu, Elsie Nallipogu, Qingwei Lin, Yingnong Dang, Saravan Rajmohan, Dongmei Zhang",2025-06-01T14:53:40Z,SWE-bench Goes Live!,SWE-Bench geht live!,SWE -BECHE GOES 现场直播!,http://arxiv.org/abs/2505.23419v2
978,"Understanding the internal mechanisms of large language models (LLMs) remains a challenging and complex endeavor. Even fundamental questions, such as how fine-tuning affects model behavior, often require extensive empirical evaluation. In this paper, we introduce a novel perspective based on the geometric properties of contextual latent embeddings to study the effects of training and fine-tuning. To that end, we measure the local dimensions of a contextual language model's latent space and analyze their shifts during training and fine-tuning. We show that the local dimensions provide insights into the model's training dynamics and generalization ability. Specifically, the mean of the local dimensions predicts when the model's training capabilities are exhausted, as exemplified in a dialogue state tracking task, overfitting, as demonstrated in an emotion recognition task, and grokking, as illustrated with an arithmetic task. Furthermore, our experiments suggest a practical heuristic: reductions in the mean local dimension tend to accompany and predict subsequent performance gains. Through this exploration, we aim to provide practitioners with a deeper understanding of the implications of fine-tuning on embedding spaces, facilitating informed decisions when configuring models for specific applications. The results of this work contribute to the ongoing discourse on the interpretability, adaptability, and generalizability of LLMs by bridging the gap between intrinsic model mechanisms and geometric properties in the respective embeddings.",,"Benjamin Matthias Ruppik, Julius von Rohrscheidt, Carel van Niekerk, Michael Heck, Renato Vukovic, Shutong Feng, Hsien-chin Lin, Nurul Lubis, Bastian Rieck, Marcus Zibrowius, Milica Gašić",2025-06-01T14:30:46Z,Less is More: Local Intrinsic Dimensions of Contextual Language Models,Weniger ist mehr: Lokale Intrinsische Dimensionen kontextueller Sprachmodelle,较少为:当地本语语言模式的内在层面,http://arxiv.org/abs/2506.01034v1
979,"We introduce LLM-as-an-Interviewer, a novel paradigm for evaluating large language models (LLMs). This approach leverages multi-turn interactions where the LLM interviewer actively provides feedback on responses and poses follow-up questions to the evaluated LLM. At the start of the interview, the LLM interviewer dynamically modifies datasets to generate initial questions, mitigating data contamination. We apply the LLM-as-an-Interviewer framework to evaluate six models on the MATH and DepthQA tasks. Our results show that the framework effectively provides insights into LLM performance, including the quality of initial responses, adaptability to feedback, and ability to address follow-up queries like clarification or additional knowledge requests. The framework also addresses key limitations of conventional methods like LLM-as-a-Judge, including verbosity bias and inconsistency across runs. Finally, we propose the Interview Report, which aggregates insights from the interview process, providing examples and a comprehensive analysis of the LLM's strengths and weaknesses. This report offers a detailed snapshot of the model's real-world applicability. The code for our framework is publicly available at https://github.com/interview-eval/.",,"Eunsu Kim, Juyoung Suk, Seungone Kim, Niklas Muennighoff, Dongkwan Kim, Alice Oh",2025-06-01T14:25:13Z,LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM   Evaluation,LLM-as-an-Interviewer: Jenseits der statischen Prüfung durch dynamische LLM-Bewertung,LLM-as-as-an-Interviewer:通过动态LLM评价超越静态测试,http://arxiv.org/abs/2412.10424v3
980,"Enabling robots to accurately interpret and execute spoken language instructions is essential for effective human-robot collaboration. Traditional methods rely on speech recognition to transcribe speech into text, often discarding crucial prosodic cues needed for disambiguating intent. We propose a novel approach that directly leverages speech prosody to infer and resolve instruction intent. Predicted intents are integrated into large language models via in-context learning to disambiguate and select appropriate task plans. Additionally, we present the first ambiguous speech dataset for robotics, designed to advance research in speech disambiguation. Our method achieves 95.79% accuracy in detecting referent intents within an utterance and determines the intended task plan of ambiguous instructions with 71.96% accuracy, demonstrating its potential to significantly improve human-robot communication.",,"David Sasu, Kweku Andoh Yamoah, Benedict Quartey, Natalie Schluter",2025-06-01T14:06:57Z,Enhancing Speech Instruction Understanding and Disambiguation in   Robotics via Speech Prosody,Verbesserung des Sprachunterrichts Verständnis und Disambiguation in der Robotik durch Sprachprosodie,通过发声推进增强对机器人的语音教学理解和差异,http://arxiv.org/abs/2506.02057v1
981,"Learning distributed representations, or embeddings, that encode the relational similarity patterns among objects is a relevant task in machine learning. A popular method to learn the embedding matrices $X, Y$ is optimizing a loss function of the term ${\rm SoftMax}(XY^T)$. The complexity required to calculate this term, however, runs quadratically with the problem size, making it a computationally heavy solution. In this article, we propose a linear-time heuristic approximation to compute the normalization constants of ${\rm SoftMax}(XY^T)$ for embedding vectors with bounded norms. We show on some pre-trained embedding datasets that the proposed estimation method achieves higher or comparable accuracy with competing methods. From this result, we design an efficient and task-agnostic algorithm that learns the embeddings by optimizing the cross entropy between the softmax and a set of probability distributions given as inputs. The proposed algorithm is interpretable and easily adapted to arbitrary embedding problems. We consider a few use cases and observe similar or higher performances and a lower computational time than similar ``2Vec'' algorithms.",,"Lorenzo Dall'Amico, Enrico Maria Belliardo",2025-06-01T14:02:31Z,Learning distributed representations with efficient SoftMax   normalization,Lernen verteilte Darstellungen mit effizienter SoftMax Normalisierung,"学习分布式陈述,并实现高效率的软式算法正常化",http://arxiv.org/abs/2303.17475v4
982,"Text editing frames grammatical error correction (GEC) as a sequence tagging problem, where edit tags are assigned to input tokens, and applying these edits results in the corrected text. This approach has gained attention for its efficiency and interpretability. However, while extensively explored for English, text editing remains largely underexplored for morphologically rich languages like Arabic. In this paper, we introduce a text editing approach that derives edit tags directly from data, eliminating the need for language-specific edits. We demonstrate its effectiveness on Arabic, a diglossic and morphologically rich language, and investigate the impact of different edit representations on model performance. Our approach achieves SOTA results on two Arabic GEC benchmarks and performs on par with SOTA on two others. Additionally, our models are over six times faster than existing Arabic GEC systems, making our approach more practical for real-world applications. Finally, we explore ensemble models, demonstrating how combining different models leads to further performance improvements. We make our code, data, and pretrained models publicly available.",,"Bashar Alhafni, Nizar Habash",2025-06-01T13:57:11Z,Enhancing Text Editing for Grammatical Error Correction: Arabic as a   Case Study,Verbesserung der Textbearbeitung für die Korrektur von Grammatikfehlern: Arabisch als Fallstudie,校正:阿拉伯文作为案例研究,http://arxiv.org/abs/2503.00985v2
983,"The meaning conveyed by a sentence often depends on the context in which it appears. Despite the progress of sentence embedding methods, it remains unclear how to best modify a sentence embedding conditioned on its context. To address this problem, we propose Condition-Aware Sentence Embeddings (CASE), an efficient and accurate method to create an embedding for a sentence under a given condition. First, CASE creates an embedding for the condition using a Large Language Model (LLM), where the sentence influences the attention scores computed for the tokens in the condition during pooling. Next, a supervised nonlinear projection is learned to reduce the dimensionality of the LLM-based text embeddings. We show that CASE significantly outperforms previously proposed Conditional Semantic Textual Similarity (C-STS) methods on an existing standard benchmark dataset. We find that subtracting the condition embedding consistently improves the C-STS performance of LLM-based text embeddings. Moreover, we propose a supervised dimensionality reduction method that not only reduces the dimensionality of LLM-based embeddings but also significantly improves their performance.",,"Gaifan Zhang, Yi Zhou, Danushka Bollegala",2025-06-01T13:19:22Z,CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic   Textual Similarity Measurement,CASE -- Condition-Aware Sentence Einbettungen für bedingte semantische textuelle Ähnlichkeitsmessung,CASE -- -- 有条件的语义文字相似性测量条件 -- -- 警告判决,http://arxiv.org/abs/2503.17279v3
984,"While contrastive learning greatly advances the representation of sentence embeddings, it is still limited by the size of the existing sentence datasets. In this paper, we present TransAug (Translate as Augmentation), which provide the first exploration of utilizing translated sentence pairs as data augmentation for text, and introduce a two-stage paradigm to advances the state-of-the-art sentence embeddings. Instead of adopting an encoder trained in other languages setting, we first distill a Chinese encoder from a SimCSE encoder (pretrained in English), so that their embeddings are close in semantic space, which can be regraded as implicit data augmentation. Then, we only update the English encoder via cross-lingual contrastive learning and frozen the distilled Chinese encoder. Our approach achieves a new state-of-art on standard semantic textual similarity (STS), outperforming both SimCSE and Sentence-T5, and the best performance in corresponding tracks on transfer tasks evaluated by SentEval.",,Jue Wang,2025-06-01T13:17:47Z,TransAug: Translate as Augmentation for Sentence Embeddings,TransAug: Übersetzen als Augmentation für Sentence-Embeddings,Transaug: 翻译为判刑嵌入的附加,http://arxiv.org/abs/2111.00157v3
985,"Recent advances in large language models (LLMs) and vision-language models (LVLMs) have shown promise across many tasks, yet their scientific reasoning capabilities remain untested, particularly in multimodal settings. We present MMSciBench, a benchmark for evaluating mathematical and physical reasoning through text-only and text-image formats, with human-annotated difficulty levels, solutions with detailed explanations, and taxonomic mappings. Evaluation of state-of-the-art models reveals significant limitations, with even the best model achieving only \textbf{63.77\%} accuracy and particularly struggling with visual reasoning tasks. Our analysis exposes critical gaps in complex reasoning and visual-textual integration, establishing MMSciBench as a rigorous standard for measuring progress in multimodal scientific understanding. The code for MMSciBench is open-sourced at GitHub, and the dataset is available at Hugging Face.",,"Xinwu Ye, Chengfan Li, Siming Chen, Wei Wei, Xiangru Tang",2025-06-01T12:49:16Z,MMSciBench: Benchmarking Language Models on Chinese Multimodal   Scientific Problems,MMSciBench: Benchmarking-Sprachmodelle zu chinesischen multimodalen wissenschaftlichen Problemen,MMSCIBENCH:中国多模式科学问题语言模式基准,http://arxiv.org/abs/2503.01891v2
986,"Multimodal Large Language Models (MLLMs) have exhibited remarkable advancements in integrating different modalities, excelling in complex understanding and generation tasks. Despite their success, MLLMs remain vulnerable to conversational adversarial inputs, particularly negation arguments. This paper systematically evaluates state-of-the-art MLLMs across diverse benchmarks, revealing significant performance drops when negation arguments are introduced to initially correct responses. Notably, we introduce the first benchmark GaslightingBench, specifically designed to evaluate the vulnerability of MLLMs to negation arguments. GaslightingBench consists of multiple-choice questions curated from existing datasets, along with generated negation prompts across 20 diverse categories. Throughout extensive evaluation, we find that proprietary models such as Gemini-1.5-flash, GPT-4o and Claude-3.5-Sonnet demonstrate better resilience compared to open-source counterparts like Qwen2-VL and LLaVA. However, all evaluated MLLMs struggle to maintain logical consistency under negation arguments during conversation. Our findings provide critical insights for improving the robustness of MLLMs against negation inputs, contributing to the development of more reliable and trustworthy multimodal AI systems.",,"Bin Zhu, Huiyan Qi, Yinxuan Gui, Jingjing Chen, Chong-Wah Ngo, Ee-Peng Lim",2025-06-01T12:45:54Z,Calling a Spade a Heart: Gaslighting Multimodal Large Language Models   via Negation,Einen Spaten ein Herz nennen: Multimodale große Sprachmodelle durch Negation gasen,拨号为“心:通过空白点燃多式大语言模型”,http://arxiv.org/abs/2501.19017v3
987,"Access to humanities research databases is often hindered by the limitations of traditional interaction formats, particularly in the methods of searching and response generation. This study introduces an LLM-based smart assistant designed to facilitate natural language communication with digital humanities data. The assistant, developed in a chatbot format, leverages the RAG approach and integrates state-of-the-art technologies such as hybrid search, automatic query generation, text-to-SQL filtering, semantic database search, and hyperlink insertion. To evaluate the effectiveness of the system, experiments were conducted to assess the response quality of various language models. The testing was based on the Prozhito digital archive, which contains diary entries from predominantly Russian-speaking individuals who lived in the 20th century. The chatbot is tailored to support anthropology and history researchers, as well as non-specialist users with an interest in the field, without requiring prior technical training. By enabling researchers to query complex databases with natural language, this tool aims to enhance accessibility and efficiency in humanities research. The study highlights the potential of Large Language Models to transform the way researchers and the public interact with digital archives, making them more intuitive and inclusive. Additional materials are presented in GitHub repository: https://github.com/alekosus/talking-to-data-intersys2025.",,"Alexander Sergeev, Valeriya Goloviznina, Mikhail Melnichenko, Evgeny Kotelnikov",2025-06-01T12:41:44Z,Talking to Data: Designing Smart Assistants for Humanities Databases,Mit Daten sprechen: Intelligente Assistenten für Geistesdatenbanken konzipieren,数据对话:设计人文数据库智能助理,http://arxiv.org/abs/2506.00986v1
988,"Diary analysis presents challenges, particularly in extracting meaningful information from large corpora, where traditional methods often fail to deliver satisfactory results. This study introduces a novel method based on Large Language Models (LLMs) to identify and cluster the various purposes of diary writing. By ""purposes,"" we refer to the intentions behind diary writing, such as documenting life events, self-reflection, or practicing language skills. Our approach is applied to Soviet-era diaries (1922-1929) from the Prozhito digital archive, a rich collection of personal narratives. We evaluate different proprietary and open-source LLMs, finding that GPT-4o and o1-mini achieve the best performance, while a template-based baseline is significantly less effective. Additionally, we analyze the retrieved purposes based on gender, age of the authors, and the year of writing. Furthermore, we examine the types of errors made by the models, providing a deeper understanding of their limitations and potential areas for improvement in future research.",,"Valeriya Goloviznina, Alexander Sergeev, Mikhail Melnichenko, Evgeny Kotelnikov",2025-06-01T12:38:01Z,Do LLMs Understand Why We Write Diaries? A Method for Purpose Extraction   and Clustering,"Verstehen LLMs, warum wir Tagebücher schreiben? Eine Methode zur gezielten Extraktion und Clusterbildung",LLM女士理解我们为什么写日记吗?,http://arxiv.org/abs/2506.00985v1
989,"Multimodal reference resolution, including phrase grounding, aims to understand the semantic relations between mentions and real-world objects. Phrase grounding between images and their captions is a well-established task. In contrast, for real-world applications, it is essential to integrate textual and multimodal reference resolution to unravel the reference relations within dialogue, especially in handling ambiguities caused by pronouns and ellipses. This paper presents a framework that unifies textual and multimodal reference resolution by mapping mention embeddings to object embeddings and selecting mentions or objects based on their similarity. Our experiments show that learning textual reference resolution, such as coreference resolution and predicate-argument structure analysis, positively affects performance in multimodal reference resolution. In particular, our model with coreference resolution performs better in pronoun phrase grounding than representative models for this task, MDETR and GLIP. Our qualitative analysis demonstrates that incorporating textual reference relations strengthens the confidence scores between mentions, including pronouns and predicates, and objects, which can reduce the ambiguities that arise in visually grounded dialogues.",,"Shun Inadumi, Nobuhiro Ueda, Koichiro Yoshino",2025-06-01T12:32:37Z,Disambiguating Reference in Visually Grounded Dialogues through Joint   Modeling of Textual and Multimodal Semantic Structures,Disambiguierung der Referenz in visuell begründeten Dialogen durch gemeinsame Modellierung von Text- und multimodalen semantischen Strukturen,"通过文本和多模式语义结构联合建模,在视觉基础对话中提供不同视角的参考",http://arxiv.org/abs/2505.11726v2
990,"Proactive search in conversations (PSC) aims to reduce user effort in formulating explicit queries by proactively retrieving useful relevant information given conversational context. Previous work in PSC either directly uses this context as input to off-the-shelf ad-hoc retrievers or further fine-tunes them on PSC data. However, ad-hoc retrievers are pre-trained on short and concise queries, while the PSC input is longer and noisier. This input mismatch between ad-hoc search and PSC limits retrieval quality. While fine-tuning on PSC data helps, its benefits remain constrained by this input gap. In this work, we propose Conv2Query, a novel conversation-to-query framework that adapts ad-hoc retrievers to PSC by bridging the input gap between ad-hoc search and PSC. Conv2Query maps conversational context into ad-hoc queries, which can either be used as input for off-the-shelf ad-hoc retrievers or for further fine-tuning on PSC data. Extensive experiments on two PSC datasets show that Conv2Query significantly improves ad-hoc retrievers' performance, both when used directly and after fine-tuning on PSC.",,"Chuan Meng, Francesco Tonolini, Fengran Mo, Nikolaos Aletras, Emine Yilmaz, Gabriella Kazai",2025-06-01T12:30:58Z,Bridging the Gap: From Ad-hoc to Proactive Search in Conversations,Bridging the Gap: Von Ad-hoc zur proaktiven Suche in Gesprächen,缩小差距:从临时搜索到主动主动搜索对话,http://arxiv.org/abs/2506.00983v1
991,"How language-specific are speech representations learned by self-supervised models? Existing work has shown that a range of linguistic features can be successfully decoded from end-to-end models trained only on speech recordings. However, it's less clear to what extent pre-training on specific languages improves language-specific linguistic information. Here we test the encoding of Dutch phonetic and lexical information in internal representations of self-supervised Wav2Vec2 models. Pre-training exclusively on Dutch improves the representation of Dutch linguistic features as compared to pre-training on similar amounts of English or larger amounts of multilingual data. This language-specific advantage is well-detected by trained clustering or classification probes, and partially observable using zero-shot metrics. Furthermore, the language-specific benefit on linguistic feature encoding aligns with downstream performance on Automatic Speech Recognition.",,"Marianne de Heer Kloots, Hosein Mohebbi, Charlotte Pouw, Gaofei Shen, Willem Zuidema, Martijn Bentum",2025-06-01T12:25:13Z,What do self-supervised speech models know about Dutch? Analyzing   advantages of language-specific pre-training,Was wissen selbstüberwachte Sprachmodelle über Niederländisch? Analysieren von Vorteilen sprachspezifischer Vorausbildung,自我监督的演讲模式对荷兰语了解多少? 分析具体语言培训前培训的优势,http://arxiv.org/abs/2506.00981v1
992,"This paper presents LEMONADE, a large-scale conflict event dataset comprising 39,786 events across 20 languages and 171 countries, with extensive coverage of region-specific entities. LEMONADE is based on a partially reannotated subset of the Armed Conflict Location & Event Data (ACLED), which has documented global conflict events for over a decade.   To address the challenge of aggregating multilingual sources for global event analysis, we introduce abstractive event extraction (AEE) and its subtask, abstractive entity linking (AEL). Unlike conventional span-based event extraction, our approach detects event arguments and entities through holistic document understanding and normalizes them across the multilingual dataset. We evaluate various large language models (LLMs) on these tasks, adapt existing zero-shot event extraction systems, and benchmark supervised models. Additionally, we introduce ZEST, a novel zero-shot retrieval-based system for AEL.   Our best zero-shot system achieves an end-to-end F1 score of 58.3%, with LLMs outperforming specialized event extraction models such as GoLLIE. For entity linking, ZEST achieves an F1 score of 45.7%, significantly surpassing OneNet, a state-of-the-art zero-shot baseline that achieves only 23.7%. However, these zero-shot results lag behind the best supervised systems by 20.1% and 37.0% in the end-to-end and AEL tasks, respectively, highlighting the need for further research.",,"Sina J. Semnani, Pingyue Zhang, Wanyue Zhai, Haozhuo Li, Ryan Beauchamp, Trey Billing, Katayoun Kishi, Manling Li, Monica S. Lam",2025-06-01T12:24:05Z,LEMONADE: A Large Multilingual Expert-Annotated Abstractive Event   Dataset for the Real World,"LEMONADE: Ein großer multilingualer, von Experten kommentierter Abstrakter Veranstaltungsdatensatz für die reale Welt",LEMONADE: 真实世界大型多语种专家 附加说明的抽象事件数据集,http://arxiv.org/abs/2506.00980v1
993,"Large Language Models (LLMs) can generate content spanning ideological rhetoric to explicit instructions for violence. However, existing safety evaluations often rely on simplistic binary labels (safe and unsafe), overlooking the nuanced spectrum of risk these outputs pose. To address this, we present XGUARD, a benchmark and evaluation framework designed to assess the severity of extremist content generated by LLMs. XGUARD includes 3,840 red teaming prompts sourced from real world data such as social media and news, covering a broad range of ideologically charged scenarios. Our framework categorizes model responses into five danger levels (0 to 4), enabling a more nuanced analysis of both the frequency and severity of failures. We introduce the interpretable Attack Severity Curve (ASC) to visualize vulnerabilities and compare defense mechanisms across threat intensities. Using XGUARD, we evaluate six popular LLMs and two lightweight defense strategies, revealing key insights into current safety gaps and trade-offs between robustness and expressive freedom. Our work underscores the value of graded safety metrics for building trustworthy LLMs.",,"Vadivel Abishethvarman, Bhavik Chandna, Pratik Jalan, Usman Naseem",2025-06-01T11:48:54Z,XGUARD: A Graded Benchmark for Evaluating Safety Failures of Large   Language Models on Extremist Content,XGUARD: Ein abgestuftes Benchmark für die Bewertung von Sicherheitsfehlern großer Sprachmodelle auf extremistische Inhalte,XGUARD:评估关于极端主义内容的大型语言模型安全失灵的分级基准,http://arxiv.org/abs/2506.00973v1
994,"Artificial agents are increasingly central to complex interactions and decision-making tasks, yet aligning their behaviors with desired human values remains an open challenge. In this work, we investigate how human-like personality traits influence agent behavior and performance within text-based interactive environments. We introduce PANDA: Personality Adapted Neural Decision Agents, a novel method for projecting human personality traits onto agents to guide their behavior. To induce personality in a text-based game agent, (i) we train a personality classifier to identify what personality type the agent's actions exhibit, and (ii) we integrate the personality profiles directly into the agent's policy-learning pipeline. By deploying agents embodying 16 distinct personality types across 25 text-based games and analyzing their trajectories, we demonstrate that an agent's action decisions can be guided toward specific personality profiles. Moreover, certain personality types, such as those characterized by higher levels of Openness, display marked advantages in performance. These findings underscore the promise of personality-adapted agents for fostering more aligned, effective, and human-centric decision-making in interactive environments.",,"Seungwon Lim, Seungbeen Lee, Dongjun Min, Youngjae Yu",2025-06-01T11:48:37Z,Persona Dynamics: Unveiling the Impact of Personality Traits on Agents   in Text-Based Games,Persona Dynamics: Enthüllen der Auswirkungen von Persönlichkeitseigenschaften auf Agenten in textbasierten Spielen,人文动态:保持个性轨迹对文本运动会代理人的影响,http://arxiv.org/abs/2504.06868v4
995,"In the age of social media, understanding public sentiment toward major corporations is crucial for investors, policymakers, and researchers. This paper presents a comprehensive sentiment analysis system tailored for corporate reputation monitoring, combining Natural Language Processing (NLP) and machine learning techniques to accurately interpret public opinion in real time. The methodology integrates a hybrid sentiment detection framework leveraging both rule-based models (VADER) and transformer-based deep learning models (DistilBERT), applied to social media data from multiple platforms. The system begins with robust preprocessing involving noise removal and text normalization, followed by sentiment classification using an ensemble approach to ensure both interpretability and contextual accuracy. Results are visualized through sentiment distribution plots, comparative analyses, and temporal sentiment trends for enhanced interpretability. Our analysis reveals significant disparities in public sentiment across major corporations, with companies like Amazon (81.2) and Samsung (45.8) receiving excellent sentiment scores, while Microsoft (21.7) and Walmart (21.9) exhibit poor sentiment profiles. These findings demonstrate the utility of our multi-source sentiment framework in providing actionable insights regarding corporate public perception, enabling stakeholders to make informed strategic decisions based on comprehensive sentiment analysis.",,"Yanampally Abhiram Reddy, Siddhi Agarwal, Vikram Parashar, Arshiya Arora",2025-06-01T11:42:50Z,Visualizing Public Opinion on X: A Real-Time Sentiment Dashboard Using   VADER and DistilBERT,Visualisierung der öffentlichen Meinung zu X: Ein Echtzeit-Sentiment-Dashboard mit VADER und DistilBERT,可视化关于X的公众意见:使用VADER和DuttilBERT的实时感光板,http://arxiv.org/abs/2504.15448v2
996,"Large language models (LLMs) are increasingly becoming valuable to corporate data management due to their ability to process text from various document formats and facilitate user interactions through natural language queries. However, LLMs must consider the sensitivity of information when communicating with employees, especially given access restrictions. Simple filtering based on user clearance levels can pose both performance and privacy challenges. To address this, we propose the concept of sensitivity awareness (SA), which enables LLMs to adhere to predefined access rights rules. In addition, we developed a benchmarking environment called ACCESS DENIED INC to evaluate SA. Our experimental findings reveal significant variations in model behavior, particularly in managing unauthorized data requests while effectively addressing legitimate queries. This work establishes a foundation for benchmarking sensitivity-aware language models and provides insights to enhance privacy-centric AI systems in corporate environments.",,"Dren Fazlija, Arkadij Orlov, Sandipan Sikdar",2025-06-01T11:24:23Z,ACCESS DENIED INC: The First Benchmark Environment for Sensitivity   Awareness,ACCESS DENIED INC: Die erste Benchmark-Umgebung für Sensitivitätsbewusstsein,敏感性意识的第一基准环境,http://arxiv.org/abs/2506.00964v1
997,"Automatically generating high-quality mathematical problems that align with educational objectives is a crucial task in NLP-based educational technology. Traditional generation methods focus primarily on textual quality, but they often overlook educational objectives. Moreover, these methods address only single-dimensional, simple question generation, failing to meet complex, multifaceted educational requirements. To address these challenges, we constructed and annotated EduMath, a dataset of 16k mathematical questions with multi-dimensional educational objectives. Based on this dataset, we developed EQGEVAL, which incorporates three evaluation dimensions and is designed to assess the ability of models to generate educational questions. Drawing inspiration from teachers' problem design processes, we propose the Educational Question Planning with self-Reflection (EQPR) method for educational mathematical question generation, following a ""plan-evaluate-optimize"" approach. Specifically, by combining planning algorithm based on Monte Carlo Tree Search with the generative capabilities of Large Language Models, we continuously optimize questions through iterative feedback. This self-optimization mechanism ensures that the generated questions both fit the educational context and strategically achieve specific basic educational objectives. Through extensive experiments based on EQGEVAL, we have demonstrated that EQPR achieves significant improvements in generating questions that meet multi-dimensional educational objectives.",,"Cheng Cheng, Zhenya Huang, Guanhao Zhao, Yuxiang Guo, Xin Lin, Jinze Wu, Xin Li, Shijin Wang",2025-06-01T11:23:18Z,From Objectives to Questions: A Planning-based Framework for Educational   Mathematical Question Generation,Von Zielen zu Fragen: Ein planungsbasierter Rahmen für die bildungsmathematische Fragestellung,从目标到问题:基于规划的教育数学问题生成框架,http://arxiv.org/abs/2506.00963v1
998,"Personalizing Large Language Models (LLMs) has become a critical step in facilitating their widespread application to enhance individual life experiences. In pursuit of personalization, distilling key preference information from an individual's historical data as instructional preference context to customize LLM generation has emerged as a promising direction. However, these methods face a fundamental limitation by overlooking the inter-user comparative analysis, which is essential for identifying the inter-user differences that truly shape preferences. To address this limitation, we propose Difference-aware Personalization Learning (DPL), a novel approach that emphasizes extracting inter-user differences to enhance LLM personalization. DPL strategically selects representative users for comparison and establishes a structured standard to extract meaningful, task-relevant differences for customizing LLM generation. Extensive experiments on real-world datasets demonstrate that DPL significantly enhances LLM personalization. We release our code at https://github.com/SnowCharmQ/DPL.",,"Yilun Qiu, Xiaoyan Zhao, Yang Zhang, Yimeng Bai, Wenjie Wang, Hong Cheng, Fuli Feng, Tat-Seng Chua",2025-06-01T11:09:47Z,Measuring What Makes You Unique: Difference-Aware User Modeling for   Enhancing LLM Personalization,"Messen, was Sie einzigartig macht: Difference-Aware User Modeling zur Verbesserung der LLM Personalisierung",衡量什么使你独一之处:提高LLM个性化的差异型用户模型,http://arxiv.org/abs/2503.02450v2
999,"Nonverbal communication is integral to human interaction, with gestures, facial expressions, and body language conveying critical aspects of intent and emotion. However, existing large language models (LLMs) fail to effectively incorporate these nonverbal elements, limiting their capacity to create fully immersive conversational experiences. We introduce MARS, a multimodal language model designed to understand and generate nonverbal cues alongside text, bridging this gap in conversational AI. Our key innovation is VENUS, a large-scale dataset comprising annotated videos with time-aligned text, facial expressions, and body language. Leveraging VENUS, we train MARS with a next-token prediction objective, combining text with vector-quantized nonverbal representations to achieve multimodal understanding and generation within a unified framework. Based on various analyses of the VENUS datasets, we validate its substantial scale and high effectiveness. Our quantitative and qualitative results demonstrate that MARS successfully generates text and nonverbal languages, corresponding to conversational input.",,"Youngmin Kim, Jiwan Chung, Jisoo Kim, Sunghyun Lee, Sangkyu Lee, Junhyeok Kim, Cheoljong Yang, Youngjae Yu",2025-06-01T11:07:25Z,Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning   Nonverbal Cues from Video-Grounded Dialogues,Das Sprechen jenseits der Sprache: Ein multimodaler Großrechner-Datensatz zum Lernen nonverbaler Queues aus Video-gerundeten Dialogen,《超语言语言:从视频四轮对话中学习非口头语库的大型多模式数据集》,http://arxiv.org/abs/2506.00958v1
1000,"Sarcasm fundamentally alters meaning through tone and context, yet detecting it in speech remains a challenge due to data scarcity. In addition, existing detection systems often rely on multimodal data, limiting their applicability in contexts where only speech is available. To address this, we propose an annotation pipeline that leverages large language models (LLMs) to generate a sarcasm dataset. Using a publicly available sarcasm-focused podcast, we employ GPT-4o and LLaMA 3 for initial sarcasm annotations, followed by human verification to resolve disagreements. We validate this approach by comparing annotation quality and detection performance on a publicly available sarcasm dataset using a collaborative gating architecture. Finally, we introduce PodSarc, a large-scale sarcastic speech dataset created through this pipeline. The detection model achieves a 73.63% F1 score, demonstrating the dataset's potential as a benchmark for sarcasm detection research.",,"Zhu Li, Yuqing Zhang, Xiyuan Gao, Shekhar Nayak, Matt Coler",2025-06-01T11:00:18Z,Leveraging Large Language Models for Sarcastic Speech Annotation in   Sarcasm Detection,Nutzung großer Sprachmodelle für sarkastische Sprachannotation in Sarkasmus-Erkennung,"利用大语言模型进行讽刺语语音批注,以探测讽刺语",http://arxiv.org/abs/2506.00955v1
1001,"In this work, we present AfriHuBERT, an extension of mHuBERT-147, a compact self-supervised learning (SSL) model pretrained on 147 languages. While mHuBERT-147 covered 16 African languages, we expand this to 1,226 through continued pretraining on 10K+ hours of speech data from diverse sources, benefiting an African population of over 600M. We evaluate AfriHuBERT on two key speech tasks, Spoken Language Identification (SLID) and Automatic Speech Recognition (ASR), using the FLEURS benchmark. Our results show a +3.6% F1 score improvement for SLID and a -2.1% average Word Error Rate (WER) reduction for ASR over mHuBERT-147, and demonstrates competitiveness with larger SSL models such as MMS and XEUS. Further analysis shows that ASR models trained on AfriHuBERT exhibit improved cross-corpus generalization and are competitive in extremely low-resource ASR scenarios.",,"Jesujoba O. Alabi, Xuechen Liu, Dietrich Klakow, Junichi Yamagishi",2025-06-01T10:49:58Z,AfriHuBERT: A self-supervised speech representation model for African   languages,AfriHuBERT: Ein selbstüberwachtes Sprachdarstellungsmodell für afrikanische Sprachen,AfriHuBERT:非洲语言自我监督的演讲代表模式,http://arxiv.org/abs/2409.20201v2
1002,"Model merging has become one of the key technologies for enhancing the capabilities and efficiency of Large Language Models (LLMs). However, our understanding of the expected performance gains and principles when merging any two models remains limited. In this work, we introduce model kinship, the degree of similarity or relatedness between LLMs, analogous to biological evolution. With comprehensive empirical analysis, we find that there is a certain relationship between model kinship and the performance gains after model merging, which can help guide our selection of candidate models. Inspired by this, we propose a new model merging strategy: Top-k Greedy Merging with Model Kinship, which can yield better performance on benchmark datasets. Specifically, we discover that using model kinship as a criterion can assist us in continuously performing model merging, alleviating the degradation (local optima) in model evolution, whereas model kinship can serve as a guide to escape these traps. Code is available at https://github.com/zjunlp/ModelKinship.",,"Yedi Hu, Yunzhi Yao, Shumin Deng, Huajun Chen, Ningyu Zhang",2025-06-01T10:39:29Z,Exploring Model Kinship for Merging Large Language Models,Erforschung von Modellkinship für das Zusammenführen von großen Sprachmodellen,探索合并大语言模式模式的示范关系,http://arxiv.org/abs/2410.12613v2
1003,"The field of neural machine translation (NMT) has changed with the advent of large language models (LLMs). Much of the recent emphasis in natural language processing (NLP) has been on modeling machine translation and many other problems using a single pre-trained Transformer decoder, while encoder-decoder architectures, which were the standard in earlier NMT models, have received relatively less attention. In this paper, we explore translation models that are universal, efficient, and easy to optimize, by marrying the world of LLMs with the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder unchanged. We also develop methods for adapting LLMs to work better with the NMT decoder. Furthermore, we construct a new dataset involving multiple tasks to assess how well the machine translation system generalizes across various tasks. Evaluations on the WMT and our datasets show that results using our method match or surpass a range of baselines in terms of translation quality, but achieve $2.4 \sim 6.5 \times$ inference speedups and a $75\%$ reduction in the memory footprint of the KV cache. It also demonstrates strong generalization across a variety of translation-related tasks.",,"Yingfeng Luo, Tong Zheng, Yongyu Mu, Bei Li, Qinghong Zhang, Yongqi Gao, Ziqiang Xu, Peinan Feng, Xiaoqian Liu, Tong Xiao, Jingbo Zhu",2025-06-01T10:36:07Z,Beyond Decoder-only: Large Language Models Can be Good Encoders for   Machine Translation,Beyond Decoder-only: Große Sprachmodelle können gute Encoder für maschinelle Übersetzung sein,仅除 Decoder 之外: 大语言模型可以是机器翻译的良好编码器,http://arxiv.org/abs/2503.06594v2
1004,"The advent of multimodal large language models (MLLMs) has sparked interest in their application to electrocardiogram (ECG) analysis. However, existing ECG-focused MLLMs primarily focus on report generation tasks, often limited to single 12-lead, short-duration (10s) ECG inputs, thereby underutilizing the potential of MLLMs. To this end, we aim to develop a MLLM for ECG analysis that supports a broader range of tasks and more flexible ECG inputs. However, existing ECG-QA datasets are often monotonous. To address this gap, we first constructed the anyECG dataset, which encompasses a wide variety of tasks, including report generation, abnormal waveform localization, and open-ended question answering. In addition to standard hospital ECGs, we introduced long-duration reduced-lead ECGs for home environments and multiple ECG comparison scenarios commonly encountered in clinical practice. Furthermore, we propose the anyECG-chat model, which supports dynamic-length ECG inputs and multiple ECG inputs. We trained the model using a three-stage curriculum training recipe with the anyECG dataset. A comprehensive evaluation was conducted, demonstrating that anyECG-chat is capable of supporting various practical application scenarios, including not only common report generation tasks but also abnormal waveform localization for long-duration reduced-lead ECGs in home environments and comprehensive comparative analysis of multiple ECGs.",,"Haitao Li, Ziyu Li, Yiheng Mao, Ziyi Liu, Zhoujian Sun, Zhengxing Huang",2025-06-01T10:17:13Z,anyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task   Understanding,anyECG-Chat: Ein Generalist EKG-MLLM für flexibles EKG-Eingangs- und Multi-Task-Verständnis,任何ECG-对话:灵活ECG投入和多任务理解通用ECG-MLM ECG-MLM ECG-MLM,http://arxiv.org/abs/2506.00942v1
1005,"Vision-language models (VLMs) aligned with general human objectives, such as being harmless and hallucination-free, have become valuable assistants of humans in managing visual tasks. However, people with diversified backgrounds have different cognition even in the same situation. Consequently, they may have personalized expectations for VLM assistants. This highlights the urgent need to align VLM assistants with personalized situated cognition for real-world assistance. To study this problem, we first simplify it by characterizing individuals based on the sociological concept of Role-Set. Then, we propose to evaluate the individuals' actions to examine whether the personalized alignment is achieved. Further, we construct a benchmark named PCogAlignBench, which includes 18k instances and 20 individuals with different Role-Sets. Finally, we present a framework called PCogAlign, which constructs a cognition-aware and action-based reward model for personalized alignment. Experimental results and human evaluations demonstrate the reliability of the PCogAlignBench and the effectiveness of our proposed PCogAlign. We will open-source the constructed benchmark and code at https://github.com/NLPGM/PCogAlign.",,"Yongqi Li, Shen Zhou, Xiaohu Li, Xin Miao, Jintao Wen, Mayi Xu, Jianhao Chen, Birong Pan, Hankun Kang, Yuanyuan Zhu, Ming Zhong, Tieyun Qian",2025-06-01T09:50:54Z,Aligning VLM Assistants with Personalized Situated Cognition,Ausrichtung von VLM-Assistenten mit personalisierter Kognition,将VLM助理与个性化地物化概念对齐,http://arxiv.org/abs/2506.00930v1
1006,"Large Language Models (LLMs) are increasingly deployed in real-world applications that demand complex reasoning. To track progress, robust benchmarks are required to evaluate their capabilities beyond superficial pattern recognition. However, current LLM reasoning benchmarks often face challenges such as insufficient interpretability, performance saturation or data contamination. To address these challenges, we introduce GAMEBoT, a gaming arena designed for rigorous and transparent assessment of LLM reasoning capabilities. GAMEBoT decomposes complex reasoning in games into predefined modular subproblems. This decomposition allows us to design a suite of Chain-of-Thought (CoT) prompts that leverage domain knowledge to guide LLMs in addressing these subproblems before action selection. Furthermore, we develop a suite of rule-based algorithms to generate ground truth for these subproblems, enabling rigorous validation of the LLMs' intermediate reasoning steps. This approach facilitates evaluation of both the quality of final actions and the accuracy of the underlying reasoning process. GAMEBoT also naturally alleviates the risk of data contamination through dynamic games and head-to-head LLM competitions. We benchmark 17 prominent LLMs across eight games, encompassing various strategic abilities and game characteristics. Our results suggest that GAMEBoT presents a significant challenge, even when LLMs are provided with detailed CoT prompts. Project page: https://visual-ai.github.io/gamebot",,"Wenye Lin, Jonathan Roberts, Yunhan Yang, Samuel Albanie, Zongqing Lu, Kai Han",2025-06-01T09:49:56Z,GAMEBoT: Transparent Assessment of LLM Reasoning in Games,GAMEBoT: Transparente Bewertung der LLM-Reasoning in Spiele,GAMEBOT: 透明评估游戏中LLM原因,http://arxiv.org/abs/2412.13602v2
1007,"Human perception of events is intrinsically tied to distinguishing between completed (perfect and telic) and ongoing (durative) actions, a process mediated by both linguistic structure and visual cues. In this work, we introduce the \textbf{Perfect Times} dataset, a novel, quadrilingual (English, Italian, Russian, and Japanese) multiple-choice question-answering benchmark designed to assess video-language models (VLMs) on temporal reasoning. By pairing everyday activity videos with event completion labels and perfectivity-tailored distractors, our dataset probes whether models truly comprehend temporal dynamics or merely latch onto superficial markers. Experimental results indicate that state-of-the-art models, despite their success on text-based tasks, struggle to mirror human-like temporal and causal reasoning grounded in video. This study underscores the necessity of integrating deep multimodal cues to capture the nuances of action duration and completion within temporal and causal video dynamics, setting a new standard for evaluating and advancing temporal reasoning in VLMs.",,"Olga Loginova, Sofía Ortega Loguinova",2025-06-01T09:45:41Z,Deep Temporal Reasoning in Video Language Models: A Cross-Linguistic   Evaluation of Action Duration and Completion through Perfect Times,Deep Temporal Reasoning in Video Language Models: Eine Cross-Linguistic Bewertung von Aktionsdauer und Vollendung durch perfekte Zeiten,视频语言模型的深时时间理由解释:通过《完美时报》对行动期限和完成的跨语言评估,http://arxiv.org/abs/2506.00928v1
1008,"Deep sequence models typically degrade in accuracy when test sequences significantly exceed their training lengths, yet many critical tasks--such as algorithmic reasoning, multi-step arithmetic, and compositional generalization--require robust length extrapolation. We introduce PRISM, a Probabilistic Relative-position Implicit Superposition Model, a novel positional encoding mechanism that enables Transformers to extrapolate accurately up to 10x beyond their training length. PRISM learns continuous relative positions through a differentiable histogram-filter update, preserving position uncertainty via a probabilistic superposition rather than conventional deterministic embeddings. Empirically, PRISM achieves state-of-the-art length extrapolation, successfully generalizing to previously intractable sequence lengths across algorithmic benchmarks--including arithmetic (addition, multiplication), SCAN compositionality tasks, and complex copy variants derived from DeepMind's recent datasets. Our analysis demonstrates that PRISM's stochastic positional encoding maintains sharp and interpretable internal states, providing a theoretical basis for reliable length generalization. These results advance the goal of neural sequence models that remain algorithmically robust at lengths far exceeding their training horizon.",,Philip Heejun Lee,2025-06-01T09:20:44Z,Position as Probability: Self-Supervised Transformers that Think Past   Their Training for Length Extrapolation,"Position als Wahrscheinlichkeit: Selbstüberwachte Transformer, die über ihr Training für Längen-Extrapolation denken",作为概率位置的自我监督变异器 思考过去对长程外推法培训的自我监督变异器,http://arxiv.org/abs/2506.00920v1
1009,"Compositionality is a key aspect of human intelligence, essential for reasoning and generalization. While transformer-based models have become the de facto standard for many language modeling tasks, little is known about how they represent compound words, and whether these representations are compositional. In this study, we test compositionality in Mistral, OpenAI Large, and Google embedding models, and compare them with BERT. First, we evaluate compositionality in the representations by examining six diverse models of compositionality (addition, multiplication, dilation, regression, etc.). We find that ridge regression, albeit linear, best accounts for compositionality. Surprisingly, we find that the classic vector addition model performs almost as well as any other model. Next, we verify that most embedding models are highly compositional, while BERT shows much poorer compositionality. We verify and visualize our findings with a synthetic dataset consisting of fully transparent adjective-noun compositions. Overall, we present a thorough investigation of compositionality.",,"Aishik Nagar, Ishaan Singh Rawal, Mansi Dhanania, Cheston Tan",2025-06-01T09:02:56Z,How do Transformer Embeddings Represent Compositions? A Functional   Analysis,Wie stellen Transformer-Embeddings Kompositionen dar? Eine funktionelle Analyse,变形嵌入式如何代表构成?功能分析,http://arxiv.org/abs/2506.00914v1
1010,"Recent advancements in Large Multimodal Models (LMMs) have shown promise in Autonomous Driving Systems (ADS). However, their direct application to ADS is hindered by challenges such as misunderstanding of traffic knowledge, complex road conditions, and diverse states of vehicle. To address these challenges, we propose the use of Knowledge Editing, which enables targeted modifications to a model's behavior without the need for full retraining. Meanwhile, we introduce ADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS, which includes various real-world scenarios, multiple data types, and comprehensive evaluation metrics. We conduct comprehensive experiments and derive several interesting conclusions. We hope that our work will contribute to the further advancement of knowledge editing applications in the field of autonomous driving. Code and data are available in https://github.com/zjunlp/EasyEdit.",,"Chenxi Wang, Jizhan Fang, Xiang Chen, Bozhong Tian, Ziwen Xu, Huajun Chen, Ningyu Zhang",2025-06-01T08:52:54Z,ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving   Systems,ADS-Edit: Ein multimodaler Wissens-Editing-Datensatz für autonome Fahrsysteme,ADS-Edid:自动驾驶系统多式知识编辑数据集,http://arxiv.org/abs/2503.20756v2
1011,"Chinese, as a linguistic system rich in depth and complexity, is characterized by distinctive elements such as ancient poetry, proverbs, idioms, and other cultural constructs. However, current Large Language Models (LLMs) face limitations in these specialized domains, highlighting the need for the development of comprehensive datasets that can assess, continuously update, and progressively improve these culturally-grounded linguistic competencies through targeted training optimizations. To address this gap, we introduce CKnowEdit, the first-ever Chinese knowledge editing dataset designed to correct linguistic, factual, and logical errors in LLMs. We collect seven types of knowledge from a wide range of sources, including classical texts, idioms, and content from Baidu Tieba Ruozhiba, taking into account the unique polyphony, antithesis, and logical structures inherent in the Chinese language. By analyzing this dataset, we highlight the challenges current LLMs face in mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge editing techniques reveals opportunities to advance the correction of Chinese knowledge. Code and dataset are available at https://github.com/zjunlp/EasyEdit.",,"Jizhan Fang, Tianhe Lu, Yunzhi Yao, Ziyan Jiang, Xin Xu, Huajun Chen, Ningyu Zhang",2025-06-01T08:49:57Z,"CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics,   Facts, and Logic Error Correction in LLMs","CKnowEdit: Ein neuer chinesischer Wissens-Editing-Datensatz für Sprach-, Fakten- und Logische Fehlerkorrektur in LLMs",CknowEdit: 一套用于语言、事实和逻辑错误校正LLMM中语言、事实和逻辑错误的新中国知识编辑数据集,http://arxiv.org/abs/2409.05806v4
1012,"Recent work in computational psycholinguistics has revealed intriguing parallels between attention mechanisms and human memory retrieval, focusing primarily on vanilla Transformers that operate on token-level representations. However, computational psycholinguistic research has also established that syntactic structures provide compelling explanations for human sentence processing that token-level factors cannot fully account for. In this paper, we investigate whether the attention mechanism of Transformer Grammar (TG), which uniquely operates on syntactic structures as representational units, can serve as a cognitive model of human memory retrieval, using Normalized Attention Entropy (NAE) as a linking hypothesis between models and humans. Our experiments demonstrate that TG's attention achieves superior predictive power for self-paced reading times compared to vanilla Transformer's, with further analyses revealing independent contributions from both models. These findings suggest that human sentence processing involves dual memory representations -- one based on syntactic structures and another on token sequences -- with attention serving as the general memory retrieval algorithm, while highlighting the importance of incorporating syntactic structures as representational units.",,"Ryo Yoshida, Shinnosuke Isono, Kohei Kajikawa, Taiga Someya, Yushi Sugimoto, Yohei Oseki",2025-06-01T08:45:26Z,"If Attention Serves as a Cognitive Model of Human Memory Retrieval, What   is the Plausible Memory Representation?","Wenn Aufmerksamkeit als Kognitives Modell des menschlichen Gedächtnisses dient, was ist die Plausible Memory Representation?","如果注意力作为人类记忆检索的认知模型,那么什么是可见的内存代表?",http://arxiv.org/abs/2502.11469v2
1013,"Despite exceptional capabilities in knowledge-intensive tasks, Large Language Models (LLMs) face a critical gap in understanding how they internalize new knowledge, particularly how to structurally embed acquired knowledge in their neural computations. We address this issue through the lens of knowledge circuit evolution, identifying computational subgraphs that facilitate knowledge storage and processing. Our systematic analysis of circuit evolution throughout continual pre-training reveals several key findings: (1) the acquisition of new knowledge is influenced by its relevance to pre-existing knowledge; (2) the evolution of knowledge circuits exhibits a distinct phase shift from formation to optimization; (3) the evolution of knowledge circuits follows a deep-to-shallow pattern. These insights not only advance our theoretical understanding of the mechanisms of new knowledge acquisition in LLMs, but also provide potential implications for improving continual pre-training strategies to enhance model performance. Code and data will be available at https://github.com/zjunlp/DynamicKnowledgeCircuits.",,"Yixin Ou, Yunzhi Yao, Ningyu Zhang, Hui Jin, Jiacheng Sun, Shumin Deng, Zhenguo Li, Huajun Chen",2025-06-01T08:43:20Z,How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on   Continual Pre-Training,Wie erwerben LLMs neues Wissen? Eine Wissenskreisperspektive auf kontinuierliches Pre-Training,如何获得新知识? 持续培训前知识电路的视角,http://arxiv.org/abs/2502.11196v2
1014,"LLMs exhibit promising Social Intelligence (SI) in modeling human behavior, raising the need to evaluate LLMs' SI and their discrepancy with humans. SI equips humans with interpersonal abilities to behave wisely in navigating social interactions to achieve social goals. This presents an operational evaluation paradigm: outcome-oriented goal achievement evaluation and process-oriented interpersonal ability evaluation, which existing work fails to address. To this end, we propose SocialEval, a script-based bilingual SI benchmark, integrating outcome- and process-oriented evaluation by manually crafting narrative scripts. Each script is structured as a world tree that contains plot lines driven by interpersonal ability, providing a comprehensive view of how LLMs navigate social interactions. Experiments show that LLMs fall behind humans on both SI evaluations, exhibit prosociality, and prefer more positive social behaviors, even if they lead to goal failure. Analysis of LLMs' formed representation space and neuronal activations reveals that LLMs have developed ability-specific functional partitions akin to the human brain.",,"Jinfeng Zhou, Yuxuan Chen, Yihan Shi, Xuanming Zhang, Leqi Lei, Yi Feng, Zexuan Xiong, Miao Yan, Xunzhi Wang, Yaru Cao, Jianing Yin, Shuai Wang, Quanyu Dai, Zhenhua Dong, Hongning Wang, Minlie Huang",2025-06-01T08:36:51Z,SocialEval: Evaluating Social Intelligence of Large Language Models,SocialEval: Soziale Intelligenz von großen Sprachmodellen bewerten,社会价值:评价大语言模式的社会智慧,http://arxiv.org/abs/2506.00900v1
1015,"Large language models (LLMs) have shown remarkable capabilities across various software engineering tasks; however, their effectiveness in code migration, adapting code to run in different environments, remains insufficiently studied. In this work, we introduce CODEMENV: Code Migration Across Environment, a new benchmark specifically designed to assess LLMs' abilities in code migration scenarios. CODEMENV consists of 922 examples spanning 19 Python and Java packages, and covers three core tasks: (1) identifying functions incompatible with specific versions, (2) detecting changes in function definitions, and (3) adapting code to target environments. Experimental evaluation with seven LLMs on CODEMENV yields an average pass@1 rate of 26.50%, with GPT-4O achieving the highest score at 43.84%. Key findings include: (i) LLMs tend to be more proficient with newer function versions, which aids in migrating legacy code, and (ii) LLMs sometimes exhibit logical inconsistencies by identifying function changes irrelevant to the intended migration environment. The datasets are available at https://github.com/xdshen-ai/Benchmark-of-Code-Migration.",,"Keyuan Cheng, Xudong Shen, Yihao Yang, Tengyue Wang, Yang Cao, Muhammad Asif Ali, Hanbin Wang, Lijie Hu, Di Wang",2025-06-01T08:29:59Z,CODEMENV: Benchmarking Large Language Models on Code Migration,CODEMENV: Benchmarking großer Sprachmodelle zur Code-Migration,CODEMENV: 确定移徙法中大语言模式的基准,http://arxiv.org/abs/2506.00894v1
1016,"Affordance theory posits that environments inherently offer action possibilities that shape perception and behavior. While Multimodal Large Language Models (MLLMs) excel in vision-language tasks, their ability to perceive affordance, which is crucial for intuitive and safe interactions, remains underexplored. To address this, we introduce A4Bench, a novel benchmark designed to evaluate the affordance perception abilities of MLLMs across two dimensions: 1) Constitutive Affordance}, assessing understanding of inherent object properties through 1,282 question-answer pairs spanning nine sub-disciplines, and 2) Transformative Affordance, probing dynamic and contextual nuances (e.g., misleading, time-dependent, cultural, or individual-specific affordance) with 718 challenging question-answer pairs. Evaluating 17 MLLMs (nine proprietary and eight open-source) against human performance, we find that proprietary models generally outperform open-source counterparts, but all exhibit limited capabilities, particularly in transformative affordance perception. Furthermore, even top-performing models, such as Gemini-2.0-Pro (18.05% overall exact match accuracy), significantly lag behind human performance (best: 85.34%, worst: 81.25%). These findings highlight critical gaps in environmental understanding of MLLMs and provide a foundation for advancing AI systems toward more robust, context-aware interactions. The dataset is available in https://github.com/JunyingWang959/A4Bench/.",,"Junying Wang, Wenzhe Li, Yalun Wu, Yingji Liang, Yijin Guo, Chunyi Li, Haodong Duan, Zicheng Zhang, Guangtao Zhai",2025-06-01T08:26:34Z,Affordance Benchmark for MLLMs,Leistungsvergleich für MLLM,MLLLMs 价格基准,http://arxiv.org/abs/2506.00893v1
1017,"User simulators are crucial for replicating human interactions with dialogue systems, supporting both collaborative training and automatic evaluation, especially for large language models (LLMs). However, current role-playing methods face challenges such as a lack of utterance-level authenticity and user-level diversity, often hindered by role confusion and dependence on predefined profiles of well-known figures. In contrast, direct simulation focuses solely on text, neglecting implicit user traits like personality and conversation-level consistency. To address these issues, we introduce the User Simulator with Implicit Profiles (USP), a framework that infers implicit user profiles from human-machine interactions to simulate personalized and realistic dialogues. We first develop an LLM-driven extractor with a comprehensive profile schema, then refine the simulation using conditional supervised fine-tuning and reinforcement learning with cycle consistency, optimizing at both the utterance and conversation levels. Finally, a diverse profile sampler captures the distribution of real-world user profiles. Experimental results show that USP outperforms strong baselines in terms of authenticity and diversity while maintaining comparable consistency. Additionally, using USP to evaluate LLM on dynamic multi-turn aligns well with mainstream benchmarks, demonstrating its effectiveness in real-world applications.",,"Kuang Wang, Xianfei Li, Shenghao Yang, Li Zhou, Feng Jiang, Haizhou Li",2025-06-01T08:16:38Z,Know You First and Be You Better: Modeling Human-Like User Simulators   via Implicit Profiles,Kennen Sie zuerst und werden Sie besser: Modellierung von Mensch-ähnlichen Benutzer-Simulatoren über Implizite Profile,"“先知你,再善待你:通过隐含描述文件模拟人像用户模拟器”",http://arxiv.org/abs/2502.18968v3
1018,"The rapid development of Multimodal Large Language Models (MLLM) has led to a wide range of MLLM applications, and a number of benchmark datasets have sprung up in order to assess MLLM abilities. However, full-coverage Q&A testing on large-scale data is resource-intensive and time-consuming. To address this issue, we propose the MLLM Interview (MITV) strategy, which aims to quickly obtain MLLM performance metrics by quizzing fewer question. First, First, we constructed the interview dataset, which was built on an existing MLLM assessment dataset, by adding difficulty labels based on the performance of some typical MLLMs in this dataset. Second, we propose an MLLM Interview strategy, which obtains an initial performance situation of the large model by quizzing a small number of topics and then continuously tries to test the model's limits. Through extensive experiments, the result shows that the MITV strategy proposed in this paper performs well on MLLM benchmark datasets, and it is able to obtain the model evaluation capability faster through a small number of questions and answers.",,"Farong Wen, Yijin Guo, Junying Wang, Jiaohao Xiao, Yingjie Zhou, Chunyi Li, Zicheng Zhang, Guangtao Zhai",2025-06-01T07:51:15Z,Improve MLLM Benchmark Efficiency through Interview,Verbesserung der Effizienz von MLLM Benchmark durch Interview,通过访谈提高最低生活水平管理基准效率,http://arxiv.org/abs/2506.00883v1
1019,"Large language models (LLMs) hold promise for sustainable manufacturing, but often hallucinate industrial codes and emission factors, undermining regulatory and investment decisions. We introduce CircuGraphRAG, a retrieval-augmented generation (RAG) framework that grounds LLMs outputs in a domain-specific knowledge graph for the circular economy. This graph connects 117,380 industrial and waste entities with classification codes and GWP100 emission data, enabling structured multi-hop reasoning. Natural language queries are translated into SPARQL and verified subgraphs are retrieved to ensure accuracy and traceability. Compared with Standalone LLMs and Naive RAG, CircuGraphRAG achieves superior performance in single-hop and multi-hop question answering, with ROUGE-L F1 scores up to 1.0, while baseline scores below 0.08. It also improves efficiency, halving the response time and reducing token usage by 16% in representative tasks. CircuGraphRAG provides fact-checked, regulatory-ready support for circular economy planning, advancing reliable, low-carbon resource decision making.",,"Yang Zhao, Chengxiao Dai, Dusit Niyato, Chuan Fu Tan, Keyi Xiang, Yueyang Wang, Zhiquan Yeo, Daren Tan Zong Loong, Jonathan Low Zhaozhi, Eugene H. Z. HO",2025-06-01T07:49:47Z,A Graph-Retrieval-Augmented Generation Framework Enhances   Decision-Making in the Circular Economy,Ein graphisch-retrieval-erweiterter Erzeugungsrahmen verbessert Entscheidungsfindung in der Kreislaufwirtschaft,图图-检索-提款一代框架加强循环经济的决策,http://arxiv.org/abs/2506.04252v1
1020,"Large Language Model (LLM) unlearning has recently gained significant attention, driven by the need to remove unwanted information, such as private, sensitive, or copyrighted content, from LLMs. However, conventional unlearning approaches indiscriminately update model parameters to forget all tokens in a target document, including common tokens (e.g., pronouns, prepositions, general nouns) that carry general knowledge. In this paper, we highlight that not every token needs forgetting. We propose Selective Unlearning (SU), which identifies a critical subset of tokens within the forgetting set that is relevant to the unwanted information, and unlearns only those tokens. Experiments on two benchmarks and six baseline unlearning algorithms demonstrate that SU not only achieves effective unlearning on the targeted forget data, but also significantly preserves the model's utility in the retaining set.",,"Yixin Wan, Anil Ramakrishna, Kai-Wei Chang, Volkan Cevher, Rahul Gupta",2025-06-01T07:36:45Z,Not Every Token Needs Forgetting: Selective Unlearning to Limit Change   in Utility in Large Language Model Unlearning,"Nicht jedes Token muss vergessen: Selektives Entlernen, um den Wandel in der Nützlichkeit im großen Sprachmodell zu begrenzen Entlernen","并非每个 Tok 都需要忘记: 有选择地放弃学习,以限制大语言模式的实用性变化",http://arxiv.org/abs/2506.00876v1
1021,"In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https://github.com/zjunlp/SynWorld.",,"Runnan Fang, Xiaobin Wang, Yuan Liang, Shuofei Qiao, Jialong Wu, Zekun Xi, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen",2025-06-01T07:35:07Z,SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge   Refinement,SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement,Synworld: 用于改进制剂行动知识的虚拟情景合成,http://arxiv.org/abs/2504.03561v3
1022,"We propose RoCoFT, a parameter-efficient fine-tuning method for large-scale language models (LMs) based on updating only a few rows and columns of the weight matrices in transformers. Through extensive experiments with medium-size LMs like BERT and RoBERTa, and larger LMs like Bloom-7B, Llama2-7B, and Llama2-13B, we show that our method gives comparable or better accuracies than state-of-art PEFT methods while also being more memory and computation-efficient. We also study the reason behind the effectiveness of our method with tools from neural tangent kernel theory. We empirically demonstrate that our kernel, constructed using a restricted set of row and column parameters, are numerically close to the full-parameter kernel and gives comparable classification performance. Ablation studies are conducted to investigate the impact of different algorithmic choices, including the selection strategy for rows and columns as well as the optimal rank for effective implementation of our method.",,"Md Kowsher, Tara Esmaeilbeig, Chun-Nam Yu, Chen Chen, Mojtaba Soltanalian, Niloofar Yousefi",2025-06-01T07:27:22Z,RoCoFT: Efficient Finetuning of Large Language Models with Row-Column   Updates,RoCoFT: Effizientes Finetuning großer Sprachmodelle mit Row-Column-Updates,"RoCoFT:对大语言模式进行高效微调,并更新Row-Column更新",http://arxiv.org/abs/2410.10075v3
1023,"Multilingual Retrieval-Augmented Generation (mRAG) systems enhance language models by integrating external multilingual information to produce context-aware responses. However, mRAG systems struggle with retrieving relevant information due to linguistic variations between queries and documents, generating inconsistent responses when multilingual sources conflict. In this work, we systematically investigate language preferences in both retrieval and generation of mRAG through a series of experiments. Our analysis indicates that retrievers tend to prefer high-resource and query languages, yet this preference does not consistently improve generation performance. Moreover, we observe that generators prefer the query language or Latin scripts, leading to inconsistent outputs. To overcome these issues, we propose Dual Knowledge Multilingual RAG (DKM-RAG), a simple yet effective framework that fuses translated multilingual passages with complementary model knowledge. Empirical results demonstrate that DKM-RAG mitigates language preference in generation and enhances performance across diverse linguistic settings. Code is available at https://github.com/jeonghyunpark2002/LanguagePreference.git",,"Jeonghyun Park, Hwanhee Lee",2025-06-01T07:23:34Z,Investigating Language Preference of Multilingual RAG Systems,Untersuchung der Sprachpräferenzen für Mehrsprachige RAG-Systeme,调查多语言RAG系统的语言偏好,http://arxiv.org/abs/2502.11175v4
1024,"Current large language models (LLMs) often exhibit imbalanced multilingual capabilities due to their English-centric training corpora. To address this, existing fine-tuning approaches operating at the data-level (e.g., through data augmentation or distillation) typically introduce implicit cross-lingual alignment, overlooking the potential for more profound, latent-level cross-lingual interactions. In this work, we propose CC-Tuning, a novel multilingual fine-tuning paradigm that explicitly establishes a cross-lingual connection mechanism at the latent level. During training, CC-Tuning fuses the feed forward activations from both English and non-English inputs, enabling the model to benefit from both linguistic resources. This process is facilitated with a trainable Decision Maker that identifies beneficial activations. Furthermore, during inference, a Transform Matrix is utilized to simulate the cross-lingual connection under monolingual setting through representation transformation. Our experiments on six benchmarks covering 22 languages show that CC-Tuning outperforms vanilla SFT and offers a strong latent-level alternative to data-level augmentation methods. Further analysis also highlights the practicality of CC-Tuning and the potential of latent-level cross-lingual interactions in advancing the multilingual performance of LLMs.",,"Yangfan Ye, Xiaocheng Feng, Zekun Yuan, Xiachong Feng, Libo Qin, Lei Huang, Weitao Ma, Yichong Huang, Zhirui Zhang, Yunfei Lu, Xiaohui Yan, Duyu Tang, Dandan Tu, Bing Qin",2025-06-01T07:20:55Z,CC-Tuning: A Cross-Lingual Connection Mechanism for Improving Joint   Multilingual Supervised Fine-Tuning,CC-Tuning: Cross-Lingual Connection Mechanism zur Verbesserung der gemeinsamen Mehrsprachigkeit überwachtes Feintuning,CC-Turning:改进多语言监督的微调联合监督的跨语言连接机制,http://arxiv.org/abs/2506.00875v1
1025,"Predicting accurate future trajectories of pedestrians is essential for autonomous systems but remains a challenging task due to the need for adaptability in different environments and domains. A common approach involves collecting scenario-specific data and performing fine-tuning via backpropagation. However, this process is often impractical on edge devices due to constrained computational resources. To address this challenge, we introduce TrajICL, an In-Context Learning (ICL) framework for pedestrian trajectory prediction that enables rapid adaptation without fine-tuning on the scenario-specific data. We propose a spatio-temporal similarity-based example selection (STES) method that selects relevant examples from previously observed trajectories within the same scene by identifying similar motion patterns at corresponding locations. To further refine this selection, we introduce prediction-guided example selection (PG-ES), which selects examples based on both the past trajectory and the predicted future trajectory, rather than relying solely on the past trajectory. This approach allows the model to account for long-term dynamics when selecting examples. Finally, instead of relying on small real-world datasets with limited scenario diversity, we train our model on a large-scale synthetic dataset to enhance its prediction ability by leveraging in-context examples. Extensive experiments demonstrate that TrajICL achieves remarkable adaptation across both in-domain and cross-domain scenarios, outperforming even fine-tuned approaches across multiple public benchmarks. The code will be released at https://fujiry0.github.io/TrajICL-project-page.",,"Ryo Fujii, Hideo Saito, Ryo Hachiuma",2025-06-01T07:18:47Z,Towards Predicting Any Human Trajectory In Context,Auf dem Weg zur Vorhersage jeder menschlichen Flugbahn im Kontext,预测任何人类轨迹,http://arxiv.org/abs/2506.00871v1
1026,"Despite the impressive performance of vision-language models (VLMs) on downstream tasks, their ability to understand and reason about causal relationships in visual inputs remains unclear. Robust causal reasoning is fundamental to solving complex high-level reasoning tasks, yet existing benchmarks often include a mixture of reasoning questions, and VLMs can frequently exploit object recognition and activity identification as shortcuts to arrive at the correct answers, making it challenging to truly assess their causal reasoning abilities. To bridge this gap, we introduce VQA-Causal and VCR-Causal, two new benchmarks specifically designed to isolate and rigorously evaluate VLMs' causal reasoning abilities. Our findings reveal that while VLMs excel in object and activity recognition, they perform poorly on causal reasoning tasks, often only marginally surpassing random guessing. Further analysis suggests that this limitation stems from a severe lack of causal expressions in widely used training datasets, where causal relationships are rarely explicitly conveyed. We additionally explore fine-tuning strategies with hard negative cases, showing that targeted fine-tuning can improve model's causal reasoning while maintaining generalization and downstream performance. Our study highlights a key gap in current VLMs and lays the groundwork for future work on causal understanding.",,"Zhaotian Weng, Haoxuan Li, Kuan-Hao Huang, Jieyu Zhao",2025-06-01T07:17:46Z,What's Missing in Vision-Language Models? Probing Their Struggles with   Causal Order Reasoning,Was fehlt in Vision-Sprachen-Modellen? Probiert ihre Kämpfe mit Kausal Order Reasoning,在视觉语言模型中缺少什么?,http://arxiv.org/abs/2506.00869v1
1027,"Time series forecasting remains a challenging task, particularly in the context of complex multiscale temporal patterns. This study presents LLM-Mixer, a framework that improves forecasting accuracy through the combination of multiscale time-series decomposition with pre-trained LLMs (Large Language Models). LLM-Mixer captures both short-term fluctuations and long-term trends by decomposing the data into multiple temporal resolutions and processing them with a frozen LLM, guided by a textual prompt specifically designed for time-series data. Extensive experiments conducted on multivariate and univariate datasets demonstrate that LLM-Mixer achieves competitive performance, outperforming recent state-of-the-art models across various forecasting horizons. This work highlights the potential of combining multiscale analysis and LLMs for effective and scalable time-series forecasting.",,"Md Kowsher, Md. Shohanur Islam Sobuj, Nusrat Jahan Prottasha, E. Alejandro Alanis, Ozlem Ozmen Garibay, Niloofar Yousefi",2025-06-01T07:07:27Z,LLM-Mixer: Multiscale Mixing in LLMs for Time Series Forecasting,LLM-Mixer: Multiscale Mixing in LLMs für die Zeitreihenprognose,LLM-混合器:用于时间序列预报的多比例混用LLM,http://arxiv.org/abs/2410.11674v2
1028,"We introduce Diffusion-based Audio Captioning (DAC), a non-autoregressive diffusion model tailored for diverse and efficient audio captioning. Although existing captioning models relying on language backbones have achieved remarkable success in various captioning tasks, their insufficient performance in terms of generation speed and diversity impede progress in audio understanding and multimedia applications. Our diffusion-based framework offers unique advantages stemming from its inherent stochasticity and holistic context modeling in captioning. Through rigorous evaluation, we demonstrate that DAC not only achieves SOTA performance levels compared to existing benchmarks in the caption quality, but also significantly outperforms them in terms of generation speed and diversity. The success of DAC illustrates that text generation can also be seamlessly integrated with audio and visual generation tasks using a diffusion backbone, paving the way for a unified, audio-related generative model across different modalities.",,"Manjie Xu, Chenxing Li, Xinyi Tu, Yong Ren, Ruibo Fu, Wei Liang, Dong Yu",2025-06-01T07:01:51Z,Towards Diverse and Efficient Audio Captioning via Diffusion Models,Auf dem Weg zu vielfältigem und effizientem Audio Captioning über Diffusionsmodelle,通过传播模型实现多样化和高效率的音频控制,http://arxiv.org/abs/2409.09401v2
1029,"Emotion recognition in low-resource languages like Marathi remains challenging due to limited annotated data. We present L3Cube-MahaEmotions, a high-quality Marathi emotion recognition dataset with 11 fine-grained emotion labels. The training data is synthetically annotated using large language models (LLMs), while the validation and test sets are manually labeled to serve as a reliable gold-standard benchmark. Building on the MahaSent dataset, we apply the Chain-of-Translation (CoTR) prompting technique, where Marathi sentences are translated into English and emotion labeled via a single prompt. GPT-4 and Llama3-405B were evaluated, with GPT-4 selected for training data annotation due to superior label quality. We evaluate model performance using standard metrics and explore label aggregation strategies (e.g., Union, Intersection). While GPT-4 predictions outperform fine-tuned BERT models, BERT-based models trained on synthetic labels fail to surpass GPT-4. This highlights both the importance of high-quality human-labeled data and the inherent complexity of emotion recognition. An important finding of this work is that generic LLMs like GPT-4 and Llama3-405B generalize better than fine-tuned BERT for complex low-resource emotion recognition tasks. The dataset and model are shared publicly at https://github.com/l3cube-pune/MarathiNLP",,"Nidhi Kowtal, Raviraj Joshi",2025-06-01T07:01:34Z,L3Cube-MahaEmotions: A Marathi Emotion Recognition Dataset with   Synthetic Annotations using CoTR prompting and Large Language Models,L3Cube-MahaEmotions: Ein Marathi-Emotionserkennungsdatensatz mit synthetischen Anmerkungen mit CoTR-Prompting und großen Sprachmodellen,"L3Cube-MahaEmotions:利用CoTR促进和大语言模型,配有合成说明的马拉地情感识别数据集",http://arxiv.org/abs/2506.00863v1
1030,"The rapid advancement of large language models (LLMs) has shown remarkable progress in complex reasoning tasks. However, a significant disparity exists between benchmark performances and real-world applications. We attribute this gap primarily to current evaluation protocols and metrics, which inadequately capture the full spectrum of LLM capabilities, especially in complex reasoning tasks where both accuracy and consistency are essential. In this paper, we introduce G-Pass@$k$, a novel evaluation metric that continuously assesses model performance across multiple sampling attempts, quantifying both the model's performance potential and its stability. Through extensive experiments on various public and newly constructed benchmarks, we employ G-Pass@$k$ in conjunction with state-of-the-art large language models to provide comprehensive insights into their potential capabilities and operational consistency. Our findings reveal a significant opportunity to enhance the realistic reasoning abilities of LLMs, underscoring the necessity for more robust evaluation metrics.",,"Junnan Liu, Hongwei Liu, Linchen Xiao, Ziyi Wang, Kuikun Liu, Songyang Gao, Wenwei Zhang, Songyang Zhang, Kai Chen",2025-06-01T06:40:28Z,Are Your LLMs Capable of Stable Reasoning?,Sind Ihre LLMs stabiler Vernunft fähig?,您的LLMS有能力 稳定的理由吗?,http://arxiv.org/abs/2412.13147v4
1031,"We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one of the earliest open-vocabulary EEG-to-text generation frameworks tailored for Chinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact pretrained language model (MiniLM), our architecture aligns multichannel brain signals with natural language representations via masked pretraining and contrastive learning. Using a subset of the ChineseEEG dataset, where each sentence contains approximately ten Chinese characters aligned with 128-channel EEG recorded at 256 Hz, we segment EEG into per-character embeddings and predict full sentences in a zero-shot setting. The decoder is trained with teacher forcing and padding masks to accommodate variable-length sequences. Evaluation on over 1,500 training-validation sentences and 300 held-out test samples shows promising lexical alignment, with a best BLEU-1 score of 6.38\%. While syntactic fluency remains a challenge, our findings demonstrate the feasibility of non-phonetic, cross-modal language decoding from EEG. This work opens a new direction in multilingual brain-to-text research and lays the foundation for future cognitive-language interfaces in Chinese.",,"Jacky Tai-Yu Lu, Jung Chiang, Chi-Sheng Chen, Anna Nai-Yun Tung, Hsiang Wei Hu, Yuan Chiao Cheng",2025-06-01T06:26:32Z,EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG   Alignment via Large Language Model and Contrastive Learning on ChineseEEG,EEG2TEXT-CN: Eine explorative Studie der offenen Vokabulären chinesischen Text-EEG-Ausrichtung über großsprachliches Modell und kontrastives Lernen auf ChinesischEEG,EEG2TEXT-CN:通过大语言模式和中经语言差异性学习对中文文本与EEEG校对开放词汇的探索性研究,http://arxiv.org/abs/2506.00854v1
1032,"Traditional transformer models often allocate a fixed amount of computational resources to every input token, leading to inefficient and unnecessary computation. To address this, the Mixture of Depths (MoD) was introduced to dynamically adjust the computational depth by skipping less important layers. Despite its promise, current MoD approaches remain under-explored and face two main challenges: (1) high training costs due to the need to train the entire model along with the routers that determine which layers to skip, and (2) the risk of performance degradation when important layers are bypassed. In response to the first issue, we propose Router-Tuning, a method that fine-tunes only the router on a small dataset, drastically reducing the computational overhead associated with full model training. For the second challenge, we propose MindSkip, which deploys Attention with Dynamic Depths. This method preserves the model's performance while significantly enhancing computational and memory efficiency. Extensive experiments demonstrate that our approach delivers competitive results while dramatically improving the computation efficiency, e.g., 21\% speedup and only a 0.2\% performance drop. The code is released at https://github.com/CASE-Lab-UMD/Router-Tuning.",,"Shwai He, Tao Ge, Guoheng Sun, Bowei Tian, Xiaoyang Wang, Dong Yu",2025-06-01T06:25:50Z,Router-Tuning: A Simple and Effective Approach for Enabling   Dynamic-Depth in Transformers,Router-Tuning: Ein einfacher und effektiver Ansatz für die Aktivierung von Dynamik in Transformatoren,路路图-路路路图:在变换器中扶持动态外能的简单而有效办法,http://arxiv.org/abs/2410.13184v5
1033,"Fine-tuning large language models (LLMs) on task-specific data is essential for their effective deployment. As dataset sizes grow, efficiently selecting optimal subsets for training becomes crucial to balancing performance and computational costs. Traditional data selection methods often require fine-tuning a scoring model on the target dataset, which is time-consuming and resource-intensive, or rely on heuristics that fail to fully leverage the model's predictive capabilities. To address these challenges, we propose Data Whisperer, an efficient, training-free, attention-based method that leverages few-shot in-context learning with the model to be fine-tuned. Comprehensive evaluations were conducted on both raw and synthetic datasets across diverse tasks and models. Notably, Data Whisperer achieves superior performance compared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just 10% of the data, and outperforms existing methods with a 3.1-point improvement and a 7.4$\times$ speedup. The code is available at https://github.com/gszfwsb/Data-Whisperer.",,"Shaobo Wang, Xiangqi Jin, Ziming Wang, Jize Wang, Jiajun Zhang, Kaixin Li, Zichen Wen, Zhong Li, Conghui He, Xuming Hu, Linfeng Zhang",2025-06-01T05:57:00Z,Data Whisperer: Efficient Data Selection for Task-Specific LLM   Fine-Tuning via Few-Shot In-Context Learning,Data Whisperer: Effiziente Datenauswahl für aufgabenspezifisches LLM-Fine-Tuning über less-Shot In-Context Learning,Data Whiseperer:通过微小热内文学习为特定任务LLM 精调调调试选择有效数据,http://arxiv.org/abs/2505.12212v3
1034,"Theory of Mind (ToM) capabilities in LLMs have recently become a central object of investigation. Cognitive science distinguishes between two steps required for ToM tasks: 1) determine whether to invoke ToM, which includes the appropriate Depth of Mentalizing (DoM), or level of recursion required to complete a task; and 2) applying the correct inference given the DoM. In this position paper, we first identify several lines of work in different communities in AI, including LLM benchmarking, ToM add-ons, ToM probing, and formal models for ToM. We argue that recent work in AI tends to focus exclusively on the second step which are typically framed as static logic problems. We conclude with suggestions for improved evaluation of ToM capabilities inspired by dynamic environments used in cognitive tasks.",,"Eitan Wagner, Nitay Alon, Joseph M. Barnby, Omri Abend",2025-06-01T05:40:43Z,Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning,Geist Ihre Theorie: Theorie des Geistes geht tiefer als Vernunft,思考你的理论:思维更深比理性更深的理论,http://arxiv.org/abs/2412.13631v3
1035,"Previous research has sought to enhance the graph reasoning capabilities of LLMs by supervised fine-tuning on synthetic graph data. While these led to specialized LLMs better at solving graph algorithm problems, we don't need LLMs for shortest path: we need generalization from synthetic graph data to real-world tasks with implicit graph structures. In this work, we propose to unlock generalizable learning of graph synthetic data with reinforcement learning. We first design solution-based and process-based rewards for synthetic graph problems: instead of rigid memorizing response patterns in direct fine-tuning, we posit that RL would help LLMs grasp the essentials underlying graph reasoning and alleviate overfitting. We employ RL algorithms such as GRPO and DPO, aligning both off-the-shelf LLMs and LLMs fine-tuned on synthetic graph data. We then compare them against existing settings on both in-domain synthetic tasks and out-of-domain real-world tasks with implicit graph structures such as multi-hop QA, structured planning, and more. Extensive experiments demonstrate that our RL recipe leads to statistically significant improvement on 5 datasets, with an average gain of 12.9\% over baseline settings. Further analysis reveals that process-based rewards consistently outperform solution-based rewards, mixing synthetic and real-world task data yields potential gains, while compositionality and explainable intermediate steps remains a critical challenge even after RL.",,"Yizhuo Zhang, Heng Wang, Shangbin Feng, Zhaoxuan Tan, Xinyun Liu, Yulia Tsvetkov",2025-06-01T05:39:56Z,Generalizable LLM Learning of Graph Synthetic Data with Reinforcement   Learning,Allgemeines LLM-Lernen von Graphen-Synthetischen Daten mit Verstärkungs-Lernen,利用强化学习学习图形合成数据学习,http://arxiv.org/abs/2506.00845v1
1036,"Personalized dialogue systems have advanced considerably with the integration of user-specific personas into large language models (LLMs). However, while LLMs can effectively generate personalized responses, the influence of persona sentiment on dialogue quality remains underexplored. In this work, we conduct a large-scale analysis of dialogues generated using a range of polarized user profiles. Our experiments reveal that dialogues involving negatively polarized users tend to overemphasize persona attributes. In contrast, positively polarized profiles yield dialogues that selectively incorporate persona information, resulting in smoother interactions. Furthermore, we find that personas with weak or neutral sentiment generally produce lower-quality dialogues. Motivated by these findings, we propose a dialogue generation approach that explicitly accounts for persona polarity by combining a turn-based generation strategy with a profile ordering mechanism and sentiment-aware prompting. Our study provides new insights into the sensitivity of LLMs to persona sentiment and offers guidance for developing more robust and nuanced personalized dialogue systems.",,"Yonghyun Jun, Hwanhee Lee",2025-06-01T05:29:30Z,Exploring Persona Sentiment Sensitivity in Personalized Dialogue   Generation,Persona-Sentiment-Sentiment-Empfindlichkeit in der persönlichen Dialog-Generierung erforschen,探索个性化对话一代人的敏感性,http://arxiv.org/abs/2502.11423v2
1037,"Factuality evaluation aims to detect factual errors produced by language models (LMs) and hence guide the development of more factual models. Towards this goal, we train a factuality evaluator, FenCE, that provides LM generators with claim-level factuality feedback. We conduct data augmentation on a combination of public judgment datasets to train FenCE to (1) generate textual critiques along with scores and (2) make claim-level judgment based on diverse source documents obtained by various tools. We then present a framework that leverages FenCE to improve the factuality of LM generators by constructing training data. Specifically, we generate a set of candidate responses, leverage FenCE to revise and score each response without introducing lesser-known facts, and train the generator by preferring highly scored revised responses. Experiments show that our data augmentation methods improve the evaluator's accuracy by 2.9% on LLM-AggreFact. With FenCE, we improve Llama2-7B-chat and Llama3-8B-chat's factuality rate by 16.86% and 14.45% on FActScore, outperforming state-of-the-art factuality finetuning methods by 8.83% and 6.96%.",,"Yiqing Xie, Wenxuan Zhou, Pradyot Prakash, Di Jin, Yuning Mao, Quintin Fettes, Arya Talebzadeh, Sinong Wang, Han Fang, Carolyn Rose, Daniel Fried, Hejia Zhang",2025-06-01T05:27:41Z,Improving Model Factuality with Fine-grained Critique-based Evaluator,Verbesserung der Model-Factuality mit feinkörnigem Critique-basiertem Evaluator,改进与精精美英国评价员的示范事实质量,http://arxiv.org/abs/2410.18359v3
1038,"Despite the remarkable successes of large language models (LLMs), the underlying Transformer architecture has inherent limitations in handling complex reasoning tasks. Chain-of-thought (CoT) prompting has emerged as a practical workaround, but most CoT-based methods rely on a single, generic prompt such as ""think step by step"", with no task-specific adaptation. These approaches expect the model to discover an effective reasoning path on its own, forcing it to search through a vast prompt space. In contrast, several studies have explored task-specific prompt designs to boost performance. However, these designs are typically developed through trial and error, lacking theoretical grounding. As a result, prompt engineering remains largely ad hoc and unguided. In this paper, we provide a theoretical framework that explains why some prompts succeed while others fail. We show that prompts function as selectors, extracting task-relevant information from the model's full hidden state during CoT reasoning. Each prompt defines a unique trajectory through the answer space, and the choice of trajectory is crucial for task performance and future navigation within the space. We analyze the complexity of finding optimal prompts and characterize the size of the prompt space for a given task. Our theory reveals principles behind effective prompt design and shows that naive CoT-using self-guided prompts like ""think step by step""-can severely hinder performance. Through experiments, we show that optimal prompt search can lead to more than a 50% improvement on reasoning tasks, providing a theoretical foundation for prompt engineering.",,"Xiang Zhang, Juntai Cao, Jiaqi Wei, Chenyu You, Dujian Ding",2025-06-01T05:23:51Z,Why Prompt Design Matters and Works: A Complexity Analysis of Prompt   Search Space in LLMs,Warum Prompt Design-Materien und -Werke: Eine Komplexitätsanalyse von Prompt-Suchraum in LLMs,为何需要即时设计事项和工作:对LLMM中即时搜索空间的复杂程度分析,http://arxiv.org/abs/2503.10084v2
1039,"Large language models (LLMs) achieve strong performance on plain text tasks but underperform on structured data like tables and databases. Potential challenges arise from their underexposure during pre-training and rigid text-to-structure transfer mechanisms. Unlike humans who seamlessly apply learned patterns across data modalities, LLMs struggle to infer implicit relationships embedded in tabular formats, especially in the absence of explicit structural guidance. To bridge this cognitive gap, we introduce Contrastive Retrieval-Augmented Generation on Experience (CoRE), a framework that builds experience memory representations and enhances generalization through contrastive In-Context Learning (ICL) to simulate human-like knowledge transfer. Experiments on Text-to-SQL and TableQA show CoRE significantly improves performance, achieving average gains of 3.44% and 4.24%, with up to 17.2% on challenging tasks. Our Monte Carlo Tree Search (MCTS)-generated Experience Memory expands training data 8-9x, enhancing diversity and domain coverage. This training-free and continual method propels LLMs toward structured knowledge expertise.",,"Jiawei Gu, Ziting Xian, Yuanzhen Xie, Ye Liu, Enjie Liu, Ruichao Zhong, Mochi Gao, Yunzhi Tan, Bo Hu, Zang Li",2025-06-01T05:22:00Z,Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented   Generation on Experience,Auf dem Weg zu strukturiertem Wissen Reasoning: Kontrastive retrieval-erweiterte Generation auf Erfahrung,实现结构化知识理由:反向取回-积累经验的一代人,http://arxiv.org/abs/2506.00842v1
1040,"The paper explores the performance of LLMs in the context of multi-dimensional analytic writing assessments, i.e. their ability to provide both scores and comments based on multiple assessment criteria. Using a corpus of literature reviews written by L2 graduate students and assessed by human experts against 9 analytic criteria, we prompt several popular LLMs to perform the same task under various conditions. To evaluate the quality of feedback comments, we apply a novel feedback comment quality evaluation framework. This framework is interpretable, cost-efficient, scalable, and reproducible, compared to existing methods that rely on manual judgments. We find that LLMs can generate reasonably good and generally reliable multi-dimensional analytic assessments. We release our corpus and code for reproducibility.",,"Zhengxiang Wang, Veronika Makarova, Zhi Li, Jordan Kodner, Owen Rambow",2025-06-01T05:11:17Z,LLMs can Perform Multi-Dimensional Analytic Writing Assessments: A Case   Study of L2 Graduate-Level Academic English Writing,LLMs können multidimensionale analytische Schreibanalysen durchführen: Eine Fallstudie von L2 Graduate-Level Academic English Writing,LLMs能够进行多种分析性分析写作评估:L2研究生级英语写作案例研究,http://arxiv.org/abs/2502.11368v2
1041,"Multimodal knowledge graphs (MMKGs) enrich traditional knowledge graphs (KGs) by incorporating diverse modalities such as images and text. Multi-modal knowledge graph completion (MMKGC) seeks to exploit these heterogeneous signals to infer missing facts, thereby mitigating the intrinsic incompleteness of MMKGs. Existing MMKGC methods typically leverage only the information contained in the MMKGs under the closed-world assumption and adopt discriminative training objectives, which limits their reasoning capacity during completion. Recent generative completion approaches powered by advanced large language models (LLMs) have shown strong reasoning abilities in unimodal knowledge graph completion, but their potential in MMKGC remains largely unexplored. To bridge this gap, we propose HERGC, a Heterogeneous Experts Representation and Generative Completion framework for MMKGs. HERGC first deploys a Heterogeneous Experts Representation Retriever that enriches and fuses multimodal information and retrieves a compact candidate set for each incomplete triple. It then uses a Generative LLM Predictor fine-tuned on minimal instruction data to accurately identify the correct answer from these candidates. Extensive experiments on three standard MMKG benchmarks demonstrate HERGC's effectiveness and robustness, achieving state-of-the-art performance.",,"Yongkang Xiao, Rui Zhang",2025-06-01T04:12:25Z,HERGC: Heterogeneous Experts Representation and Generative Completion   for Multimodal Knowledge Graphs,HERGC: Heterogene Expertendarstellung und Generative Fertigstellung für multimodale Wissensgraphen,HHGC: 多式联运知识图的多样化专家代表性和生成完成,http://arxiv.org/abs/2506.00826v1
1042,"Large language models (LLMs) are trained on extensive datasets that encapsulate substantial world knowledge. However, their outputs often include confidently stated inaccuracies. Earlier works suggest that LLMs encode truthfulness as a distinct linear feature, termed the ""truth direction"", which can classify truthfulness reliably. We address several open questions about the truth direction: (i) whether LLMs universally exhibit consistent truth directions; (ii) whether sophisticated probing techniques are necessary to identify truth directions; and (iii) how the truth direction generalizes across diverse contexts. Our findings reveal that not all LLMs exhibit consistent truth directions, with stronger representations observed in more capable models, particularly in the context of logical negation. Additionally, we demonstrate that truthfulness probes trained on declarative atomic statements can generalize effectively to logical transformations, question-answering tasks, in-context learning, and external knowledge sources. Finally, we explore the practical application of truthfulness probes in selective question-answering, illustrating their potential to improve user trust in LLM outputs. These results advance our understanding of truth directions and provide new insights into the internal representations of LLM beliefs. Our code is public at https://github.com/colored-dye/truthfulness_probe_generalization",,"Yuntai Bao, Xuhong Zhang, Tianyu Du, Xinkui Zhao, Zhengwen Feng, Hao Peng, Jianwei Yin",2025-06-01T03:55:53Z,Probing the Geometry of Truth: Consistency and Generalization of Truth   Directions in LLMs Across Logical Transformations and Question Answering   Tasks,Die Geometrie der Wahrheit beweisen: Konsistenz und Verallgemeinerung der Wahrheitsrichtungen in LLMs über logische Transformationen und Fragen beantwortende Aufgaben,检验真相的几何几何:贯穿逻辑转变和回答问题任务LLMs的真理方向的一致性和普遍性,http://arxiv.org/abs/2506.00823v1
1043,"Large language models (LLMs) encode vast world knowledge but struggle to stay up-to-date, often leading to errors and hallucinations. Knowledge editing offers an efficient alternative to retraining, enabling targeted modifications by updating specific model parameters. However, existing methods primarily focus on individual models, posing challenges in efficiently updating multiple models and adapting to new models. To address this, we propose OnceEdit, a novel ensemble-based approach that employs a plug-in model as the editing module, enabling stable knowledge updates across multiple models. Building on the model ensemble, OnceEdit introduces two key mechanisms to enhance its effectiveness. First, we introduce a dynamic weight mechanism through a \weight token for distinguishing between edit-related and non-edit-related instances, ensuring the appropriate utilization of knowledge from integrated models. Second, we incorporate an ensemble enhancement mechanism to mitigate the excessive reliance on the central model inherent in the model ensemble technique, making it more suitable for knowledge editing. Extensive experiments on diverse LLMs demonstrate that OnceEdit consistently outperforms existing methods while achieving superior editing efficiency. Further analysis confirms its adaptability and stability in multi-model editing scenarios. Our code will be available.",,"Weitao Ma, Xiyuan Du, Xiaocheng Feng, Lei Huang, Yichong Huang, Huiyi Zhang, Xiaoliang Yang, Baohang Li, Xiachong Feng, Ting Liu, Bing Qin",2025-06-01T03:48:54Z,One for All: Update Parameterized Knowledge Across Multiple Models,Eins für alle: Parametriertes Wissen über mehrere Modelle aktualisieren,人人共享:更新多模式的参数知识,http://arxiv.org/abs/2506.00817v1
1044,"Recent advances in large language models (LLMs) have significantly improved natural language generation, including creative tasks like poetry composition. However, most progress remains concentrated in high-resource languages. This raises an important question: Can LLMs be adapted for structured poetic generation in a low-resource, morphologically rich language such as Sanskrit? In this work, we introduce a dataset designed for translating English prose into structured Sanskrit verse, with strict adherence to classical metrical patterns, particularly the Anushtub meter. We evaluate a range of generative models-both open-source and proprietary-under multiple settings. Specifically, we explore constrained decoding strategies and instruction-based fine-tuning tailored to metrical and semantic fidelity. Our decoding approach achieves over 99% accuracy in producing syntactically valid poetic forms, substantially outperforming general-purpose models in meter conformity. Meanwhile, instruction-tuned variants show improved alignment with source meaning and poetic style, as supported by human assessments, albeit with marginal trade-offs in metrical precision.",,"Manoj Balaji Jagadeeshan, Samarth Bhatia, Pretam Ray, Harshul Raj Surana, Akhil Rajeev P, Priya Mishra, Annarao Kulkarni, Ganesh Ramakrishnan, Prathosh AP, Pawan Goyal",2025-06-01T03:35:46Z,From Plain Text to Poetic Form: Generating Metrically-Constrained   Sanskrit Verses,Vom einfachen Text zur poetischen Form: Erzeugen von Metrically Constrained Sanskrit Versen,从纯文字到诗样: 产生中度约束的梵文诗句,http://arxiv.org/abs/2506.00815v1
1045,"We propose GuessBench, a novel benchmark that evaluates Vision Language Models (VLMs) on modeling the pervasive, noisy, and pluralistic human creativity. GuessBench sources data from ""Guess the Build"", an online multiplayer Minecraft minigame where one player constructs a Minecraft build given a concept (e.g. caterpillar) and others try to guess it with natural language hints, presenting a pristine testbed for sensemaking creativity in the wild with VLMs acting as guessers. We curate 1500 images from the actual gameplay and design 2000 problems spanning static and dynamic image settings, natural language hints of varying completeness, and more. Extensive experiments with six open/API VLMs and five reasoning enhancement approaches demonstrate that GuessBench presents a uniquely challenging task in creativity modeling: even the start-of-the-art GPT-4o is incorrect on 34% of instances, while we observe a huge performance gap (13.87% vs. 53.93% on average) between open and API models. When used as a resource to improve VLMs, fine-tuning on the reasoning traces for GuessBench problems improves visual perception tasks by 15.36% on average. Further analysis reveals that VLM performance in creativity sensemaking correlates with the frequency of the concept in training data, while the accuracy drops sharply for concepts in underrepresented cultural contexts and low-resource languages.",,"Zifeng Zhu, Shangbin Feng, Herun Wan, Ningnan Wang, Minnan Luo, Yulia Tsvetkov",2025-06-01T03:32:36Z,GuessBench: Sensemaking Multimodal Creativity in the Wild,GuessBench: Sensemaking Multimodale Kreativität in der Wildnis,Gisguess B Beench: 荒野中思想制造的多模式创造性,http://arxiv.org/abs/2506.00814v1
1046,"Multimodal large language models (MLLMs) still struggle with complex reasoning tasks in Visual Question Answering (VQA). While current methods have advanced by incorporating visual prompts, our study uncovers critical limitations: these approaches indiscriminately annotate all detected objects for every visual question, generating excessive visual markers that degrade task performance. This issue stems primarily from a lack of focus on key visual elements, raising two important questions: Are all objects equally important, and do all questions require visual prompts? Motivated by Dual Process Theory, which distinguishes between instinctive and deliberate cognitive modes in human reasoning, we propose FOCUS, a plug-and-play approach that dynamically adapts to the complexity of questions, combining fast intuitive judgments with deliberate analytical reasoning to enhance the vision-language reasoning capability of the MLLM. For straightforward questions, FOCUS supports efficient zero-shot reasoning. For more complex tasks, it employs the conceptualizing before observation strategy to highlight critical elements. Extensive experiments on four benchmarks, ScienceQA, TextQA, VizWiz, and MME, demonstrate that FOCUS consistently improves the performance of both open-source and black-box MLLMs, achieving significant gains across all datasets. Ablation studies further validate the importance of combining diverse cognitive strategies with refined visual information for superior performance. Code will be released.",,"Songtao Jiang, Chenyi Zhou, Yan Zhang, Yeying Jin, Zuozhu Liu",2025-06-01T03:15:29Z,Fast or Slow? Integrating Fast Intuition and Deliberate Thinking for   Enhancing Visual Question Answering,Schnelle oder langsame Integration? Schnelle Intuition und überlegtes Denken für die Verbesserung der visuellen Frage beantworten,"快速或慢?结合快速思维和深思熟虑,以加强视觉问答",http://arxiv.org/abs/2506.00806v1
1047,"Medical Vision-Language Models (Med-VLMs) have achieved success across various tasks, yet most existing methods overlook the modality misalignment issue that can lead to untrustworthy responses in clinical settings. In this paper, we propose Hierarchical Self-Contrastive Rewarding (HSCR), a novel approach that addresses two critical challenges in Med-VLM alignment: 1) Cost-effective generation of high-quality preference data; 2) Capturing nuanced and context-aware preferences for improved alignment. HSCR first leverages the inherent capability of Med-VLMs to generate dispreferred responses with higher sampling probability. By analyzing output logit shifts after visual token dropout, we identify modality-coupled tokens that induce misalignment and derive an implicit alignment reward function. This function guides token replacement with hallucinated ones during decoding, producing high-quality dispreferred data. Furthermore, HSCR introduces a multi-level preference optimization strategy, which extends beyond traditional adjacent-level optimization by incorporating nuanced implicit preferences, leveraging relative quality in dispreferred data to capture subtle alignment cues for more precise and context-aware optimization. Extensive experiments across multiple medical tasks, including Med-VQA, medical image captioning and instruction following, demonstrate that HSCR not only enhances zero-shot performance but also significantly improves modality alignment and trustworthiness with just 2,000 training entries.",,"Songtao Jiang, Yan Zhang, Yeying Jin, Zhihang Tang, Yangyang Wu, Yang Feng, Jian Wu, Zuozhu Liu",2025-06-01T03:11:00Z,HSCR: Hierarchical Self-Contrastive Rewarding for Aligning Medical   Vision Language Models,HSCR: Hierarchische Selbst-Kontrastive Belohnung für die Ausrichtung medizinischer Visions-Sprachmodelle,HSCR: 医疗愿景语言模型对齐的等级制自我评级,http://arxiv.org/abs/2506.00805v1
1048,"Understanding the creation, evolution, and dissemination of scientific knowledge is crucial for bridging diverse subject areas and addressing complex global challenges such as pandemics, climate change, and ethical AI. Scientometrics, the quantitative and qualitative study of scientific literature, provides valuable insights into these processes. We introduce SciEvo, a longitudinal scientometric dataset with over two million academic publications, providing comprehensive contents information and citation graphs to support cross-disciplinary analyses. SciEvo is easy to use and available across platforms, including GitHub, Kaggle, and HuggingFace. Using SciEvo, we conduct a temporal study spanning over 30 years to explore key questions in scientometrics: the evolution of academic terminology, citation patterns, and interdisciplinary knowledge exchange. Our findings reveal critical insights, such as disparities in epistemic cultures, knowledge production modes, and citation practices. For example, rapidly developing, application-driven fields like LLMs exhibit significantly shorter citation age (2.48 years) compared to traditional theoretical disciplines like oral history (9.71 years). Our data and analytic tools can be accessed at https://github.com/Ahren09/SciEvo.",,"Yiqiao Jin, Yijia Xiao, Yiyang Wang, Jindong Wang",2025-06-01T02:58:47Z,"SciEvo: A 2 Million, 30-Year Cross-disciplinary Dataset for Temporal   Scientometric Analysis","SciEvo: 2 Millionen, 30-jährige disziplinübergreifende Datensätze für die zeitliche Scientometrische Analyse",SciEvo:用于时间科学测定分析的200万年、30年跨学科数据集,http://arxiv.org/abs/2410.09510v2
1049,"While densely annotated image captions significantly facilitate the learning of robust vision-language alignment, methodologies for systematically optimizing human annotation efforts remain underexplored. We introduce Chain-of-Talkers (CoTalk), an AI-in-the-loop methodology designed to maximize the number of annotated samples and improve their comprehensiveness under fixed budget constraints (e.g., total human annotation time). The framework is built upon two key insights. First, sequential annotation reduces redundant workload compared to conventional parallel annotation, as subsequent annotators only need to annotate the ``residual'' -- the missing visual information that previous annotations have not covered. Second, humans process textual input faster by reading while outputting annotations with much higher throughput via talking; thus a multimodal interface enables optimized efficiency. We evaluate our framework from two aspects: intrinsic evaluations that assess the comprehensiveness of semantic units, obtained by parsing detailed captions into object-attribute trees and analyzing their effective connections; extrinsic evaluation measures the practical usage of the annotated captions in facilitating vision-language alignment. Experiments with eight participants show our Chain-of-Talkers (CoTalk) improves annotation speed (0.42 vs. 0.30 units/sec) and retrieval performance (41.13% vs. 40.52%) over the parallel method.",,"Yijun Shen, Delong Chen, Fan Liu, Xingyu Wang, Chuanyi Zhang, Liang Yao, Yuhui Zheng",2025-06-01T02:55:39Z,Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions,Chain-of-Talkers (CoTalk): Schnelle menschliche Anmerkung von Dense Image Captions,谈话链(Contalk):人类对高密度图像描述的快速记号,http://arxiv.org/abs/2505.22627v2
1050,"Retrieval-Augmented Generation (RAG) enhances recency and factuality in answers. However, existing evaluations rarely test how well these systems cope with real-world noise, conflicting between internal and external retrieved contexts, or fast-changing facts. We introduce Retrieval-Aware Robustness Evaluation (RARE), a unified framework and large-scale benchmark that jointly stress-tests query and document perturbations over dynamic, time-sensitive corpora. One of the central features of RARE is a knowledge-graph-driven synthesis pipeline (RARE-Get) that automatically extracts single and multi-hop relations from the customized corpus and generates multi-level question sets without manual intervention. Leveraging this pipeline, we construct a dataset (RARE-Set) spanning 400 expert-level time-sensitive finance, economics, and policy documents and 48,322 questions whose distribution evolves as the underlying sources change. To quantify resilience, we formalize retrieval-conditioned robustness metrics (RARE-Met) that capture a model's ability to remain correct or recover when queries, documents, or real-world retrieval results are systematically altered. Our results show that RAG systems exhibit surprising vulnerability to perturbations, with document robustness consistently being the weakest point regardless of generator size or architecture. RAG systems consistently show lower robustness on multi-hop queries than single-hop queries across all domains.",,"Yixiao Zeng, Tianyu Cao, Danqing Wang, Xinran Zhao, Zimeng Qiu, Morteza Ziyadi, Tongshuang Wu, Lei Li",2025-06-01T02:42:36Z,RARE: Retrieval-Aware Robustness Evaluation for Retrieval-Augmented   Generation Systems,RARE: Retrieval-Aware Robustheitsbewertung für Retrieval-Augmented Generation Systems,RARE: 回收-加速发电系统的检索-软件强度评价,http://arxiv.org/abs/2506.00789v1
1051,"Improving cultural competence of language technologies is important. However most recent works rarely engage with the communities they study, and instead rely on synthetic setups and imperfect proxies of culture. In this work, we take a human-centered approach to discover and measure language-based cultural norms, and cultural competence of LLMs. We focus on a single kind of culture, research cultures, and a single task, adapting writing across research cultures. Through a set of interviews with interdisciplinary researchers, who are experts at moving between cultures, we create a framework of structural, stylistic, rhetorical, and citational norms that vary across research cultures. We operationalise these features with a suite of computational metrics and use them for (a) surfacing latent cultural norms in human-written research papers at scale; and (b) highlighting the lack of cultural competence of LLMs, and their tendency to homogenise writing. Overall, our work illustrates the efficacy of a human-centered approach to measuring cultural norms in human-written and LLM-generated texts.",,"Shaily Bhatt, Tal August, Maria Antoniak",2025-06-01T02:23:55Z,Research Borderlands: Analysing Writing Across Research Cultures,Forschungsgrenzen: Analysieren des Schreibens über Forschungskulturen hinweg,研究边界地区:分析跨研究文化的写作,http://arxiv.org/abs/2506.00784v1
1052,"Large Language Models (LLMs), such as the GPT series, have driven significant industrial applications, leading to economic and societal transformations. However, a comprehensive understanding of their real-world applications remains limited. To address this, we introduce REALM, a dataset of over 94,000 LLM use cases collected from Reddit and news articles. REALM captures two key dimensions: the diverse applications of LLMs and the demographics of their users. It categorizes LLM applications and explores how users' occupations relate to the types of applications they use. By integrating real-world data, REALM offers insights into LLM adoption across different domains, providing a foundation for future research on their evolving societal roles.",,"Jingwen Cheng, Kshitish Ghate, Wenyue Hua, William Yang Wang, Hong Shen, Fei Fang",2025-06-01T02:21:49Z,REALM: A Dataset of Real-World LLM Use Cases,REAM: Ein Datensatz von realen LLM-Use Cases,真实世界LLM使用案例数据集,http://arxiv.org/abs/2503.18792v2
1053,"Large language models (LLMs) have made remarkable strides in various natural language processing tasks, but their performance on complex reasoning problems remains hindered by a lack of explainability and trustworthiness. This issue, often manifesting as hallucinations or unattributable reasoning processes, limits their applicability in complex reasoning scenarios. To address this, we propose Knowledge Graph-constrained Trajectory Reasoning Attribution and Chain Explanation Supervision (KG-TRACES), a novel framework that enhances the reasoning ability of LLMs through explicit supervision over reasoning paths and processes. KG-TRACES jointly supervises the model to: (1) predict symbolic relation paths, (2) predict full triple-level reasoning paths, and (3) generate attribution-aware reasoning processes grounded in the reasoning paths. At inference phase, the model adapts to both KG-available and KG-unavailable scenarios, retrieving reasoning paths from a KG when possible or predicting plausible reasoning paths with only intrinsic knowledge when not. This design enables the model to reason in an explainable and source-attributable pattern. Through extensive experiments on complex reasoning tasks, we demonstrate that KG-TRACES significantly outperforms existing SOTA: it improves Hits@1 by 1.6% and F1 by 4.7% on WebQSP, and achieves improvements of 4.8% in Hits@1 and 2.1% in F1 on CWQ. Moreover, we show its transferability to specialized domains such as medicine. By visualizing the intermediate steps of reasoning processes, we further show that the explicit supervision introduced by KG-TRACES leads to more stable and goal-directed reasoning processes, aligning closely with correct answers. Code is available at https://github.com/Edaizi/KG-TRACES.",,"Rong Wu, Pinlong Cai, Jianbiao Mei, Licheng Wen, Tao Hu, Xuemeng Yang, Daocheng Fu, Botian Shi",2025-06-01T02:20:45Z,KG-TRACES: Enhancing Large Language Models with Knowledge   Graph-constrained Trajectory Reasoning and Attribution Supervision,KG-TRACES: Erweiterung großer Sprachmodelle mit wissensgraphisch bedingter Trajektorie-Aufklärung und Zuordnungsüberwachung,KG-TRACES:加强具有受限制的跟踪原因和职责监督的知识图表限制的大型语言模式,http://arxiv.org/abs/2506.00783v1
1054,"Multiple choice question answering (MCQA) is popular for LLM evaluation due to its simplicity and human-like testing, but we argue for its reform. We first reveal flaws in MCQA's format, as it struggles to: 1) test generation/subjectivity; 2) match LLM use cases; and 3) fully test knowledge. We instead advocate for generative formats based on human testing, where LLMs construct and explain answers, better capturing user needs and knowledge while remaining easy to score. We then show even when MCQA is a useful format, its datasets suffer from: leakage; unanswerability; shortcuts; and saturation. In each issue, we give fixes from education, like rubrics to guide MCQ writing; scoring methods to bridle guessing; and Item Response Theory to build harder MCQs. Lastly, we discuss LLM errors in MCQA, robustness, biases, and unfaithful explanations, showing how our prior solutions better measure or address these issues. While we do not need to desert MCQA, we encourage more efforts in refining the task based on educational testing, advancing evaluations.",,"Nishant Balepur, Rachel Rudinger, Jordan Lee Boyd-Graber",2025-06-01T02:05:44Z,Which of These Best Describes Multiple Choice Evaluation with LLMs? A)   Forced B) Flawed C) Fixable D) All of the Above,Welche dieser besten Beschreibungen beschreibt die Multiple-Choice-Bewertung mit LLMs? A) Gezwungen B) Abgeflachtes C) Fixierbares D) Alles oben,A) 强迫(B) 易燃(C) 易燃(D),http://arxiv.org/abs/2502.14127v2
1055,"LLMs are aligned to follow input instructions by learning which of two responses users prefer for a prompt. However, such preference data do not convey why users prefer responses that are chosen or rejected, so LLMs trained on these datasets cannot tailor responses to varied user needs. To surface these parameters of personalization, we apply abductive reasoning to preference data, inferring needs and interests of users, i.e., personas, that may prefer either response. We test this idea in two steps: Persona Inference (PI), abductively inferring personas of users who prefer chosen or rejected outputs, and Persona Tailoring (PT), training models to tailor outputs to personas from PI. We show: 1) LLMs infer personas accurately explaining why different users may prefer both chosen or rejected outputs; 2) Training on preference data augmented with PI personas via PT boosts personalization and generalizes to supporting user-written personas; and 3) Rejected response personas form harder personalization evaluations, showing PT better aids users with uncommon preferences versus typical alignment methods. We argue for an abductive view of preferences for personalization, asking not only which response is better but when, why, and for whom.",,"Nishant Balepur, Vishakh Padmakumar, Fumeng Yang, Shi Feng, Rachel Rudinger, Jordan Lee Boyd-Graber",2025-06-01T02:03:00Z,Whose Boat Does it Float? Improving Personalization in Preference Tuning   via Inferred User Personas,Wessen Boot schwimmt es? Verbesserung der Personalisierung in Präferenz Tuning über abgeleitete Benutzer Personas,谁的船会浮在谁的船上?,http://arxiv.org/abs/2501.11549v2
1056,"Faced with an expensive human annotation process, creators of NLP systems increasingly turn to synthetic data generation. While this method shows promise, the extent to which synthetic data can replace human annotation is poorly understood. We investigate the use of synthetic data in Fact Verification (FV) and Question Answering (QA) by studying the effects of incrementally replacing human generated data with synthetic points on eight diverse datasets. Strikingly, replacing up to 90% of the training data only marginally decreases performance, but replacing the final 10% leads to severe declines. We find that models trained on purely synthetic data can be reliably improved by including as few as 125 human generated data points. We show that matching the performance gain of just a little additional human data (only 200 points) requires an order of magnitude more synthetic data and estimate price ratios at which human annotation would be a more cost-effective solution. Our results suggest that even when human annotation at scale is infeasible, there is great value to having a small proportion of the dataset being human generated.",,"Dhananjay Ashok, Jonathan May",2025-06-01T02:02:34Z,A Little Human Data Goes A Long Way,Ein wenig menschliche Daten gehen einen langen Weg,一个小小的人类数据会走很长的路,http://arxiv.org/abs/2410.13098v2
1057,"Large Language Models (LLMs) have demonstrated impressive performance in biomedical relation extraction, even in zero-shot scenarios. However, evaluating LLMs in this task remains challenging due to their ability to generate human-like text, often producing synonyms or abbreviations of gold-standard answers, making traditional automatic evaluation metrics unreliable. On the other hand, while human evaluation is more reliable, it is costly and time-consuming, making it impractical for real-world applications. This paper investigates the use of LLMs-as-the-Judge as an alternative evaluation method for biomedical relation extraction. We benchmark 8 LLMs as judges to evaluate the responses generated by 5 other LLMs across 3 biomedical relation extraction datasets. Unlike other text-generation tasks, we observe that LLM-based judges perform quite poorly (usually below 50% accuracy) in the biomedical relation extraction task. Our findings reveal that it happens mainly because relations extracted by LLMs do not adhere to any standard format. To address this, we propose structured output formatting for LLM-generated responses that helps LLM-Judges to improve their performance by about 15% (on average). We also introduce a domain adaptation technique to further enhance LLM-Judge performance by effectively transferring knowledge between datasets. We release both our human-annotated and LLM-annotated judgment data (36k samples in total) for public use here: https://github.com/tahmedge/llm_judge_biomedical_re.",,"Md Tahmid Rahman Laskar, Israt Jahan, Elham Dolatabadi, Chun Peng, Enamul Hoque, Jimmy Huang",2025-06-01T02:01:52Z,Improving Automatic Evaluation of Large Language Models (LLMs) in   Biomedical Relation Extraction via LLMs-as-the-Judge,Verbesserung der automatischen Bewertung großer Sprachmodelle (LLMs) in der biomedizinischen Beziehungsextraktion über LLMs-as-the-Judge,改进对生物医学关系中大语言模型(LLM)的自动评价,http://arxiv.org/abs/2506.00777v1
1058,"While ChatGPT and GPT-based models are able to effectively perform many tasks without additional fine-tuning, they struggle with tasks related to extremely low-resource languages and indigenous languages. Uniform Meaning Representation (UMR), a semantic representation designed to capture the meaning of texts in many languages, is well-positioned to be leveraged in the development of low-resource language technologies. In this work, we explore the downstream utility of UMR for low-resource languages by incorporating it into GPT-4 prompts. Specifically, we examine the ability of GPT-4 to perform translation from three indigenous languages (Navajo, Ar\'apaho, and Kukama), with and without demonstrations, as well as with and without UMR annotations. Ultimately, we find that in the majority of our test cases, integrating UMR into the prompt results in a statistically significant increase in performance, which is a promising indication of future applications of the UMR formalism.",,Shira Wein,2025-06-01T01:53:16Z,Can Uniform Meaning Representation Help GPT-4 Translate from Indigenous   Languages?,Kann einheitliche Bedeutung Darstellung Hilfe GPT-4 Übersetzen aus indigenen Sprachen?,统一代表方式能帮助GPT-4翻译土著语言吗?,http://arxiv.org/abs/2502.08900v2
1059,"Recent studies have shown that supervised fine-tuning of LLMs on a small number of high-quality datasets can yield strong reasoning capabilities. However, full fine-tuning (Full FT), while powerful, is computationally expensive and susceptible to overfitting and catastrophic forgetting, particularly when data is limited. Sparse fine-tuning, which previously achieved notable success by updating only a small subset of model parameters, offers a promising trade-off between efficiency and effectiveness. Yet, it has lagged behind in the LLM era due to the difficulty of identifying parameters truly critical for reasoning. In this work, we state that weights with the largest magnitude after low-rank approximation are critical weights for fine-tuning, which we call Principal Weights. Surprisingly, while magnitude-based sparse fine-tuning performs poorly as a baseline on LLM fine-tuning, it becomes highly effective after rank reduction. These insights motivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only updates the top 5% Principal Weights throughout training and consistently achieves better performance on reasoning tasks than Full FT, while maintaining memory efficiency on par with popular parameter-efficient fine-tuning methods. In addition to strong performance on target domains such as arithmetic reasoning, LIFT also retains up to 20% more source-domain knowledge, compared to Full FT and LoRA. Our code is available at: https://github.com/zihanghliu/LIFT.",,"Zihang Liu, Tianyu Pang, Oleg Balabanov, Chaoqun Yang, Tianjin Huang, Lu Yin, Yaoqing Yang, Shiwei Liu",2025-06-01T01:31:50Z,LIFT the Veil for the Truth: Principal Weights Emerge after Rank   Reduction for Reasoning-Focused Supervised Fine-Tuning,LIFT the Veil for the Truth: Hauptgewichte nach Rang-Reduktion für vernünftig-fokussierte Supervised Fine-Tuning,自由真理之光:因受监督的罚款调整而降级后的主要重负,http://arxiv.org/abs/2506.00772v1
1060,"As online platforms grow, comment sections increasingly host harassment that undermines user experience and well-being. This study benchmarks three leading large language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic Claude 3 Opus, on a corpus of 5,080 YouTube comments sampled from high-abuse threads in gaming, lifestyle, food vlog, and music channels. The dataset comprises 1,334 harmful and 3,746 non-harmful messages in English, Arabic, and Indonesian, annotated independently by two reviewers with substantial agreement (Cohen's kappa = 0.83). Using a unified prompt and deterministic settings, GPT-4.1 achieved the best overall balance with an F1 score of 0.863, precision of 0.887, and recall of 0.841. Gemini flagged the highest share of harmful posts (recall = 0.875) but its precision fell to 0.767 due to frequent false positives. Claude delivered the highest precision at 0.920 and the lowest false-positive rate of 0.022, yet its recall dropped to 0.720. Qualitative analysis showed that all three models struggle with sarcasm, coded insults, and mixed-language slang. These results underscore the need for moderation pipelines that combine complementary models, incorporate conversational context, and fine-tune for under-represented languages and implicit abuse. A de-identified version of the dataset and full prompts is publicly released to promote reproducibility and further progress in automated content moderation.",,Amel Muminovic,2025-06-01T01:17:36Z,Moderating Harm: Benchmarking Large Language Models for Cyberbullying   Detection in YouTube Comments,Moderating Harm: Benchmarking von großen Sprachmodellen für Cyberbullying Detection in YouTube Kommentare,在YouTube评论中为网络欺欺欺欺欺欺欺欺欺欺欺欺欺凌探测大语言模式制定基准,http://arxiv.org/abs/2505.18927v3
1061,"We introduce TESS 2, a general instruction-following diffusion language model that outperforms contemporary instruction-tuned diffusion models, as well as matches and sometimes exceeds strong autoregressive (AR) models. We train TESS 2 by first adapting a strong AR model via continued pretraining with the usual cross-entropy as diffusion loss, and then performing further instruction tuning. We find that adaptation training as well as the choice of the base model is crucial for training good instruction-following diffusion models. We further propose reward guidance, a novel and modular inference-time guidance procedure to align model outputs without needing to train the underlying model. Finally, we show that TESS 2 further improves with increased inference-time compute, highlighting the utility of diffusion LMs in having fine-grained controllability over the amount of compute used at inference time. Code and models are available at https://github.com/hamishivi/tess-2.",,"Jaesung Tae, Hamish Ivison, Sachin Kumar, Arman Cohan",2025-06-01T00:59:12Z,TESS 2: A Large-Scale Generalist Diffusion Language Model,"TESS 2: Ein großformatiges, generalistisches Diffusions-Sprachenmodell",TESS 2: 大规模通用通用传播语言模式,http://arxiv.org/abs/2502.13917v2
1062,"Parameter-efficient fine-tuning (PEFT) methods typically assume that Large Language Models (LLMs) are trained on data from a single device or client. However, real-world scenarios often require fine-tuning these models on private data distributed across multiple devices. Federated Learning (FL) offers an appealing solution by preserving user privacy, as sensitive data remains on local devices during training. Nonetheless, integrating PEFT methods into FL introduces two main challenges: communication overhead and data heterogeneity. In this paper, we introduce FedTT and FedTT+, methods for adapting LLMs by integrating tensorized adapters into client-side models' encoder/decoder blocks. FedTT is versatile and can be applied to both cross-silo FL and large-scale cross-device FL. FedTT+, an extension of FedTT tailored for cross-silo FL, enhances robustness against data heterogeneity by adaptively freezing portions of tensor factors, further reducing the number of trainable parameters. Experiments on BERT and LLaMA models demonstrate that our proposed methods successfully address data heterogeneity challenges and perform on par or even better than existing federated PEFT approaches while achieving up to 10$\times$ reduction in communication cost.",,"Sajjad Ghiasvand, Yifan Yang, Zhiyu Xue, Mahnoosh Alizadeh, Zheng Zhang, Ramtin Pedarsani",2025-06-01T00:41:09Z,Communication-Efficient and Tensorized Federated Fine-Tuning of Large   Language Models,Kommunikationseffiziente und spannungsorientierte Federated Fine-Tuning von großen Sprachmodellen,大语言模式的通讯效率和通量化联-联式先进和联-联-联-联-联-联-联-联-联-联-联-联-联-联-联-联-联-联-联-联-联,http://arxiv.org/abs/2410.13097v2
1063,"Large Language Models (LLMs) trained on massive data capture rich information embedded in the training data. However, this also introduces the risk of privacy leakage, particularly involving personally identifiable information (PII). Although previous studies have shown that this risk can be mitigated through methods such as privacy neurons, they all assume that both the (sensitive) training data and user queries are in English. We show that they cannot defend against the privacy leakage in cross-lingual contexts: even if the training data is exclusively in one language, these (private) models may still reveal private information when queried in another language. In this work, we first investigate the information flow of cross-lingual privacy leakage to give a better understanding. We find that LLMs process private information in the middle layers, where representations are largely shared across languages. The risk of leakage peaks when converted to a language-specific space in later layers. Based on this, we identify privacy-universal neurons and language-specific privacy neurons. Privacy-universal neurons influence privacy leakage across all languages, while language-specific privacy neurons are only related to specific languages. By deactivating these neurons, the cross-lingual privacy leakage risk is reduced by 23.3%-31.6%.",,"Wenshuo Dong, Qingsong Yang, Shu Yang, Lijie Hu, Meng Ding, Wanyu Lin, Tianhang Zheng, Di Wang",2025-06-01T00:10:30Z,Understanding and Mitigating Cross-lingual Privacy Leakage via   Language-specific and Universal Privacy Neurons,Verständnis und Abmilderung von Cross-lingual Privacy Leakage über sprachspezifische und Universal Privacy Neuronen,通过特定语言和通用隐私中世纪理解和减少跨语言隐私疏漏,http://arxiv.org/abs/2506.00759v1
1064,"Computer-use agents (CUAs) promise to automate complex tasks across operating systems (OS) and the web, but remain vulnerable to indirect prompt injection. Current evaluations of this threat either lack support realistic but controlled environments or ignore hybrid web-OS attack scenarios involving both interfaces. To address this, we propose RedTeamCUA, an adversarial testing framework featuring a novel hybrid sandbox that integrates a VM-based OS environment with Docker-based web platforms. Our sandbox supports key features tailored for red teaming, such as flexible adversarial scenario configuration, and a setting that decouples adversarial evaluation from navigational limitations of CUAs by initializing tests directly at the point of an adversarial injection. Using RedTeamCUA, we develop RTC-Bench, a comprehensive benchmark with 864 examples that investigate realistic, hybrid web-OS attack scenarios and fundamental security vulnerabilities. Benchmarking current frontier CUAs identifies significant vulnerabilities: Claude 3.7 Sonnet | CUA demonstrates an ASR of 42.9%, while Operator, the most secure CUA evaluated, still exhibits an ASR of 7.6%. Notably, CUAs often attempt to execute adversarial tasks with an Attempt Rate as high as 92.5%, although failing to complete them due to capability limitations. Nevertheless, we observe concerning ASRs of up to 50% in realistic end-to-end settings, with the recently released frontier Claude 4 Opus | CUA showing an alarming ASR of 48%, demonstrating that indirect prompt injection presents tangible risks for even advanced CUAs despite their capabilities and safeguards. Overall, RedTeamCUA provides an essential framework for advancing realistic, controlled, and systematic analysis of CUA vulnerabilities, highlighting the urgent need for robust defenses to indirect prompt injection prior to real-world deployment.",,"Zeyi Liao, Jaylen Jones, Linxi Jiang, Eric Fosler-Lussier, Yu Su, Zhiqiang Lin, Huan Sun",2025-06-01T00:04:07Z,RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in   Hybrid Web-OS Environments,RedTeamCUA: Realistisches Adversarial Testen von Computer-Use-Agenten in hybriden Web-OS-Umgebungen,Red TeamCUA:对混合网络-OS环境的计算机使用代理器进行现实的反反向测试,http://arxiv.org/abs/2505.21936v2
1065,"Large language models (LLMs) have become the backbone of modern natural language processing but pose privacy concerns about leaking sensitive training data. Membership inference attacks (MIAs), which aim to infer whether a sample is included in a model's training dataset, can serve as a foundation for broader privacy threats. Existing defenses designed for traditional classification models do not account for the sequential nature of text data. As a result, they either require significant computational resources or fail to effectively mitigate privacy risks in LLMs. In this work, we propose \methodname, a lightweight yet effective empirical privacy defense for protecting training data of language models by leveraging token-specific characteristics. By analyzing token dynamics during training, we propose a token selection strategy that categorizes tokens into hard tokens for learning and memorized tokens for unlearning. Subsequently, our training-phase defense optimizes a novel dual-purpose token-level loss to achieve a Pareto-optimal balance between utility and privacy. Extensive experiments demonstrate that our approach not only provides strong protection against MIAs but also improves language modeling performance by around 10\% across various LLM architectures and datasets compared to the baselines.",,"Toan Tran, Ruixuan Liu, Li Xiong",2025-05-31T23:53:12Z,"Tokens for Learning, Tokens for Unlearning: Mitigating Membership   Inference Attacks in Large Language Models via Dual-Purpose Training","Tokens for Learning, Tokens for Unlearning: Mildernde Schlussfolgerungsangriffe in großen Sprachmodellen durch Dual-Purpose-Training",学习的代言人、不学习的代代代人:通过双重目的培训在大语言模式中减少成员推论攻击,http://arxiv.org/abs/2502.19726v2
1066,"Aligning large language models with humans is challenging due to the inherently multifaceted nature of preference feedback. While existing approaches typically frame this as a multi-objective optimization problem, they often overlook how humans actually make decisions. Research on bounded rationality suggests that human decision making follows satisficing strategies-optimizing primary objectives while ensuring others meet acceptable thresholds. To bridge this gap and operationalize the notion of satisficing alignment, we propose SITAlign: an inference time framework that addresses the multifaceted nature of alignment by maximizing a primary objective while satisfying threshold-based constraints on secondary criteria. We provide theoretical insights by deriving sub-optimality bounds of our satisficing based inference alignment approach. We empirically validate SITAlign's performance through extensive experimentation on multiple benchmarks. For instance, on the PKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while ensuring a threshold on harmlessness, SITAlign outperforms the state-of-the-art multi objective decoding strategy by a margin of 22.3% in terms of GPT-4 win-tie rate for helpfulness reward while adhering to the threshold on harmlessness.",,"Mohamad Chehade, Soumya Suvra Ghosal, Souradip Chakraborty, Avinash Reddy, Dinesh Manocha, Hao Zhu, Amrit Singh Bedi",2025-05-31T23:47:06Z,Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time,Begrenzte Rationalität für LLMs: Zufriedene Ausrichtung zur Folgezeit,LLM女士的理 理 理 理:在推断时满足一致,http://arxiv.org/abs/2505.23729v2
1067,"Large Language Models (LLMs) have recently demonstrated impressive few-shot learning capabilities through in-context learning (ICL). However, ICL performance is highly dependent on the choice of few-shot demonstrations, making the selection of the most optimal examples a persistent research challenge. This issue is further amplified in low-resource Indic languages, where the scarcity of ground-truth data complicates the selection process. In this work, we propose PromptRefine, a novel Alternating Minimization approach for example selection that improves ICL performance on low-resource Indic languages. PromptRefine leverages auxiliary example banks from related high-resource Indic languages and employs multi-task learning techniques to align language-specific retrievers, enabling effective cross-language retrieval. Additionally, we incorporate diversity in the selected examples to enhance generalization and reduce bias. Through comprehensive evaluations on four text generation tasks -- Cross-Lingual Question Answering, Multilingual Question Answering, Machine Translation, and Cross-Lingual Summarization using state-of-the-art LLMs such as LLAMA-3.1-8B, LLAMA-2-7B, Qwen-2-7B, and Qwen-2.5-7B, we demonstrate that PromptRefine significantly outperforms existing frameworks for retrieving examples.",,"Soumya Suvra Ghosal, Soumyabrata Pal, Koyel Mukherjee, Dinesh Manocha",2025-05-31T23:34:02Z,PromptRefine: Enhancing Few-Shot Performance on Low-Resource Indic   Languages with Example Selection from Related Example Banks,PromptRefine: Verbesserung der weniger scharfen Leistung auf Low-Resource-Indic-Sprachen mit Beispielauswahl aus verwandten Beispielbanken,"快速注释:通过从相关实例银行中选用示例,提高低资源型印度语言的微小热性能",http://arxiv.org/abs/2412.05710v2
1068,"Retrieval-augmented generation (RAG) has shown great promise for knowledge-intensive tasks and recently advanced with agentic RAG, where language agents engage in multi-round interactions with external knowledge sources for adaptive information retrieval. However, existing agentic RAG methods often depend on ad-hoc prompt engineering and lack a unified optimization framework. We introduce RAG-Gym, a comprehensive platform that systematically explores three optimization dimensions: (1) prompt engineering, (2) actor tuning, and (3) critic training. For prompt engineering, we propose Re$^2$Search, a novel agent incorporating reasoning reflection that significantly outperforms standard prompts. In actor tuning, we evaluate three popular post-training algorithms with fine-grained process supervision and identify direct preference optimization as the most effective. We further demonstrate that a trained critic can enhance inference by selecting higher-quality intermediate reasoning steps. Together, these findings lead to the optimized Re$^2$Search++ agent, which surpasses most recent methods like Search-R1 by a relative increase of 3.2% to 11.6% in average F1. Finally, we examine the impact of different reward sources and analyze scaling properties in training and inference, offering practical insights for agentic RAG optimization. The project homepage is available at https://rag-gym.github.io.",,"Guangzhi Xiong, Qiao Jin, Xiao Wang, Yin Fang, Haolin Liu, Yifan Yang, Fangyuan Chen, Zhixing Song, Dengyu Wang, Minjia Zhang, Zhiyong Lu, Aidong Zhang",2025-05-31T23:32:16Z,RAG-Gym: Systematic Optimization of Language Agents for   Retrieval-Augmented Generation,RAG-Gym: Systematische Optimierung von Sprachagenten für retrieval-Augmented Generation,RAG-Gym: 将语文物剂系统优化用于回收-提款一代人,http://arxiv.org/abs/2502.13957v2
1069,"We propose Model Swarms, a collaborative search algorithm to adapt LLMs via swarm intelligence, the collective behavior guiding individual systems. Specifically, Model Swarms starts with a pool of LLM experts and a utility function. Guided by the best-found checkpoints across models, diverse LLM experts collaboratively move in the weight space and optimize a utility function representing model adaptation objectives. Compared to existing model composition approaches, Model Swarms offers tuning-free model adaptation, works in low-data regimes with as few as 200 examples, and does not require assumptions about specific experts in the swarm or how they should be composed. Extensive experiments demonstrate that Model Swarms could flexibly adapt LLM experts to a single task, multi-task domains, reward models, as well as diverse human interests, improving over 12 model composition baselines by up to 21.0% across tasks and contexts. Further analysis reveals that LLM experts discover previously unseen capabilities in initial checkpoints and that Model Swarms enable the weak-to-strong transition of experts through the collaborative search process.",,"Shangbin Feng, Zifeng Wang, Yike Wang, Sayna Ebrahimi, Hamid Palangi, Lesly Miculicich, Achin Kulshrestha, Nathalie Rauschmayr, Yejin Choi, Yulia Tsvetkov, Chen-Yu Lee, Tomas Pfister",2025-05-31T23:27:47Z,Model Swarms: Collaborative Search to Adapt LLM Experts via Swarm   Intelligence,"Modellswarms: Gemeinsame Suche, um LLM-Experten über Swarm Intelligence anzupassen",模模蜂群:通过Swarm情报系统合作搜索适应LLM专家,http://arxiv.org/abs/2410.11163v2
1070,"Addressing gender bias and maintaining logical coherence in machine translation remains challenging, particularly when translating between natural gender languages, like English, and genderless languages, such as Persian, Indonesian, and Finnish. We introduce the Translate-with-Care (TWC) dataset, comprising 3,950 challenging scenarios across six low- to mid-resource languages, to assess translation systems' performance. Our analysis of diverse technologies, including GPT-4, mBART-50, NLLB-200, and Google Translate, reveals a universal struggle in translating genderless content, resulting in gender stereotyping and reasoning errors. All models preferred masculine pronouns when gender stereotypes could influence choices. Google Translate and GPT-4 showed particularly strong bias, favoring male pronouns 4-6 times more than feminine ones in leadership and professional success contexts. Fine-tuning mBART-50 on TWC substantially resolved these biases and errors, led to strong generalization, and surpassed proprietary LLMs while remaining open-source. This work emphasizes the need for targeted approaches to gender and semantic coherence in machine translation, particularly for genderless languages, contributing to more equitable and accurate translation systems.",,"Pardis Sadat Zahraei, Ali Emami",2025-05-31T23:27:07Z,"Translate With Care: Addressing Gender Bias, Neutrality, and Reasoning   in Large Language Model Translations","Übersetzen mit Sorgfalt: Adressieren von Gender-Bias, Neutralität und Vernunft in großsprachigen Modellübersetzungen","用 "" 照护翻译:解决性别偏见、中立和大语言模型翻译的理由 """,http://arxiv.org/abs/2506.00748v1
1071,"Large language models (LLMs) excel at answering questions but remain passive learners-absorbing static data without the ability to question and refine knowledge. This paper explores how LLMs can transition to interactive, question-driven learning through student-teacher dialogues. We introduce INTERACT (INTERactive learning for Adaptive Concept Transfer), a framework in which a ""student"" LLM engages a ""teacher"" LLM through iterative inquiries to acquire knowledge across 1,347 contexts, including song lyrics, news articles, movie plots, academic papers, and images. Our experiments show that across a wide range of scenarios and LLM architectures, interactive learning consistently enhances performance, achieving up to a 25% improvement, with 'cold-start' student models matching static learning baselines in as few as five dialogue turns. Interactive setups can also mitigate the disadvantages of weaker teachers, showcasing the robustness of question-driven learning.",,"Aum Kendapadi, Kerem Zaman, Rakesh R. Menon, Shashank Srivastava",2025-05-31T23:21:54Z,"INTERACT: Enabling Interactive, Question-Driven Learning in Large   Language Models","INTERACT: Interaktives, fragwürdiges Lernen in großen Sprachmodellen ermöglichen","区域间:促进互动,以大语言模式进行有问题、有问题的学习",http://arxiv.org/abs/2412.11388v2
1072,"Active Learning (AL) has been a powerful paradigm for improving model efficiency and performance by selecting the most informative data points for labeling and training. In recent active learning frameworks, Large Language Models (LLMs) have been employed not only for selection but also for generating entirely new data instances and providing more cost-effective annotations. Motivated by the increasing importance of high-quality data and efficient model training in the era of LLMs, we present a comprehensive survey on LLM-based Active Learning. We introduce an intuitive taxonomy that categorizes these techniques and discuss the transformative roles LLMs can play in the active learning loop. We further examine the impact of AL on LLM learning paradigms and its applications across various domains. Finally, we identify open challenges and propose future research directions. This survey aims to serve as an up-to-date resource for researchers and practitioners seeking to gain an intuitive understanding of LLM-based AL techniques and deploy them to new applications.",,"Yu Xia, Subhojyoti Mukherjee, Zhouhang Xie, Junda Wu, Xintong Li, Ryan Aponte, Hanjia Lyu, Joe Barrow, Hongjie Chen, Franck Dernoncourt, Branislav Kveton, Tong Yu, Ruiyi Zhang, Jiuxiang Gu, Nesreen K. Ahmed, Yu Wang, Xiang Chen, Hanieh Deilamsalehy, Sungchul Kim, Zhengmian Hu, Yue Zhao, Nedim Lipka, Seunghyun Yoon, Ting-Hao Kenneth Huang, Zichao Wang, Puneet Mathur, Soumyabrata Pal, Koyel Mukherjee, Zhehao Zhang, Namyong Park, Thien Huu Nguyen, Jiebo Luo, Ryan A. Rossi, Julian McAuley",2025-05-31T23:20:09Z,From Selection to Generation: A Survey of LLM-based Active Learning,Von der Auswahl zur Generation: Eine Umfrage des LLM-basierten aktiven Lernens,从选择到产生:基于LLM的主动学习调查,http://arxiv.org/abs/2502.11767v2
1073,"Evacuation decision prediction is critical for efficient and effective wildfire response by helping emergency management anticipate traffic congestion and bottlenecks, allocate resources, and minimize negative impacts. Traditional statistical methods for evacuation decision prediction fail to capture the complex and diverse behavioral logic of different individuals. In this work, for the first time, we introduce FLARE, short for facilitating LLM for advanced reasoning on wildfire evacuation decision prediction, a Large Language Model (LLM)-based framework that integrates behavioral theories and models to streamline the Chain-of-Thought (CoT) reasoning and subsequently integrate with memory-based Reinforcement Learning (RL) module to provide accurate evacuation decision prediction and understanding. Our proposed method addresses the limitations of using existing LLMs for evacuation behavioral predictions, such as limited survey data, mismatching with behavioral theory, conflicting individual preferences, implicit and complex mental states, and intractable mental state-behavior mapping. Experiments on three post-wildfire survey datasets show an average of 20.47% performance improvement over traditional theory-informed behavioral models, with strong cross-event generalizability. Our complete code is publicly available at https://github.com/SusuXu-s-Lab/FLARE",,"Ruxiao Chen, Chenguang Wang, Yuran Sun, Xilei Zhao, Susu Xu",2025-05-31T23:18:46Z,From Perceptions to Decisions: Wildfire Evacuation Decision Prediction   with Behavioral Theory-informed LLMs,Von Wahrnehmungen zu Entscheidungen: Wildfire Evacuation Decision Prediction with Behavioral Theory-informed LLMs,从概念到决定:野火撤离决定 预测行为理论 -- -- 知情的LMs,http://arxiv.org/abs/2502.17701v2
1074,"Parameter Efficient Fine-Tuning (PEFT) has become the de-facto approach in adapting Large Language Models (LLMs) for downstream tasks in Natural Language Processing. However, its adoption in privacy-preserving distributed learning frameworks, such as Federated Learning (FL), remains relatively limited. This is mainly due to challenges specific to FL, such as resource-constrained devices and diverse data distributions among clients. In this paper, we propose an efficient method to perform PEFT within the FL framework for Multi-Head Attention (MHA) based language models. We address the challenges through head pruning, a novel head-specific weighted aggregation mechanism, and a client selection strategy. Head pruning minimizes training complexity within the clients, guided by the importance score computed based on the confidence of the attention head. Weighted aggregation of heads ensures the global model captures crucial updates from diverse clients complementing our client selection strategy. We show results on the MultiNLI benchmark along with 20 Newsgroups, XL-Sum, and E2E NLG datasets. We use the MultiNLI dataset and T5-small model with LoRA as our PEFT method, attaining sparsity levels of up to 90%, resulting in a communication advantage of up to 1.8x and a reduction in training OPs of 3.9x while maintaining the accuracy drop under 2%.",,"Yeshwanth Venkatesha, Souvik Kundu, Priyadarshini Panda",2025-05-31T23:09:26Z,Assortment of Attention Heads: Accelerating Federated PEFT with Head   Pruning and Strategic Client Selection,Auswahl von Aufmerksamkeitsköpfen: Beschleunigen von Federated PEFT mit Head Pruning und strategischer Kundenauswahl,"关注对象负责人组:快速联邦PEFT,由主管谨慎和战略客户选择",http://arxiv.org/abs/2506.00743v1
1075,"We propose Data Swarms, an algorithm to optimize the generation of synthetic evaluation data and advance quantitative desiderata of LLM evaluation. We first train a swarm of initial data generators using existing data, and define various evaluation objectives to reflect the desired properties of evaluation (e.g., generate more difficult problems for the evaluated models) and quantitatively evaluate data generators. We then employ particle swarm optimization to optimize the swarm of data generators, where they collaboratively search through the model parameter space to find new generators that advance these objectives. We further extend it to Adversarial Swarms, where the data generator swarm generates harder data while the test taker model swarm learns from such data, co-evolving dynamically for better data and models simultaneously. Extensive experiments demonstrate that Data Swarms outperforms eight data generation baselines across five evaluation objectives, while Adversarial Swarms produce more robust learning of synthetic data and stronger generalization. Further analysis reveals that Data Swarms successfully optimizes compositions of multiple evaluation objectives and generalizes to new off-the-shelf LLMs, unseen at optimization time.",,"Shangbin Feng, Yike Wang, Weijia Shi, Yulia Tsvetkov",2025-05-31T23:03:46Z,Data Swarms: Optimizable Generation of Synthetic Evaluation Data,Datenschwärme: Optimierbare Generierung synthetischer Auswertungsdaten,Data Swarms: 最佳生成合成评价数据,http://arxiv.org/abs/2506.00741v1
1076,"In video dubbing, aligning translated audio with the source audio is a significant challenge. Our focus is on achieving this efficiently, tailored for real-time, on-device video dubbing scenarios. We developed a phoneme-based end-to-end length-sensitive speech translation (LSST) model, which generates translations of varying lengths short, normal, and long using predefined tags. Additionally, we introduced length-aware beam search (LABS), an efficient approach to generate translations of different lengths in a single decoding pass. This approach maintained comparable BLEU scores compared to a baseline without length awareness while significantly enhancing synchronization quality between source and target audio, achieving a mean opinion score (MOS) gain of 0.34 for Spanish and 0.65 for Korean, respectively.",,"Harveen Singh Chadha, Aswin Shanmugam Subramanian, Vikas Joshi, Shubham Bansal, Jian Xue, Rupeshkumar Mehta, Jinyu Li",2025-05-31T23:01:50Z,Length Aware Speech Translation for Video Dubbing,Länge Aware Speech Übersetzung für Video-Dubbing,视频 Dubbing 视频语音翻译,http://arxiv.org/abs/2506.00740v1
1077,"Large language model (LLM) agents have shown impressive capabilities in human language comprehension and reasoning, yet their potential in cybersecurity remains underexplored. We introduce DefenderBench, a practical, open-source toolkit for evaluating language agents across offense, defense, and cybersecurity knowledge-based tasks. DefenderBench includes environments for network intrusion, malicious content detection, code vulnerability analysis, and cybersecurity knowledge assessment. It is intentionally designed to be affordable and easily accessible for researchers while providing fair and rigorous assessment. We benchmark several state-of-the-art (SoTA) and popular LLMs, including both open- and closed-weight models, using a standardized agentic framework. Our results show that Claude-3.7-sonnet performs best with a DefenderBench score of 81.65, followed by Claude-3.7-sonnet-think with 78.40, while the best open-weight model, Llama 3.3 70B, is not far behind with a DefenderBench score of 71.81. DefenderBench's modular design allows seamless integration of custom LLMs and tasks, promoting reproducibility and fair comparisons. An anonymized version of DefenderBench is available at https://github.com/microsoft/DefenderBench.",,"Chiyu Zhang, Marc-Alexandre Cote, Michael Albada, Anush Sankaran, Jack W. Stokes, Tong Wang, Amir Abdi, William Blum, Muhammad Abdul-Mageed",2025-05-31T23:00:29Z,DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity   Environments,DefenderBench: Ein Toolkit zur Bewertung von Sprachagenten in Cybersicherheitsumgebungen,""" 捍卫堡垒:网络安全环境中评价语言代理工具工具包 """,http://arxiv.org/abs/2506.00739v1
1078,"Narrative frames are a powerful way of conceptualizing and communicating complex, controversial ideas, however automated frame analysis to date has mostly overlooked this framing device. In this paper, we connect elements of narrativity with fundamental aspects of framing, and present a framework which formalizes and operationalizes such aspects. We annotate and release a data set of news articles in the climate change domain, analyze the dominance of narrative frame components across political leanings, and test LLMs in their ability to predict narrative frames and their components. Finally, we apply our framework in an unsupervised way to elicit components of narrative framing in a second domain, the COVID-19 crisis, where our predictions are congruent with prior theoretical work showing the generalizability of our approach.",,"Yulia Otmakhova, Lea Frermann",2025-05-31T22:55:08Z,Narrative Media Framing in Political Discourse,Erzählende Medien im politischen Diskurs,A. 政治争论中的描述性媒体,http://arxiv.org/abs/2506.00737v1
1079,"We propose a novel discriminative model for sequence labeling called Bregman conditional random fields (BCRF). Contrary to standard linear-chain conditional random fields, BCRF allows fast parallelizable inference algorithms based on iterative Bregman projections. We show how such models can be learned using Fenchel-Young losses, including extension for learning from partial labels. Experimentally, our approach delivers comparable results to CRF while being faster, and achieves better results in highly constrained settings compared to mean field, another parallelizable alternative.",,"Caio Corro, Mathieu Lacroix, Joseph Le Roux",2025-05-31T22:36:21Z,Bregman Conditional Random Fields: Sequence Labeling with Parallelizable   Inference Algorithms,Bregman Conditional Random Fields: Sequenzierung mit parallelisierbaren Inferenzalgorithmen,Bregman 有条件随机字段: 与可平行推断值的序列标签,http://arxiv.org/abs/2506.00732v1
1080,"Large Language Models have been found to create plans that are neither executable nor verifiable in grounded environments. An emerging line of work demonstrates success in using the LLM as a formalizer to generate a formal representation of the planning domain in some language, such as Planning Domain Definition Language (PDDL). This formal representation can be deterministically solved to find a plan. We systematically evaluate this methodology while bridging some major gaps. While previous work only generates a partial PDDL representation, given templated, and therefore unrealistic environment descriptions, we generate the complete representation given descriptions of various naturalness levels. Among an array of observations critical to improve LLMs' formal planning abilities, we note that most large enough models can effectively formalize descriptions as PDDL, outperforming those directly generating plans, while being robust to lexical perturbation. As the descriptions become more natural-sounding, we observe a decrease in performance and provide detailed error analysis.",,"Cassie Huang, Li Zhang",2025-05-31T22:14:36Z,On the Limit of Language Models as Planning Formalizers,An der Grenze von Sprachmodellen als Planungsformalisatoren,关于作为规划正规化机构的语言模式限制,http://arxiv.org/abs/2412.09879v4
1081,"Large language models have demonstrated remarkable performance; however, their massive parameter counts make deployment highly expensive. Low-rank approximation offers a promising compression solution, yet existing approaches have two main limitations: (1) They focus on minimizing the output error of individual linear layers, without considering the architectural characteristics of Transformers, and (2) they decompose a large weight matrix into two small low-rank matrices. Consequently, these methods often fall short compared to other compression techniques like pruning and quantization, and introduce runtime overhead such as the extra GEMM kernel launches for decomposed small matrices. To address these limitations, we propose $\tt A^\tt 3$, a post-training low-rank approximation framework. $\tt A^\tt 3$ splits a Transformer layer into three functional components, namely $\tt QK$, $\tt OV$, and $\tt MLP$. For each component, $\tt A^\tt 3$ provides an analytical solution that reduces the hidden dimension size inside each component while minimizing the component's functional loss ($\it i.e.$, error in attention scores, attention outputs, and MLP outputs). This approach directly reduces model sizes, KV cache sizes, and FLOPs without introducing any runtime overheads. In addition, it provides a new narrative in advancing the optimization problem from singular linear layer loss optimization toward improved end-to-end performance. Through extensive experiments, we show that $\tt A^\tt 3$ maintains superior performance compared to SoTAs. For example, under the same reduction budget in computation and memory, our low-rank approximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2, outperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the versatility of $\tt A^\tt 3$, including KV cache compression, quantization, and mixed-rank assignments for enhanced performance.",,"Jeffrey T. H. Wong, Cheng Zhang, Xinye Cao, Pedro Gimenes, George A. Constantinides, Wayne Luk, Yiren Zhao",2025-05-31T22:12:10Z,A3 : an Analytical Low-Rank Approximation Framework for Attention,A3: ein analytischer Rahmen für die Annäherung an den Low-Rank-Wert,A3: 分析性低Rank接近度关注框架,http://arxiv.org/abs/2505.12942v2
1082,"This paper presents a gradient-informed fine-tuning method for large language models under few-shot conditions. The goal is to enhance task adaptability and training stability when data is limited. The method builds on a base loss function and introduces two gradient-related regularization terms. The first enforces gradient direction consistency to guide parameter updates along task-relevant directions and prevent drift. The second controls gradient magnitude to avoid abnormal updates. Together, these components support a more efficient and stable optimization path. To further improve cross-task generalization, the method incorporates a gradient alignment mechanism. This mechanism measures the consistency between optimization directions of the source and target tasks. It enhances fine-tuning performance in multi-task and cross-domain scenarios. Across various natural language understanding tasks, the method outperforms existing fine-tuning strategies in average accuracy, gradient stability, and directional alignment. Empirical evaluations under different sample sizes and domain-specific tasks confirm the method's robustness and broad applicability in low-resource environments. In particular, the method shows clear advantages in controlling parameter update paths. The results demonstrate that a gradient-based fine-tuning framework can effectively leverage the representational power of large language models. It ensures training stability while reducing dependence on large volumes of labeled data.",,"Hongye Zheng, Yichen Wang, Ray Pan, Guiran Liu, Binrong Zhu, Hanlu Zhang",2025-05-31T21:59:01Z,Structured Gradient Guidance for Few-Shot Adaptation in Large Language   Models,Strukturierte Gradienten-Leitlinien für die wenige-heiße Anpassung in großen Sprachmodellen,关于大语言模型中小热适应的结构性渐进式指南,http://arxiv.org/abs/2506.00726v1
1083,"Unlike traditional cascaded pipelines, end-to-end (E2E) spoken dialogue systems preserve full differentiability and capture non-phonemic information, making them well-suited for modeling spoken interactions. However, existing E2E approaches often require large-scale training data and generates responses lacking semantic coherence. We propose a simple yet effective strategy leveraging a chain-of-thought (CoT) formulation, ensuring that training on conversational data remains closely aligned with the multimodal language model (LM)'s pre-training on speech recognition~(ASR), text-to-speech synthesis (TTS), and text LM tasks. Our method achieves over 1.5 ROUGE-1 improvement over the baseline, successfully training spoken dialogue systems on publicly available human-human conversation datasets, while being compute-efficient enough to train on just 300 hours of public human-human conversation data, such as the Switchboard. We will publicly release our models and training code.",,"Siddhant Arora, Jinchuan Tian, Hayato Futami, Jee-weon Jung, Jiatong Shi, Yosuke Kashiwagi, Emiru Tsunoo, Shinji Watanabe",2025-05-31T21:43:37Z,Chain-of-Thought Training for Open E2E Spoken Dialogue Systems,Chain-of-Thought-Training für offene E2E-Gesprochene Dialogsysteme,开放 E2E 口语对话系统研究链培训,http://arxiv.org/abs/2506.00722v1
1084,"Language models (LMs) tend to show human-like preferences on a number of syntactic phenomena, but the extent to which these are attributable to direct exposure to the phenomena or more general properties of language is unclear. We explore this with the English dative alternation (DO: ""gave Y the X"" vs. PO: ""gave the X to Y""), using a controlled rearing paradigm wherein we iteratively train small LMs on systematically manipulated input. We focus on properties that affect the choice of alternant: length and animacy. Both properties are directly present in datives but also reflect more global tendencies for shorter elements to precede longer ones and animates to precede inanimates. First, by manipulating and ablating datives for these biases in the input, we show that direct evidence of length and animacy matters, but easy-first preferences persist even without such evidence. Then, using LMs trained on systematically perturbed datasets to manipulate global length effects (re-linearizing sentences globally while preserving dependency structure), we find that dative preferences can emerge from indirect evidence. We conclude that LMs' emergent syntactic preferences come from a mix of direct and indirect sources.",,"Qing Yao, Kanishka Misra, Leonie Weissweiler, Kyle Mahowald",2025-05-31T21:35:06Z,Both Direct and Indirect Evidence Contribute to Dative Alternation   Preferences in Language Models,Sowohl direkte als auch indirekte Beweise tragen zu Dative-Alternation-Präferenzen in Sprachmodellen bei,直接证据和间接证据都有助于语文模式中的替代选择,http://arxiv.org/abs/2503.20850v2
1085,"This paper presents a framework to convert argumentative texts into argument knowledge graphs (AKG). Starting with basic annotations of argumentative components (ACs) and argumentative relations (ARs), we enrich the information by constructing a knowledge base (KB) graph with metadata attributes for nodes. Next, we use premises and inference rules from the KB to form arguments by applying modus ponens. From these arguments, we create an AKG. The nodes and edges of the AKG have attributes that capture important argumentative features. We also find missing inference rules by identifying markers. This makes it possible to identify undercut attacks that were previously undetectable in existing datasets. The AKG gives a graphical view of the argumentative structure that is easier to understand than theoretical formats. It also prepares the ground for future reasoning tasks, including checking the coherence of arguments and identifying opportunities for revision. For this, it is important to find indirect relations, many of which are implicit. Our proposed AKG format, with annotated inference rules and modus ponens, will help reasoning models learn the implicit indirect relations that require inference over arguments and the relations between them.",,"Debarati Bhattacharjee, Ashish Anand",2025-05-31T21:11:30Z,From Argumentative Text to Argument Knowledge Graph: A New Framework for   Structured Argumentation,Vom argumentativen Text zum argumentativen Wissensgraph: Ein neuer Rahmen für strukturierte Argumentation,从参数文字到参数知识图:结构化参数新框架,http://arxiv.org/abs/2506.00713v1
1086,"Knowledge graph completion (KGC) aims to predict missing triples in knowledge graphs (KGs) by leveraging existing triples and textual information. Recently, generative large language models (LLMs) have been increasingly employed for graph tasks. However, current approaches typically encode graph context in textual form, which fails to fully exploit the potential of LLMs for perceiving and reasoning about graph structures. To address this limitation, we propose DrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion). DrKGC employs a flexible lightweight model training strategy to learn structural embeddings and logical rules within the KG. It then leverages a novel bottom-up graph retrieval method to extract a subgraph for each query guided by the learned rules. Finally, a graph convolutional network (GCN) adapter uses the retrieved subgraph to enhance the structural embeddings, which are then integrated into the prompt for effective LLM fine-tuning. Experimental results on two general domain benchmark datasets and two biomedical datasets demonstrate the superior performance of DrKGC. Furthermore, a realistic case study in the biomedical domain highlights its interpretability and practical utility.",,"Yongkang Xiao, Sinian Zhang, Yi Dai, Huixue Zhou, Jue Hou, Jie Ding, Rui Zhang",2025-05-31T20:56:54Z,DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph   Completion across General and Biomedical Domains,DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion over General and Biomedical Domains,DrKGC: 通用和生物医学领域知识图完成全域和生物医学领域知识图完成动态子集检索推荐LMs,http://arxiv.org/abs/2506.00708v1
1087,"The advent of Large Language Models (LLMs) has significantly reshaped the landscape of machine translation (MT), particularly for low-resource languages and domains that lack sufficient parallel corpora, linguistic tools, and computational infrastructure. This survey presents a comprehensive overview of recent progress in leveraging LLMs for MT. We analyze techniques such as few-shot prompting, cross-lingual transfer, and parameter-efficient fine-tuning (e.g., LoRA, adapters) that enable effective adaptation to under-resourced settings. The paper also explores synthetic data generation strategies using LLMs, including back-translation and lexical augmentation. Additionally, we compare LLM-based translation with traditional encoder-decoder models across diverse language pairs, highlighting the strengths and limitations of each. We discuss persistent challenges - such as hallucinations, evaluation inconsistencies, and inherited biases, while also evaluating emerging LLM-driven metrics for translation quality. This survey offers practical insights and outlines future directions for building robust, inclusive, and scalable MT systems in the era of large-scale generative models.",,"Baban Gain, Dibyanayan Bandyopadhyay, Asif Ekbal",2025-05-31T20:46:21Z,Bridging the Linguistic Divide: A Survey on Leveraging Large Language   Models for Machine Translation,Überbrückung der sprachlichen Kluft: Eine Umfrage über die Nutzung großer Sprachmodelle für die maschinelle Übersetzung,弥合语言分歧:关于利用大语言模型进行机器翻译的调查,http://arxiv.org/abs/2504.01919v3
1088,"The field of machine translation has achieved significant advancements, yet domain-specific terminology translation, particularly in AI, remains challenging. We introduce GIST, a large-scale multilingual AI terminology dataset containing 5K terms extracted from top AI conference papers spanning 2000 to 2023. The terms are translated into Arabic, Chinese, French, Japanese, and Russian using a hybrid framework that combines LLMs for extraction with human expertise for translation. The dataset's quality is benchmarked against existing resources, demonstrating superior translation accuracy through crowdsourced evaluation. GIST is integrated into translation workflows using post-translation refinement methods that require no retraining, where LLM prompting consistently improves BLEU and COMET scores. A web demonstration on the ACL Anthology platform highlights its practical application, showcasing improved accessibility for non-English speakers. This work aims to address critical gaps in AI terminology resources and fosters global inclusivity and collaboration in AI research. Our data is at https://huggingface.co/datasets/Jerry999/multilingual-terminology",,"Jiarui Liu, Iman Ouzzani, Wenkai Li, Lechen Zhang, Tianyue Ou, Houda Bouamor, Zhijing Jin, Mona Diab",2025-05-31T20:39:16Z,Towards Global AI Inclusivity: A Large-Scale Multilingual Terminology   Dataset (GIST),Auf dem Weg zu globaler KI-Inklusivität: Ein multilingualer Terminologie-Datensatz (GIST),实现全球独立独的包容性:大型多语种术语数据集(GIST),http://arxiv.org/abs/2412.18367v6
1089,"Math reasoning is an active area of Large Language Model (LLM) research because it is a hallmark of artificial intelligence and has implications in several domains, including math education. However, few works have explored how math reasoning is encoded within LLM parameters and if it is a skill that can be isolated within models. Doing so could allow targeted intervention to improve math performance without altering non-math behavior and foster understanding of how models encode math reasoning. We introduce Math Neurosurgery (MathNeuro), a computationally efficient method we use to isolate math-specific parameters in LLMs using only forward passes. MathNeuro builds on existing work by using weights and activations to calculate parameter importance, but isolates math-specific parameters by filtering out those important for general language tasks. Through pruning parameters MathNeuro identifies, we delete a LLM's math reasoning ability without significantly impacting its general language ability. Scaling the identified parameters by a small constant improves a pretrained or instruction-tuned LLM's performance by 4-17% on GSM8K and 5-35% on MATH while leaving non-math behavior unaltered. MathNeuro is also data efficient: most of its effectiveness holds when identifying math-specific parameters using a single sample. MathNeuro highlights the potential for future work to intervene on math-specific parameters.",,"Bryan R. Christ, Zack Gottesman, Jonathan Kropko, Thomas Hartvigsen",2025-05-31T20:37:42Z,Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities   Using Only Forward Passes,Math Neurochirurgie: Die Math-Reasoning-Fähigkeiten von Sprachmodellen mit nur Vorwärtspassagen isolieren,数学神经外科:仅使用前方通道的孤立语言模型理据能力,http://arxiv.org/abs/2410.16930v3
1090,"Generative models have become widely used in biomedical entity linking (BioEL) due to their excellent performance and efficient memory usage. However, these models are usually trained only with positive samples, i.e., entities that match the input mention's identifier, and do not explicitly learn from hard negative samples, which are entities that look similar but have different meanings. To address this limitation, we introduce ANGEL (Learning from Negative Samples in Biomedical Generative Entity Linking), the first framework that trains generative BioEL models using negative samples. Specifically, a generative model is initially trained to generate positive entity names from the knowledge base for given input entities. Subsequently, both correct and incorrect outputs are gathered from the model's top-k predictions. Finally, the model is updated to prioritize the correct predictions through preference optimization. Our models outperform the previous best baseline models by up to an average top-1 accuracy of 1.4% on five benchmarks. When incorporating our framework into pre-training, the performance improvement increases further to 1.7%, demonstrating its effectiveness in both the pre-training and fine-tuning stages. The code and model weights are available at https://github.com/dmis-lab/ANGEL.",,"Chanhwi Kim, Hyunjae Kim, Sihyeon Park, Jiwoo Lee, Mujeen Sung, Jaewoo Kang",2025-05-31T20:25:08Z,Learning from Negative Samples in Generative Biomedical Entity Linking,Aus Negativproben lernen in generativem biomedizinischem Entity Linking,从生生生物医学实体联系中的负面抽样学习,http://arxiv.org/abs/2408.16493v2
1091,"Tool-integrated reasoning (TIR) augments large language models (LLMs) with the ability to invoke external tools during long-form reasoning, such as search engines and code interpreters, to solve tasks beyond the capabilities of internal reasoning. While reinforcement learning (RL) has shown promise in training such agents, most of existing approaches typically optimize only for final correctness without considering the efficiency or necessity of external tool use. This often leads to excessive tool calling, incurring high computational costs and hindering the development of internal reasoning capabilities - a phenomenon known as \textit{cognitive offloading}. To this end, we propose Optimal Tool Call-controlled Policy Optimization (OTC-PO), a simple yet effective RL-based framework that encourages models to produce accurate answers with minimal tool calls. Our method introduces a tool-integrated reward that jointly considers answer correctness and corresponding tool use behavior of model to reach that answer. To validate the effectiveness, we introduce the metric of \textit{tool productivity}, defined as the ratio between the number of correct answers and the total number of tool calls across all test cases. This metric reflects how efficiently tool usage contributes to successful task completion, with higher values indicating smarter and more autonomous reasoning. We instantiate this framework within both Proximal Policy Optimization (PPO) and Group Relative Preference Optimization (GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and Qwen-Math across multiple QA benchmarks show that our approach reduces tool calls by up to 68.3\% and improves tool productivity by up to 215.4\%, while maintaining comparable answer accuracy.",,"Hongru Wang, Cheng Qian, Wanjun Zhong, Xiusi Chen, Jiahao Qiu, Shijue Huang, Bowen Jin, Mengdi Wang, Kam-Fai Wong, Heng Ji",2025-05-31T20:08:42Z,Acting Less is Reasoning More! Teaching Model to Act Efficiently,Weniger handeln ist mehr Grund! Lehrmodell effizient handeln,少演就是多讲道理!,http://arxiv.org/abs/2504.14870v2
1092,"Taxonomies are hierarchical knowledge graphs crucial for recommendation systems, and web applications. As data grows, expanding taxonomies is essential, but existing methods face key challenges: (1) discriminative models struggle with representation limits and generalization, while (2) generative methods either process all candidates at once, introducing noise and exceeding context limits, or discard relevant entities by selecting noisy candidates. We propose LORex (Lineage-Oriented Reasoning for Taxonomy Expansion), a plug-and-play framework that combines discriminative ranking and generative reasoning for efficient taxonomy expansion. Unlike prior methods, LORex ranks and chunks candidate terms into batches, filtering noise and iteratively refining selections by reasoning candidates' hierarchy to ensure contextual efficiency. Extensive experiments across four benchmarks and twelve baselines show that LORex improves accuracy by 12% and Wu & Palmer similarity by 5% over state-of-the-art methods.",,"Sahil Mishra, Kumar Arjun, Tanmoy Chakraborty",2025-05-31T20:07:12Z,"Rank, Chunk and Expand: Lineage-Oriented Reasoning for Taxonomy   Expansion","Rang, Chunk und Expand: Linienorientierte Begründung für Taxonomie-Erweiterung",级、级、块块和扩展:以直线为主的扩展分类学理由,http://arxiv.org/abs/2505.13282v4
1093,"Machine unlearning aims to remove sensitive or undesired data from large language models. However, recent studies suggest that unlearning is often shallow, claiming that removed knowledge can easily be recovered. In this work, we critically examine standard unlearning evaluation practices and uncover key limitations that shake our trust in those findings. First, we show that some evaluations introduce substantial new information into the model, potentially masking true unlearning performance by re-teaching the model during testing. Second, we demonstrate that evaluation outcomes vary significantly across tasks, undermining the generalizability of current evaluation routines. Finally, we find that many evaluations rely on spurious correlations, making their results difficult to trust and interpret. Taken together, these issues suggest that current evaluation protocols may both overstate and understate unlearning success. To address this, we propose two principles for future unlearning evaluations: minimal information injection and downstream task awareness. We validate these principles through a series of targeted experiments, showing how violations of each can lead to misleading conclusions.",,"Zhili Feng, Yixuan Even Xu, Alexander Robey, Robert Kirk, Xander Davies, Yarin Gal, Avi Schwarzschild, J. Zico Kolter",2025-05-31T19:43:00Z,Existing Large Language Model Unlearning Evaluations Are Inconclusive,Bestehende große Sprachmodell Unlearning Bewertungen sind nicht schlüssig,现有的大语言示范模式不学习评价是结论性评价,http://arxiv.org/abs/2506.00688v1
1094,"Large language models are finetuned to refuse questions about hazardous knowledge, but these protections can often be bypassed. Unlearning methods aim at completely removing hazardous capabilities from models and make them inaccessible to adversaries. This work challenges the fundamental differences between unlearning and traditional safety post-training from an adversarial perspective. We demonstrate that existing jailbreak methods, previously reported as ineffective against unlearning, can be successful when applied carefully. Furthermore, we develop a variety of adaptive methods that recover most supposedly unlearned capabilities. For instance, we show that finetuning on 10 unrelated examples or removing specific directions in the activation space can recover most hazardous capabilities for models edited with RMU, a state-of-the-art unlearning method. Our findings challenge the robustness of current unlearning approaches and question their advantages over safety training.",,"Jakub Łucki, Boyi Wei, Yangsibo Huang, Peter Henderson, Florian Tramèr, Javier Rando",2025-05-31T19:22:41Z,An Adversarial Perspective on Machine Unlearning for AI Safety,Eine Kehrtwende zum Thema Maschinelles Lernen für die KI-Sicherheit,关于为AI安全而机械脱学的反向视角,http://arxiv.org/abs/2409.18025v6
1095,"We propose DeepRAG, a novel framework that integrates DeepSeek hierarchical question decomposition capabilities with RAG Gym unified retrieval-augmented generation optimization using process level supervision. Targeting the challenging MedHopQA biomedical question answering task, DeepRAG systematically decomposes complex queries into precise sub-queries and employs concept level reward signals informed by the UMLS ontology to enhance biomedical accuracy. Preliminary evaluations on the MedHopQA dataset indicate that DeepRAG significantly outperforms baseline models, including standalone DeepSeek and RAG Gym, achieving notable improvements in both Exact Match and concept level accuracy.",,"Yuelyu Ji, Hang Zhang, Shiven Verma, Hui Ji, Chun Li, Yushui Han, Yanshan Wang",2025-05-31T18:52:05Z,DeepRAG: Integrating Hierarchical Reasoning and Process Supervision for   Biomedical Multi-Hop QA,DeepRAG: Integration von Hierarchical Reasoning und Prozessüberwachung für biomedizinische Multi-Hop QA,深层RAG:生物医学多层次质量评估综合等级原因和流程监督,http://arxiv.org/abs/2506.00671v1
1096,"Malicious attackers can exploit large language models (LLMs) by engaging them in multi-turn dialogues to achieve harmful objectives, posing significant safety risks to society. To address this challenge, we propose a novel defense mechanism: SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues (STREAM). STREAM defends LLMs against multi-turn attacks while preserving their functional capabilities. Our approach involves constructing a human-annotated dataset, the Safety Reasoning Multi-turn Dialogues dataset, which is used to fine-tune a plug-and-play safety reasoning moderator. This model is designed to identify malicious intent hidden within multi-turn conversations and alert the target LLM of potential risks. We evaluate STREAM across multiple LLMs against prevalent multi-turn attack strategies. Experimental results demonstrate that our method significantly outperforms existing defense techniques, reducing the Attack Success Rate (ASR) by 51.2%, all while maintaining comparable LLM capability.",,"Martin Kuo, Jianyi Zhang, Aolin Ding, Louis DiValentin, Amin Hass, Benjamin F Morris, Isaac Jacobson, Randolph Linderman, James Kiessling, Nicolas Ramos, Bhavna Gopal, Maziyar Baran Pouyan, Changwei Liu, Hai Li, Yiran Chen",2025-05-31T18:38:23Z,SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues,Sichere und sinnvolle Ausleuchtung für Multi-Turn-Dialoge,多发对话的安全、有合理理由的求费协调,http://arxiv.org/abs/2506.00668v1
1097,"Large language models (LLMs) deliver superior performance but require substantial computational resources and operate with relatively low efficiency, while smaller models can efficiently handle simpler tasks with fewer resources. LLM routing is a crucial paradigm that dynamically selects the most suitable large language models from a pool of candidates to process diverse inputs, ensuring optimal resource utilization while maintaining response quality. Existing routing frameworks typically model this as a locally optimal decision-making problem, selecting the presumed best-fit LLM for each query individually, which overlook global budget constraints, resulting in ineffective resource allocation. To tackle this problem, we introduce OmniRouter, a fundamentally controllable routing framework for multi-LLM serving. Instead of making per-query greedy choices, OmniRouter models the routing task as a constrained optimization problem, assigning models that minimize total cost while ensuring the required performance level. Specifically, a hybrid retrieval-augmented predictor is designed to predict the capabilities and costs of LLMs and a constrained optimizer is employed to control globally optimal query-model allocation. Experiments show that OmniRouter achieves up to 6.30% improvement in response accuracy while simultaneously reducing computational costs by at least 10.15% compared to competitive router baselines. The code and the dataset are available at https://github.com/agiresearch/OmniRouter.",,"Kai Mei, Wujiang Xu, Shuhang Lin, Yongfeng Zhang",2025-05-31T18:35:45Z,OmniRouter: Budget and Performance Controllable Multi-LLM Routing,OmniRouter: Budget- und Performancesteuerbares Multi-LLM Routing,omniRouter: 预算和绩效控制可控多LLM 路由,http://arxiv.org/abs/2502.20576v5
1098,"Foundation model-based agents are increasingly used to automate complex tasks, enhancing efficiency and productivity. However, their access to sensitive resources and autonomous decision-making also introduce significant security risks, where successful attacks could lead to severe consequences. To systematically uncover these vulnerabilities, we propose AdvAgent, a black-box red-teaming framework for attacking web agents. Unlike existing approaches, AdvAgent employs a reinforcement learning-based pipeline to train an adversarial prompter model that optimizes adversarial prompts using feedback from the black-box agent. With careful attack design, these prompts effectively exploit agent weaknesses while maintaining stealthiness and controllability. Extensive evaluations demonstrate that AdvAgent achieves high success rates against state-of-the-art GPT-4-based web agents across diverse web tasks. Furthermore, we find that existing prompt-based defenses provide only limited protection, leaving agents vulnerable to our framework. These findings highlight critical vulnerabilities in current web agents and emphasize the urgent need for stronger defense mechanisms. We release code at https://ai-secure.github.io/AdvAgent/.",,"Chejian Xu, Mintong Kang, Jiawei Zhang, Zeyi Liao, Lingbo Mo, Mengqi Yuan, Huan Sun, Bo Li",2025-05-31T18:34:01Z,AdvAgent: Controllable Blackbox Red-teaming on Web Agents,AdvAgent: Kontrollierbare Blackbox Red-Teaming auf Web-Agenten,助理:在网络代理上可控黑箱红队,http://arxiv.org/abs/2410.17401v4
1099,"Sarcasm is a form of humor where expressions convey meanings opposite to their literal interpretations. Classifying and generating sarcasm using large language models is vital for interpreting human communication. Sarcasm poses challenges for computational models, due to its nuanced nature. We introduce Sarc7, a benchmark that classifies 7 types of sarcasm: self-deprecating, brooding, deadpan, polite, obnoxious, raging, and manic by annotating entries of the MUStARD dataset. Classification was evaluated using zero-shot, few-shot, chain-of-thought (CoT), and a novel emotion-based prompting technique. We propose an emotion-based generation method developed by identifying key components of sarcasm-incongruity, shock value, and context dependency. Our classification experiments show that Gemini 2.5, using emotion-based prompting, outperforms other setups with an F1 score of 0.3664. Human evaluators preferred our emotion-based prompting, with 38.46% more successful generations than zero-shot prompting.",,"Lang Xiong, Raina Gao, Alyssa Jeong, Yicheng Fu, Sean O'Brien, Vasu Sharma, Kevin Zhu",2025-05-31T18:01:23Z,Sarc7: Evaluating Sarcasm Detection and Generation with Seven Types and   Emotion-Informed Techniques,Sarc7: Sarkasmuserkennung und -generierung mit sieben Typen und emotional-informierten Techniken bewerten,Sarc7:评估七种类型和情感化技术的讽刺性探测和代代评估,http://arxiv.org/abs/2506.00658v1
1100,"Information Extraction (IE) systems are traditionally domain-specific, requiring costly adaptation that involves expert schema design, data annotation, and model training. While Large Language Models have shown promise in zero-shot IE, performance degrades significantly in unseen domains where label definitions differ. This paper introduces GUIDEX, a novel method that automatically defines domain-specific schemas, infers guidelines, and generates synthetically labeled instances, allowing for better out-of-domain generalization. Fine-tuning Llama 3.1 with GUIDEX sets a new state-of-the-art across seven zeroshot Named Entity Recognition benchmarks. Models trained with GUIDEX gain up to 7 F1 points over previous methods without humanlabeled data, and nearly 2 F1 points higher when combined with it. Models trained on GUIDEX demonstrate enhanced comprehension of complex, domain-specific annotation schemas. Code, models, and synthetic datasets are available at neilus03.github.io/guidex.com",,"Neil De La Fuente, Oscar Sainz, Iker García-Ferrero, Eneko Agirre",2025-05-31T17:36:18Z,GuideX: Guided Synthetic Data Generation for Zero-Shot Information   Extraction,GuideX: Geführte synthetische Datengenerierung für Zero-Shot-Informationsextraktion,指南X:为零热信息采掘工作制导合成数据,http://arxiv.org/abs/2506.00649v1
1101,"Stuttering is a complex disorder that requires specialized expertise for effective assessment and treatment. This paper presents an effort to enhance the FluencyBank dataset with a new stuttering annotation scheme based on established clinical standards. To achieve high-quality annotations, we hired expert clinicians to label the data, ensuring that the resulting annotations mirror real-world clinical expertise. The annotations are multi-modal, incorporating audiovisual features for the detection and classification of stuttering moments, secondary behaviors, and tension scores. In addition to individual annotations, we additionally provide a test set with highly reliable annotations based on expert consensus for assessing individual annotators and machine learning models. Our experiments and analysis illustrate the complexity of this task that necessitates extensive clinical expertise for valid training and evaluation of stuttering assessment models.",,"Ana Rita Valente, Rufael Marew, Hawau Olamide Toyin, Hamdan Al-Ali, Anelise Bohnen, Inma Becerra, Elsa Marta Soares, Goncalo Leal, Hanan Aldarmaki",2025-05-31T17:18:20Z,Clinical Annotations for Automatic Stuttering Severity Assessment,Klinische Anmerkungen zur automatischen Stuttering Severity Assessment,自动排量重度评估临床说明,http://arxiv.org/abs/2506.00644v1
1102,"Large language models (LLMs) are increasingly evaluated on single-answer multiple-choice tasks, yet many real-world problems require identifying all correct answers from a set of options. This capability remains underexplored. We introduce SATA-BENCH, the first dedicated benchmark for evaluating LLMs on Select All That Apply (SATA) questions across diverse domains, including reading comprehension, law, and biomedicine. Our evaluation of 27 open-source and proprietary models reveals a significant gap: even the strongest model achieves only 41.8% exact match, exposing LLMs' inability to reliably identify all correct answers. We find that this weakness stems from two core challenges: selection bias - models favor certain choices regardless of content, and count bias - models fail to predict the correct number of answers. To address these issues, we propose Choice Funnel, a decoding strategy that combines token debiasing with adaptive thresholding to guide models toward complete and accurate selections. Choice Funnel achieves up to 29% higher exact match than competitive baselines while reducing inference cost by over 64%. Our findings expose fundamental limitations in current LLMs and introduce a new framework for diagnosing and improving multi-answer reasoning. We release SATA-BENCH and Choice Funnel to promote LLM development for robust decision-making in realistic, multi-answer applications.",,"Weijie Xu, Shixian Cui, Xi Fang, Chi Xue, Stephanie Eckman, Chandan Reddy",2025-05-31T17:14:21Z,SATA-BENCH: Select All That Apply Benchmark for Multiple Choice   Questions,"SATA-BENCH: Wählen Sie alle, die Benchmark für Multiple-Choice-Fragen anwenden",SATA-BENCH: 选择多个选择问题的所有应用基准,http://arxiv.org/abs/2506.00643v1
1103,"Well-calibrated model confidence scores can improve the usefulness of text generation models. For example, users can be prompted to review predictions with low confidence scores, to prevent models from returning bad or potentially dangerous predictions. However, confidence metrics are not always well calibrated in text generation. One reason is that in generation, there can be many valid answers, which previous methods do not always account for. Hence, a confident model could distribute its output probability among multiple sequences because they are all valid. We propose task-agnostic confidence metrics suited to generation, which rely solely on the probabilities associated with the model outputs without the need for further fine-tuning or heuristics. Using these, we are able to improve the calibration of BART and Flan-T5 on summarization, translation, and QA datasets.",,"Lorenzo Jaime Yu Flores, Ori Ernst, Jackie Chi Kit Cheung",2025-05-31T17:01:45Z,Improving the Calibration of Confidence Scores in Text Generation Using   the Output Distribution's Characteristics,Verbesserung der Kalibrierung von Vertrauens-Scores bei der Textgenerierung anhand der Eigenschaften der Output-Distribution,利用产出分配特点改进对文本制作中信任分数的校准,http://arxiv.org/abs/2506.00637v1
1104,"Toxic speech on online platforms is a growing concern, impacting user experience and online safety. While text-based toxicity detection is well-studied, audio-based approaches remain underexplored, especially for low-resource languages like Vietnamese. This paper introduces ViToSA (Vietnamese Toxic Spans Audio), the first dataset for toxic spans detection in Vietnamese speech, comprising 11,000 audio samples (25 hours) with accurate human-annotated transcripts. We propose a pipeline that combines ASR and toxic spans detection for fine-grained identification of toxic content. Our experiments show that fine-tuning ASR models on ViToSA significantly reduces WER when transcribing toxic speech, while the text-based toxic spans detection (TSD) models outperform existing baselines. These findings establish a novel benchmark for Vietnamese audio-based toxic spans detection, paving the way for future research in speech content moderation.",,"Huy Ba Do, Vy Le-Phuong Huynh, Luan Thanh Nguyen",2025-05-31T17:01:18Z,ViToSA: Audio-Based Toxic Spans Detection on Vietnamese Speech   Utterances,ViToSA: Audiobasierte Toxikologische Span-Erkennung auf vietnamesischen Sprach-Utterances,VTOSA:在越南言语变化中探测以音频为基础的有毒气体,http://arxiv.org/abs/2506.00636v1
1105,"Rental listings offer a unique window into how urban space is socially constructed through language. We analyze Chicago Craigslist rental advertisements from 2018 to 2024 to examine how listing agents characterize neighborhoods, identifying mismatches between institutional boundaries and neighborhood claims. Through manual and large language model annotation, we classify unstructured listings from Craigslist according to their neighborhood. Geospatial analysis reveals three distinct patterns: properties with conflicting neighborhood designations due to competing spatial definitions, border properties with valid claims to adjacent neighborhoods, and ``reputation laundering"" where listings claim association with distant, desirable neighborhoods. Through topic modeling, we identify patterns that correlate with spatial positioning: listings further from neighborhood centers emphasize different amenities than centrally-located units. Our findings demonstrate that natural language processing techniques can reveal how definitions of urban spaces are contested in ways that traditional methods overlook.",,"Adam Visokay, Ruth Bagley, Ian Kennedy, Chris Hess, Kyle Crowder, Rob Voigt, Denis Peskoff",2025-05-31T16:42:46Z,Social Construction of Urban Space: Understanding Neighborhood   Boundaries Using Rental Listings,Soziale Konstruktion des städtischen Raumes: Grenzen der Nachbarschaft verstehen mit Mietlisten,城市空间的社会建设:利用租赁清单了解邻里边界,http://arxiv.org/abs/2506.00634v1
1106,"In psychological practices, standardized questionnaires serve as essential tools for assessing mental health through structured, clinically-validated questions (i.e., items). While social media platforms offer rich data for mental health screening, computational approaches often bypass these established clinical assessment tools in favor of black-box classification. We propose a novel questionnaire-guided screening framework that bridges psychological practice and computational methods through adaptive Retrieval-Augmented Generation (\textit{aRAG}). Our approach links unstructured social media content and standardized clinical assessments by retrieving relevant posts for each questionnaire item and using Large Language Models (LLMs) to complete validated psychological instruments. Our findings demonstrate two key advantages of questionnaire-guided screening: First, when completing the Beck Depression Inventory-II (BDI-II), our approach matches or outperforms state-of-the-art performance on Reddit-based benchmarks without requiring training data. Second, we show that guiding LLMs through standardized questionnaires can yield superior results compared to directly prompting them for depression screening, while also providing a more interpretable assessment by linking model outputs to clinically validated diagnostic criteria. Additionally, we show, as a proof-of-concept, how our questionnaire-based methodology can be extended to other mental conditions' screening, highlighting the promising role of LLMs as psychological assessors.",,"Federico Ravenda, Seyed Ali Bahrainian, Andrea Raballo, Antonietta Mira, Noriko Kando",2025-05-31T16:41:57Z,Are LLMs effective psychological assessors? Leveraging adaptive RAG for   interpretable mental health screening through psychometric practice,Sind LLMs effektive psychologische Assessoren? Nutzung adaptiver RAG für ein interpretierbares Screening der psychischen Gesundheit durch psychometrische Praxis,"LLM LLM是否有效的心理评估员?利用适应性RAG,通过心理计量做法进行可解释的心理健康检查",http://arxiv.org/abs/2501.00982v2
1107,"Recent Pre-trained Language Models (PLMs) usually only provide users with the inference APIs, namely the emerging Model-as-a-Service (MaaS) setting. To adapt MaaS PLMs to downstream tasks without accessing their parameters and gradients, some existing methods focus on the output-side adaptation of PLMs, viewing the PLM as an encoder and then optimizing a task-specific decoder for decoding the output hidden states and class scores of the PLM. Despite the effectiveness of these methods, they only use a single prompt to query PLMs for decoding, leading to a heavy reliance on the quality of the adopted prompt. In this paper, we propose a simple yet effective Multi-Prompting Decoder (MPD) framework for MaaS adaptation. The core idea is to query PLMs with multiple different prompts for each sample, thereby obtaining multiple output hidden states and class scores for subsequent decoding. Such multi-prompting decoding paradigm can simultaneously mitigate reliance on the quality of a single prompt, alleviate the issue of data scarcity under the few-shot setting, and provide richer knowledge extracted from PLMs. Specifically, we propose two decoding strategies: multi-prompting decoding with optimal transport for hidden states and calibrated decoding for class scores. Extensive experiments demonstrate that our method achieves new state-of-the-art results on multiple natural language understanding datasets under the few-shot setting.",,"Zifeng Cheng, Zhaoling Chen, Zhiwei Jiang, Yafeng Yin, Cong Wang, Shiping Ge, Qing Gu",2025-05-31T16:39:12Z,Multi-Prompting Decoder Helps Better Language Understanding,Multi-Prompting-Decoder hilft besser Sprachverständnis,多发式解码器帮助更好地理解语言,http://arxiv.org/abs/2406.06279v2
1108,"Current speech encoding pipelines often rely on an additional text-based LM to get robust representations of human communication, even though SotA speech-to-text models often have a LM within. This work proposes an approach to improve the LM within an audio model such that the subsequent text-LM is unnecessary. We introduce WhiSPA (Whisper with Semantic and Psychological Alignment), which leverages a novel audio training objective: contrastive loss with a language model embedding as a teacher. Using over 500k speech segments from mental health audio interviews, we evaluate the utility of aligning Whisper's latent space with semantic representations from a text autoencoder (SBERT) and lexically derived embeddings of basic psychological dimensions: emotion and personality. Over self-supervised affective tasks and downstream psychological tasks, WhiSPA surpasses current speech encoders, achieving an average error reduction of 73.4% and 83.8%, respectively. WhiSPA demonstrates that it is not always necessary to run a subsequent text LM on speech-to-text output in order to get a rich psychological representation of human communication.",,"Rajath Rao, Adithya Ganesan, Oscar Kjell, Jonah Luby, Akshay Raghavan, Scott Feltman, Whitney Ringwald, Ryan L. Boyd, Benjamin Luft, Camilo Ruggero, Neville Ryant, Roman Kotov, H. Andrew Schwartz",2025-05-31T16:37:32Z,WhiSPA: Semantically and Psychologically Aligned Whisper with   Self-Supervised Contrastive and Student-Teacher Learning,WhiSPA: Semantisch und Psychologisch ausgerichteter Whisper mit selbstüberwachtem Kontrast und studentisch-Lehrer-Lernen,WhiSPA: 与自我控制的对抗和学生-教师学习相矛盾的言语在时间和心理上趋于一致,http://arxiv.org/abs/2501.16344v4
1109,"Prior research indicates that LID model performance significantly declines on accented speech; however, the specific causes, extent, and characterization of these errors remain under-explored. (i) We identify a common failure mode on accented speech whereby LID systems often misclassify L2 accented speech as the speaker's native language or a related language. (ii) We present evidence suggesting that state-of-the-art models are invariant to permutations of short spans of speech, implying they classify on the basis of short phonotactic features indicative of accent rather than language. Our analysis reveals a simple method to enhance model robustness to accents through input chunking. (iii) We present an approach that integrates sequence-level information into our model without relying on monolingual ASR systems; this reduces accent-language confusion and significantly enhances performance on accented speech while maintaining comparable results on standard LID.",,"Niyati Bafna, Matthew Wiesner",2025-05-31T16:35:40Z,LID Models are Actually Accent Classifiers: Implications and Solutions   for LID on Accented Speech,LID-Modelle sind eigentlich Accent Klassifikatoren: Implikationen und Lösungen für LID auf Accented Speech,LID 模型实际上是浓度分级器:LID对发声的影响和解决办法,http://arxiv.org/abs/2506.00628v1
1110,"During the finetuning stage of text generation tasks, standard cross-entropy loss treats all tokens equally. This can lead models to overemphasize high-frequency, low-information tokens, neglecting lower-frequency tokens crucial for specificity and informativeness in generated content. This paper introduces a novel loss function, Power-Law Decay Loss (PDL), specifically designed to optimize the finetuning process for text generation. The core motivation for PDL stems from observations in information theory and linguistics: the informativeness of a token is often inversely proportional to its frequency of occurrence. PDL re-weights the contribution of each token in the standard cross-entropy loss based on its frequency in the training corpus, following a power-law decay. Specifically, the weights for high-frequency tokens are reduced, while low-frequency, information-dense tokens are assigned higher weights. This mechanism guides the model during finetuning to focus more on learning and generating tokens that convey specific and unique information, thereby enhancing the quality, diversity, and informativeness of the generated text. We theoretically elaborate on the motivation and construction of PDL and discuss its potential applications and advantages across various text generation finetuning tasks, such as abstractive summarization, dialogue systems, and style transfer.",,Jintian Shao,2025-05-31T16:32:33Z,Power-Law Decay Loss for Large Language Model Finetuning: A Theory   Perspective,Macht-Rechts-Dekay Verlust für große Sprachmodell Finetuning: Eine Theorie-Perspektive,大语言模型微调的功率法减缩损失:理论视角,http://arxiv.org/abs/2505.16900v4
1111,"Large Reasoning Models(LRMs) such as OpenAI o1 and DeepSeek-R1 have shown remarkable reasoning capabilities by scaling test-time compute and generating long Chain-of-Thought(CoT). Distillation--post-training on LRMs-generated data--is a straightforward yet effective method to enhance the reasoning abilities of smaller models, but faces a critical bottleneck: we found that distilled long CoT data poses learning difficulty for small models and leads to the inheritance of biases (i.e. over-thinking) when using Supervised Fine-tuning (SFT) and Reinforcement Learning (RL) methods. To alleviate this bottleneck, we propose constructing tree-based CoT data from scratch via Monte Carlo Tree Search(MCTS). We then exploit a set of CoT-aware approaches, including Thoughts Length Balance, Fine-grained DPO, and Joint Post-training Objective, to enhance SFT and RL on the constructed data. We conduct evaluation on various benchmarks such as math (GSM8K, MATH, AIME). instruction-following (Multi-IF) and planning (Blocksworld), results demonstrate our approaches substantially improve the reasoning performance of distilled models compared to standard distilled models via reducing the hallucinations in long-time thinking. The project homepage is https://github.com/AIDC-AI/Marco-o1.",,"Huifeng Yin, Yu Zhao, Minghao Wu, Xuanfan Ni, Bo Zeng, Hao Wang, Tianqi Shi, Liangying Shao, Chenyang Lyu, Longyue Wang, Weihua Luo, Kaifu Zhang",2025-05-31T16:16:36Z,Marco-o1 v2: Towards Widening The Distillation Bottleneck for Reasoning   Models,Marco-o1 v2: Auf dem Weg zur Erweiterung der Destillation Engpässe für sinnvolle Modelle,Marco- o1 v2: 努力扩大蒸馏瓶瓶子用于解释模型,http://arxiv.org/abs/2503.01461v2
1112,"Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but often at the cost of reduced output diversity. This trade-off between diversity and alignment quality remains a significant challenge. Drawing inspiration from curiosity-driven exploration in reinforcement learning, we introduce curiosity-driven RLHF (CD-RLHF), a framework that incorporates intrinsic rewards for novel states, alongside traditional sparse extrinsic rewards, to optimize both output diversity and alignment quality. We demonstrate the effectiveness of CD-RLHF through extensive experiments on a range of tasks, including text summarization and instruction following. Our approach achieves significant gains in diversity on multiple diversity-oriented metrics while maintaining alignment with human preferences comparable to standard RLHF. We make our code publicly available at https://github.com/ernie-research/CD-RLHF.",,"Haoran Sun, Yekun Chai, Shuohuan Wang, Yu Sun, Hua Wu, Haifeng Wang",2025-05-31T16:08:55Z,Curiosity-Driven Reinforcement Learning from Human Feedback,Kuriosität-getriebene Stärkung Lernen aus menschlichem Feedback,从人类反馈中学习,http://arxiv.org/abs/2501.11463v2
1113,"Test-time compute has empowered multimodal large language models to generate extended reasoning chains, yielding strong performance on tasks such as multimodal math reasoning. However, this improved reasoning ability often comes with increased hallucination: as generations become longer, models tend to drift away from image-grounded content and rely more heavily on language priors. Attention analysis shows that longer reasoning chains lead to reduced focus on visual inputs, which contributes to hallucination. To systematically study this phenomenon, we introduce RH-AUC, a metric that quantifies how a model's perception accuracy changes with reasoning length, allowing us to evaluate whether the model preserves visual grounding during reasoning. We also release RH-Bench, a diagnostic benchmark that spans a variety of multimodal tasks, designed to assess the trade-off between reasoning ability and hallucination. Our analysis reveals that (i) larger models typically achieve a better balance between reasoning and perception, and (ii) this balance is influenced more by the types and domains of training data than by its overall volume. These findings underscore the importance of evaluation frameworks that jointly consider both reasoning quality and perceptual fidelity.",,"Chengzhi Liu, Zhongxing Xu, Qingyue Wei, Juncheng Wu, James Zou, Xin Eric Wang, Yuyin Zhou, Sheng Liu",2025-05-31T16:02:34Z,"More Thinking, Less Seeing? Assessing Amplified Hallucination in   Multimodal Reasoning Models","Mehr denken, weniger sehen? Bewertung verstärkter Halluzinationen in multimodalen Vernunftmodellen","更多思考,少见? 评估多模式理由模型中放大的幻觉",http://arxiv.org/abs/2505.21523v2
1114,"The proliferation of transliterated texts in digital spaces has emphasized the need for detecting and classifying hate speech in languages beyond English, particularly in low-resource languages. As online discourse can perpetuate discrimination based on target groups, e.g. gender, religion, and origin, multi-label classification of hateful content can help in comprehending hate motivation and enhance content moderation. While previous efforts have focused on monolingual or binary hate classification tasks, no work has yet addressed the challenge of multi-label hate speech classification in transliterated Bangla. We introduce BanTH, the first multi-label transliterated Bangla hate speech dataset comprising 37.3k samples. The samples are sourced from YouTube comments, where each instance is labeled with one or more target groups, reflecting the regional demographic. We establish novel transformer encoder-based baselines by further pre-training on transliterated Bangla corpus. We also propose a novel translation-based LLM prompting strategy for transliterated text. Experiments reveal that our further pre-trained encoders are achieving state-of-the-art performance on the BanTH dataset, while our translation-based prompting outperforms other strategies in the zero-shot setting. The introduction of BanTH not only fills a critical gap in hate speech research for Bangla but also sets the stage for future exploration into code-mixed and multi-label classification challenges in underrepresented languages.",,"Fabiha Haider, Fariha Tanjim Shifat, Md Farhan Ishmam, Deeparghya Dutta Barua, Md Sakib Ul Rahman Sourove, Md Fahim, Md Farhad Alam",2025-05-31T16:01:10Z,BanTH: A Multi-label Hate Speech Detection Dataset for Transliterated   Bangla,BanTH: Ein Multi-Label Hate Speech Detection Dataset für Transliterated Bangla,BANTH: 一个多标签的反光孟加拉语仇恨言论检测数据集,http://arxiv.org/abs/2410.13281v4
1115,"Trajectory modeling, which includes research on trajectory data pattern mining and future prediction, has widespread applications in areas such as life services, urban transportation, and public administration. Numerous methods have been proposed to address specific problems within trajectory modeling. However, the heterogeneity of data and the diversity of trajectory tasks make effective and reliable trajectory modeling an important yet highly challenging endeavor, even for domain experts. In this paper, we propose \textit{TrajAgent}, a agent framework powered by large language models (LLMs), designed to facilitate robust and efficient trajectory modeling through automation modeling. This framework leverages and optimizes diverse specialized models to address various trajectory modeling tasks across different datasets effectively. In \textit{TrajAgent}, we first develop \textit{UniEnv}, an execution environment with a unified data and model interface, to support the execution and training of various models. Building on \textit{UniEnv}, we introduce an agentic workflow designed for automatic trajectory modeling across various trajectory tasks and data. Furthermore, we introduce collaborative learning schema between LLM-based agents and small speciallized models, to enhance the performance of the whole framework effectively. Extensive experiments on four tasks using four real-world datasets demonstrate the effectiveness of \textit{TrajAgent} in automated trajectory modeling, achieving a performance improvement of 2.38\%-34.96\% over baseline methods.",,"Yuwei Du, Jie Feng, Jie Zhao, Yong Li",2025-05-31T16:00:06Z,TrajAgent: An LLM-based Agent Framework for Automated Trajectory   Modeling via Collaboration of Large and Small Models,TrajAgent: Ein LLM-basiertes Agent-Framework für automatisierte Trajektorienmodellierung über die Zusammenarbeit von großen und kleinen Modellen,TrajAgendy:一个基于LLM的通过大型和小型模型合作进行自动轨迹建模的LLM代理框架,http://arxiv.org/abs/2410.20445v2
1116,"Infographic charts are a powerful medium for communicating abstract data by combining visual elements (e.g., charts, images) with textual information. However, their visual and structural richness poses challenges for large vision-language models (LVLMs), which are typically trained on plain charts. To bridge this gap, we introduce ChartGalaxy, a million-scale dataset designed to advance the understanding and generation of infographic charts. The dataset is constructed through an inductive process that identifies 75 chart types, 330 chart variations, and 68 layout templates from real infographic charts and uses them to create synthetic ones programmatically. We showcase the utility of this dataset through: 1) improving infographic chart understanding via fine-tuning, 2) benchmarking code generation for infographic charts, and 3) enabling example-based infographic chart generation. By capturing the visual and structural complexity of real design, ChartGalaxy provides a useful resource for enhancing multimodal reasoning and generation in LVLMs.",,"Zhen Li, Duan Li, Yukai Guo, Xinyuan Guo, Bowen Li, Lanxi Xiao, Shenyu Qiao, Jiashu Chen, Zijian Wu, Hui Zhang, Xinhuan Shu, Shixia Liu",2025-05-31T15:53:58Z,ChartGalaxy: A Dataset for Infographic Chart Understanding and   Generation,ChartGalaxy: Ein Datensatz für Infografik Chart Verstehen und Generieren,图表银河:用于了解和生成信息图表的数据集,http://arxiv.org/abs/2505.18668v2
1117,"Recent studies emphasize that manually ensuring a consistent response style and maintaining high data quality in training sets can significantly improve the performance of fine-tuned Large Language Models (LLMs) while reducing the number of training examples needed. However, the precise definition of style and the relationship between style, data quality, and LLM performance remains unclear. This research identifies two key stylistic elements in responses: linguistic form and instructional surprisal. We find that, among training data of comparable quality, higher consistency in these response elements leads to better LLM performance. Inspired by this, we introduce Style Consistency-Aware Response Ranking (SCAR), which automatically prioritizes instruction-response pairs in the training set based on their response stylistic consistency. By selecting the most style-consistent examples, using only 0.7% of the full dataset in the best case, the fine-tuned LLMs can match or even surpass the performance of models trained on the entire dataset in coding and open-ended question-answering benchmarks. Code and data are available at https://github.com/zhuang-li/SCAR .",,"Zhuang Li, Yuncheng Hua, Thuy-Trang Vu, Haolan Zhan, Lizhen Qu, Gholamreza Haffari",2025-05-31T15:53:22Z,SCAR: Data Selection via Style Consistency-Aware Response Ranking for   Efficient Instruction-Tuning of Large Language Models,SCAR: Datenauswahl über Style Consistency-Aware Response Ranking für effizientes Instruction-Tuning von großen Sprachmodellen,SCAR:通过对大语言模式的高效教学指导进行样式一致性-软件响应排名来进行数据选择,http://arxiv.org/abs/2406.10882v9
1118,"As large language models (LLMs) continue to advance and gain widespread use, establishing systematic and reliable evaluation methodologies for LLMs and vision-language models (VLMs) has become essential to ensure their real-world effectiveness and reliability. There have been some early explorations about the usability of LLMs for limited urban tasks, but a systematic and scalable evaluation benchmark is still lacking. The challenge in constructing a systematic evaluation benchmark for urban research lies in the diversity of urban data, the complexity of application scenarios and the highly dynamic nature of the urban environment. In this paper, we design \textit{CityBench}, an interactive simulator based evaluation platform, as the first systematic benchmark for evaluating the capabilities of LLMs for diverse tasks in urban research. First, we build \textit{CityData} to integrate the diverse urban data and \textit{CitySimu} to simulate fine-grained urban dynamics. Based on \textit{CityData} and \textit{CitySimu}, we design 8 representative urban tasks in 2 categories of perception-understanding and decision-making as the \textit{CityBench}. With extensive results from 30 well-known LLMs and VLMs in 13 cities around the world, we find that advanced LLMs and VLMs can achieve competitive performance in diverse urban tasks requiring commonsense and semantic understanding abilities, e.g., understanding the human dynamics and semantic inference of urban images. Meanwhile, they fail to solve the challenging urban tasks requiring professional knowledge and high-level numerical abilities, e.g., geospatial prediction and traffic control task.",,"Jie Feng, Jun Zhang, Tianhui Liu, Xin Zhang, Tianjian Ouyang, Junbo Yan, Yuwei Du, Siqi Guo, Yong Li",2025-05-31T15:40:15Z,CityBench: Evaluating the Capabilities of Large Language Models for   Urban Tasks,CityBench: Bewertung der Fähigkeiten großer Sprachmodelle für städtische Aufgaben,城市责任:评价城市任务大语言模式的能力,http://arxiv.org/abs/2406.13945v3
1119,"Contract review is a complex and time-intensive task that typically demands specialized legal expertise, rendering it largely inaccessible to non-experts. Moreover, legal interpretation is rarely straightforward-ambiguity is pervasive, and judgments often hinge on subjective assessments. Compounding these challenges, contracts are usually confidential, restricting their use with proprietary models and necessitating reliance on open-source alternatives. To address these challenges, we introduce PAKTON: a fully open-source, end-to-end, multi-agent framework with plug-and-play capabilities. PAKTON is designed to handle the complexities of contract analysis through collaborative agent workflows and a novel retrieval-augmented generation (RAG) component, enabling automated legal document review that is more accessible, adaptable, and privacy-preserving. Experiments demonstrate that PAKTON outperforms both general-purpose and pretrained models in predictive accuracy, retrieval performance, explainability, completeness, and grounded justifications as evaluated through a human study and validated with automated metrics.",,"Petros Raptopoulos, Giorgos Filandrianos, Maria Lymperaiou, Giorgos Stamou",2025-05-31T15:38:21Z,PAKTON: A Multi-Agent Framework for Question Answering in Long Legal   Agreements,PAKTON: Ein Multi-Agenten-Rahmen für die Beantwortung von Fragen in langen Rechtsvereinbarungen,长期法律协议中问题解答的多机构框架,http://arxiv.org/abs/2506.00608v1
1120,"High-quality multilingual training data is essential for effectively pretraining large language models (LLMs). Yet, the availability of suitable open-source multilingual datasets remains limited. Existing state-of-the-art datasets mostly rely on heuristic filtering methods, restricting both their cross-lingual transferability and scalability. Here, we introduce JQL, a systematic approach that efficiently curates diverse and high-quality multilingual data at scale while significantly reducing computational demands. JQL distills LLMs' annotation capabilities into lightweight annotators based on pretrained multilingual embeddings. These models exhibit robust multilingual and cross-lingual performance, even for languages and scripts unseen during training. Evaluated empirically across 35 languages, the resulting annotation pipeline substantially outperforms current heuristic filtering methods like Fineweb2. JQL notably enhances downstream model training quality and increases data retention rates. Our research provides practical insights and valuable resources for multilingual data curation, raising the standards of multilingual dataset development.",,"Mehdi Ali, Manuel Brack, Max Lübbering, Elias Wendt, Abbas Goher Khan, Richard Rutmann, Alex Jude, Maurice Kraus, Alexander Arno Weber, David Kaczér, Florian Mai, Lucie Flek, Rafet Sifa, Nicolas Flores-Herr, Joachim Köhler, Patrick Schramowski, Michael Fromm, Kristian Kersting",2025-05-31T15:28:40Z,Judging Quality Across Languages: A Multilingual Approach to Pretraining   Data Filtering with Language Models,Qualität Across-Sprachen beurteilen: Ein mehrsprachiger Ansatz zur Vorschulung von Datenfiltern mit Sprachmodellen,"判断各语文的质量:采用多种语文办法,利用语言模式进行培训前数据过滤",http://arxiv.org/abs/2505.22232v2
1121,"Large language models(LLMs), with their powerful language generation and reasoning capabilities, have already achieved notable success in many domains, e.g., math and code generation. However, they often fall short when tackling real-life geospatial tasks within urban environments. This limitation stems from a lack of physical world knowledge and relevant data during training. To address this gap, we propose \textit{CityGPT}, a systematic framework designed to enhance LLMs' understanding of urban space and improve their ability to solve the related urban tasks by integrating a city-scale `world model' into the model. Firstly, we construct a diverse instruction tuning dataset, \textit{CityInstruction}, for injecting urban knowledge into LLMs and effectively boosting their spatial reasoning capabilities. Using a combination of \textit{CityInstruction} and open source general instruction data, we introduce a novel and easy-to-use self-weighted fine-tuning method (\textit{SWFT}) to train various LLMs (including ChatGLM3-6B, Llama3-8B, and Qwen2.5-7B) to enhance their urban spatial capabilities without compromising, or even improving, their general abilities. Finally, to validate the effectiveness of our proposed framework, we develop a comprehensive text-based spatial benchmark \textit{CityEval} for evaluating the performance of LLMs across a wide range of urban scenarios and geospatial tasks. Extensive evaluation results demonstrate that smaller LLMs trained with \textit{CityInstruction} by \textit{SWFT} method can achieve performance that is competitive with, and in some cases superior to, proprietary LLMs when assessed using \textit{CityEval}.",,"Jie Feng, Tianhui Liu, Yuwei Du, Siqi Guo, Yuming Lin, Yong Li",2025-05-31T15:26:01Z,CityGPT: Empowering Urban Spatial Cognition of Large Language Models,CityGPT: Ermächtigung städtischer räumlicher Kognition großer Sprachmodelle,城市GPT:增强城市对大语言模式的空间认识,http://arxiv.org/abs/2406.13948v2
1122,"Despite near-perfect results reported in the literature, the effectiveness of model editing in real-world applications remains unclear. To bridge this gap, we introduce QAEdit, a new benchmark aligned with widely used question answering (QA) datasets, and WILD, a task-agnostic evaluation framework designed to better reflect real-world usage of model editing. Our single editing experiments show that current editing methods perform substantially worse than previously reported (38.5% vs. 96.8%). We demonstrate that it stems from issues in the synthetic evaluation practices of prior work. Among them, the most severe is the use of teacher forcing during testing, which leaks both content and length of the ground truth, leading to overestimated performance. Furthermore, we simulate practical deployment by sequential editing, revealing that current approaches fail drastically with only 1000 edits. This work calls for a shift in model editing research toward rigorous evaluation and the development of robust, scalable methods that can reliably update knowledge in LLMs for real-world use.",,"Wanli Yang, Fei Sun, Jiajun Tan, Xinyu Ma, Qi Cao, Dawei Yin, Huawei Shen, Xueqi Cheng",2025-05-31T15:12:14Z,The Mirage of Model Editing: Revisiting Evaluation in the Wild,Die Mirage der Modellbearbeitung: Die Bewertung in der Wildnis erneut besuchen,《示范编辑的幻影:野生动物中的重新审视评价》,http://arxiv.org/abs/2502.11177v5
1123,"Chain-of-thought (CoT) reasoning enables large language models (LLMs) to move beyond fast System-1 responses and engage in deliberative System-2 reasoning. However, this comes at the cost of significant inefficiency due to verbose intermediate output. Recent latent-space reasoning methods improve efficiency by operating on hidden states without decoding into language, yet they treat all steps uniformly, failing to distinguish critical deductions from auxiliary steps and resulting in suboptimal use of computational resources. In this paper, we propose System-1.5 Reasoning, an adaptive reasoning framework that dynamically allocates computation across reasoning steps through shortcut paths in latent space. Specifically, System-1.5 Reasoning introduces two types of dynamic shortcuts. The model depth shortcut (DS) adaptively reasons along the vertical depth by early exiting non-critical tokens through lightweight adapter branches, while allowing critical tokens to continue through deeper Transformer layers. The step shortcut (SS) reuses hidden states across the decoding steps to skip trivial steps and reason horizontally in latent space. Training System-1.5 Reasoning involves a two-stage self-distillation process: first distilling natural language CoT into latent-space continuous thought, and then distilling full-path System-2 latent reasoning into adaptive shortcut paths (System-1.5 Reasoning). Experiments on reasoning tasks demonstrate the superior performance of our method. For example, on GSM8K, System-1.5 Reasoning achieves reasoning performance comparable to traditional CoT fine-tuning methods while accelerating inference by over 20x and reducing token generation by 92.31% on average.",,"Xiaoqiang Wang, Suyuchen Wang, Yun Zhu, Bang Liu",2025-05-31T15:08:41Z,System-1.5 Reasoning: Traversal in Language and Latent Spaces with   Dynamic Shortcuts,System-1.5 Reasoning: Traversal in Sprach- und Latentenräumen mit dynamischen Shortcuts,系统-1.5 理由:具有动态快捷键的语言和隐藏空间的变化,http://arxiv.org/abs/2505.18962v3
1124,"A retriever, which retrieves relevant knowledge pieces from a knowledge base given a context, is an important component in many natural language processing (NLP) tasks. Retrievers have been introduced in knowledge-grounded dialog systems to improve knowledge acquisition. In knowledge-grounded dialog systems, when conditioning on a given context, there may be multiple relevant and correlated knowledge pieces. However, knowledge pieces are usually assumed to be conditionally independent in current retriever models. To address this issue, we propose Entriever, an energy-based retriever. Entriever directly models the candidate retrieval results as a whole instead of modeling the knowledge pieces separately, with the relevance score defined by an energy function. We explore various architectures of energy functions and different training methods for Entriever, and show that Entriever substantially outperforms the strong cross-encoder baseline in knowledge retrieval tasks. Furthermore, we show that in semi-supervised training of knowledge-grounded dialog systems, Entriever enables effective scoring of retrieved knowledge pieces and significantly improves end-to-end performance of dialog systems.",,"Yucheng Cai, Ke Li, Yi Huang, Junlan Feng, Zhijian Ou",2025-05-31T14:42:34Z,Entriever: Energy-based Retriever for Knowledge-Grounded Dialog Systems,Entriever: Energiebasierter Retriever für wissensbasierte Dialogsysteme,尝试:知识四面对话系统以能源为基础的探索,http://arxiv.org/abs/2506.00585v1
1125,"It is crucial for large language models (LLMs) to follow instructions that involve multiple constraints. However, it is an unexplored area to enhance LLMs' ability to follow soft constraints. To bridge the gap, we initially design a pipeline to construct datasets with high-quality outputs automatically. Additionally, to fully utilize the positive and negative samples generated during the data construction process, we choose Direct Preference Optimization (DPO) as the training method. Furthermore, taking into account the difficulty of soft constraints indicated by the number of constraints, we design a curriculum learning training paradigm based on the constraint quantity. We experimentally evaluate the effectiveness of our methods in improving LLMs' soft constraint following ability and analyze the factors driving the improvements.The datasets and code are publicly available at https://github.com/Rainier-rq/FollowSoftConstraint.",,"Qingyu Ren, Jie Zeng, Qianyu He, Jiaqing Liang, Yanghua Xiao, Weikang Zhou, Zeye Sun, Fei Yu",2025-05-31T14:42:28Z,Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of   Large Language Models,Schritt-für-Schritt-Meisterschaft: Erweitern von Soft Constraint nach Fähigkeit von großen Sprachmodellen,逐步掌握:加强大型语言模型的软约束能力,http://arxiv.org/abs/2501.04945v4
1126,"Scalable Vector Graphics (SVGs) are vital for modern image rendering due to their scalability and versatility. Previous SVG generation methods have focused on curve-based vectorization, lacking semantic understanding, often producing artifacts, and struggling with SVG primitives beyond path curves. To address these issues, we introduce StarVector, a multimodal large language model for SVG generation. It performs image vectorization by understanding image semantics and using SVG primitives for compact, precise outputs. Unlike traditional methods, StarVector works directly in the SVG code space, leveraging visual understanding to apply accurate SVG primitives. To train StarVector, we create SVG-Stack, a diverse dataset of 2M samples that enables generalization across vectorization tasks and precise use of primitives like ellipses, polygons, and text. We address challenges in SVG evaluation, showing that pixel-based metrics like MSE fail to capture the unique qualities of vector graphics. We introduce SVG-Bench, a benchmark across 10 datasets, and 3 tasks: Image-to-SVG, Text-to-SVG generation, and diagram generation. Using this setup, StarVector achieves state-of-the-art performance, producing more compact and semantically rich SVGs.",,"Juan A. Rodriguez, Abhay Puri, Shubham Agarwal, Issam H. Laradji, Pau Rodriguez, Sai Rajeswar, David Vazquez, Christopher Pal, Marco Pedersoli",2025-05-31T14:41:26Z,StarVector: Generating Scalable Vector Graphics Code from Images and   Text,StarVector: Erzeugen von skalierbarem Vektorgrafikcode aus Bildern und Text,StarVector: 从图像和文字生成可缩放矢量图形代码,http://arxiv.org/abs/2312.11556v4
1127,"Social media platforms have become central to modern communication, yet they also harbor offensive content that challenges platform safety and inclusivity. While prior research has primarily focused on textual indicators of offense, the role of emojis, ubiquitous visual elements in online discourse, remains underexplored. Emojis, despite being rarely offensive in isolation, can acquire harmful meanings through symbolic associations, sarcasm, and contextual misuse. In this work, we systematically examine emoji contributions to offensive Twitter messages, analyzing their distribution across offense categories and how users exploit emoji ambiguity. To address this, we propose an LLM-powered, multi-step moderation pipeline that selectively replaces harmful emojis while preserving the tweet's semantic intent. Human evaluations confirm our approach effectively reduces perceived offensiveness without sacrificing meaning. Our analysis also reveals heterogeneous effects across offense types, offering nuanced insights for online communication and emoji moderation.",,"Yuhang Zhou, Yimin Xiao, Wei Ai, Ge Gao",2025-05-31T14:39:08Z,The Hidden Language of Harm: Examining the Role of Emojis in Harmful   Online Communication and Content Moderation,Die verborgene Sprache des Schadens: Prüfung der Rolle von Emojis in schädlicher Online-Kommunikation und Inhaltsmoderation,《伤害的隐藏语言:研究Emojis在有害在线通信和内容调节中的作用》,http://arxiv.org/abs/2506.00583v1
1128,"Since the adoption of large language models (LLMs) for text evaluation has become increasingly prevalent in the field of natural language processing (NLP), a series of existing works attempt to optimize the prompts for LLM evaluators to improve their alignment with human judgment. However, their efforts are limited to optimizing individual factors of evaluation prompts, such as evaluation criteria or output formats, neglecting the combinatorial impact of multiple factors, which leads to insufficient optimization of the evaluation pipeline. Nevertheless, identifying well-behaved prompting strategies for adjusting multiple factors requires extensive enumeration. To this end, we comprehensively integrate 8 key factors for evaluation prompts and propose a novel automatic prompting strategy optimization method called Heuristic Prompting Strategy Search (HPSS). Inspired by the genetic algorithm, HPSS conducts an iterative search to find well-behaved prompting strategies for LLM evaluators. A heuristic function is employed to guide the search process, enhancing the performance of our algorithm. Extensive experiments across four evaluation tasks demonstrate the effectiveness of HPSS, consistently outperforming both human-designed evaluation prompts and existing automatic prompt optimization methods. Our code is available at https://github.com/thu-coai/HPSS.",,"Bosi Wen, Pei Ke, Yufei Sun, Cunxiang Wang, Xiaotao Gu, Jinfeng Zhou, Jie Tang, Hongning Wang, Minlie Huang",2025-05-31T14:34:40Z,HPSS: Heuristic Prompting Strategy Search for LLM Evaluators,HPSS: Heuristic Prompting Strategy Suche nach LLM Evaluatoren,HPS: 对LLM评估员的超速快速战略搜索,http://arxiv.org/abs/2502.13031v2
1129,"In Self-Supervised Learning (SSL), pre-training and evaluation are resource intensive. In the speech domain, current indicators of the quality of SSL models during pre-training, such as the loss, do not correlate well with downstream performance. Consequently, it is often difficult to gauge the final downstream performance in a cost efficient manner during pre-training. In this work, we propose unsupervised efficient methods that give insights into the quality of the pre-training of SSL speech models, namely, measuring the cluster quality and rank of the embeddings of the SSL model. Results show that measures of cluster quality and rank correlate better with downstream performance than the pre-training loss with only one hour of unlabeled audio, reducing the need for GPU hours and labeled data in SSL model evaluation.",,"Ryan Whetten, Lucas Maison, Titouan Parcollet, Marco Dinarelli, Yannick Estève",2025-05-31T14:29:13Z,Towards Early Prediction of Self-Supervised Speech Model Performance,Auf dem Weg zu einer frühen Vorhersage der selbstüberwachten Sprachmodellleistung,早日预测自我发言示范表现的早期预测,http://arxiv.org/abs/2501.05966v2
1130,"Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS) remains challenging due to intricate reward modeling, dynamic agent interactions, and demanding generalization requirements. This paper explores whether post-training techniques, specifically Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR), can effectively $\textit{generalize}$ to multi-agent scenarios. We use economic reasoning as a testbed, leveraging its strong foundations in mathematics and game theory, its demand for structured analytical reasoning, and its relevance to real-world applications such as market design, resource allocation, and policy analysis. We introduce $\textbf{Recon}$ ($\textbf{R}$easoning like an $\textbf{ECON}$omist), a 7B-parameter open-source LLM post-trained on a hand-curated dataset of 2,100 high-quality economic reasoning problems. Comprehensive evaluation on economic reasoning benchmarks and multi-agent games reveals clear improvements in structured reasoning and economic rationality. These results underscore the promise of domain-aligned post-training for enhancing reasoning and agent alignment, shedding light on the roles of SFT and RL in shaping model behavior. Code is available at https://github.com/MasterZhou1/Recon .",,"Yufa Zhou, Shaobo Wang, Xingyu Dong, Xiangqi Jin, Yifang Chen, Yue Min, Kexin Yang, Xingzhang Ren, Dayiheng Liu, Linfeng Zhang",2025-05-31T14:22:40Z,Reasoning Like an Economist: Post-Training on Economic Problems Induces   Strategic Generalization in LLMs,Reasoning Like a Economist: Post-Training zu wirtschaftlichen Problemen führt zu einer strategischen Generalisierung in LLMs,象一名经济学家一样的理由:关于经济问题的培训后在LLM中将战略普遍化,http://arxiv.org/abs/2506.00577v1
1131,"Large language models (LLMs) have significantly advanced human language understanding and generation, with pretraining data quality and organization being crucial to their performance. Multi-stage pretraining is a promising approach, but existing methods often lack quantitative criteria for data partitioning and instead rely on intuitive heuristics. In this paper, we propose the novel Four-quadRAnt Multi-stage prEtraining strategy (FRAME), guided by the established principle of organizing the pretraining process into four stages to achieve significant loss reductions four times. This principle is grounded in two key findings: first, training on high Perplexity (PPL) data followed by low PPL data, and second, training on low PPL difference (PD) data followed by high PD data, both causing the loss to drop significantly twice and performance enhancements. By partitioning data into four quadrants and strategically organizing them, FRAME achieves a remarkable 16.8% average improvement over random across MMLU and CMMLU for the 3B model, effectively boosting LLM performance.",,"Xuemiao Zhang, Feiyu Duan, Liangyu Xu, Yongwei Zhou, Sirui Wang, Rongxiang Weng, Jingang Wang, Xunliang Cai",2025-05-31T14:08:15Z,FRAME: Boosting LLMs with A Four-Quadrant Multi-Stage Pretraining   Strategy,FRAME: Förderung von LLMs mit einer Vier-Quadrant-Mehrstufen-Vorschulungsstrategie,FRAME:利用四Quadrant多阶段培训前战略促进LMs,http://arxiv.org/abs/2502.05551v4
1132,"Large Reasoning Models (LRMs) demonstrate exceptional capability in tackling complex mathematical, logical, and coding tasks by leveraging extended Chain-of-Thought (CoT) reasoning. Test-time scaling methods, such as prolonging CoT with explicit token-level exploration, can push LRMs' accuracy boundaries, but they incur significant decoding overhead. A key inefficiency source is LRMs often generate redundant thinking CoTs, which demonstrate clear structured overthinking and underthinking patterns. Inspired by human cognitive reasoning processes and numerical optimization theories, we propose TrimR, a verifier-based, training-free, efficient framework for dynamic CoT compression to trim reasoning and enhance test-time scaling, explicitly tailored for production-level deployment. Our method employs a lightweight, pretrained, instruction-tuned verifier to detect and truncate redundant intermediate thoughts of LRMs without any LRM or verifier fine-tuning. We present both the core algorithm and asynchronous online system engineered for high-throughput industrial applications. Empirical evaluations on Ascend NPUs and vLLM show that our framework delivers substantial gains in inference efficiency under large-batch workloads. In particular, on the four MATH500, AIME24, AIME25, and GPQA benchmarks, the reasoning runtime of Pangu Pro MoE, Pangu-R-38B, QwQ-32B, and DeepSeek-R1-Distill-Qwen-32B is improved by up to 70% with negligible impact on accuracy.",,"Weizhe Lin, Xing Li, Zhiyuan Yang, Xiaojin Fu, Hui-Ling Zhen, Yaoyuan Wang, Xianzhi Yu, Wulong Liu, Xiaosong Li, Mingxuan Yuan",2025-05-31T13:54:42Z,TrimR: Verifier-based Training-Free Thinking Compression for Efficient   Test-Time Scaling,TrimR: Verifier-basierte Trainings-freie Denkkompression für effizientes Test-Time Scaling,TrimR: 有效试验时间缩放的无思维压力,http://arxiv.org/abs/2505.17155v2
1133,"This paper presents a pioneering exploration of reinforcement learning (RL) via group relative policy optimization for unified multimodal large language models (ULMs), aimed at simultaneously reinforcing generation and understanding capabilities. Through systematic pilot studies, we uncover the significant potential of ULMs to enable the synergistic co-evolution of dual capabilities within a shared policy optimization framework. Building on this insight, we introduce CoRL, a co-reinforcement learning framework comprising a unified RL stage for joint optimization and a refined RL stage for task-specific enhancement. With the proposed CoRL, our resulting model, ULM-R1, achieves average improvements of 7% on three text-to-image generation datasets and 23% on nine multimodal understanding benchmarks. These results demonstrate the effectiveness of CoRL and highlight the substantial benefit of reinforcement learning in facilitating cross-task synergy and optimization for ULMs. Code is available at https://github.com/mm-vl/ULM-R1.",,"Jingjing Jiang, Chongjie Si, Jun Luo, Hanwang Zhang, Chao Ma",2025-05-31T13:51:12Z,Co-Reinforcement Learning for Unified Multimodal Understanding and   Generation,Ko-Reinforcement-Lernen für ein einheitliches multimodales Verständnis und -Generierung,统一多模式理解和产生共同强化学习,http://arxiv.org/abs/2505.17534v2
1134,"Interpreting the law is always essential for the law to adapt to the ever-changing society. It is a critical and challenging task even for legal practitioners, as it requires meticulous and professional annotations and summarizations by legal experts, which are admittedly time-consuming and expensive to collect at scale. To alleviate the burden on legal experts, we propose a method for automated legal interpretation. Specifically, by emulating doctrinal legal research, we introduce a novel framework, ATRIE, to address Legal Concept Interpretation, a typical task in legal interpretation. ATRIE utilizes large language models (LLMs) to AuTomatically Retrieve concept-related information, Interpret legal concepts, and Evaluate generated interpretations, eliminating dependence on legal experts. ATRIE comprises a legal concept interpreter and a legal concept interpretation evaluator. The interpreter uses LLMs to retrieve relevant information from previous cases and interpret legal concepts. The evaluator uses performance changes on Legal Concept Entailment, a downstream task we propose, as a proxy of interpretation quality. Automated and multifaceted human evaluations indicate that the quality of our interpretations is comparable to those written by legal experts, with superior comprehensiveness and readability. Although there remains a slight gap in accuracy, it can already assist legal practitioners in improving the efficiency of legal interpretation.",,"Kangcheng Luo, Quzhe Huang, Cong Jiang, Yansong Feng",2025-05-31T13:48:07Z,"Automating Legal Interpretation with LLMs: Retrieval, Generation, and   Evaluation","Automatisieren der rechtlichen Interpretation mit LLMs: Retrieval, Generation und Evaluation","使法律解释自动化,LLM:检索、生成和评价",http://arxiv.org/abs/2501.01743v3
1135,"Autoregressive modeling has been a huge success in the field of natural language processing (NLP). Recently, autoregressive models have emerged as a significant area of focus in computer vision, where they excel in producing high-quality visual content. Autoregressive models in NLP typically operate on subword tokens. However, the representation strategy in computer vision can vary in different levels, i.e., pixel-level, token-level, or scale-level, reflecting the diverse and hierarchical nature of visual data compared to the sequential structure of language. This survey comprehensively examines the literature on autoregressive models applied to vision. To improve readability for researchers from diverse research backgrounds, we start with preliminary sequence representation and modeling in vision. Next, we divide the fundamental frameworks of visual autoregressive models into three general sub-categories, including pixel-based, token-based, and scale-based models based on the representation strategy. We then explore the interconnections between autoregressive models and other generative models. Furthermore, we present a multifaceted categorization of autoregressive models in computer vision, including image generation, video generation, 3D generation, and multimodal generation. We also elaborate on their applications in diverse domains, including emerging domains such as embodied AI and 3D medical AI, with about 250 related references. Finally, we highlight the current challenges to autoregressive models in vision with suggestions about potential research directions. We have also set up a Github repository to organize the papers included in this survey at: https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey.",,"Jing Xiong, Gongye Liu, Lun Huang, Chengyue Wu, Taiqiang Wu, Yao Mu, Yuan Yao, Hui Shen, Zhongwei Wan, Jinfa Huang, Chaofan Tao, Shen Yan, Huaxiu Yao, Lingpeng Kong, Hongxia Yang, Mi Zhang, Guillermo Sapiro, Jiebo Luo, Ping Luo, Ngai Wong",2025-05-31T13:35:55Z,Autoregressive Models in Vision: A Survey,Autoregressive Modelle in der Vision: Eine Umfrage,展望中的自动递减模式:调查,http://arxiv.org/abs/2411.05902v2
1136,"Current approaches for training Process Reward Models (PRMs) often involve breaking down responses into multiple reasoning steps using rule-based techniques, such as using predefined placeholder tokens or setting the reasoning step's length into a fixed size. These approaches overlook the fact that specific words do not typically mark true decision points in a text. To address this, we propose AdaptiveStep, a method that divides reasoning steps based on the model's confidence in predicting the next word. This division method provides more decision-making information at each step, enhancing downstream tasks, such as reward model learning. Moreover, our method does not require manual annotation. We demonstrate its effectiveness through experiments with AdaptiveStep-trained PRMs in mathematical reasoning and code generation tasks. Experimental results indicate that the outcome PRM achieves state-of-the-art Best-of-N performance, surpassing greedy search strategy with token-level value-guided decoding, while also reducing construction costs by over 30% compared to existing open-source PRMs. In addition, we provide a thorough analysis and case study on the PRM's performance, transferability, and generalization capabilities.",,"Yuliang Liu, Junjie Lu, Zhaoling Chen, Chaofeng Qu, Jason Klein Liu, Chonghan Liu, Zefan Cai, Yunhui Xia, Li Zhao, Jiang Bian, Chuheng Zhang, Wei Shen, Zhouhan Lin",2025-05-31T13:35:35Z,AdaptiveStep: Automatically Dividing Reasoning Step through Model   Confidence,AdaptiveStep: Automatisch dividierende Vernunft Schritt durch Modellvertrauen,适应性步骤:通过示范信任自动分解理由说明步骤,http://arxiv.org/abs/2502.13943v2
1137,"The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed/variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding.",,"Shunsuke Kando, Yusuke Miyao, Shinnosuke Takamichi",2025-05-31T13:32:13Z,Exploring the Effect of Segmentation and Vocabulary Size on Speech   Tokenization for Speech Language Models,Untersuchung des Einflusses von Segmentierung und Vokabelgröße auf die Sprach-Tokenisierung für Sprachmodelle,探讨分部和词汇对语音语言模式的语音教学的影响,http://arxiv.org/abs/2505.17446v2
1138,"Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential in multimodal diagnostic tasks. However, existing single-agent models struggle to generalize across diverse medical specialties, limiting their performance. Recent efforts introduce multi-agent collaboration frameworks inspired by clinical workflows, where general practitioners (GPs) and specialists interact in a fixed sequence. Despite improvements, these static pipelines lack flexibility and adaptability in reasoning. To address this, we propose MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that enables dynamic, optimized collaboration among medical agents. Specifically, we train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to assign patients to appropriate specialties, while the attending physician integrates the judgments from multi-specialists and its own knowledge to make final decisions. To address the inconsistency in specialist outputs, we introduce a curriculum learning (CL)-guided RL strategy that progressively teaches the attending physician to balance between imitating specialists and correcting their mistakes. Experiments on five medical VQA benchmarks demonstrate that MMedAgent-RL not only outperforms both open-source and proprietary Med-LVLMs, but also exhibits human-like reasoning patterns. Notably, it achieves an average performance gain of 18.4% over supervised fine-tuning baselines.",,"Peng Xia, Jinglu Wang, Yibo Peng, Kaide Zeng, Xian Wu, Xiangru Tang, Hongtu Zhu, Yun Li, Shujie Liu, Yan Lu, Huaxiu Yao",2025-05-31T13:22:55Z,MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal   Medical Reasoning,MMedAgent-RL: Optimierung der Multi-Agenten-Kollaboration für multimodale medizinische Vernunft,MMedAgender-RL:优化多机构协作促进多式联运医疗理由,http://arxiv.org/abs/2506.00555v1
1139,"Lexical semantic change detection aims to identify shifts in word meanings over time. While existing methods using embeddings from a diachronic corpus pair estimate the degree of change for target words, they offer limited insight into changes at the level of individual usage instances. To address this, we apply Unbalanced Optimal Transport (UOT) to sets of contextualized word embeddings, capturing semantic change through the excess and deficit in the alignment between usage instances. In particular, we propose Sense Usage Shift (SUS), a measure that quantifies changes in the usage frequency of a word sense at each usage instance. By leveraging SUS, we demonstrate that several challenges in semantic change detection can be addressed in a unified manner, including quantifying instance-level semantic change and word-level tasks such as measuring the magnitude of semantic change and the broadening or narrowing of meaning.",,"Ryo Kishino, Hiroaki Yamagiwa, Ryo Nagata, Sho Yokoi, Hidetoshi Shimodaira",2025-05-31T13:22:24Z,Quantifying Lexical Semantic Shift via Unbalanced Optimal Transport,Quantifizierung der Lexical Semantic Shift durch unausgewogenen optimalen Transport,通过不均匀最佳运输方式量化逻辑语义转换,http://arxiv.org/abs/2412.12569v2
1140,"This study investigates the machine unlearning techniques within the context of large language models (LLMs), referred to as \textit{LLM unlearning}. LLM unlearning offers a principled approach to removing the influence of undesirable data (e.g., sensitive or illegal information) from LLMs, while preserving their overall utility without requiring full retraining. Despite growing research interest, there is no comprehensive survey that systematically organizes existing work and distills key insights; here, we aim to bridge this gap. We begin by introducing the definition and the paradigms of LLM unlearning, followed by a comprehensive taxonomy of existing unlearning studies. Next, we categorize current unlearning approaches, summarizing their strengths and limitations. Additionally, we review evaluation metrics and benchmarks, providing a structured overview of current assessment methodologies. Finally, we outline promising directions for future research, highlighting key challenges and opportunities in the field.",,"Jiahui Geng, Qing Li, Herbert Woisetschlaeger, Zongxiong Chen, Fengyu Cai, Yuxia Wang, Preslav Nakov, Hans-Arno Jacobsen, Fakhri Karray",2025-05-31T13:19:34Z,A Comprehensive Survey of Machine Unlearning Techniques for Large   Language Models,Eine umfassende Übersicht über Techniken des maschinellen Lernens für große Sprachmodelle,大型语言模型机离学技术综合调查,http://arxiv.org/abs/2503.01854v2
1141,"Constrained by the cost and ethical concerns of involving real seekers in AI-driven mental health, researchers develop LLM-based conversational agents (CAs) with tailored configurations, such as profiles, symptoms, and scenarios, to simulate seekers. While these efforts advance AI in mental health, achieving more realistic seeker simulation remains hindered by two key challenges: dynamic evolution and multi-session memory. Seekers' mental states often fluctuate during counseling, which typically spans multiple sessions. To address this, we propose AnnaAgent, an emotional and cognitive dynamic agent system equipped with tertiary memory. AnnaAgent incorporates an emotion modulator and a complaint elicitor trained on real counseling dialogues, enabling dynamic control of the simulator's configurations. Additionally, its tertiary memory mechanism effectively integrates short-term and long-term memory across sessions. Evaluation results, both automated and manual, demonstrate that AnnaAgent achieves more realistic seeker simulation in psychological counseling compared to existing baselines. The ethically reviewed and screened code can be found on https://github.com/sci-m-wang/AnnaAgent.",,"Ming Wang, Peidong Wang, Lin Wu, Xiaocui Yang, Daling Wang, Shi Feng, Yuxin Chen, Bixuan Wang, Yifei Zhang",2025-05-31T13:15:51Z,AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for   Realistic Seeker Simulation,AnnaAgent: Dynamisches Evolutions-Agentensystem mit Multi-Session-Speicher für realistische Suchersimulation,"AnnAAgenti:动态进化剂系统,具有多会议记忆系统,用于模拟现实探索者模拟",http://arxiv.org/abs/2506.00551v1
1142,"Evaluation frameworks for text summarization have evolved in terms of both domain coverage and metrics. However, existing benchmarks still lack domain-specific assessment criteria, remain predominantly English-centric, and face challenges with human annotation due to the complexity of reasoning. To address these, we introduce MSumBench, which provides a multi-dimensional, multi-domain evaluation of summarization in English and Chinese. It also incorporates specialized assessment criteria for each domain and leverages a multi-agent debate system to enhance annotation quality. By evaluating eight modern summarization models, we discover distinct performance patterns across domains and languages. We further examine large language models as summary evaluators, analyzing the correlation between their evaluation and summarization capabilities, and uncovering systematic bias in their assessment of self-generated summaries. Our benchmark dataset is publicly available at https://github.com/DISL-Lab/MSumBench.",,"Hyangsuk Min, Yuho Lee, Minjeong Ban, Jiaqi Deng, Nicole Hee-Yeon Kim, Taewon Yun, Hang Su, Jason Cai, Hwanjun Song",2025-05-31T13:12:35Z,Towards Multi-dimensional Evaluation of LLM Summarization across Domains   and Languages,Auf dem Weg zu einer multidimensionalen Bewertung der LLM-Zusammenfassung über Domains und Sprachen hinweg,走向对全域和所有语文的LLM概括的多维评价,http://arxiv.org/abs/2506.00549v1
1143,"Existing attacks against multimodal language models (MLLMs) primarily communicate instructions through text accompanied by adversarial images. In contrast, we exploit the capabilities of MLLMs to interpret non-textual instructions, specifically, adversarial images or audio generated by our novel method, Con Instruction. We optimize these adversarial examples to align closely with target instructions in the embedding space, revealing the detrimental implications of MLLMs' sophisticated understanding. Unlike prior work, our method does not require training data or preprocessing of textual instructions. While these non-textual adversarial examples can effectively bypass MLLM safety mechanisms, their combination with various text inputs substantially amplifies attack success. We further introduce a new Attack Response Categorization (ARC) framework, which evaluates both the quality of the model's response and its relevance to the malicious instructions. Experimental results demonstrate that Con Instruction effectively bypasses safety mechanisms in multiple vision- and audio-language models, including LLaVA-v1.5, InternVL, Qwen-VL, and Qwen-Audio, evaluated on two standard benchmarks: AdvBench and SafeBench. Specifically, our method achieves the highest attack success rates, reaching 81.3% and 86.6% on LLaVA-v1.5 (13B). On the defense side, we explore various countermeasures against our attacks and uncover a substantial performance gap among existing techniques. Our implementation is made publicly available.",,"Jiahui Geng, Thy Thy Tran, Preslav Nakov, Iryna Gurevych",2025-05-31T13:11:14Z,Con Instruction: Universal Jailbreaking of Multimodal Large Language   Models via Non-Textual Modalities,Con Instruction: Universal Jailbreaking von multimodalen großen Sprachmodellen über nicht-textuelle Modalitäten,Con 指令:通过非形式方式普遍破禁多式大语言模式,http://arxiv.org/abs/2506.00548v1
1144,"To compare autoregressive language models at scale, we propose using log-likelihood vectors computed on a predefined text set as model features. This approach has a solid theoretical basis: when treated as model coordinates, their squared Euclidean distance approximates the Kullback-Leibler divergence of text-generation probabilities. Our method is highly scalable, with computational cost growing linearly in both the number of models and text samples, and is easy to implement as the required features are derived from cross-entropy loss. Applying this method to over 1,000 language models, we constructed a ""model map,"" providing a new perspective on large-scale model analysis.",,"Momose Oyama, Hiroaki Yamagiwa, Yusuke Takase, Hidetoshi Shimodaira",2025-05-31T13:03:13Z,"Mapping 1,000+ Language Models via the Log-Likelihood Vector",Kartierung von 1.000+ Sprachmodellen über den Log-Likelihood-Vektor,通过日志-权益矢量矢量绘制1000+语言模型,http://arxiv.org/abs/2502.16173v2
1145,"Recently, LLM agents have made rapid progress in improving their programming capabilities. However, existing benchmarks lack the ability to automatically evaluate from users' perspective, and also lack the explainability of the results of LLM agents' code generation capabilities. Thus, we introduce ProjectEval, a new benchmark for LLM agents project-level code generation's automated evaluation by simulating user interaction. ProjectEval is constructed by LLM with human reviewing. It has three different level inputs of natural languages or code skeletons. ProjectEval can evaluate the generated projects by user interaction simulation for execution, and by code similarity through existing objective indicators. Through ProjectEval, we find that systematic engineering project code, overall understanding of the project and comprehensive analysis capability are the keys for LLM agents to achieve practical projects. Our findings and benchmark provide valuable insights for developing more effective programming agents that can be deployed in future real-world production.",,"Kaiyuan Liu, Youcheng Pan, Yang Xiang, Daojing He, Jing Li, Yexing Du, Tianrun Gao",2025-05-31T12:53:50Z,ProjectEval: A Benchmark for Programming Agents Automated Evaluation on   Project-Level Code Generation,ProjectEval: Ein Benchmark für die automatisierte Evaluierung von Programmierern auf Projektebene,项目Val:方案拟订代理器基准项目一级代码生成自动评价,http://arxiv.org/abs/2503.07010v2
1146,"Reinforcement learning (RL) has significantly advanced the reasoning capabilities of vision-language models (VLMs). However, the use of RL beyond reasoning tasks remains largely unexplored, especially for perceptionintensive tasks like object detection and grounding. We propose V-Triune, a Visual Triple Unified Reinforcement Learning system that enables VLMs to jointly learn visual reasoning and perception tasks within a single training pipeline. V-Triune comprises triple complementary components: Sample-Level Data Formatting (to unify diverse task inputs), Verifier-Level Reward Computation (to deliver custom rewards via specialized verifiers) , and Source-Level Metric Monitoring (to diagnose problems at the data-source level). We further introduce a novel Dynamic IoU reward, which provides adaptive, progressive, and definite feedback for perception tasks handled by V-Triune. Our approach is instantiated within off-the-shelf RL training framework using open-source 7B and 32B backbone models. The resulting model, dubbed Orsta (One RL to See Them All), demonstrates consistent improvements across both reasoning and perception tasks. This broad capability is significantly shaped by its training on a diverse dataset, constructed around four representative visual reasoning tasks (Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding, Detection, Counting, and OCR). Subsequently, Orsta achieves substantial gains on MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1 across its various 7B and 32B model variants, with performance benefits extending to a wide range of downstream tasks. These results highlight the effectiveness and scalability of our unified RL approach for VLMs. The V-Triune system, along with the Orsta models, is publicly available at https://github.com/MiniMax-AI.",,"Yan Ma, Linge Du, Xuyang Shen, Shaoxiang Chen, Pengfei Li, Qibing Ren, Lizhuang Ma, Yuchao Dai, Pengfei Liu, Junjie Yan",2025-05-31T12:52:01Z,One RL to See Them All: Visual Triple Unified Reinforcement Learning,"Ein RL, um sie alle zu sehen: Visual Triple Unified Verstärkung Lernen",1RL 全部见他们:视觉三重统一强化学习,http://arxiv.org/abs/2505.18129v2
1147,"Knowledge editing aims to efficiently update Large Language Models (LLMs) by modifying specific knowledge without retraining the entire model. Among knowledge editing approaches, in-context editing (ICE) offers a lightweight solution by injecting new knowledge directly into the input context, leaving model parameters unchanged. However, existing ICE approaches do not explicitly separate the newly injected knowledge from the model's original reasoning process. This entanglement often results in conflicts between external updates and internal parametric knowledge, undermining the consistency and accuracy of the reasoning path.In this work, we conduct preliminary experiments to examine how parametric knowledge influences reasoning path planning. We find that the model's reasoning is tightly coupled with its internal knowledge, and that naively injecting new information without adapting the reasoning path often leads to performance degradation, particularly in multi-hop tasks. To this end, we propose DecKER, a novel ICE framework that decouples reasoning from knowledge editing by generating a masked reasoning path and then resolving knowledge edits via hybrid retrieval and model-based validation. Experiments on multi-hop QA benchmarks show that DecKER significantly outperforms existing ICE methods by mitigating knowledge conflicts and preserving reasoning consistency. Our code is available at: https://github.com/bebr2/DecKER .",,"Changyue Wang, Weihang Su, Qingyao Ai, Yujia Zhou, Yiqun Liu",2025-05-31T12:51:12Z,Decoupling Reasoning and Knowledge Injection for In-Context Knowledge   Editing,Entkoppelung von Vernunft und Wissensinjektion für In-Context-Wissensbearbeitung,用于Intext知识编辑的脱钩理由和知识输入,http://arxiv.org/abs/2506.00536v1
1148,"Large Language Models (LLMs) are revolutionizing bioinformatics, enabling advanced analysis of DNA, RNA, proteins, and single-cell data. This survey provides a systematic review of recent advancements, focusing on genomic sequence modeling, RNA structure prediction, protein function inference, and single-cell transcriptomics. Meanwhile, we also discuss several key challenges, including data scarcity, computational complexity, and cross-omics integration, and explore future directions such as multimodal learning, hybrid AI models, and clinical applications. By offering a comprehensive perspective, this paper underscores the transformative potential of LLMs in driving innovations in bioinformatics and precision medicine.",,"Zhenyu Wang, Zikang Wang, Jiyue Jiang, Pengan Chen, Xiangyu Shi, Yu Li",2025-05-31T12:40:57Z,Large Language Models in Bioinformatics: A Survey,Große Sprachmodelle in der Bioinformatik: Eine Umfrage,生物信息学中大语言模型:调查,http://arxiv.org/abs/2503.04490v2
1149,"Large Language Models (LLMs) have demonstrated remarkable generalization capabilities across diverse tasks and languages. In this study, we focus on natural language understanding in three classical languages -- Sanskrit, Ancient Greek and Latin -- to investigate the factors affecting cross-lingual zero-shot generalization. First, we explore named entity recognition and machine translation into English. While LLMs perform equal to or better than fine-tuned baselines on out-of-domain data, smaller models often struggle, especially with niche or abstract entity types. In addition, we concentrate on Sanskrit by presenting a factoid question-answering (QA) dataset and show that incorporating context via retrieval-augmented generation approach significantly boosts performance. In contrast, we observe pronounced performance drops for smaller LLMs across these QA tasks. These results suggest model scale as an important factor influencing cross-lingual generalization. Assuming that models used such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical languages, our findings provide insights into how LLMs may generalize on these languages and their consequent utility in classical studies.",,"V. S. D. S. Mahesh Akavarapu, Hrishikesh Terdalkar, Pramit Bhattacharyya, Shubhangi Agarwal, Vishakha Deulgaonkar, Pralay Manna, Chaitali Dangarikar, Arnab Bhattacharya",2025-05-31T12:29:42Z,A Case Study of Cross-Lingual Zero-Shot Generalization for Classical   Languages in LLMs,Eine Fallstudie zur Cross-Lingual Zero-Shot Generalization für klassische Sprachen in LLMs,"关于LLMM中古典语言的 "" 不同语言零热零 "" 通用案例研究",http://arxiv.org/abs/2505.13173v2
1150,"State-of-the-art large language models (LLMs) have demonstrated impressive code generation capabilities but struggle with real-world software engineering tasks, such as revising source code to address code reviews, hindering their practical use. Code review comments are often implicit, ambiguous, and colloquial, requiring models to grasp both code and human intent. This challenge calls for evaluating large language models' ability to bridge both technical and conversational contexts. While existing work has employed the automated code refinement (ACR) task to resolve these comments, current evaluation methods fall short, relying on text matching metrics that provide limited insight into model failures and remain susceptible to training data contamination. To address these limitations, we introduce a novel evaluation benchmark, $\textbf{CodeReviewQA}$ that enables us to conduct fine-grained assessment of model capabilities and mitigate data contamination risks. In CodeReviewQA, we decompose the generation task of code refinement into $\textbf{three essential reasoning steps}$: $\textit{change type recognition}$ (CTR), $\textit{change localisation}$ (CL), and $\textit{solution identification}$ (SI). Each step is reformulated as multiple-choice questions with varied difficulty levels, enabling precise assessment of model capabilities, while mitigating data contamination risks. Our comprehensive evaluation spans 72 recently released large language models on $\textbf{900 manually curated, high-quality examples}$ across nine programming languages. Our results show that CodeReviewQA is able to expose specific model weaknesses in code review comprehension, disentangled from their generative automated code refinement results.",,"Hong Yi Lin, Chunhua Liu, Haoyu Gao, Patanamon Thongtanunam, Christoph Treude",2025-05-31T12:26:00Z,CodeReviewQA: The Code Review Comprehension Assessment for Large   Language Models,CodeReviewQA: Das CodeReview-Verständnis für große Sprachmodelle,守则审查QA:对大语言模式的守则审查理解评估,http://arxiv.org/abs/2503.16167v2
1151,"Understanding urban socioeconomic conditions through visual data is a challenging yet essential task for sustainable urban development and policy planning. In this work, we introduce $\textbf{CityLens}$, a comprehensive benchmark designed to evaluate the capabilities of large language-vision models (LLVMs) in predicting socioeconomic indicators from satellite and street view imagery. We construct a multi-modal dataset covering a total of 17 globally distributed cities, spanning 6 key domains: economy, education, crime, transport, health, and environment, reflecting the multifaceted nature of urban life. Based on this dataset, we define 11 prediction tasks and utilize three evaluation paradigms: Direct Metric Prediction, Normalized Metric Estimation, and Feature-Based Regression. We benchmark 17 state-of-the-art LLVMs across these tasks. Our results reveal that while LLVMs demonstrate promising perceptual and reasoning capabilities, they still exhibit limitations in predicting urban socioeconomic indicators. CityLens provides a unified framework for diagnosing these limitations and guiding future efforts in using LLVMs to understand and predict urban socioeconomic patterns. Our codes and datasets are open-sourced via https://github.com/tsinghua-fib-lab/CityLens.",,"Tianhui Liu, Jie Feng, Hetian Pang, Xin Zhang, Tianjian Ouyang, Zhiyuan Zhang, Yong Li",2025-05-31T12:25:33Z,CityLens: Benchmarking Large Language-Vision Models for Urban   Socioeconomic Sensing,CityLens: Benchmarking von großen Sprach-Vision-Modellen für die urbane sozioökonomische Sensing,城市:城市社会经济遥感大型语言远景模型基准,http://arxiv.org/abs/2506.00530v1
1152,"This paper introduces BMIKE-53, a comprehensive benchmark for cross-lingual in-context knowledge editing (IKE) across 53 languages, unifying three knowledge editing (KE) datasets: zsRE, CounterFact, and WikiFactDiff. Cross-lingual KE, which requires knowledge edited in one language to generalize across others while preserving unrelated knowledge, remains underexplored. To address this gap, we systematically evaluate IKE under zero-shot, one-shot, and few-shot setups, incorporating tailored metric-specific demonstrations. Our findings reveal that model scale and demonstration alignment critically govern cross-lingual IKE efficacy, with larger models and tailored demonstrations significantly improving performance. Linguistic properties, particularly script type, strongly influence performance variation across languages, with non-Latin languages underperforming due to issues like language confusion. Code and data are publicly available at: https://github.com/ercong21/MultiKnow/.",,"Ercong Nie, Bo Shao, Zifeng Ding, Mingyang Wang, Helmut Schmid, Hinrich Schütze",2025-05-31T12:21:04Z,BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context   Learning,BMIKE-53: Untersuchung von Cross-Lingual Knowledge Editing mit In-Context Learning,BMIKE-53:用内文学习调查跨语言知识编辑,http://arxiv.org/abs/2406.17764v3
1153,"Retrieval-Augmented Generation (RAG) systems in the Intellectual Property (IP) field often struggle with diverse user queries, including colloquial expressions, spelling errors, and ambiguous terminology, leading to inaccurate retrieval and suboptimal responses. To address this challenge, we propose Multi-Angle Question Generation and Retrieval Fine-Tuning Method (MQG-RFM), a novel framework that leverages large language models (LLMs) to simulate varied user inquiries and fine-tunes retrieval models to align semantically equivalent but linguistically diverse questions. Unlike complex architectural modifications, MQG-RFM adopts a lightweight Data-to-Tune paradigm, combining prompt-engineered query generation with hard negative mining to enhance retrieval robustness without costly infrastructure changes. Experimental results on a Taiwan patent Q&A dataset show 185.62% improvement in retrieval accuracy on the Patent Consultation dataset and 262.26% improvement on the Novel Patent Technology Report dataset, with 14.22% and 53.58% improvements in generation quality over the baselines, respectively. By bridging the gap between user intent and system comprehension through semantic-aware retrieval optimization, MQG-RFM offers a practical, scalable approach for rapid, cost-effective deployment among small and medium-sized agencies seeking reliable patent intelligence solutions. Additionally, our proposed method has already been adopted by ScholarMate, the largest professional research social networking platform in China, to support real-world development and deployment. A demo version of the instantiated is available at https://github.com/renruntao/patent_rag.",,"Runtao Ren, Jian Ma, Jianxi Luo",2025-05-31T12:19:35Z,Retrieval-Augmented Generation Systems for Intellectual Property via   Synthetic Multi-Angle Fine-tuning,Retrieval-Augmented Generation Systems for Intellectual Property via Synthetic Multi-Angle Fine-Tuning,通过合成多角度多角度微调实现知识产权回收-提款生成系统,http://arxiv.org/abs/2506.00527v1
1154,"Large language models hold promise for addressing medical challenges, such as medical diagnosis reasoning, research knowledge acquisition, clinical decision-making, and consumer health inquiry support. However, they often generate hallucinations due to limited medical knowledge. Incorporating external knowledge is therefore critical, which necessitates multi-source knowledge acquisition. We address this challenge by framing it as a source planning problem, which is to formulate context-appropriate queries tailored to the attributes of diverse sources. Existing approaches either overlook source planning or fail to achieve it effectively due to misalignment between the model's expectation of the sources and their actual content. To bridge this gap, we present MedOmniKB, a repository comprising multigenre and multi-structured medical knowledge sources. Leveraging these sources, we propose the Source Planning Optimisation method, which enhances multi-source utilisation. Our approach involves enabling an expert model to explore and evaluate potential plans while training a smaller model to learn source alignment. Experimental results demonstrate that our method substantially improves multi-source planning performance, enabling the optimised small model to achieve state-of-the-art results in leveraging diverse medical knowledge sources.",,"Zhe Chen, Yusheng Liao, Shuyang Jiang, Pingjie Wang, Yiqiu Guo, Yanfeng Wang, Yu Wang",2025-05-31T12:13:46Z,Towards Omni-RAG: Comprehensive Retrieval-Augmented Generation for Large   Language Models in Medical Applications,Auf dem Weg zu Omni-RAG: Umfassende Retrieval-Augmented Generation für große Sprachmodelle in medizinischen Anwendungen,走向Omni-RAG:医学应用中大语言模型综合检索-推广一代,http://arxiv.org/abs/2501.02460v3
1155,"Cognitive tasks originally developed for humans are now increasingly used to study language models. While applying these tasks is often straightforward, interpreting their results can be challenging. In particular, when a model underperforms, it is often unclear whether this results from a limitation in the cognitive ability being tested or a failure to understand the task itself. A recent study argues that GPT 3.5's declining performance on 2-back and 3-back tasks reflects a working memory capacity limit similar to humans (Gong et al., 2024). By analyzing a range of open-source language models of varying performance levels on these tasks, we show that the poor performance is due at least in part to a limitation in task comprehension and task set maintenance. We challenge the best-performing model with progressively harder versions of the task (up to 10-back) and experiment with alternative prompting strategies, before analyzing model attentions. Our larger aim is to contribute to the ongoing conversation around refining methodologies for the cognitive evaluation of language models.",,"Xiaoyang Hu, Richard L. Lewis",2025-05-31T12:13:06Z,Do Language Models Understand the Cognitive Tasks Given to Them?   Investigations with the N-Back Paradigm,"Verstehen Sprachmodelle die kognitiven Aufgaben, die ihnen gegeben werden? Untersuchungen mit dem N-Back Paradigma",语言模式是否理解由他们承担的认知任务?,http://arxiv.org/abs/2412.18120v3
1156,"Large language models possess general linguistic abilities but acquire language less efficiently than humans. This study proposes a method for integrating the developmental characteristics of working memory during the critical period, a stage when human language acquisition is particularly efficient, into the training process of language models. The proposed method introduces a mechanism that initially constrains working memory during the early stages of training and gradually relaxes this constraint in an exponential manner as learning progresses. Targeted syntactic evaluation shows that the proposed method outperforms conventional methods without memory constraints or with static memory constraints. These findings not only provide new directions for designing data-efficient language models but also offer indirect evidence supporting the role of the developmental characteristics of working memory as the underlying mechanism of the critical period in language acquisition.",,"Masato Mita, Ryo Yoshida, Yohei Oseki",2025-05-31T12:07:46Z,Developmentally-plausible Working Memory Shapes a Critical Period for   Language Acquisition,Entwicklunglich-plausible Arbeitsgedächtnis formt eine kritische Periode für Spracherwerb,语文购置关键时期的可发展可促进的工作记忆形状,http://arxiv.org/abs/2502.04795v3
1157,"Most toxicity detection models treat toxicity as an intrinsic property of text, overlooking the role of context in shaping its impact. Drawing on interdisciplinary research, we reconceptualise toxicity as a socially emergent stress signal. We introduce a new framework for toxicity detection, including a formal definition and metric, and validate our approach on a novel dataset, demonstrating improved contextual sensitivity and adaptability.",,"Sergey Berezin, Reza Farahbakhsh, Noel Crespi",2025-05-31T12:02:49Z,Redefining Toxicity: An Objective and Context-Aware Approach for   Stress-Level-Based Detection,Redefining Toxizität: Ein objektiver und kontextbewusster Ansatz für Stress-Level-basierte Erkennung,重新界定毒性:基于压力水平检测的目标和背景意识方法,http://arxiv.org/abs/2503.16072v2
1158,"We present a novel class of jailbreak adversarial attacks on LLMs, termed Task-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks (e.g., cipher decoding, riddles, code execution) into the model's prompt to indirectly generate prohibited inputs. To systematically assess the effectiveness of these attacks, we introduce the PHRYGE benchmark. We demonstrate that our techniques successfully circumvent safeguards in six state-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings highlight critical weaknesses in current LLM safety alignments and underscore the urgent need for more sophisticated defence strategies.   Warning: this paper contains examples of unethical inquiries used solely for research purposes.",,"Sergey Berezin, Reza Farahbakhsh, Noel Crespi",2025-05-31T11:52:11Z,The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt   Adversarial Attacks on LLMs,Die TIPP des Eisbergs: Enthüllen einer versteckten Klasse von Task-in-Prompt-Adversarial-Angriffen auf LLMs,"Iceberg的TIP: 揭露对LLMs的 "" 临时反向袭击 "" 的隐蔽任务类别",http://arxiv.org/abs/2501.18626v4
1159,"Medical imaging provides essential visual insights for diagnosis, and multimodal large language models (MLLMs) are increasingly utilized for its analysis due to their strong generalization capabilities; however, the underlying factors driving this generalization remain unclear. Current research suggests that multi-task training outperforms single-task as different tasks can benefit each other, but they often overlook the internal relationships within these tasks. To analyze this phenomenon, we attempted to employ compositional generalization (CG), which refers to the models' ability to understand novel combinations by recombining learned elements, as a guiding framework. Since medical images can be precisely defined by Modality, Anatomical area, and Task, naturally providing an environment for exploring CG, we assembled 106 medical datasets to create Med-MAT for comprehensive experiments. The experiments confirmed that MLLMs can use CG to understand unseen medical images and identified CG as one of the main drivers of the generalization observed in multi-task training. Additionally, further studies demonstrated that CG effectively supports datasets with limited data and confirmed that MLLMs can achieve CG across classification and detection tasks, underscoring its broader generalization potential. Med-MAT is available at https://github.com/FreedomIntelligence/Med-MAT.",,"Zhenyang Cai, Junying Chen, Rongsheng Wang, Weihong Wang, Yonglin Deng, Dingjie Song, Yize Chen, Zixu Zhang, Benyou Wang",2025-05-31T11:45:12Z,Exploring Compositional Generalization of Multimodal LLMs for Medical   Imaging,Erforschung der kompositorischen Generalisierung multimodaler LLMs für die medizinische Bildgebung,探索医疗成像多式灵丹,http://arxiv.org/abs/2412.20070v2
1160,"Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there is a notable lack of attention for assessing essays according to individual traits. In this work, we propose TRATES, a novel trait-specific and rubric-based cross-prompt AES framework that is generic yet specific to the underlying trait. The framework leverages a Large Language Model (LLM) that utilizes the trait grading rubrics to generate trait-specific features (represented by assessment questions), then assesses those features given an essay. The trait-specific features are eventually combined with generic writing-quality and prompt-specific features to train a simple classical regression model that predicts trait scores of essays from an unseen prompt. Experiments show that TRATES achieves a new state-of-the-art performance across all traits on a widely-used dataset, with the generated LLM-based features being the most significant.",,"Sohaila Eltanbouly, Salam Albatarni, Tamer Elsayed",2025-05-31T11:33:14Z,TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring,TRATES: Trait-spezifische Rubric-Assisted Cross-Prompt Essay Scoring,TRATTES: 转轨、特定卢布、辅助跨期热量日志,http://arxiv.org/abs/2505.14577v2
1161,"Large Language Models (LLMs) achieve remarkable performance through pretraining on extensive data. This enables efficient adaptation to diverse downstream tasks. However, the lack of interpretability in their underlying mechanisms limits the ability to effectively steer LLMs for specific applications. In this work, we investigate the intrinsic mechanisms of LLMs from a cognitive perspective using eye movement measures. Specifically, we analyze the layer-wise correlation between human cognitive indicators and LLM representations. Building on these insights, we propose a heuristic approach for selecting the optimal steering layer to modulate LLM semantics. To this end, we introduce an efficient selective layer intervention based on prominent parameter-efficient fine-tuning methods, which conventionally adjust either all layers or only the final layer. Additionally, we present an implicit layer contrastive intervention during inference to steer LLMs away from toxic outputs. Extensive experiments on natural language understanding, reasoning, and generation tasks, conducted on GPT-2, Llama2-7B, and Mistral-7B, demonstrate the effectiveness and efficiency of our approach. As a model-agnostic framework, it enhances the interpretability of LLMs while improving efficiency for safe deployment.",,"Xintong Wang, Jingheng Pan, Liang Ding, Longyue Wang, Longqin Jiang, Xingshan Li, Chris Biemann",2025-05-31T11:23:27Z,CogSteer: Cognition-Inspired Selective Layer Intervention for   Efficiently Steering Large Language Models,CogSteer: Cognition-inspirierte selektive Layer-Intervention zur effizienten Steuerung großer Sprachmodelle,COgSteer:高效引导大语言模型的认知-受启发选择性图层干预,http://arxiv.org/abs/2410.17714v3
1162,"Large Language Models (LLMs) such as ChatGPT demonstrate significant potential in the medical domain and are often evaluated using multiple-choice questions (MCQs) modeled on exams like the USMLE. However, such benchmarks may overestimate true clinical understanding by rewarding pattern recognition and test-taking heuristics. To investigate this, we created a fictional medical benchmark centered on an imaginary organ, the Glianorex, allowing us to separate memorized knowledge from reasoning ability. We generated textbooks and MCQs in English and French using leading LLMs, then evaluated proprietary, open-source, and domain-specific models in a zero-shot setting. Despite the fictional content, models achieved an average score of 64%, while physicians scored only 27%. Fine-tuned medical models outperformed base models in English but not in French. Ablation and interpretability analyses revealed that models frequently relied on shallow cues, test-taking strategies, and hallucinated reasoning to identify the correct choice. These results suggest that standard MCQ-based evaluations may not effectively measure clinical reasoning and highlight the need for more robust, clinically meaningful assessment methods for LLMs.",,"Maxime Griot, Jean Vanderdonckt, Demet Yuksel, Coralie Hemptinne",2025-05-31T11:21:22Z,Pattern Recognition or Medical Knowledge? The Problem with   Multiple-Choice Questions in Medicine,Mustererkennung oder medizinisches Wissen? Das Problem mit Multiple-Choice-Fragen in der Medizin,模式识别还是医学知识?医学中多选择问题的问题,http://arxiv.org/abs/2406.02394v2
1163,"We study whether Large Language Models (LLMs) latently perform multi-hop reasoning with complex prompts such as ""The mother of the singer of 'Superstition' is"". We look for evidence of a latent reasoning pathway where an LLM (1) latently identifies ""the singer of 'Superstition'"" as Stevie Wonder, the bridge entity, and (2) uses its knowledge of Stevie Wonder's mother to complete the prompt. We analyze these two hops individually and consider their co-occurrence as indicative of latent multi-hop reasoning. For the first hop, we test if changing the prompt to indirectly mention the bridge entity instead of any other entity increases the LLM's internal recall of the bridge entity. For the second hop, we test if increasing this recall causes the LLM to better utilize what it knows about the bridge entity. We find strong evidence of latent multi-hop reasoning for the prompts of certain relation types, with the reasoning pathway used in more than 80% of the prompts. However, the utilization is highly contextual, varying across different types of prompts. Also, on average, the evidence for the second hop and the full multi-hop traversal is rather moderate and only substantial for the first hop. Moreover, we find a clear scaling trend with increasing model size for the first hop of reasoning but not for the second hop. Our experimental findings suggest potential challenges and opportunities for future development and applications of LLMs.",,"Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, Sebastian Riedel",2025-05-31T11:20:49Z,Do Large Language Models Latently Perform Multi-Hop Reasoning?,Führen große Sprachmodelle latent Multi-Hop-Reasoning aus?,大语言模型是否实际表现多功能理由 ?,http://arxiv.org/abs/2402.16837v2
1164,"In commonsense generation, given a set of input concepts, a model must generate a response that is not only commonsense bearing, but also capturing multiple diverse viewpoints. Numerous evaluation metrics based on form- and content-level overlap have been proposed in prior work for evaluating the diversity of a commonsense generation model. However, it remains unclear as to which metrics are best suited for evaluating the diversity in commonsense generation. To address this gap, we conduct a systematic meta-evaluation of diversity metrics for commonsense generation. We find that form-based diversity metrics tend to consistently overestimate the diversity in sentence sets, where even randomly generated sentences are assigned overly high diversity scores. We then use an Large Language Model (LLM) to create a novel dataset annotated for the diversity of sentences generated for a commonsense generation task, and use it to conduct a meta-evaluation of the existing diversity evaluation metrics. Our experimental results show that content-based diversity evaluation metrics consistently outperform the form-based counterparts, showing high correlations with the LLM-based ratings. We recommend that future work on commonsense generation should use content-based metrics for evaluating the diversity of their outputs.",,"Tianhui Zhang, Bei Peng, Danushka Bollegala",2025-05-31T11:18:26Z,Evaluating the Evaluation of Diversity in Commonsense Generation,Bewertung der Bewertung der Vielfalt in der Commonsense-Generation,评估对共同一代人多样性的评价,http://arxiv.org/abs/2506.00514v1
1165,"Chain-of-thought (CoT) prompting demonstrates varying performance under different reasoning tasks. Previous work attempts to evaluate it but falls short in providing an in-depth analysis of patterns that influence the CoT. In this paper, we study the CoT performance from the perspective of effectiveness and faithfulness. For the former, we identify key factors that influence CoT effectiveness on performance improvement, including problem difficulty, information gain, and information flow. For the latter, we interpret the unfaithful CoT issue by conducting a joint analysis of the information interaction among the question, CoT, and answer. The result demonstrates that, when the LLM predicts answers, it can recall correct information missing in the CoT from the question, leading to the problem. Finally, we propose a novel algorithm to mitigate this issue, in which we recall extra information from the question to enhance the CoT generation and evaluate CoTs based on their information gain. Extensive experiments demonstrate that our approach enhances both the faithfulness and effectiveness of CoT.",,"Jiachun Li, Pengfei Cao, Yubo Chen, Jiexin Xu, Huaijun Li, Xiaojian Jiang, Kang Liu, Jun Zhao",2025-05-31T11:18:02Z,Towards Better Chain-of-Thought: A Reflection on Effectiveness and   Faithfulness,Auf dem Weg zu einer besseren Kette von Gedanken: Eine Reflexion über Wirksamkeit und Treue,寻求更好的思考链:对有效性和信仰性的反思,http://arxiv.org/abs/2405.18915v3
1166,"We evaluate how well Large Language Models (LLMs) latently recall and compose facts to answer multi-hop queries like ""In the year Scarlett Johansson was born, the Summer Olympics were hosted in the country of"". One major challenge in such evaluation is that LLMs may have developed shortcuts by encountering the head entity ""Scarlett Johansson"" and the answer entity ""United States"" in the same training sequences or merely guess the answer based on frequency-based priors. To prevent shortcuts, we exclude test queries where the head and answer entities might have co-appeared during training. Through careful selection of relations and facts and systematic removal of cases where models might guess answers or exploit partial matches, we construct an evaluation dataset SOCRATES (ShOrtCut-fRee lATent rEaSoning). We observe that LLMs demonstrate promising latent multi-hop reasoning abilities without exploiting shortcuts, but only for certain types of queries. For queries requiring latent recall of countries as the intermediate answer, the best models achieve 80% latent composability, but this drops to just 5% for the recall of years. Comparisons with Chain-of-Thought highlight a significant gap between the ability of models to reason latently versus explicitly. Analysis reveals that latent representations of the intermediate answer are constructed more often in queries with higher latent composability, and shows the emergence of latent multi-hop reasoning during pretraining.",,"Sohee Yang, Nora Kassner, Elena Gribovskaya, Sebastian Riedel, Mor Geva",2025-05-31T11:16:44Z,Do Large Language Models Perform Latent Multi-Hop Reasoning without   Exploiting Shortcuts?,"Führen große Sprachmodelle latente Multi-Hop-Reasoning aus, ohne Shortcuts auszunutzen?",大语言模型是否在不使用快捷键的情况下执行前端多处偏差?,http://arxiv.org/abs/2411.16679v2
1167,"Large Language Model-based Multi-Agent Systems (MASs) have demonstrated strong advantages in addressing complex real-world tasks. However, due to the introduction of additional attack surfaces, MASs are particularly vulnerable to misinformation injection. To facilitate a deeper understanding of misinformation propagation dynamics within these systems, we introduce MisinfoTask, a novel dataset featuring complex, realistic tasks designed to evaluate MAS robustness against such threats. Building upon this, we propose ARGUS, a two-stage, training-free defense framework leveraging goal-aware reasoning for precise misinformation rectification within information flows. Our experiments demonstrate that in challenging misinformation scenarios, ARGUS exhibits significant efficacy across various injection attacks, achieving an average reduction in misinformation toxicity of approximately 28.17% and improving task success rates under attack by approximately 10.33%. Our code and dataset is available at: https://github.com/zhrli324/ARGUS.",,"Zherui Li, Yan Mi, Zhenhong Zhou, Houcheng Jiang, Guibin Zhang, Kun Wang, Junfeng Fang",2025-05-31T11:02:26Z,Goal-Aware Identification and Rectification of Misinformation in   Multi-Agent Systems,Ziel-Aware-Identifizierung und Berichtigung von Fehlinformationen in Multi-Agent-Systemen,目标-多机构系统中错误信息的识别和纠正,http://arxiv.org/abs/2506.00509v1
1168,"Large language models (LLMs) have demonstrated strong performance across various tasks, leveraging their exceptional in-context learning ability with only a few examples. Accordingly, the selection of optimal in-context examples has been actively studied in the field of machine translation. However, these studies presuppose the presence of a demonstration pool with human-annotated pairs, making them less applicable to low-resource languages where such an assumption is challenging to meet. To overcome this limitation, this paper explores the research direction of in-context example generation for machine translation. Specifically, we propose Demonstration Augmentation for Translation (DAT), a simple yet effective approach that generates example pairs without relying on any external resources. This method builds upon two prior criteria, relevance and diversity, which have been highlighted in previous work as key factors for in-context example selection. Through experiments and analysis on low-resource languages where human-annotated pairs are scarce, we show that DAT achieves superior translation quality compared to the baselines. Furthermore, we investigate the potential of progressively accumulating generated pairs during test time to build and reuse a demonstration pool. Our implementation is publicly available at https://github.com/aiclaudev/DAT.",,"Dohyun Lee, Seungil Chad Lee, Chanwoo Yang, Yujin Baek, Jaegul Choo",2025-05-31T11:00:49Z,Exploring In-context Example Generation for Machine Translation,Exploring In-Context Beispiel Erzeugung für maschinelle Übersetzung,探索机器翻译的内文实例生成,http://arxiv.org/abs/2506.00507v1
1169,"Many sensitive domains -- such as the clinical domain -- lack widely available datasets due to privacy risks. The increasing generative capabilities of large language models (LLMs) have made synthetic datasets a viable path forward. In this study, we domain-adapt LLMs to the clinical domain and generate synthetic clinical texts that are machine-annotated with tags for personally identifiable information using capable encoder-based NER models. The synthetic corpora are then used to train synthetic NER models. The results show that training NER models using synthetic corpora incurs only a small drop in predictive performance. The limits of this process are investigated in a systematic ablation study -- using both Swedish and Spanish data. Our analysis shows that smaller datasets can be sufficient for domain-adapting LLMs for data synthesis. Instead, the effectiveness of this process is almost entirely contingent on the performance of the machine-annotating NER models trained using the original data.",,"Thomas Vakili, Aron Henriksson, Hercules Dalianis",2025-05-31T10:43:20Z,Data-Constrained Synthesis of Training Data for De-Identification,Datenkonzentrierte Synthese von Trainingsdaten zur De-Identifizierung,数据 - 数据 - 数据 - 识别培训数据综合汇编,http://arxiv.org/abs/2502.14677v3
1170,"Large Multimodal Models (LMMs) have demonstrated strong performance in English, but their effectiveness in Japanese remains limited due to the lack of high-quality training data. Current Japanese LMMs often rely on translated English datasets, restricting their ability to capture Japan-specific cultural knowledge. To address this, we explore the potential of Japanese PDF data as a training resource, an area that remains largely underutilized. We introduce a fully automated pipeline that leverages pretrained models to extract image-text pairs from PDFs through layout analysis, OCR, and vision-language pairing, removing the need for manual annotation. Additionally, we construct instruction data from extracted image-text pairs to enrich the training data. To evaluate the effectiveness of PDF-derived data, we train Japanese LMMs and assess their performance on the Japanese LMM Benchmark. Our results demonstrate substantial improvements, with performance gains ranging from 2.1% to 13.8% on Heron-Bench. Further analysis highlights the impact of PDF-derived data on various factors, such as model size and language models, reinforcing its value as a multimodal resource for Japanese LMMs.",,"Jeonghun Baek, Akiko Aizawa, Kiyoharu Aizawa",2025-05-31T10:33:53Z,Harnessing PDF Data for Improving Japanese Large Multimodal Models,Verwendung von PDF-Daten zur Verbesserung japanischer großer multimodaler Modelle,利用PDF数据改进日本大型多式联运模型,http://arxiv.org/abs/2502.14778v2
1171,"Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a widely adopted strategy for adapting pre-trained Large Language Models (LLMs) to downstream tasks, significantly reducing memory and computational costs. However, most existing PEFT techniques uniformly deploy LoRA adapters across all layers, disregarding the intrinsic heterogeneity of layer contributions and task-specific rank requirements. This uniform paradigm leads to redundant parameter allocation and suboptimal adaptation efficiency. To address these limitations, we propose FLoE, a novel PEFT framework that introduces two key innovations: (i) a Fisher information-guided importance scoring mechanism to dynamically identify task-critical transformer layers for MoE-based low-rank adaptation, enabling sparse adapter deployment; and (ii) a Bayesian optimization-driven rank allocator that automatically determines optimal LoRA ranks on specific datasets without exhaustive grid search. Extensive experiments across diverse LLMs and benchmarks reveal that FLoE achieves impressive efficiency-accuracy trade-offs, making FLoE particularly advantageous in resource-constrained environments that necessitate rapid adaptation.",,"Xinyi Wang, Lirong Gao, Haobo Wang, Yiming Zhang, Junbo Zhao",2025-05-31T10:27:08Z,FLoE: Fisher-Based Layer Selection for Efficient Sparse Adaptation of   Low-Rank Experts,FLoE: Fisher-Based-Layer-Auswahl für effiziente Sparse-Anpassung von Low-Rank-Experten,"FLOE:以渔业为基础的图层选择,以便高效率地对低兰克专家进行零散的适应",http://arxiv.org/abs/2506.00495v1
1172,"Machine Unlearning (MU) is critical for removing private or hazardous information from deep learning models. While MU has advanced significantly in unimodal (text or vision) settings, multimodal unlearning (MMU) remains underexplored due to the lack of open benchmarks for evaluating cross-modal data removal. To address this gap, we introduce CLEAR, the first open-source benchmark designed specifically for MMU. CLEAR contains 200 fictitious individuals and 3,700 images linked with corresponding question-answer pairs, enabling a thorough evaluation across modalities. We conduct a comprehensive analysis of 11 MU methods (e.g., SCRUB, gradient ascent, DPO) across four evaluation sets, demonstrating that jointly unlearning both modalities outperforms single-modality approaches. The dataset is available at https://huggingface.co/datasets/therem/CLEAR",,"Alexey Dontsov, Dmitrii Korzh, Alexey Zhavoronkin, Boris Mikheev, Denis Bobkov, Aibek Alanov, Oleg Y. Rogov, Ivan Oseledets, Elena Tutubalina",2025-05-31T10:24:54Z,CLEAR: Character Unlearning in Textual and Visual Modalities,CLEAR: Character Unlearning in Text- und Visual Modalities,CLEAR: 在文字和视觉模式方面不学习的特性,http://arxiv.org/abs/2410.18057v4
1173,"Human evaluation is the gold standard for evaluating text generation models. However, it is expensive. In order to fit budgetary constraints, a random subset of the test data is often chosen in practice for human evaluation. However, randomly selected data may not accurately represent test performance, making this approach economically inefficient for model comparison. Thus, in this work, we develop and analyze a suite of selectors to get the most informative datapoints for human evaluation, taking the evaluation costs into account. We show that selectors based on variance in automated metric scores, diversity in model outputs, or Item Response Theory outperform random selection. We further develop an approach to distill these selectors to the scenario where the model outputs are not yet available. In particular, we introduce source-based estimators, which predict item usefulness for human evaluation just based on the source texts. We demonstrate the efficacy of our selectors in two common NLG tasks, machine translation and summarization, and show that only $\sim$70\% of the test data is needed to produce the same evaluation result as the entire data.",,"Vilém Zouhar, Peng Cui, Mrinmaya Sachan",2025-05-31T09:55:39Z,How to Select Datapoints for Efficient Human Evaluation of NLG Models?,Wie wählt man Datenpunkte für eine effiziente menschliche Bewertung von NLG-Modellen aus?,"如何选择数据点,以便高效地对NLG模型进行人类评估?",http://arxiv.org/abs/2501.18251v2
1174,"Misinformation is prevalent in various fields such as education, politics, health, etc., causing significant harm to society. However, current methods for cross-domain misinformation detection rely on effort- and resource-intensive fine-tuning and complex model structures. With the outstanding performance of LLMs, many studies have employed them for misinformation detection. Unfortunately, they focus on in-domain tasks and do not incorporate significant sentiment and emotion features (which we jointly call {\em affect}). In this paper, we propose RAEmoLLM, the first retrieval augmented (RAG) LLMs framework to address cross-domain misinformation detection using in-context learning based on affective information. RAEmoLLM includes three modules. (1) In the index construction module, we apply an emotional LLM to obtain affective embeddings from all domains to construct a retrieval database. (2) The retrieval module uses the database to recommend top K examples (text-label pairs) from source domain data for target domain contents. (3) These examples are adopted as few-shot demonstrations for the inference module to process the target domain content. The RAEmoLLM can effectively enhance the general performance of LLMs in cross-domain misinformation detection tasks through affect-based retrieval, without fine-tuning. We evaluate our framework on three misinformation benchmarks. Results show that RAEmoLLM achieves significant improvements compared to the other few-shot methods on three datasets, with the highest increases of 15.64%, 31.18%, and 15.73% respectively. This project is available at https://github.com/lzw108/RAEmoLLM.",,"Zhiwei Liu, Kailai Yang, Qianqian Xie, Christine de Kock, Sophia Ananiadou, Eduard Hovy",2025-05-31T09:54:19Z,RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation   Detection Using In-Context Learning Based on Emotional Information,RAEmoLLM: Retrieval Augmented LLMs für Cross-Domain-Missinformationserkennung mittels In-Context-Lernen basierend auf emotionalen Informationen,"RAEmoLLM:利用基于情感信息的内文学习,进行跨域错误信息探测的检索增强LML",http://arxiv.org/abs/2406.11093v2
1175,"Transformers have achieved state-of-the-art performance in morphological inflection tasks, yet their ability to generalize across languages and morphological rules remains limited. One possible explanation for this behavior can be the degree to which these models are able to capture implicit phenomena at the phonological and subphonemic levels. We introduce a language-agnostic probing method to investigate phonological feature encoding in transformers trained directly on phonemes, and perform it across seven morphologically diverse languages. We show that phonological features which are local, such as final-obstruent devoicing in Turkish, are captured well in phoneme embeddings, whereas long-distance dependencies like vowel harmony are better represented in the transformer's encoder. Finally, we discuss how these findings inform empirical strategies for training morphological models, particularly regarding the role of subphonemic feature acquisition.",,"Gal Astrach, Yuval Pinter",2025-05-31T09:53:40Z,Probing Subphonemes in Morphology Models,Probing Subphonemes in morphologischen Modellen,体理模型中检测子电话,http://arxiv.org/abs/2505.11297v2
1176,"Large Language Models (LLMs) can assist multimodal fake news detection by predicting pseudo labels. However, LLM-generated pseudo labels alone demonstrate poor performance compared to traditional detection methods, making their effective integration non-trivial. In this paper, we propose Global Label Propagation Network with LLM-based Pseudo Labeling (GLPN-LLM) for multimodal fake news detection, which integrates LLM capabilities via label propagation techniques. The global label propagation can utilize LLM-generated pseudo labels, enhancing prediction accuracy by propagating label information among all samples. For label propagation, a mask-based mechanism is designed to prevent label leakage during training by ensuring that training nodes do not propagate their own labels back to themselves. Experimental results on benchmark datasets show that by synergizing LLMs with label propagation, our model achieves superior performance over state-of-the-art baselines.",,"Shuguo Hu, Jun Hu, Huaiwen Zhang",2025-05-31T09:50:40Z,Synergizing LLMs with Global Label Propagation for Multimodal Fake News   Detection,Synergie von LLMs mit globaler Labelverbreitung für multimodale Fake News-Erkennung,将LLMs与全球标签促销促进多式联运假冒新闻探测同步,http://arxiv.org/abs/2506.00488v1
1177,"Visualizations play a pivotal role in daily communication in an increasingly datadriven world. Research on multimodal large language models (MLLMs) for automated chart understanding has accelerated massively, with steady improvements on standard benchmarks. However, for MLLMs to be reliable, they must be robust to misleading visualizations, i.e., charts that distort the underlying data, leading readers to draw inaccurate conclusions that may support disinformation. Here, we uncover an important vulnerability: MLLM questionanswering (QA) accuracy on misleading visualizations drops on average to the level of the random baseline. To address this, we introduce the first inference-time methods to improve QA performance on misleading visualizations, without compromising accuracy on non-misleading ones. We find that two methods, table-based QA and redrawing the visualization, are effective, with improvements of up to 19.6 percentage points. We make our code and data available.",,"Jonathan Tonglet, Tinne Tuytelaars, Marie-Francine Moens, Iryna Gurevych",2025-05-31T09:48:25Z,Protecting multimodal large language models against misleading   visualizations,Schutz multimodaler Großsprachenmodelle gegen irreführende Visualisierungen,"保护大型多式联运语言模式,防止误导可视化",http://arxiv.org/abs/2502.20503v4
1178,"Retrieval-Augmented Generation (RAG) has emerged as a crucial method for addressing hallucinations in large language models (LLMs). While recent research has extended RAG models to complex noisy scenarios, these explorations often confine themselves to limited noise types and presuppose that noise is inherently detrimental to LLMs, potentially deviating from real-world retrieval environments and restricting practical applicability. In this paper, we define seven distinct noise types from a linguistic perspective and establish a Noise RAG Benchmark (NoiserBench), a comprehensive evaluation framework encompassing multiple datasets and reasoning tasks. Through empirical evaluation of eight representative LLMs with diverse architectures and scales, we reveal that these noises can be further categorized into two practical groups: noise that is beneficial to LLMs (aka beneficial noise) and noise that is harmful to LLMs (aka harmful noise). While harmful noise generally impairs performance, beneficial noise may enhance several aspects of model capabilities and overall performance. Our analysis offers insights for developing more robust, adaptable RAG solutions and mitigating hallucinations across diverse retrieval scenarios. Code is available at https://github.com/jinyangwu/NoiserBench.",,"Jinyang Wu, Shuai Zhang, Feihu Che, Mingkuan Feng, Chuyuan Zhang, Pengpeng Shao, Jianhua Tao",2025-05-31T09:45:31Z,Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the   Role of RAG Noise in Large Language Models,"Pandora's Box oder Aladdin's Lampe: Eine umfassende Analyse, die die Rolle des RAG-Geräuschs in großen Sprachmodellen aufzeigt",Pandora的盒子或Aladdin的灯光:全面分析RAG噪音在大语言模型中的作用,http://arxiv.org/abs/2408.13533v4
1179,"A radiology report comprises several sections, including the Findings and Impression of the diagnosis. Automatically generating the Impression from the Findings is crucial for reducing radiologists' workload and improving diagnostic accuracy. Pretrained models that excel in common abstractive summarization problems encounter challenges when applied to specialized medical domains largely due to the complex terminology and the necessity for accurate clinical context. Such tasks in medical domains demand extracting core information, avoiding context shifts, and maintaining proper flow. Misuse of medical terms can lead to drastic clinical errors. To address these issues, we introduce a sequential transfer learning that ensures key content extraction and coherent summarization. Sequential transfer learning often faces challenges like initial parameter decay and knowledge loss, which we resolve with the Fisher matrix regularization. Using MIMIC-CXR and Open-I datasets, our model, CSTRL - Context-driven Sequential TRansfer Learning - achieved state-of-the-art performance, showing 56.2% improvement in BLEU-1, 40.5% in BLEU-2, 84.3% in BLEU-3, 28.9% in ROUGE-1, 41.0% in ROUGE-2 and 26.5% in ROGUE-3 score over benchmark studies. We also analyze factual consistency scores while preserving the medical context. Our code is publicly available at https://github.com/fahmidahossain/Report_Summarization.",,"Mst. Fahmida Sultana Naznin, Adnan Ibney Faruq, Mostafa Rifat Tazwar, Md Jobayer, Md. Mehedi Hasan Shawon, Md Rakibul Hasan",2025-05-31T09:39:55Z,CSTRL: Context-Driven Sequential Transfer Learning for Abstractive   Radiology Report Summarization,CSTRL: Context-Driven Sequential Transfer Learning for Abstraktive Radiology Report Zusammenfassung,CSTRSL: 用于简易放射学报告摘要总结的内向序列转移学习,http://arxiv.org/abs/2503.05750v2
1180,"Time plays a critical role in how information is generated, retrieved, and interpreted. In this survey, we provide a comprehensive overview of Temporal Information Retrieval and Temporal Question Answering, two research areas aimed at handling and understanding time-sensitive information. As the amount of time-stamped content from sources like news articles, web archives, and knowledge bases increases, systems must address challenges such as detecting temporal intent, normalizing time expressions, ordering events, and reasoning over evolving or ambiguous facts. These challenges are critical across many dynamic and time-sensitive domains, from news and encyclopedias to science, history, and social media. We review both traditional approaches and modern neural methods, including those that use transformer models and Large Language Models (LLMs). We also review recent advances in temporal language modeling, multi-hop reasoning, and retrieval-augmented generation (RAG), alongside benchmark datasets and evaluation strategies that test temporal robustness, recency awareness, and generalization.",,"Bhawna Piryani, Abdelrahman Abdallah, Jamshid Mozafari, Avishek Anand, Adam Jatowt",2025-05-31T09:34:42Z,It's High Time: A Survey of Temporal Information Retrieval and Question   Answering,Es ist höchste Zeit: Eine Umfrage der zeitlichen Informationen Retrieval und Fragen beantworten,《高时:时间信息检索和回答问题调查》,http://arxiv.org/abs/2505.20243v2
1181,"Multi-hop questions still stump large language models (LLMs), which struggle to link information across multiple reasoning steps. We introduce Auto-Patch, a novel method that dynamically patches hidden states during inference to enhance multi-hop reasoning in LLMs. Building on the PatchScopes framework, Auto-Patch selectively modifies internal representations using a learned classifier. Evaluated on the MuSiQue dataset, Auto-Patch improves the solve rate from 18.45\% (baseline) to 23.63~$\pm$~0.7\% (3 runs), narrowing the gap to Chain-of-Thought prompting (27.44\%). Our results highlight the potential of dynamic hidden state interventions for advancing complex reasoning in LLMs.",,"Aviv Jan, Dean Tahory, Omer Talmi, Omar Abo Mokh",2025-05-31T09:30:59Z,Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models,Auto-Patching: Multi-Hop-Reasoning in Sprachmodellen verbessern,自动配配:加强语文模式中的多功能理由,http://arxiv.org/abs/2506.00483v1
1182,"As large language models (LLMs) continue to advance, the need for up-to-date and well-organized benchmarks becomes increasingly critical. However, many existing datasets are scattered, difficult to manage, and make it challenging to perform evaluations tailored to specific needs or domains, despite the growing importance of domain-specific models in areas such as math or code. In this paper, we introduce BenchHub, a dynamic benchmark repository that empowers researchers and developers to evaluate LLMs more effectively. BenchHub aggregates and automatically classifies benchmark datasets from diverse domains, integrating 303K questions across 38 benchmarks. It is designed to support continuous updates and scalable data management, enabling flexible and customizable evaluation tailored to various domains or use cases. Through extensive experiments with various LLM families, we demonstrate that model performance varies significantly across domain-specific subsets, emphasizing the importance of domain-aware benchmarking. We believe BenchHub can encourage better dataset reuse, more transparent model comparisons, and easier identification of underrepresented areas in existing benchmarks, offering a critical infrastructure for advancing LLM evaluation research.",,"Eunsu Kim, Haneul Yoo, Guijin Son, Hitesh Patel, Amit Agarwal, Alice Oh",2025-05-31T09:24:32Z,BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM   Evaluation,BenchHub: Eine einheitliche Benchmark-Suite für die ganzheitliche und anpassbare LLM-Bewertung,Bench Hub:综合和可定制的LLM评价统一基准套件,http://arxiv.org/abs/2506.00482v1
1183,"Visual persuasion, which uses visual elements to influence cognition and behaviors, is crucial in fields such as advertising and political communication. With recent advancements in artificial intelligence, there is growing potential to develop persuasive systems that automatically generate persuasive images tailored to individuals. However, a significant bottleneck in this area is the lack of comprehensive datasets that connect the persuasiveness of images with the personal information about those who evaluated the images. To address this gap and facilitate technological advancements in personalized visual persuasion, we release the Personalized Visual Persuasion (PVP) dataset, comprising 28,454 persuasive images across 596 messages and 9 persuasion strategies. Importantly, the PVP dataset provides persuasiveness scores of images evaluated by 2,521 human annotators, along with their demographic and psychological characteristics (personality traits and values). We demonstrate the utility of our dataset by developing a persuasive image generator and an automated evaluator, and establish benchmark baselines. Our experiments reveal that incorporating psychological characteristics enhances the generation and evaluation of persuasive images, providing valuable insights for personalized visual persuasion.",,"Junseo Kim, Jongwook Han, Dongmin Choi, Jongwook Yoon, Eun-Ju Lee, Yohan Jo",2025-05-31T09:21:57Z,"PVP: An Image Dataset for Personalized Visual Persuasion with Persuasion   Strategies, Viewer Characteristics, and Persuasiveness Ratings","PVP: Ein Bilddatensatz für personalisierte visuelle Überzeugung mit Überzeugungsstrategien, Viewer-Eigenschaften und Persuasiveness-Bewertungen","PVP:一个图像数据集,用于用预测战略、查看器特征和概率评级进行个性化视觉分析",http://arxiv.org/abs/2506.00481v1
1184,"Alignment techniques have become central to ensuring that Large Language Models (LLMs) generate outputs consistent with human values. However, existing alignment paradigms often model an averaged or monolithic preference, failing to account for the diversity of perspectives across cultures, demographics, and communities. This limitation is particularly critical in health-related scenarios, where plurality is essential due to the influence of culture, religion, personal values, and conflicting opinions. Despite progress in pluralistic alignment, no prior work has focused on health, likely due to the unavailability of publicly available datasets. To address this gap, we introduce VITAL, a new benchmark dataset comprising 13.1K value-laden situations and 5.4K multiple-choice questions focused on health, designed to assess and benchmark pluralistic alignment methodologies. Through extensive evaluation of eight LLMs of varying sizes, we demonstrate that existing pluralistic alignment techniques fall short in effectively accommodating diverse healthcare beliefs, underscoring the need for tailored AI alignment in specific domains. This work highlights the limitations of current approaches and lays the groundwork for developing health-specific alignment solutions.",,"Anudeex Shetty, Amin Beheshti, Mark Dras, Usman Naseem",2025-05-31T09:13:38Z,VITAL: A New Dataset for Benchmarking Pluralistic Alignment in   Healthcare,VITAL: Ein neuer Datensatz zur Benchmarking pluralistischer Ausrichtung im Gesundheitswesen,VITAL:保健领域多元化协调基准衡量新数据集,http://arxiv.org/abs/2502.13775v2
1185,"Empathetic response generation necessitates the integration of emotional and intentional dynamics to foster meaningful interactions. Existing research either neglects the intricate interplay between emotion and intent, leading to suboptimal controllability of empathy, or resorts to large language models (LLMs), which incur significant computational overhead. In this paper, we introduce ReflectDiffu, a lightweight and comprehensive framework for empathetic response generation. This framework incorporates emotion contagion to augment emotional expressiveness and employs an emotion-reasoning mask to pinpoint critical emotional elements. Additionally, it integrates intent mimicry within reinforcement learning for refinement during diffusion. By harnessing an intent twice reflect mechanism of Exploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional decision-making into precise intent actions, thereby addressing empathetic response misalignments stemming from emotional misrecognition. Through reflection, the framework maps emotional states to intents, markedly enhancing both response empathy and flexibility. Comprehensive experiments reveal that ReflectDiffu outperforms existing models regarding relevance, controllability, and informativeness, achieving state-of-the-art results in both automatic and human evaluations.",,"Jiahao Yuan, Zixiang Di, Zhiqing Cui, Guisong Yang, Usman Naseem",2025-05-31T09:12:51Z,ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for   Empathetic Response Generation via a RL-Diffusion Framework,ReflectDiffu: Reflect zwischen emotional-intent Ansteckung und Mimicry für Empathetic Response Generation über ein RL-Diffusion Framework,"反省:通过RL-扩散框架,对情感-情感内聚变和Mmimimicry之间的反射,以便产生同情性反应",http://arxiv.org/abs/2409.10289v4
1186,"Large Vision-Language Models (LVLMs) have achieved remarkable success, yet their significant computational demands hinder practical deployment. While efforts to improve LVLM efficiency are growing, existing methods lack comprehensive evaluation across diverse backbones, benchmarks, and metrics. In this work, we systematically evaluate mainstream acceleration techniques for LVLMs, categorized into token and parameter compression. We introduce EffiVLM-Bench, a unified framework for assessing not only absolute performance but also generalization and loyalty, while exploring Pareto-optimal trade-offs. Our extensive experiments and in-depth analyses offer insights into optimal strategies for accelerating LVLMs. We open-source code and recipes for EffiVLM-Bench to foster future research.",,"Zekun Wang, Minghua Ma, Zexin Wang, Rongchuan Mu, Liping Shan, Ming Liu, Bing Qin",2025-05-31T09:10:43Z,EffiVLM-BENCH: A Comprehensive Benchmark for Evaluating Training-Free   Acceleration in Large Vision-Language Models,EffiVLM-BENCH: Ein umfassender Benchmark für die Bewertung von Trainings-freien Beschleunigungen in großen Vision-Sprachen-Modellen,EffiVLM-BENCH:评价大型愿景-语言模型无培训加速的综合基准,http://arxiv.org/abs/2506.00479v1
1187,"Multiword expressions (MWEs) refer to idiomatic sequences of multiple words. MWE identification, i.e., detecting MWEs in text, can play a key role in downstream tasks such as machine translation, but existing datasets for the task are inconsistently annotated, limited to a single type of MWE, or limited in size. To enable reliable and comprehensive evaluation, we created CoAM: Corpus of All-Type Multiword Expressions, a dataset of 1.3K sentences constructed through a multi-step process to enhance data quality consisting of human annotation, human review, and automated consistency checking. Additionally, for the first time in a dataset of MWE identification, CoAM's MWEs are tagged with MWE types, such as Noun and Verb, enabling fine-grained error analysis. Annotations for CoAM were collected using a new interface created with our interface generator, which allows easy and flexible annotation of MWEs in any form. Through experiments using CoAM, we find that a fine-tuned large language model outperforms MWEasWSD, which achieved the state-of-the-art performance on the DiMSUM dataset. Furthermore, analysis using our MWE type tagged data reveals that Verb MWEs are easier than Noun MWEs to identify across approaches.",,"Yusuke Ide, Joshua Tanner, Adam Nohejl, Jacob Hoffman, Justin Vasselli, Hidetaka Kamigaito, Taro Watanabe",2025-05-31T09:09:06Z,CoAM: Corpus of All-Type Multiword Expressions,CoAM: Corpus von Multiwort-Ausdrücken aller Art,CoAM: 全类型多字表达式组合体,http://arxiv.org/abs/2412.18151v2
1188,"Predicting personality traits automatically has become a challenging problem in computer vision. This paper introduces an innovative multimodal feature learning framework for personality analysis in short video clips. For visual processing, we construct a facial graph and design a Geo-based two-stream network incorporating an attention mechanism, leveraging both Graph Convolutional Networks (GCN) and Convolutional Neural Networks (CNN) to capture static facial expressions. Additionally, ResNet18 and VGGFace networks are employed to extract global scene and facial appearance features at the frame level. To capture dynamic temporal information, we integrate a BiGRU with a temporal attention module for extracting salient frame representations. To enhance the model's robustness, we incorporate the VGGish CNN for audio-based features and XLM-Roberta for text-based features. Finally, a multimodal channel attention mechanism is introduced to integrate different modalities, and a Multi-Layer Perceptron (MLP) regression model is used to predict personality traits. Experimental results confirm that our proposed framework surpasses existing state-of-the-art approaches in performance.",,"Kangsheng Wang, Chengwei Ye, Huanzhen Zhang, Linuo Xu, Shuyan Liu",2025-05-31T09:09:03Z,Graph-Driven Multimodal Feature Learning Framework for Apparent   Personality Assessment,Graph-Driven Multimodal Feature Learning Framework für die scheinbare Persönlichkeitsbewertung,图图 - 发展图 - 图 - 图 - 图 - 图 - 图 - 图 - 图 - 图 - 图 - 图 - 图 - 图 - 图 - 图 - 图 - 图 - 图,http://arxiv.org/abs/2504.11515v2
1189,"Multimodal Continual Instruction Tuning (MCIT) aims to finetune Multimodal Large Language Models (MLLMs) to continually align with human intent across sequential tasks. Existing approaches often rely on the Mixture-of-Experts (MoE) LoRA framework to preserve previous instruction alignments. However, these methods are prone to Catastrophic Forgetting (CF), as they aggregate all LoRA blocks via simple summation, which compromises performance over time. In this paper, we identify a critical parameter inefficiency in the MoELoRA framework within the MCIT context. Based on this insight, we propose BranchLoRA, an asymmetric framework to enhance both efficiency and performance. To mitigate CF, we introduce a flexible tuning-freezing mechanism within BranchLoRA, enabling branches to specialize in intra-task knowledge while fostering inter-task collaboration. Moreover, we incrementally incorporate task-specific routers to ensure an optimal branch distribution over time, rather than favoring the most recent task. To streamline inference, we introduce a task selector that automatically routes test inputs to the appropriate router without requiring task identity. Extensive experiments on the latest MCIT benchmark demonstrate that BranchLoRA significantly outperforms MoELoRA and maintains its superiority across various MLLM sizes.",,"Duzhen Zhang, Yong Ren, Zhong-Zhi Li, Yahan Yu, Jiahua Dong, Chenxing Li, Zhilong Ji, Jinfeng Bai",2025-05-31T09:02:38Z,Enhancing Multimodal Continual Instruction Tuning with BranchLoRA,Verbesserung der multimodalen kontinuierlichen Instruktionssteuerung mit BranchLoRA,与分局LORA加强多模式连续教学,http://arxiv.org/abs/2506.02041v1
1190,"Retrieval-augmented generation (RAG) encounters challenges when addressing complex queries, particularly multi-hop questions. While several methods tackle multi-hop queries by iteratively generating internal queries and retrieving external documents, these approaches are computationally expensive. In this paper, we identify a three-stage information processing pattern in LLMs during layer-by-layer reasoning, consisting of extraction, processing, and subsequent extraction steps. This observation suggests that the representations in intermediate layers contain richer information compared to those in other layers. Building on this insight, we propose Layer-wise RAG (L-RAG). Unlike prior methods that focus on generating new internal queries, L-RAG leverages intermediate representations from the middle layers, which capture next-hop information, to retrieve external knowledge. L-RAG achieves performance comparable to multi-step approaches while maintaining inference overhead similar to that of standard RAG. Experimental results show that L-RAG outperforms existing RAG methods on open-domain multi-hop question-answering datasets, including MuSiQue, HotpotQA, and 2WikiMultiHopQA. The code is available in https://anonymous.4open.science/r/L-RAG-ADD5/",,"Jiaen Lin, Jingyu Liu, Yingbo Liu",2025-05-31T09:02:25Z,Optimizing Multi-Hop Document Retrieval Through Intermediate   Representations,Multi-Hop-Dokument-Retrieval durch Zwischendarstellungen optimieren,通过中间代表方式优化多功能文档检索优化,http://arxiv.org/abs/2503.04796v2
1191,"This paper investigates a critical design decision in the practice of massively multilingual continual pre-training -- the inclusion of parallel data. Specifically, we study the impact of bilingual translation data for massively multilingual language adaptation of the Llama3 family of models to 500 languages. To this end, we construct the MaLA bilingual translation corpus, containing data from more than 2,500 language pairs. Subsequently, we develop the EMMA-500 Llama 3 suite of four massively multilingual models -- continually pre-trained from the Llama 3 family of base models extensively on diverse data mixes up to 671B tokens -- and explore the effect of continual pre-training with or without bilingual translation data. Comprehensive evaluation across 7 tasks and 12 benchmarks demonstrates that bilingual data tends to enhance language transfer and performance, particularly for low-resource languages. We open-source the MaLA corpus, EMMA-500 Llama 3 suite artefacts, code, and model generations.",,"Shaoxiong Ji, Zihao Li, Jaakko Paavola, Indraneil Paul, Hengyu Luo, Jörg Tiedemann",2025-05-31T08:37:17Z,Massively Multilingual Adaptation of Large Language Models Using   Bilingual Translation Data,Massive mehrsprachige Anpassung von großen Sprachmodellen mit zweisprachigen Übersetzungsdaten,利用双语翻译数据大规模多语种多语种应用大语言模型,http://arxiv.org/abs/2506.00469v1
1192,"Recent advances in audio generation led to an increasing number of deepfakes, making the general public more vulnerable to financial scams, identity theft, and misinformation. Audio deepfake detectors promise to alleviate this issue, with many recent studies reporting accuracy rates close to 99%. However, these methods are typically tested in an in-domain setup, where the deepfake samples from the training and test sets are produced by the same generative models. To this end, we introduce XMAD-Bench, a large-scale cross-domain multilingual audio deepfake benchmark comprising 668.8 hours of real and deepfake speech. In our novel dataset, the speakers, the generative methods, and the real audio sources are distinct across training and test splits. This leads to a challenging cross-domain evaluation setup, where audio deepfake detectors can be tested ``in the wild''. Our in-domain and cross-domain experiments indicate a clear disparity between the in-domain performance of deepfake detectors, which is usually as high as 100%, and the cross-domain performance of the same models, which is sometimes similar to random chance. Our benchmark highlights the need for the development of robust audio deepfake detectors, which maintain their generalization capacity across different languages, speakers, generative methods, and data sources. Our benchmark is publicly released at https://github.com/ristea/xmad-bench/.",,"Ioan-Paul Ciobanu, Andrei-Iulian Hiji, Nicolae-Catalin Ristea, Paul Irofti, Cristian Rusu, Radu Tudor Ionescu",2025-05-31T08:28:36Z,XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark,XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark,XMAD-Bench:跨主题多语种音频深藏基准,http://arxiv.org/abs/2506.00462v1
1193,"Speech translation for Indian languages remains a challenging task due to the scarcity of large-scale, publicly available datasets that capture the linguistic diversity and domain coverage essential for real-world applications. Existing datasets cover a fraction of Indian languages and lack the breadth needed to train robust models that generalize beyond curated benchmarks. To bridge this gap, we introduce BhasaAnuvaad, the largest speech translation dataset for Indian languages, spanning over 44 thousand hours of audio and 17 million aligned text segments across 14 Indian languages and English. Our dataset is built through a threefold methodology: (a) aggregating high-quality existing sources, (b) large-scale web crawling to ensure linguistic and domain diversity, and (c) creating synthetic data to model real-world speech disfluencies. Leveraging BhasaAnuvaad, we train IndicSeamless, a state-of-the-art speech translation model for Indian languages that performs better than existing models. Our experiments demonstrate improvements in the translation quality, setting a new standard for Indian language speech translation. We will release all the code, data and model weights in the open-source, with permissive licenses to promote accessibility and collaboration.",,"Ashwin Sankar, Sparsh Jain, Nikhil Narasimhan, Devilal Choudhary, Dhairya Suman, Mohammed Safi Ur Rahman Khan, Anoop Kunchukuttan, Mitesh M Khapra, Raj Dabre",2025-05-31T08:18:52Z,Towards Building Large Scale Datasets and State-of-the-Art Automatic   Speech Translation Systems for 14 Indian Languages,Auf dem Weg zum Aufbau von Großdatensätzen und modernen automatischen Sprachübersetzungssystemen für 14 indische Sprachen,努力为14种印度语言建立大规模数据集和最新最先进的自动语音翻译系统,http://arxiv.org/abs/2411.04699v3
1194,"Embeddings-as-a-Service (EaaS) is a service offered by large language model (LLM) developers to supply embeddings generated by LLMs. Previous research suggests that EaaS is prone to imitation attacks -- attacks that clone the underlying EaaS model by training another model on the queried embeddings. As a result, EaaS watermarks are introduced to protect the intellectual property of EaaS providers. In this paper, we first show that existing EaaS watermarks can be removed by paraphrasing when attackers clone the model. Subsequently, we propose a novel watermarking technique that involves linearly transforming the embeddings, and show that it is empirically and theoretically robust against paraphrasing.",,"Anudeex Shetty, Qiongkai Xu, Jey Han Lau",2025-05-31T08:16:14Z,WET: Overcoming Paraphrasing Vulnerabilities in Embeddings-as-a-Service   with Linear Transformation Watermarks,WET: Paraphrasierung von Schwachstellen in der Einbettung-as-a-Service mit linearen Transformations-Wasserzeichen überwinden,WET: 克服以线性转化水印嵌入服务中的参数脆弱性,http://arxiv.org/abs/2409.04459v2
1195,"Hallucinations in large language models (LLMs) during summarization of patient-clinician dialogues pose significant risks to patient care and clinical decision-making. However, the phenomenon remains understudied in the clinical domain, with uncertainty surrounding the applicability of general-domain hallucination detectors. The rarity and randomness of hallucinations further complicate their investigation. In this paper, we conduct an evaluation of hallucination detection methods in the medical domain, and construct two datasets for the purpose: A fact-controlled Leave-N-out dataset -- generated by systematically removing facts from source dialogues to induce hallucinated content in summaries; and a natural hallucination dataset -- arising organically during LLM-based medical summarization. We show that general-domain detectors struggle to detect clinical hallucinations, and that performance on fact-controlled hallucinations does not reliably predict effectiveness on natural hallucinations. We then develop fact-based approaches that count hallucinations, offering explainability not available with existing methods. Notably, our LLM-based detectors, which we developed using fact-controlled hallucinations, generalize well to detecting real-world clinical hallucinations. This research contributes a suite of specialized metrics supported by expert-annotated datasets to advance faithful clinical summarization systems.",,"Suhas BN, Han-Chin Shing, Lei Xu, Mitch Strong, Jon Burnsky, Jessica Ofor, Jordan R. Mason, Susan Chen, Sundararajan Srinivasan, Chaitanya Shivade, Jack Moriarty, Joseph Paul Cohen",2025-05-31T08:04:37Z,Fact-Controlled Diagnosis of Hallucinations in Medical Text   Summarization,Fact-Controlled Diagnose von Halluzinationen in medizinischen Text Zusammenfassung,医学文本摘要中的幻觉事实控制诊断,http://arxiv.org/abs/2506.00448v1
1196,"Forecasting over Temporal Knowledge Graphs (TKGs) which predicts future facts based on historical ones has received much attention. Recent studies have introduced Large Language Models (LLMs) for this task to enhance the models' generalization abilities. However, these models perform forecasting via simultaneously learning two kinds of entangled knowledge in the TKG: (1) general patterns, i.e., invariant temporal structures shared across different scenarios; and (2) scenario information, i.e., factual knowledge engaged in specific scenario, such as entities and relations. As a result, the learning processes of these two kinds of knowledge may interfere with each other, which potentially impact the generalization abilities of the models. To enhance the generalization ability of LLMs on this task, in this paper, we propose a General-to-Specific learning framework (G2S) that disentangles the learning processes of the above two kinds of knowledge. In the general learning stage, we mask the scenario information in different TKGs and convert it into anonymous temporal structures. After training on these structures, the model is able to capture the general patterns across different TKGs. In the specific learning stage, we inject the scenario information into the structures via either in-context learning or fine-tuning modes. Experimental results show that G2S effectively improves the generalization abilities of LLMs.",,"Long Bai, Zixuan Li, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng, Tat-Seng Chua",2025-05-31T07:57:19Z,G2S: A General-to-Specific Learning Framework for Temporal Knowledge   Graph Forecasting with Large Language Models,G2S: Ein allgemein-spezifischer Lernrahmen für die zeitliche Wissensprognose mit großen Sprachmodellen,G2S:用大语言模型预测时际知识图预测的一般到具体学习框架,http://arxiv.org/abs/2506.00445v1
1197,"Test-time computing approaches, which leverage additional computational resources during inference, have been proven effective in enhancing large language model performance. This work introduces a novel, linearly scaling approach, TestNUC, that improves test-time predictions by leveraging the local consistency of neighboring unlabeled data-it classifies an input instance by considering not only the model's prediction on that instance but also on neighboring unlabeled instances. We evaluate TestNUC across eight diverse datasets, spanning intent classification, topic mining, domain discovery, and emotion detection, demonstrating its consistent superiority over baseline methods such as standard prompting and self-consistency. Furthermore, TestNUC can be seamlessly integrated with existing test-time computing approaches, substantially boosting their performance. Our analysis reveals that TestNUC scales effectively with increasing amounts of unlabeled data and performs robustly across different embedding models, making it practical for real-world applications. Our code is available at https://github.com/HenryPengZou/TestNUC.",,"Henry Peng Zou, Zhengyao Gu, Yue Zhou, Yankai Chen, Weizhi Zhang, Liancheng Fang, Yibo Wang, Yangning Li, Kay Liu, Philip S. Yu",2025-05-31T07:53:48Z,TestNUC: Enhancing Test-Time Computing Approaches and Scaling through   Neighboring Unlabeled Data Consistency,TestNUC: Verbesserung von Test-Time Computing-Ansätzen und Skalierung durch benachbarte unmarkierte Datenkonsistenz,"测试NUC:通过邻接无标签数据一致性,加强试验时间计算机计算方法,并推广",http://arxiv.org/abs/2502.19163v2
1198,"The increasing number of academic papers poses significant challenges for researchers to efficiently acquire key details. While retrieval augmented generation (RAG) shows great promise in large language model (LLM) based automated question answering, previous works often isolate neural and symbolic retrieval despite their complementary strengths. Moreover, conventional single-view chunking neglects the rich structure and layout of PDFs, e.g., sections and tables. In this work, we propose NeuSym-RAG, a hybrid neural symbolic retrieval framework which combines both paradigms in an interactive process. By leveraging multi-view chunking and schema-based parsing, NeuSym-RAG organizes semi-structured PDF content into both the relational database and vectorstore, enabling LLM agents to iteratively gather context until sufficient to generate answers. Experiments on three full PDF-based QA datasets, including a self-annotated one AIRQA-REAL, show that NeuSym-RAG stably defeats both the vector-based RAG and various structured baselines, highlighting its capacity to unify both retrieval schemes and utilize multiple views. Code and data are publicly available at https://github.com/X-LANCE/NeuSym-RAG.",,"Ruisheng Cao, Hanchong Zhang, Tiancheng Huang, Zhangyi Kang, Yuxin Zhang, Liangtai Sun, Hanqi Li, Yuxun Miao, Shuai Fan, Lu Chen, Kai Yu",2025-05-31T07:51:54Z,NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring   for PDF Question Answering,NeuSym-RAG: Hybrides neurales Symbolisches Retrieval mit Multiview-Strukturierung für PDF-Fragebeantwortung,NeuSym-RAG: PDF 问题解答混合神经符号回收与多视图结构结构,http://arxiv.org/abs/2505.19754v2
1199,"Counterspeech has proven to be a powerful tool to combat hate speech online. Previous studies have focused on generating counterspeech conditioned only on specific intents (single attributed). However, a holistic approach considering multiple attributes simultaneously can yield more nuanced and effective responses. Here, we introduce HiPPrO, Hierarchical Prefix learning with Preference Optimization, a novel two-stage framework that utilizes the effectiveness of attribute-specific prefix embedding spaces hierarchically optimized during the counterspeech generation process in the first phase. Thereafter, we incorporate both reference and reward-free preference optimization to generate more constructive counterspeech. Furthermore, we extend IntentCONANv2 by annotating all 13,973 counterspeech instances with emotion labels by five annotators. HiPPrO leverages hierarchical prefix optimization to integrate these dual attributes effectively. An extensive evaluation demonstrates that HiPPrO achieves a ~38 % improvement in intent conformity and a ~3 %, ~2 %, ~3 % improvement in Rouge-1, Rouge-2, and Rouge-L, respectively, compared to several baseline models. Human evaluations further substantiate the superiority of our approach, highlighting the enhanced relevance and appropriateness of the generated counterspeech. This work underscores the potential of multi-attribute conditioning in advancing the efficacy of counterspeech generation systems. Our code is available on Github and dataset is open-sourced on Hugging-face.",,"Aswini Kumar, Anil Bandhakavi, Tanmoy Chakraborty",2025-05-31T07:50:41Z,Counterspeech the ultimate shield! Multi-Conditioned Counterspeech   Generation through Attributed Prefix Learning,Counterspeech der ultimative Schild! Multi-conditioned Counterspeech Generation durch Attributed Prefix Learning,"对抗终极屏蔽 ! 通过特定前缀学习, 多功能化的反声音一代 。",http://arxiv.org/abs/2505.11958v3
1200,"Recent advancements in table-based reasoning have expanded beyond factoid-level QA to address insight-level tasks, where systems should synthesize implicit knowledge in the table to provide explainable analyses. Although effective, existing studies remain confined to scenarios where a single gold table is given alongside the user query, failing to address cases where users seek comprehensive insights from multiple unknown tables. To bridge these gaps, we propose MT-RAIG Bench, design to evaluate systems on Retrieval-Augmented Insight Generation over Mulitple-Tables. Additionally, to tackle the suboptimality of existing automatic evaluation methods in the table domain, we further introduce a fine-grained evaluation framework MT-RAIG Eval, which achieves better alignment with human quality judgments on the generated insights. We conduct extensive experiments and reveal that even frontier LLMs still struggle with complex multi-table reasoning, establishing our MT-RAIG Bench as a challenging testbed for future research.",,"Kwangwook Seo, Donguk Kwon, Dongha Lee",2025-05-31T07:47:26Z,MT-RAIG: Novel Benchmark and Evaluation Framework for   Retrieval-Augmented Insight Generation over Multiple Tables,MT-RAIG: Neuer Benchmark- und Evaluationsrahmen für die neu entwickelte Insight-Generierung über mehrere Tabellen,MT-RAIG:多张表格检索增强的视觉一代的新基准和评估框架,http://arxiv.org/abs/2502.11735v3
1201,"Mental health is increasingly critical in contemporary healthcare, with psychotherapy demanding dynamic, context-sensitive interactions that traditional NLP methods struggle to capture. Large Language Models (LLMs) offer significant potential for addressing this gap due to their ability to handle extensive context and multi-turn reasoning. This review introduces a conceptual taxonomy dividing psychotherapy into interconnected stages--assessment, diagnosis, and treatment--to systematically examine LLM advancements and challenges. Our comprehensive analysis reveals imbalances in current research, such as a focus on common disorders, linguistic biases, fragmented methods, and limited theoretical integration. We identify critical challenges including capturing dynamic symptom fluctuations, overcoming linguistic and cultural biases, and ensuring diagnostic reliability. Highlighting future directions, we advocate for continuous multi-stage modeling, real-time adaptive systems grounded in psychological theory, and diversified research covering broader mental disorders and therapeutic approaches, aiming toward more holistic and clinically integrated psychotherapy LLMs systems.",,"Hongbin Na, Yining Hua, Zimu Wang, Tao Shen, Beibei Yu, Lilin Wang, Wei Wang, John Torous, Ling Chen",2025-05-31T07:40:23Z,A Survey of Large Language Models in Psychotherapy: Current Landscape   and Future Directions,Eine Umfrage zu großen Sprachmodellen in der Psychotherapie: Aktuelle Landschaft und zukünftige Richtungen,心理治疗中大语言模式调查:当前景观和未来方向,http://arxiv.org/abs/2502.11095v2
1202,"Multi-answer question answering (QA), where questions can have many valid answers, presents a significant challenge for existing retrieval-augmented generation-based QA systems, as these systems struggle to retrieve and then synthesize a large number of evidence passages. To tackle these challenges, we propose a new multi-answer QA framework -- Retrieval-augmented Independent Reading with Inter-passage Verification (RI$^2$VER). Our framework retrieves a large set of passages and processes each passage individually to generate an initial high-recall but noisy answer set. Then we propose a new inter-passage verification pipeline that validates every candidate answer through (1) Verification Question Generation, (2) Gathering Additional Evidence, and (3) Verification with inter-passage synthesis. Evaluations on the QAMPARI and RoMQA datasets demonstrate that our framework significantly outperforms existing baselines across various model sizes, achieving an average F1 score improvement of 11.17%. Further analysis validates that our inter-passage verification pipeline enables our framework to be particularly beneficial for questions requiring multi-evidence synthesis.",,"Bingsen Chen, Shengjie Wang, Xi Ye, Chen Zhao",2025-05-31T07:03:52Z,Inter-Passage Verification for Multi-evidence Multi-answer QA,Interpassage-Überprüfung für Multi-Evidence-Multi-Antwort QA,多证据、多证据、多答、多答、多答、多方核查,http://arxiv.org/abs/2506.00425v1
1203,"Diffusion Language Models (DLMs) have emerged as a promising new paradigm for text generative modeling, potentially addressing limitations of autoregressive (AR) models. However, current DLMs have been studied at a smaller scale compared to their AR counterparts and lack fair comparison on language modeling benchmarks. Additionally, training diffusion models from scratch at scale remains challenging. Given the prevalence of open-source AR language models, we propose adapting these models to build text diffusion models. We demonstrate connections between AR and diffusion modeling objectives and introduce a simple continual pre-training approach for training diffusion models. Through systematic evaluation on language modeling, reasoning, and commonsense benchmarks, we show that we can convert AR models ranging from 127M to 7B parameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA, using less than 200B tokens for training. Our experimental results reveal that these models outperform earlier DLMs and are competitive with their AR counterparts. We release a suite of DLMs (127M-355M-7B) capable of generating fluent text, performing in-context learning, filling in the middle without prompt re-ordering, and following instructions https://github.com/HKUNLP/DiffuLLaMA.",,"Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, Hao Peng, Lingpeng Kong",2025-05-31T07:01:57Z,Scaling Diffusion Language Models via Adaptation from Autoregressive   Models,Skalierung von Diffusions-Sprachmodellen durch Anpassung an autoregressive Modelle,"通过自动递减模型的适应, 扩展传播语言模型",http://arxiv.org/abs/2410.17891v3
1204,"Advances in large language models (LLMs) have spurred research into enhancing their reasoning capabilities, particularly in math-rich STEM (Science, Technology, Engineering, and Mathematics) documents. While LLMs can generate equations or solve math-related queries, their ability to fully understand and interpret abstract mathematical symbols in long, math-rich documents remains limited. In this paper, we introduce STEM-PoM, a comprehensive benchmark dataset designed to evaluate LLMs' reasoning abilities on math symbols within contextual scientific text. The dataset, sourced from real-world ArXiv documents, contains over 2K math symbols classified as main attributes of variables, constants, operators, and unit descriptors, with additional sub-attributes including scalar/vector/matrix for variables and local/global/discipline-specific labels for both constants and operators. Our extensive experiments demonstrate that state-of-the-art LLMs achieve an average accuracy of 20-60% under in-context learning and 50-60% with fine-tuning, highlighting a substantial gap in their ability to classify mathematical symbols. By improving LLMs' mathematical symbol classification, STEM-PoM further enhances models' downstream mathematical reasoning capabilities. The code and data are available at https://github.com/jiaruzouu/STEM-PoM.",,"Jiaru Zou, Qing Wang, Pratyush Thakur, Nickvash Kani",2025-05-31T06:56:47Z,STEM-POM: Evaluating Language Models Math-Symbol Reasoning in Document   Parsing,STEM-POM: Bewertung von Sprachmodellen Mathe-Symbol-Reasoning in Document Parsing,STEM-POM: 评估文档分析中的语言模型数学类比理由,http://arxiv.org/abs/2411.00387v3
1205,"Contextual biasing (CB) improves automatic speech recognition for rare and unseen phrases. Recent studies have introduced dynamic vocabulary, which represents context phrases as expandable tokens in autoregressive (AR) models. This method improves CB accuracy but with slow inference speed. While dynamic vocabulary can be applied to non-autoregressive (NAR) models, such as connectionist temporal classification (CTC), the conditional independence assumption fails to capture dependencies between static and dynamic tokens. This paper proposes DYNAC (Dynamic Vocabulary-based NAR Contextualization), a self-conditioned CTC method that integrates dynamic vocabulary into intermediate layers. Conditioning the encoder on dynamic vocabulary, DYNAC effectively captures dependencies between static and dynamic tokens while reducing the real-time factor (RTF). Experimental results show that DYNAC reduces RTF by 81% with a 0.1-point degradation in word error rate on the LibriSpeech 960 test-clean set.",,"Yui Sudo, Yosuke Fukumoto, Muhammad Shakeel, Yifan Peng, Chyi-Jiunn Lin, Shinji Watanabe",2025-05-31T06:53:25Z,DYNAC: Dynamic Vocabulary based Non-Autoregressive Contextualization for   Speech Recognition,DYNAC: Dynamisches Vokabeln basiertes Nicht-Autoregressives Kontextualisieren für die Spracherkennung,DYNAC: 语音识别的动态词汇表基础非自动递减背景,http://arxiv.org/abs/2506.00422v1
1206,"As chatbots continue to evolve toward human-like, real-world, interactions, multimodality remains an active area of research and exploration. So far, efforts to integrate multimodality into chatbots have primarily focused on image-centric tasks, such as visual dialogue and image-based instructions, placing emphasis on the ""eyes"" of human perception while neglecting the ""ears"", namely auditory aspects. Moreover, these studies often center around static interactions that focus on discussing the modality rather than naturally incorporating it into the conversation, which limits the richness of simultaneous, dynamic engagement. Furthermore, while multimodality has been explored in multi-party and multi-session conversations, task-specific constraints have hindered its seamless integration into dynamic, natural conversations. To address these challenges, this study aims to equip chatbots with ""eyes and ears"" capable of more immersive interactions with humans. As part of this effort, we introduce a new multimodal conversation dataset, Multimodal Multi-Session Multi-Party Conversation ($M^3C$), and propose a novel multimodal conversation model featuring multimodal memory retrieval. Our model, trained on the $M^3C$, demonstrates the ability to seamlessly engage in long-term conversations with multiple speakers in complex, real-world-like settings, effectively processing visual and auditory inputs to understand and respond appropriately. Human evaluations highlight the model's strong performance in maintaining coherent and dynamic interactions, demonstrating its potential for advanced multimodal conversational agents.",,"Jihyoung Jang, Minwook Bae, Minji Kim, Dilek Hakkani-Tur, Hyounghun Kim",2025-05-31T06:50:51Z,Enabling Chatbots with Eyes and Ears: An Immersive Multimodal   Conversation System for Dynamic Interactions,Chatbots mit Augen und Ohren aktivieren: Ein immersives multimodales Gesprächssystem für dynamische Interaktionen,启用有眼睛和耳的聊天室:动态互动的混合多模式对话系统,http://arxiv.org/abs/2506.00421v1
1207,"In context learning (ICL) relies heavily on high quality demonstrations drawn from large annotated corpora. Existing approaches detect noisy annotations by ranking local perplexities, presuming that noisy samples yield higher perplexities than their clean counterparts. However, this assumption breaks down when the noise ratio is high and many demonstrations are flawed. We reexamine the perplexity based paradigm for text generation under noisy annotations, highlighting two sources of bias in perplexity: the annotation itself and the domain specific knowledge inherent in large language models (LLMs). To overcome these biases, we introduce a dual debiasing framework that uses synthesized neighbors to explicitly correct perplexity estimates, yielding a robust Sample Cleanliness Score. This metric uncovers absolute sample cleanliness regardless of the overall corpus noise level. Extensive experiments demonstrate our method's superior noise detection capabilities and show that its final ICL performance is comparable to that of a fully clean demonstration corpus. Moreover, our approach remains robust even when noise ratios are extremely high.",,"Siqi Liang, Sumyeong Ahn, Paramveer S. Dhillon, Jiayu Zhou",2025-05-31T06:44:48Z,Dual Debiasing for Noisy In-Context Learning for Text Generation,Dual Debiasing für lautes In-Context-Lernen für Textgenerierung,为产生文本进行有噪音的内文学习双向偏差,http://arxiv.org/abs/2506.00418v1
1208,"As large language models (LLMs) evolve, their ability to deliver personalized and context-aware responses offers transformative potential for improving user experiences. Existing personalization approaches, however, often rely solely on user history to augment the prompt, limiting their effectiveness in generating tailored outputs, especially in cold-start scenarios with sparse data. To address these limitations, we propose Personalized Graph-based Retrieval-Augmented Generation (PGraphRAG), a framework that leverages user-centric knowledge graphs to enrich personalization. By directly integrating structured user knowledge into the retrieval process and augmenting prompts with user-relevant context, PGraphRAG enhances contextual understanding and output quality. We also introduce the Personalized Graph-based Benchmark for Text Generation, designed to evaluate personalized text generation tasks in real-world settings where user history is sparse or unavailable. Experimental results show that PGraphRAG significantly outperforms state-of-the-art personalization methods across diverse tasks, demonstrating the unique advantages of graph-based retrieval for personalization.",,"Steven Au, Cameron J. Dimacali, Ojasmitha Pedirappagari, Namyong Park, Franck Dernoncourt, Yu Wang, Nikos Kanakaris, Hanieh Deilamsalehy, Ryan A. Rossi, Nesreen K. Ahmed",2025-05-31T06:33:39Z,Personalized Graph-Based Retrieval for Large Language Models,Personalisiertes Graph-basiertes Retrieval für große Sprachmodelle,用于大语言模型的以图为基础的个人化图图检索,http://arxiv.org/abs/2501.02157v2
1209,"Simultaneous machine translation (SMT) takes streaming input utterances and incrementally produces target text. Existing SMT methods mainly use the partial utterance that has already arrived at the input and the generated hypothesis. Motivated by human interpreters' technique to forecast future words before hearing them, we propose $\textbf{T}$ranslation by $\textbf{A}$nticipating $\textbf{F}$uture (TAF), a method to improve translation quality while retraining low latency. Its core idea is to use a large language model (LLM) to predict future source words and opportunistically translate without introducing too much risk. We evaluate our TAF and multiple baselines of SMT on four language directions. Experiments show that TAF achieves the best translation quality-latency trade-off and outperforms the baselines by up to 5 BLEU points at the same latency (three words). Code is released at https://github.com/owaski/TAF",,"Siqi Ouyang, Oleksii Hrinchuk, Zhehuai Chen, Vitaly Lavrukhin, Jagadeesh Balam, Lei Li, Boris Ginsburg",2025-05-31T06:32:52Z,Anticipating Future with Large Language Model for Simultaneous Machine   Translation,Zukunft mit großem Sprachmodell für simultane maschinelle Übersetzung antizipieren,"预测未来,同时用大语言模式进行同声机翻译",http://arxiv.org/abs/2410.22499v2
1210,"Transformers often struggle to generalize to longer sequences than those seen during training, a limitation known as length extrapolation. Most existing Relative Positional Encoding (RPE) methods attempt to address this by introducing either fixed linear biases or globally learned biases, which lack the capacity to adapt to different input contexts. In this work, we propose an additive RPE, Context-Aware Biases for Length Extrapolation (CABLE), a method that learns token-specific, context-aware biases for each attention head in transformers. By dynamically adjusting positional biases based on the input sequence, CABLE overcomes the rigidity of fixed RPEs. When evaluated on sequences longer than originally trained with, GPT-2 Medium (334M parameters) with CABLE achieves lower perplexity than counterparts using other widely adopted positional encoding methods. Additionally, by applying CABLE to the BERT base model we improved performance in long-context retrieval tasks. Our method significantly enhances the extrapolation performance of existing RPE methods tested on the FineWeb-Edu10B and WikiText-103 datasets. Code is available at: https://github.com/axiomlab/cable",,"Ali Veisi, Hamidreza Amirzadeh, Amir Mansourian",2025-05-31T06:24:36Z,Context-aware Biases for Length Extrapolation,Kontext-Bewusst-Biasen für Längen-Extrapolation,长长外推法的因地认知比数,http://arxiv.org/abs/2503.08067v2
1211,"Knowledge editing (KE) methods offer an efficient way to modify knowledge in large language models. Current KE evaluations typically assess editing success by considering only the edited knowledge without any preceding contexts. In real-world applications, however, preceding contexts often trigger the retrieval of the original knowledge and undermine the intended edit. To address this issue, we develop CHED -- a benchmark designed to evaluate the context robustness of KE methods. Evaluations on CHED show that they often fail when preceding contexts are present. To mitigate this shortcoming, we introduce CoRE, a KE method designed to strengthen context robustness by minimizing context-sensitive variance in hidden states of the model for edited knowledge. This method not only improves the editing success rate in situations where a preceding context is present but also preserves the overall capabilities of the model. We provide an in-depth analysis of the differing impacts of preceding contexts when introduced as user utterances versus assistant responses, and we dissect attention-score patterns to assess how specific tokens influence editing success.",,"Haewon Park, Gyubin Choi, Minjun Kim, Yohan Jo",2025-05-31T06:20:21Z,Context-Robust Knowledge Editing for Language Models,Kontext-Robuste Wissensbearbeitung für Sprachmodelle,语言模式背景操作- 浏览语言模型知识编辑,http://arxiv.org/abs/2505.23026v2
1212,"The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks.",,"Daniel Israel, Guy Van den Broeck, Aditya Grover",2025-05-31T06:10:10Z,Accelerating Diffusion LLMs via Adaptive Parallel Decoding,Beschleunigung von Diffusion LLMs durch adaptive parallele Dekodierung,通过适应性平行解码加速扩散LMS,http://arxiv.org/abs/2506.00413v1
1213,"Children's automatic speech recognition (ASR) often underperforms compared to that of adults due to a confluence of interdependent factors: physiological (e.g., smaller vocal tracts), cognitive (e.g., underdeveloped pronunciation), and extrinsic (e.g., vocabulary limitations, background noise). Existing analysis methods examine the impact of these factors in isolation, neglecting interdependencies-such as age affecting ASR accuracy both directly and indirectly via pronunciation skills. In this paper, we introduce a causal structure discovery to unravel these interdependent relationships among physiology, cognition, extrinsic factors, and ASR errors. Then, we employ causal quantification to measure each factor's impact on children's ASR. We extend the analysis to fine-tuned models to identify which factors are mitigated by fine-tuning and which remain largely unaffected. Experiments on Whisper and Wav2Vec2.0 demonstrate the generalizability of our findings across different ASR systems.",,"Vishwanath Pratap Singh, Md. Sahidullah, Tomi Kinnunen",2025-05-31T05:44:43Z,Causal Structure Discovery for Error Diagnostics of Children's ASR,Causal Structure Discovery für Fehlerdiagnosen von Kinder-ASR,儿童ASR误差诊断发现因果结构,http://arxiv.org/abs/2506.00402v1
1214,"In designing multiple-choice questions (MCQs) in education, creating plausible distractors is crucial for identifying students' misconceptions and gaps in knowledge and accurately assessing their understanding. However, prior studies on distractor generation have not paid sufficient attention to enhancing the difficulty of distractors, resulting in reduced effectiveness of MCQs. This study presents a pipeline for training a model to generate distractors that are more likely to be selected by students. First, we train a pairwise ranker to reason about students' misconceptions and assess the relative plausibility of two distractors. Using this model, we create a dataset of pairwise distractor ranks and then train a distractor generator via Direct Preference Optimization (DPO) to generate more plausible distractors. Experiments on computer science subjects (Python, DB, MLDL) demonstrate that our pairwise ranker effectively identifies students' potential misunderstandings and achieves ranking accuracy comparable to human experts. Furthermore, our distractor generator outperforms several baselines in generating plausible distractors and produces questions with a higher item discrimination index (DI).",,"Yooseop Lee, Suin Kim, Yohan Jo",2025-05-31T05:37:00Z,Generating Plausible Distractors for Multiple-Choice Questions via   Student Choice Prediction,Generieren plausibler Distraktoren für Multiple-Choice-Fragen durch Choice Prediction,通过学生选择预测为多选择问题生成可分解器,http://arxiv.org/abs/2501.13125v3
1215,"As prompts play an increasingly critical role in large language models (LLMs), optimizing textual prompts has become a crucial challenge. The Textual Gradient Descent (TGD) framework has emerged as a promising data-driven approach that iteratively refines textual prompts using LLM - suggested updates (or textual gradients) over minibatches of training samples. In this paper, we empirically demonstrate that scaling the number of training examples initially improves but later degrades TGD's performance across multiple downstream NLP tasks. However, while data scaling improves results for most tasks, it also significantly increases the computational cost when leveraging LLMs. To address this, we draw inspiration from numerical gradient descent and propose Textual Stochastic Gradient Descent with Momentum (TSGD-M) - a method that facilitates scalable in-context learning by reweighting prompt sampling based on past batch distributions. Across nine NLP tasks spanning three domains - including BIG-Bench Hard (BBH), natural language understanding tasks, and reasoning tasks - TSGD-M significantly outperforms TGD baselines that do not incorporate reweighted sampling, while also reducing variance in most tasks.",,"Zixin Ding, Junyuan Hong, Jiachen T. Wang, Zinan Lin, Zhangyang Wang, Yuxin Chen",2025-05-31T05:35:45Z,Scaling Textual Gradients via Sampling-Based Momentum,Skalierung von Textgradienten über Sampling-Based Momentum,"通过基于取样的时速, 通过取样增强文本梯度",http://arxiv.org/abs/2506.00400v1
1216,"Effective decision-making in Large Language Models (LLMs) is essential for handling intricate tasks. However, existing approaches prioritize performance but often overlook the balance between effectiveness and computational cost. To address this, we first introduce the 3E Criteria to systematically assess the cost-effectiveness of search strategies, revealing that existing methods often trade significant efficiency for marginal performance gains. To improve LLM decision-making while maintaining efficiency, we propose the Speculative Reward Model (SRM), a plug-and-play framework that seamlessly integrates with existing search strategies. Specifically, SRM employs an external reward assigner to predict optimal actions, reducing reliance on LLMs' internal self-evaluation. And a speculative verification mechanism is used to prune suboptimal choices and guide the search toward more promising steps. We evaluate SRM on several complex decision-making tasks including mathematical reasoning, planning and numerical reasoning in specialized domains. Experimental results show that SRM reduces costs to 1/10 of the original search framework on average while maintaining effectiveness.",,"Jiawei Gu, Shangsong Liang",2025-05-31T05:32:12Z,Speculative Reward Model Boosts Decision Making Ability of LLMs   Cost-Effectively,Spekulatives Prämienmodell steigert die Entscheidungsfähigkeit von LLMs kosteneffektiv,具有成本效益的LLMs的投机性奖励模式促进决策能力,http://arxiv.org/abs/2506.00396v1
1217,"Empirical evidence indicates that LLMs exhibit spontaneous cross-lingual alignment. However, although LLMs show promising cross-lingual alignment in Information Extraction (IE), a significant imbalance across languages persists, highlighting an underlying deficiency. To address this, we propose KnowCoder-X, a powerful code LLM with advanced cross-lingual and multilingual capabilities for universal IE. Firstly, it standardizes the representation of multilingual schemas using Python classes, ensuring a consistent ontology across different languages. Then, IE across languages is formulated as a unified code generation task. Secondly, we conduct IE cross-lingual alignment instruction tuning on the translated instance prediction task to enhance the model's cross-lingual transferability. During this phase, we also construct a high-quality and diverse bilingual IE parallel dataset with 257k samples, called ParallelNER, synthesized by our proposed robust three-stage pipeline, with manual annotation to ensure quality. Although without training in 29 unseen languages, KnowCoder-X surpasses ChatGPT by 30.17\% and SoTA by 20.03\%, thereby demonstrating superior cross-lingual IE capabilities. Comprehensive evaluations on 64 IE benchmarks in Chinese and English under various settings demonstrate that KnowCoder-X significantly enhances cross-lingual IE transfer through boosting the IE alignment. Our code and dataset are available at: https://github.com/ICT-GoKnow/KnowCoder",,"Yuxin Zuo, Wenxuan Jiang, Wenxuan Liu, Zixuan Li, Long Bai, Hanbin Wang, Yutao Zeng, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng",2025-05-31T05:13:09Z,KnowCoder-X: Boosting Multilingual Information Extraction via Code,KnowCoder-X: Förderung der Mehrsprachigkeit durch Code,KnowCoder-X:通过代码促进多语言信息提取,http://arxiv.org/abs/2411.04794v3
1218,"Current self-correction approaches in text-to-SQL face two critical limitations: 1) Conventional self-correction methods rely on recursive self-calls of LLMs, resulting in multiplicative computational overhead, and 2) LLMs struggle to implement effective error detection and correction for declarative SQL queries, as they fail to demonstrate the underlying reasoning path. In this work, we propose SHARE, an SLM-based Hierarchical Action corREction assistant that enables LLMs to perform more precise error localization and efficient correction. SHARE orchestrates three specialized Small Language Models (SLMs) in a sequential pipeline, where it first transforms declarative SQL queries into stepwise action trajectories that reveal underlying reasoning, followed by a two-phase granular refinement. We further propose a novel hierarchical self-evolution strategy for data-efficient training. Experimental results demonstrate that SHARE effectively enhances self-correction capabilities while proving robust across various LLMs. Furthermore, our comprehensive analysis shows that SHARE maintains strong performance even in low-resource training settings, which is particularly valuable for text-to-SQL applications with data privacy constraints.",,"Ge Qu, Jinyang Li, Bowen Qin, Xiaolong Li, Nan Huo, Chenhao Ma, Reynold Cheng",2025-05-31T04:51:12Z,SHARE: An SLM-based Hierarchical Action CorREction Assistant for   Text-to-SQL,SHARE: Ein SLM-basierter Hierarchischer Aktionskorrektionsassistent für Text-zu-SQL,文本到SQL的基于可持续土地管理的等级行动校验助理,http://arxiv.org/abs/2506.00391v1
1219,"AI copilots represent a new generation of AI-powered systems designed to assist users, particularly knowledge workers and developers, in complex, context-rich tasks. As these systems become more embedded in daily workflows, personalization has emerged as a critical factor for improving usability, effectiveness, and user satisfaction. Central to this personalization is preference optimization: the system's ability to detect, interpret, and align with individual user preferences. While prior work in intelligent assistants and optimization algorithms is extensive, their intersection within AI copilots remains underexplored. This survey addresses that gap by examining how user preferences are operationalized in AI copilots. We investigate how preference signals are sourced, modeled across different interaction stages, and refined through feedback loops. Building on a comprehensive literature review, we define the concept of an AI copilot and introduce a taxonomy of preference optimization techniques across pre-, mid-, and post-interaction phases. Each technique is evaluated in terms of advantages, limitations, and design implications. By consolidating fragmented efforts across AI personalization, human-AI interaction, and language model adaptation, this work offers both a unified conceptual foundation and a practical design perspective for building user-aligned, persona-aware AI copilots that support end-to-end adaptability and deployment.",,"Saleh Afzoon, Zahra Jahanandish, Phuong Thao Huynh, Amin Beheshti, Usman Naseem",2025-05-31T04:48:02Z,Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive   Survey and Taxonomy,Modellierung und Optimierung von Benutzereinstellungen in AI-Copiloten: Eine umfassende Umfrage und Taxonomie,AI中模拟和优化用户首选模式:全面调查和分类,http://arxiv.org/abs/2505.21907v2
1220,"KV cache quantization can improve Large Language Models (LLMs) inference throughput and latency in long contexts and large batch-size scenarios while preserving LLMs effectiveness. However, current methods have three unsolved issues: overlooking layer-wise sensitivity to KV cache quantization, high overhead of online fine-grained decision-making, and low flexibility to different LLMs and constraints. Therefore, we theoretically analyze the inherent correlation of layer-wise transformer attention patterns to KV cache quantization errors and study why key cache is generally more important than value cache for quantization error reduction. We further propose a simple yet effective framework KVTuner to adaptively search for the optimal hardware-friendly layer-wise KV quantization precision pairs for coarse-grained KV cache with multi-objective optimization and directly utilize the offline searched configurations during online inference. To reduce the computational cost of offline calibration, we utilize the intra-layer KV precision pair pruning and inter-layer clustering to reduce the search space. Experimental results show that we can achieve nearly lossless 3.25-bit mixed precision KV cache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum inference throughput can be improved by 21.25\% compared with KIVI-KV8 quantization over various context lengths. Our code and searched configurations are available at https://github.com/cmd2001/KVTuner.",,"Xing Li, Zeyu Xing, Yiming Li, Linping Qu, Hui-Ling Zhen, Wulong Liu, Yiwu Yao, Sinno Jialin Pan, Mingxuan Yuan",2025-05-31T04:45:23Z,KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache   Quantization for Efficient and Nearly Lossless LLM Inference,KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache Quantization für effiziente und nahezu verlustfreie LLM-Inferenz,KVTunner: 用于高效和几乎无损的LLM 推断的敏感度- 敏感度- 警示图层- 精密混合精密 KV 缓存量,http://arxiv.org/abs/2502.04420v4
1221,"Effective communication training is essential to preparing nurses for high-quality patient care. While standardized patient (SP) simulations provide valuable experiential learning, they are often costly and inflexible. Virtual patient (VP) systems offer a scalable alternative, but most fail to adapt to the varying communication skills of trainees. In particular, when trainees respond ineffectively, VPs should escalate in hostility or become uncooperative--yet this level of adaptive interaction remains largely unsupported. To address this gap, we introduce Adaptive-VP, a VP dialogue generation framework that leverages large language models (LLMs) to dynamically adapt VP behavior based on trainee input. The framework features a pipeline for constructing clinically grounded yet flexible VP scenarios and a modular system for assessing trainee communication and adjusting VP responses in real time, while ensuring learner safety. We validated Adaptive-VP by simulating challenging patient conversations. Automated evaluation using a corpus from practicing nurses showed that our communication skill evaluation mechanism reflected real-world proficiency levels. Expert nurses further confirmed that Adaptive-VP produced more natural and realistic interactions than existing approaches, demonstrating its potential as a scalable and effective tool for nursing communication training.",,"Keyeun Lee, Seolhee Lee, Esther Hehsun Kim, Yena Ko, Jinsu Eun, Dahee Kim, Hyewon Cho, Haiyi Zhu, Robert E. Kraut, Eunyoung Suh, Eun-mee Kim, Hajin Lim",2025-05-31T04:34:55Z,Adaptive-VP: A Framework for LLM-Based Virtual Patients that Adapts to   Trainees' Dialogue to Facilitate Nurse Communication Training,"Adaptive-VP: Ein Rahmen für LLM-basierte virtuelle Patienten, der sich an den Dialog der Auszubildenden anpasst, um das Krankenschwestern-Kommunikationstraining zu erleichtern","适应性-VP:基于LLM的虚拟病人框架,适应受训人员的对话,便利护士交流培训",http://arxiv.org/abs/2506.00386v1
1222,"Large language models (LLMs) are increasingly deployed in real-world applications, raising concerns about the unauthorized use of copyrighted or sensitive data. Machine unlearning aims to remove such 'forget' data while preserving utility and information from the 'retain' set. However, existing evaluations typically assume that forget and retain sets are fully disjoint, overlooking realistic scenarios where they share overlapping content. For instance, a news article may need to be unlearned, even though the same event, such as an earthquake in Japan, is also described factually on Wikipedia. Effective unlearning should remove the specific phrasing of the news article while preserving publicly supported facts. In this paper, we introduce DUSK, a benchmark designed to evaluate unlearning methods under realistic data overlap. DUSK constructs document sets that describe the same factual content in different styles, with some shared information appearing across all sets and other content remaining unique to each. When one set is designated for unlearning, an ideal method should remove its unique content while preserving shared facts. We define seven evaluation metrics to assess whether unlearning methods can achieve this selective removal. Our evaluation of nine recent unlearning methods reveals a key limitation: while most can remove surface-level text, they often fail to erase deeper, context-specific knowledge without damaging shared content. We release DUSK as a public benchmark to support the development of more precise and reliable unlearning techniques for real-world applications.",,"Wonje Jeung, Sangyeon Yoon, Hyesoo Hong, Soeun Kim, Seungju Han, Youngjae Yu, Albert No",2025-05-31T04:26:58Z,DUSK: Do Not Unlearn Shared Knowledge,DUSK: Gemeinsames Wissen nicht entschärfen,DUSK: 不共享未读共享知识,http://arxiv.org/abs/2505.15209v3
1223,"Large Reasoning Models (LRMs) have become powerful tools for complex problem solving, but their structured reasoning pathways can lead to unsafe outputs when exposed to harmful prompts. Existing safety alignment methods reduce harmful outputs but can degrade reasoning depth, leading to significant trade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated jailbreak attacks. To address this, we introduce SAFEPATH, a lightweight alignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at the start of their reasoning, in response to harmful prompts, while leaving the rest of the reasoning process unsupervised. Empirical results across multiple benchmarks indicate that SAFEPATH effectively reduces harmful outputs while maintaining reasoning performance. Specifically, SAFEPATH reduces harmful responses by up to 90.0% and blocks 83.3% of jailbreak attempts in the DeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than Direct Refusal and 314.1x less than SafeChain. We further introduce a zero-shot variant that requires no fine-tuning. In addition, we provide a comprehensive analysis of how existing methods in LLMs generalize, or fail, when applied to reasoning-centric models, revealing critical gaps and new directions for safer AI.",,"Wonje Jeung, Sangyeon Yoon, Minsuk Kahng, Albert No",2025-05-31T04:18:54Z,SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early   Alignment,SAFEPATH: Verhindern schädlicher Vernunft in der Kette der Gedanken durch frühzeitige Ausrichtung,SAFPATH:通过早期协调防止在研究链中产生有害理由,http://arxiv.org/abs/2505.14667v3
1224,"Decoding continuous language from neural signals remains a significant challenge in the intersection of neuroscience and artificial intelligence. We introduce Neuro2Semantic, a novel framework that reconstructs the semantic content of perceived speech from intracranial EEG (iEEG) recordings. Our approach consists of two phases: first, an LSTM-based adapter aligns neural signals with pre-trained text embeddings; second, a corrector module generates continuous, natural text directly from these aligned embeddings. This flexible method overcomes the limitations of previous decoding approaches and enables unconstrained text generation. Neuro2Semantic achieves strong performance with as little as 30 minutes of neural data, outperforming a recent state-of-the-art method in low-data settings. These results highlight the potential for practical applications in brain-computer interfaces and neural decoding technologies.",,"Siavash Shams, Richard Antonello, Gavin Mischler, Stephan Bickel, Ashesh Mehta, Nima Mesgarani",2025-05-31T04:17:19Z,Neuro2Semantic: A Transfer Learning Framework for Semantic   Reconstruction of Continuous Language from Human Intracranial EEG,Neuro2Semantic: Ein Transfer-Lernrahmen für die semantische Rekonstruktion kontinuierlicher Sprache vom menschlichen intrakraniellen EEG,Neuro2Semantic: 人类内部电子教育中持续语言的语义重建转移学习框架,http://arxiv.org/abs/2506.00381v1
1225,"Speculative decoding is a powerful technique for reducing the latency of Large Language Models (LLMs), offering a fault-tolerant framework that enables the use of highly compressed draft models. In this work, we introduce Self-Distilled Sparse Drafters (SD$^2$), a novel methodology that leverages self-data distillation and fine-grained weight sparsity to produce highly efficient and well-aligned draft models. SD$^2$ systematically enhances draft token acceptance rates while significantly reducing Multiply-Accumulate operations (MACs), even in the Universal Assisted Generation (UAG) setting, where draft and target models originate from different model families. On a Llama-3.1-70B target model, SD$^2$ provides a 1.59$\times$ higher Mean Accepted Length (MAL) compared to layer-pruned draft models and reduces MACs by over 43.87% with a 8.36% reduction in MAL compared to a dense draft models. Our 1.5B and 3B unstructured sparse drafters outperform both dense and layer-pruned models in terms of end-to-end latency improvements; highlighting the potential of sparsity-aware fine-tuning and compression strategies to improve LLM inference efficiency while maintaining alignment with target models.",,"Mike Lasby, Nish Sinnadurai, Valavan Manohararajah, Sean Lie, Yani Ioannou, Vithursan Thangarasa",2025-05-31T04:11:05Z,SD$^2$: Self-Distilled Sparse Drafters,SD$^2$: Selbstdestillierte Sparse-Zeichner,2美元SD$2美元:自行提炼的零散起草人,http://arxiv.org/abs/2504.08838v2
1226,"Retrieval augmented generation (RAG), while effectively integrating external knowledge to address the inherent limitations of large language models (LLMs), can be hindered by imperfect retrieval that contain irrelevant, misleading, or even malicious information. Previous studies have rarely connected the behavior of RAG through joint analysis, particularly regarding error propagation coming from imperfect retrieval and potential conflicts between LLMs' internal knowledge and external sources. Through comprehensive and controlled analyses under realistic conditions, we find that imperfect retrieval augmentation is inevitable, common, and harmful. We identify the knowledge conflicts between LLM-internal and external knowledge from retrieval as a bottleneck to overcome imperfect retrieval in the post-retrieval stage of RAG. To address this, we propose Astute RAG, a novel RAG approach designed to be resilient to imperfect retrieval augmentation. It adaptively elicits essential information from LLMs' internal knowledge, iteratively consolidates internal and external knowledge with source-awareness, and finalizes the answer according to information reliability. Our experiments with Gemini and Claude demonstrate the superior performance of Astute RAG compared to previous robustness-enhanced RAG approaches. Specifically, Astute RAG is the only RAG method that achieves performance comparable to or even surpassing conventional use of LLMs under the worst-case scenario. Further analysis reveals the effectiveness of Astute RAG in resolving knowledge conflicts, thereby improving the trustworthiness of RAG.",,"Fei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen, Sercan Ö. Arık",2025-05-31T04:07:21Z,Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge   Conflicts for Large Language Models,Astute RAG: Unvollkommene Retrieval-Augmentation und Wissenskonflikte für große Sprachmodelle überwinden,RAG 高级 RAG: 克服大语言模型的不全检索增强和知识冲突,http://arxiv.org/abs/2410.07176v2
1227,"Tokenization, a crucial initial step in natural language processing, is governed by several key parameters, such as the tokenization algorithm, vocabulary size, pre-tokenization strategy, inference strategy, and training data corpus. This paper investigates the impact of an often-overlooked hyperparameter, tokenizer training data size. We train BPE, UnigramLM, and WordPiece tokenizers across various vocabulary sizes using English training data ranging from 1GB to 900GB. Our findings reveal diminishing returns as training data size increases beyond roughly 150GB, suggesting a practical limit to the improvements in tokenization quality achievable through additional data. We analyze this phenomenon and attribute the saturation effect to constraints introduced by the pre-tokenization stage. We then demonstrate the extent to which these findings can generalize by experimenting on data in Russian, a language typologically distant from English. While the limit appears to materialize at a later phase of pre-training, around 200GB, it is in fact observed. These results provide valuable insights for optimizing the tokenization process by reducing the compute required for training on large corpora and suggest promising directions for future research in tokenization algorithms.",,"Varshini Reddy, Craig W. Schmidt, Yuval Pinter, Chris Tanner",2025-05-31T04:06:12Z,How Much is Enough? The Diminishing Returns of Tokenization Training   Data,Wie viel ist genug? Die Diminishing Rückgaben von Tokenization Trainingsdaten,有多少足够?,http://arxiv.org/abs/2502.20273v2
1228,"Large language models (LLMs) have demonstrated remarkable capabilities across various professional domains, with their performance typically evaluated through standardized benchmarks. However, the development of financial RAG benchmarks has been constrained by data confidentiality issues and the lack of dynamic data integration. To address this issue, we introduces FinS-Pilot, a novel benchmark for evaluating RAG systems in online financial applications. Constructed from real-world financial assistant interactions, our benchmark incorporates both real-time API data and structured text sources, organized through an intent classification framework covering critical financial domains such as equity analysis and macroeconomic forecasting. The benchmark enables comprehensive evaluation of financial assistants' capabilities in handling both static knowledge and time-sensitive market information. Through systematic experiments with multiple Chinese leading LLMs, we demonstrate FinS-Pilot's effectiveness in identifying models suitable for financial applications while addressing the current gap in specialized evaluation tools for the financial domain. Our work contributes both a practical evaluation framework and a curated dataset to advance research in financial NLP systems. The code and dataset are accessible on GitHub\footnote{https://github.com/PhealenWang/financial\_rag\_benchmark}.",,"Feng Wang, Yiding Sun, Jiaxin Mao, Wei Xue, Danqing Xu",2025-05-31T03:50:19Z,FinS-Pilot: A Benchmark for Online Financial System,FinS-Pilot: Ein Benchmark für das Online-Finanzsystem,FinS-Pilot:在线财务系统基准,http://arxiv.org/abs/2506.02037v1
1229,"The performance of Large Language Models (LLMs) is intrinsically linked to the quality of its training data. Although several studies have proposed methods for high-quality data selection, they do not consider the importance of knowledge richness in text corpora. In this paper, we propose a novel and gradient-free High-Knowledge Scorer (HKS) to select high-quality data from the dimension of knowledge, to alleviate the problem of knowledge scarcity in the pre-trained corpus. We propose a comprehensive multi-domain knowledge element pool and introduce knowledge density and coverage as metrics to assess the knowledge content of the text. Based on this, we propose a comprehensive knowledge scorer to select data with intensive knowledge, which can also be utilized for domain-specific high-knowledge data selection by restricting knowledge elements to the specific domain. We train models on a high-knowledge bilingual dataset, and experimental results demonstrate that our scorer improves the model's performance in knowledge-intensive and general comprehension tasks, and is effective in enhancing both the generic and domain-specific capabilities of the model.",,"Feiyu Duan, Xuemiao Zhang, Sirui Wang, Haoran Que, Yuqi Liu, Wenge Rong, Xunliang Cai",2025-05-31T03:47:36Z,Enhancing LLMs via High-Knowledge Data Selection,Verbesserung der LLMs durch hochwissende Datenauswahl,通过高知识数据选择加强LLM,http://arxiv.org/abs/2505.14070v2
1230,"Recent advancements in language modeling have shown promising results when applied to time series data. In particular, fine-tuning pre-trained large language models (LLMs) for time series classification tasks has achieved state-of-the-art (SOTA) performance on standard benchmarks. However, these LLM-based models have a significant drawback due to the large model size, with the number of trainable parameters in the millions. In this paper, we propose an alternative approach to leveraging the success of language modeling in the time series domain. Instead of fine-tuning LLMs, we utilize a text embedding model to embed time series and then pair the embeddings with a simple classification head composed of convolutional neural networks (CNN) and multilayer perceptron (MLP). We conducted extensive experiments on a well-established time series classification benchmark. We demonstrated LETS-C not only outperforms the current SOTA in classification accuracy but also offers a lightweight solution, using only 14.5% of the trainable parameters on average compared to the SOTA model. Our findings suggest that leveraging text embedding models to encode time series data, combined with a simple yet effective classification head, offers a promising direction for achieving high-performance time series classification while maintaining a lightweight model architecture.",,"Rachneet Kaur, Zhen Zeng, Tucker Balch, Manuela Veloso",2025-05-31T03:41:19Z,LETS-C: Leveraging Text Embedding for Time Series Classification,LETS-C: Leveraging Text Embedding für Zeitreihenklassifikation,LETS-C:利用文本嵌入时间序列分类,http://arxiv.org/abs/2407.06533v2
1231,"Recent calls for pluralistic alignment emphasize that AI systems should address the diverse needs of all people. Yet, efforts in this space often require sorting people into fixed buckets of pre-specified diversity-defining dimensions (e.g., demographics), risking smoothing out individualistic variations or even stereotyping. To achieve an authentic representation of diversity that respects individuality, we propose individualistic alignment. While individualistic alignment can take various forms, we introduce IndieValueCatalog, a dataset transformed from the influential World Values Survey (WVS), to study language models (LMs) on the specific challenge of individualistic value reasoning. Given a sample of an individual's value-expressing statements, models are tasked with predicting this person's value judgments in novel cases. With IndieValueCatalog, we reveal critical limitations in frontier LMs, which achieve only 55 % to 65% accuracy in predicting individualistic values. Moreover, our results highlight that a precise description of individualistic values cannot be approximated only with demographic information. We also identify a partiality of LMs in reasoning about global individualistic values, as measured by our proposed Value Inequity Index ({\sigma}Inequity). Finally, we train a series of IndieValueReasoners to reveal new patterns and dynamics into global human values.",,"Liwei Jiang, Taylor Sorensen, Sydney Levine, Yejin Choi",2025-05-31T03:22:11Z,Can Language Models Reason about Individualistic Human Values and   Preferences?,Können Sprachmodelle Grund zu individualistischen menschlichen Werten und Präferenzen sein?,语言模式能解释个人主义的人的价值和偏好吗?,http://arxiv.org/abs/2410.03868v2
1232,"Text embedding models play a cornerstone role in AI applications, such as retrieval-augmented generation (RAG). While general-purpose text embedding models demonstrate strong performance on generic retrieval benchmarks, their effectiveness diminishes when applied to private datasets (e.g., company-specific proprietary data), which often contain specialized terminology and lingo. In this work, we introduce BMEmbed, a novel method for adapting general-purpose text embedding models to private datasets. By leveraging the well-established keyword-based retrieval technique (BM25), we construct supervisory signals from the ranking of keyword-based retrieval results to facilitate model adaptation. We evaluate BMEmbed across a range of domains, datasets, and models, showing consistent improvements in retrieval performance. Moreover, we provide empirical insights into how BM25-based signals contribute to improving embeddings by fostering alignment and uniformity, highlighting the value of this approach in adapting models to domain-specific data. We release the source code available at https://github.com/BaileyWei/BMEmbed for the research community.",,"Yubai Wei, Jiale Han, Yi Yang",2025-05-31T03:06:09Z,Adapting General-Purpose Embedding Models to Private Datasets Using   Keyword-based Retrieval,Anpassen von allgemeinen Einbettungsmodellen an private Datensätze mit Hilfe von Keyword-basierten Retrieval,使用基于关键字的检索检索法将通用嵌入模型适应于私有数据集,http://arxiv.org/abs/2506.00363v1
1233,"This paper explores the problem of commonsense level vision-knowledge conflict in Multimodal Large Language Models (MLLMs), where visual information contradicts model's internal commonsense knowledge. To study this issue, we introduce an automated framework, augmented with human-in-the-loop quality control, to generate inputs designed to simulate and evaluate these conflicts in MLLMs. Using this framework, we have crafted a diagnostic benchmark consisting of 374 original images and 1,122 high-quality question-answer (QA) pairs. The benchmark covers two aspects of conflict and three question types, providing a thorough assessment tool. We apply this benchmark to assess the conflict-resolution capabilities of nine representative MLLMs from various model families. Our results indicate an evident over-reliance on parametric knowledge for approximately 20% of all queries, especially among Yes-No and action-related problems. Based on these findings, we evaluate the effectiveness of existing approaches to mitigating the conflicts and compare them to our ""Focus-on-Vision"" prompting strategy. Despite some improvement, the vision-knowledge conflict remains unresolved and can be further scaled through our data construction framework. Our proposed framework, benchmark, and analysis contribute to the understanding and mitigation of vision-knowledge conflicts in MLLMs.",,"Xiaoyuan Liu, Wenxuan Wang, Youliang Yuan, Jen-tse Huang, Qiuzhi Liu, Pinjia He, Zhaopeng Tu",2025-05-31T02:59:15Z,Insight Over Sight: Exploring the Vision-Knowledge Conflicts in   Multimodal LLMs,Einblick in die Sicht: Die visionswissenden Konflikte in multimodalen LLMs erforschen,透视视觉:探索多模式LMM中的视觉-知识冲突,http://arxiv.org/abs/2410.08145v2
1234,"In the evolving landscape of online discourse, misinformation increasingly adopts humorous tones to evade detection and gain traction. This work introduces Deceptive Humor as a novel research direction, emphasizing how false narratives, when coated in humor, can become more difficult to detect and more likely to spread. To support research in this space, we present the Deceptive Humor Dataset (DHD) a collection of humor-infused comments derived from fabricated claims using the ChatGPT-4o model. Each entry is labeled with a Satire Level (from 1 for subtle satire to 3 for overt satire) and categorized into five humor types: Dark Humor, Irony, Social Commentary, Wordplay, and Absurdity. The dataset spans English, Telugu, Hindi, Kannada, Tamil, and their code-mixed forms, making it a valuable resource for multilingual analysis. DHD offers a structured foundation for understanding how humor can serve as a vehicle for the propagation of misinformation, subtly enhancing its reach and impact. Strong baselines are established to encourage further research and model development in this emerging area.",,"Sai Kartheek Reddy Kasu, Shankar Biradar, Sunil Saumya",2025-05-31T02:26:21Z,Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging   Fabricated Claims with Humorous Content,Täuschender Humor: Ein synthetischer Mehrsprachiger Benchmark-Datensatz zur Überbrückung von fabrizierten Claims mit humorvollem Inhalt,"欺骗性幽默:一个合成多语种基准数据集,用于将制造索赔与幽默内容连接起来",http://arxiv.org/abs/2503.16031v2
1235,"Scaling test-time computation--generating and analyzing multiple or sequential outputs for a single input--has become a promising strategy for improving the reliability and quality of large language models (LLMs), as evidenced by advances in uncertainty quantification and multi-step reasoning. A key shared component is semantic clustering, which groups outputs that differ in form but convey the same meaning. Semantic clustering enables estimation of the distribution over the semantics of outputs and helps avoid redundant exploration of reasoning paths. However, existing approaches typically rely on external models, which introduce substantial computational overhead and often fail to capture context-aware semantics. We propose Latent Semantic Clustering (LSC), a lightweight and context-sensitive method that leverages the generator LLM's internal hidden states for clustering, eliminating the need for external models. Our extensive experiment across various LLMs and datasets shows that LSC significantly improves the computational efficiency of test-time scaling while maintaining or exceeding the performance of existing methods.",,"Sungjae Lee, Hoyoung Kim, Jeongyeon Hwang, Eunhyeok Park, Jungseul Ok",2025-05-31T02:08:32Z,Efficient Latent Semantic Clustering for Scaling Test-Time Computation   of LLMs,Effizientes Latent Semantic Clustering für die Skalierung von Test-Time Computation von LLMs,用于按比例测试时计算LLMML的高效中流中语义集成集成,http://arxiv.org/abs/2506.00344v1
1236,"Developing high-performance software is a complex task that requires specialized expertise. We introduce GSO, a benchmark for evaluating language models' capabilities in developing high-performance software. We develop an automated pipeline that generates and executes performance tests to analyze repository commit histories to identify 102 challenging optimization tasks across 10 codebases, spanning diverse domains and programming languages. An agent is provided with a codebase and performance test as a precise specification, and tasked to improve the runtime efficiency, which is measured against the expert developer optimization. Our quantitative evaluation reveals that leading SWE-Agents struggle significantly, achieving less than 5% success rate, with limited improvements even with inference-time scaling. Our qualitative analysis identifies key failure modes, including difficulties with low-level languages, practicing lazy optimization strategies, and challenges in accurately localizing bottlenecks. We release the code and artifacts of our benchmark along with agent trajectories to enable future research.",,"Manish Shetty, Naman Jain, Jinjian Liu, Vijay Kethanaboyina, Koushik Sen, Ion Stoica",2025-05-31T01:53:06Z,GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents,GSO: Herausfordernde Software-Optimierungsaufgaben zur Bewertung von SWE-Agenten,GSO:评估SWE-Agentics的有挑战的软件优化任务,http://arxiv.org/abs/2505.23671v2
1237,"The Open Whisper-style Speech Models (OWSM) project has developed a series of fully open speech foundation models using academic-scale resources, but their training data remains insufficient. This work enhances OWSM by integrating YODAS, a large-scale web-crawled dataset with a Creative Commons license. However, incorporating YODAS is nontrivial due to its wild nature, which introduces challenges such as incorrect language labels and audio-text misalignments. To address this, we develop a scalable data-cleaning pipeline using public toolkits, yielding a dataset with 166,000 hours of speech across 75 languages. Our new series of OWSM v4 models, trained on this curated dataset alongside existing OWSM data, significantly outperform previous versions on multilingual benchmarks. Our models even match or surpass frontier industrial models like Whisper and MMS in multiple scenarios. We will publicly release the cleaned YODAS data, pre-trained models, and all associated scripts via the ESPnet toolkit.",,"Yifan Peng, Shakeel Muhammad, Yui Sudo, William Chen, Jinchuan Tian, Chyi-Jiunn Lin, Shinji Watanabe",2025-05-31T01:44:44Z,OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and   Cleaning,OWSM v4: Open Whisper-Style Sprachmodelle durch Datenskalierung und -reinigung verbessern,OWSM v4:通过数据缩放和清理改进开放耳语语音模型,http://arxiv.org/abs/2506.00338v1
1238,"Evaluating personalized text generated by large language models (LLMs) is challenging, as only the LLM user, i.e., prompt author, can reliably assess the output, but re-engaging the same individuals across studies is infeasible. This paper addresses the challenge of evaluating personalized text generation by introducing ExPerT, an explainable reference-based evaluation framework. ExPerT leverages an LLM to extract atomic aspects and their evidence from the generated and reference texts, match the aspects, and evaluate their alignment based on content and writing style -- two key attributes in personalized text generation. Additionally, ExPerT generates detailed, fine-grained explanations for every step of the evaluation process, enhancing transparency and interpretability. Our experiments demonstrate that ExPerT achieves a 7.2% relative improvement in alignment with human judgments compared to the state-of-the-art text generation evaluation methods. Furthermore, human evaluators rated the usability of ExPerT's explanations at 4.7 out of 5, highlighting its effectiveness in making evaluation decisions more interpretable.",,"Alireza Salemi, Julian Killingback, Hamed Zamani",2025-05-31T01:44:04Z,ExPerT: Effective and Explainable Evaluation of Personalized Long-Form   Text Generation,ExPerT: Effektive und erklärbare Bewertung der personalisierten Langform-Textgenerierung,ExPERT: 有效和可解释的对个性化长龄长龄制文本生成的有效和可解释的评价,http://arxiv.org/abs/2501.14956v2
1239,"Emotional support dialogue systems aim to reduce help-seekers' distress and help them overcome challenges. While human values$\unicode{x2013}$core beliefs that shape an individual's priorities$\unicode{x2013}$are increasingly emphasized in contemporary psychological therapy for their role in fostering internal transformation and long-term emotional well-being, their integration into emotional support systems remains underexplored. To bridge this gap, we present a value-driven method for training emotional support dialogue systems designed to reinforce positive values in seekers. Notably, our model identifies which values to reinforce at each turn and how to do so, by leveraging online support conversations from Reddit. We evaluate the method across support skills, seekers' emotional intensity, and value reinforcement. Our method consistently outperforms various baselines, effectively exploring and eliciting values from seekers. Additionally, leveraging crowd knowledge from Reddit significantly enhances its effectiveness. Therapists highlighted its ability to validate seekers' challenges and emphasize positive aspects of their situations$\unicode{x2013}$both crucial elements of value reinforcement. Our work, being the first to integrate value reinforcement into emotional support systems, demonstrates its promise and establishes a foundation for future research.",,"Juhee Kim, Chunghu Mok, Jisun Lee, Hyang Sook Kim, Yohan Jo",2025-05-31T01:43:02Z,Dialogue Systems for Emotional Support via Value Reinforcement,Dialogsysteme für emotionale Unterstützung durch Wertverstärkung,通过价值观强化促进情感支持的对话系统,http://arxiv.org/abs/2501.17182v3
1240,"This paper presents ICAT, an evaluation framework for measuring coverage of diverse factual information in long-form text generation. ICAT breaks down a long output text into a list of atomic claims and not only verifies each claim through retrieval from a (reliable) knowledge source, but also computes the alignment between the atomic factual claims and various aspects expected to be presented in the output. We study three implementations of the ICAT framework, each with a different assumption on the availability of aspects and alignment method. By adopting data from the diversification task in the TREC Web Track and the ClueWeb corpus, we evaluate the ICAT framework. We demonstrate strong correlation with human judgments and provide comprehensive evaluation across multiple state-of-the-art LLMs. Our framework further offers interpretable and fine-grained analysis of diversity and coverage. Its modular design allows for easy adaptation to different domains and datasets, making it a valuable tool for evaluating the qualitative aspects of long-form responses produced by LLMs.",,"Chris Samarinas, Alexander Krubner, Alireza Salemi, Youngwoo Kim, Hamed Zamani",2025-05-31T01:23:14Z,Beyond Factual Accuracy: Evaluating Coverage of Diverse Factual   Information in Long-form Text Generation,Beyond Factual Accuracy: Bewertung der Abdeckung unterschiedlicher Factual Information in der langformalen Textgenerierung,超越事实准确性:评估长式文本生成中多种事实信息覆盖面,http://arxiv.org/abs/2501.03545v4
1241,"Datasets used for emotion recognition tasks typically contain overt cues that can be used in predicting the emotions expressed in a text. However, one challenge is that texts sometimes contain covert contextual cues that are rich in affective semantics, which warrant higher-order reasoning abilities to infer emotional states, not simply the emotions conveyed. This study advances beyond surface-level perceptual features to investigate how large language models (LLMs) reason about others' emotional states using contextual information, within a Theory-of-Mind (ToM) framework. Grounded in Cognitive Appraisal Theory, we curate a specialized ToM evaluation dataset1 to assess both forward reasoning - from context to emotion- and backward reasoning - from emotion to inferred context. We showed that LLMs can reason to a certain extent, although they are poor at associating situational outcomes and appraisals with specific emotions. Our work highlights the need for psychological theories in the training and evaluation of LLMs in the context of emotion reasoning.",,"Gerard Christopher Yeo, Kokil Jaidka",2025-05-31T01:18:04Z,Beyond Context to Cognitive Appraisal: Emotion Reasoning as a Theory of   Mind Benchmark for Large Language Models,Beyond Context to Cognitive Appraisal: Emotion Reasoning as a Theory of Mind Benchmark for Large Language Models,超越认知评估的背景:作为大语言模式思想基准理论的情感原因,http://arxiv.org/abs/2506.00334v1
1242,"Do language models (LMs) offer insights into human language learning? A common argument against this idea is that because their architecture and training paradigm are so vastly different from humans, LMs can learn arbitrary inputs as easily as natural languages. We test this claim by training LMs to model impossible and typologically unattested languages. Unlike previous work, which has focused exclusively on English, we conduct experiments on 12 languages from 4 language families with two newly constructed parallel corpora. Our results show that while GPT-2 small can largely distinguish attested languages from their impossible counterparts, it does not achieve perfect separation between all the attested languages and all the impossible ones. We further test whether GPT-2 small distinguishes typologically attested from unattested languages with different NP orders by manipulating word order based on Greenberg's Universal 20. We find that the model's perplexity scores do not distinguish attested vs. unattested word orders, while its performance on the generalization test does. These findings suggest that LMs exhibit some human-like inductive biases, though these biases are weaker than those found in human learners.",,"Xiulin Yang, Tatsuya Aoyama, Yuekun Yao, Ethan Wilcox",2025-05-31T01:14:47Z,Anything Goes? A Crosslinguistic Study of (Im)possible Language Learning   in LMs,Irgendetwas geht? Eine Crosslinguistische Studie über (Un)mögliches Sprachenlernen in LMs,"任何变化吗? 关于(一)可能语言学习的跨语言研究,LMs",http://arxiv.org/abs/2502.18795v2
1243,"Code-mixing involves the seamless integration of linguistic elements from multiple languages within a single discourse, reflecting natural multilingual communication patterns. Despite its prominence in informal interactions such as social media, chat messages and instant-messaging exchanges, there has been a lack of publicly available corpora that are author-labeled and suitable for modeling human conversations and relationships. This study introduces the first labeled and general-purpose corpus for understanding code-mixing in context while maintaining rigorous privacy and ethical standards. Our live project will continuously gather, verify, and integrate code-mixed messages into a structured dataset released in JSON format, accompanied by detailed metadata and linguistic statistics. To date, it includes over 355,641 messages spanning various code-mixing patterns, with a primary focus on English, Mandarin, and other languages. We expect the Codemix Corpus to serve as a foundational dataset for research in computational linguistics, sociolinguistics, and NLP applications.",,"Svetlana Churina, Akshat Gupta, Insyirah Mujtahid, Kokil Jaidka",2025-05-31T01:09:04Z,Disentangling Codemixing in Chats: The NUS ABC Codemixed Corpus,Entwirren von Codemixing in Chats: Der NUS ABC Codemixed Corpus,在聊天区拆解编码混合: NUS ABC 编码混合公司,http://arxiv.org/abs/2506.00332v1
1244,"In real practice, questions are typically complex and knowledge-intensive, requiring Large Language Models (LLMs) to recognize the multifaceted nature of the question and reason across multiple information sources. Iterative and adaptive retrieval, where LLMs decide when and what to retrieve based on their reasoning, has been shown to be a promising approach to resolve complex, knowledge-intensive questions. However, the performance of such retrieval frameworks is limited by the accumulation of reasoning errors and misaligned retrieval results. To overcome these limitations, we propose TreeRare (Syntax Tree-Guided Retrieval and Reasoning), a framework that utilizes syntax trees to guide information retrieval and reasoning for question answering. Following the principle of compositionality, TreeRare traverses the syntax tree in a bottom-up fashion, and in each node, it generates subcomponent-based queries and retrieves relevant passages to resolve localized uncertainty. A subcomponent question answering module then synthesizes these passages into concise, context-aware evidence. Finally, TreeRare aggregates the evidence across the tree to form a final answer. Experiments across five question answering datasets involving ambiguous or multi-hop reasoning demonstrate that TreeRare achieves substantial improvements over existing state-of-the-art methods.",,"Boyi Zhang, Zhuo Liu, Hangfeng He",2025-05-31T01:07:50Z,TreeRare: Syntax Tree-Guided Retrieval and Reasoning for   Knowledge-Intensive Question Answering,TreeRare: Syntax Baum-geführte Retrieval und Begründung für wissensintensive Frageantworten,树数: 用于知识强化问题解答的语法树指南检索和理由,http://arxiv.org/abs/2506.00331v1
1245,"Minimum Bayes Risk (MBR) decoding optimizes output selection by maximizing the expected utility value of an underlying human distribution. While prior work has shown the effectiveness of MBR decoding through empirical evaluation, few studies have analytically investigated why the method is effective. As a result of our analysis, we show that, given the size $n$ of the reference hypothesis set used in computation, MBR decoding approaches the optimal solution with high probability at a rate of $O\left(n^{-\frac{1}{2}}\right)$, under certain assumptions, even though the language space $Y$ is significantly larger $|Y|\gg n$. This result helps to theoretically explain the strong performance observed in several prior empirical studies on MBR decoding. In addition, we provide the performance gap for maximum-a-posteriori (MAP) decoding and compare it to MBR decoding. The result of this paper indicates that MBR decoding tends to converge to the optimal solution faster than MAP decoding in several cases.",,"Yuki Ichihara, Yuu Jinnai, Kaito Ariu, Tetsuro Morimura, Eiji Uchibe",2025-05-31T00:33:16Z,Theoretical Guarantees for Minimum Bayes Risk Decoding,Theoretische Garantien für die Risikodekodierung von Mindestbuchten,最低比亚最低风险编码理论保障,http://arxiv.org/abs/2502.12685v2
1246,"Recent progress in reasoning with large language models (LLMs), such as DeepSeek-R1, demonstrates impressive capabilities in domains like mathematics and coding, by exhibiting complex cognitive behaviors such as verification, goal decomposition, and self-reflection. However, it is unclear what behavior is effective and what behavior is missing for long-horizon AI agents tasks. In this work, we propose Dyna-Think, a thinking framework that integrates planning with an internal world model with reasoning and acting to enhance AI agent performance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning (DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with Dyna-Think, DIT reconstructs the thinking process of R1 to focus on performing world model simulation relevant to the proposed (and planned) action, and trains the policy using this reconstructed data. To enhance Dyna-Think, DDT uses a two-stage training process to first improve the agent's world modeling ability via objectives such as state prediction or critique generation, and then improve the agent's action via policy training. We evaluate our methods on OSWorld, and demonstrate that Dyna-Think improves the agent's in-domain and out-of-domain performance, achieving similar best-of-n performance compared to R1 while generating 2x less tokens on average. Our extensive empirical studies reveal that 1) using critique generation for world model training is effective to improve policy performance; and 2) AI agents with better performance correlate with better world modeling abilities. We believe our results suggest a promising research direction to integrate world model simulation into AI agents to enhance their reasoning, planning, and acting capabilities.",,"Xiao Yu, Baolin Peng, Ruize Xu, Michel Galley, Hao Cheng, Suman Nath, Jianfeng Gao, Zhou Yu",2025-05-31T00:10:18Z,"Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in   AI Agents","Dyna-Think: Synergizing Reasoning, Acting und World Model Simulation in KI-Agenten",Dyn-Tyn-Think:在AI剂中合成理性、行为和世界模拟模型模拟,http://arxiv.org/abs/2506.00320v1
1247,"As language models evolve to tackle complex, multifaceted tasks, their evaluation must adapt to capture this intricacy. A granular, skill-specific understanding of model capabilities can empower researchers to make informed model development plans. In this paper, we introduce SkillVerse, an unsupervised tree-structured diagnosis framework for understanding model proficiency in specific abilities. With LLM as a judge, SkillVerse first critiques the model responses, and then organizes them into a hierarchical structure termed dendrogram. Given proficiency at arbitrary levels of granularity, SkillVerse is flexible to produce insights of behaviors of modern large models. We also demonstrate its efficacy in two downstream tasks: 1) improving model in-context learning by 25% using a tree-search algorithm to select more informative few-shot demonstrations, and 2) accurately predicting new model weaknesses with a 55% success rate, 22% higher than without SkillVerse.",,"Yufei Tian, Jiao Sun, Nanyun Peng, Zizhao Zhang",2025-05-31T00:08:59Z,SkillVerse : Assessing and Enhancing LLMs with Tree Evaluation,SkillVerse : Bewertung und Verbesserung von LLMs mit Baumbewertung,SkillVerse:评估和加强植树评估LLMs,http://arxiv.org/abs/2506.00319v1
1248,"Large language models (LLMs) have been prominent in various tasks, including text generation and summarisation. The applicability of LLMs to the generation of product reviews is gaining momentum, paving the way for the generation of movie reviews. In this study, we propose a framework that generates movie reviews using three LLMs (GPT-4o, DeepSeek-V3, and Gemini-2.0), and evaluate their performance by comparing the generated outputs with IMDb user reviews. We use movie subtitles and screenplays as input to the LLMs and investigate how they affect the quality of reviews generated. We review the LLM-based movie reviews in terms of vocabulary, sentiment polarity, similarity, and thematic consistency in comparison to IMDB user reviews. The results demonstrate that LLMs are capable of generating syntactically fluent and structurally complete movie reviews. Nevertheless, there is still a noticeable gap in emotional richness and stylistic coherence between LLM-generated and IMDb reviews, suggesting that further refinement is needed to improve the overall quality of movie review generation. We provided a survey-based analysis where participants were told to distinguish between LLM and IMDb user reviews. The results show that LLM-generated reviews are difficult to distinguish from IMDB user reviews. We found that DeepSeek-V3 produced the most balanced reviews, closely matching IMDb reviews. GPT-4o overemphasised positive emotions, while Gemini-2.0 captured negative emotions better but showed excessive emotional intensity.",,"Brendan Sands, Yining Wang, Chenhao Xu, Yuxuan Zhou, Lai Wei, Rohitash Chandra",2025-05-30T23:45:53Z,"An evaluation of LLMs for generating movie reviews: GPT-4o, Gemini-2.0   and DeepSeek-V3","Eine Auswertung von LLMs zur Generierung von Filmrezensionen: GPT-4o, Gemini-2.0 und DeepSeek-V3","GPT-4o、Gemini-2.0和DeepSeek-V3,对制作电影评论的LLMs的评价:GPT-4o、Gemini-2.0和DeepSeek-V3",http://arxiv.org/abs/2506.00312v1
1249,"Effective decision-making and problem-solving in conversational systems require the ability to identify and acquire missing information through targeted questioning. A key challenge lies in efficiently narrowing down a large space of possible outcomes by posing questions that minimize uncertainty. To address this, we introduce a novel framework that leverages Large Language Models (LLMs) to generate information-seeking questions, with Monte Carlo Tree Search (MCTS) to strategically select questions that maximize information gain, as a part of inference-time planning. Our primary contribution includes a hierarchical feedback mechanism that exploits past interaction patterns to guide future strategy. Specifically, each new problem is mapped to a cluster based on semantic similarity, and our UCT (Upper Confidence bound for Trees) formulation employs a cluster-specific bonus reward to prioritize successful question trajectories that have proven effective for similar problems in the past. Extensive empirical evaluation across medical diagnosis and technical troubleshooting domains shows that our method achieves an average of 12% improvement in success rates and about 10x reduction in the number of LLM calls made for planning per conversation, compared to the state of the art. An additional 8% gain in success rate is observed on average when we start with a constrained set of possibilities. Our results underscore the efficacy of feedback-aware MCTS in enhancing information-seeking in goal-oriented dialogues.",,"Harshita Chopra, Chirag Shah",2025-05-30T23:44:37Z,Feedback-Aware Monte Carlo Tree Search for Efficient Information Seeking   in Goal-Oriented Conversations,Feedback-Bewusst Monte Carlo Tree Suche nach effizienten Informationen Suche nach zielorientierten Gesprächen,在以目标为导向的对话中寻找有效信息,http://arxiv.org/abs/2501.15056v2
1250,"Learning from human feedback has enabled the alignment of language models (LMs) with human preferences. However, collecting human preferences is expensive and time-consuming, with highly variable annotation quality. An appealing alternative is to distill preferences from LMs as a source of synthetic annotations, offering a cost-effective and scalable alternative, albeit susceptible to other biases and errors. In this work, we introduce HyPER, a Hybrid Preference routER that defers an annotation to either humans or LMs, achieving better annotation quality while reducing the cost of human-only annotation. We formulate this as an optimization problem: given a preference dataset and an evaluation metric, we (1) train a performance prediction model (PPM) to predict a reward model's (RM) performance on an arbitrary combination of human and LM annotations and (2) employ a routing strategy that selects a combination that maximizes the predicted performance. We train the PPM on MultiPref, a new preference dataset with 10k instances paired with humans and LM labels. We show that the selected hybrid mixture of synthetic and direct human preferences using HyPER achieves better RM performance compared to using either one exclusively by 7-13% on RewardBench and generalizes across unseen preference datasets and other base models. We also observe the same trend in other benchmarks using Best-of-N reranking, where the hybrid mix has 2-3% better performance. Finally, we analyze features from HyPER and find that prompts with moderate safety concerns or complexity benefit the most from human feedback.",,"Lester James V. Miranda, Yizhong Wang, Yanai Elazar, Sachin Kumar, Valentina Pyatkin, Faeze Brahman, Noah A. Smith, Hannaneh Hajishirzi, Pradeep Dasigi",2025-05-30T23:40:44Z,Hybrid Preferences: Learning to Route Instances for Human vs. AI   Feedback,"Hybrid Preferences: Lernen, Instanzen für den Menschen gegen AI-Feedback zu routen",混合优惠:学习人类道路与AI. AI. 之间的路由实例,http://arxiv.org/abs/2410.19133v5
1251,"Understanding the prevalence of misinformation in health topics online can inform public health policies and interventions. However, measuring such misinformation at scale remains a challenge, particularly for high-stakes but understudied topics like opioid-use disorder (OUD)--a leading cause of death in the U.S. We present the first large-scale study of OUD-related myths on YouTube, a widely-used platform for health information. With clinical experts, we validate 8 pervasive myths and release an expert-labeled video dataset. To scale labeling, we introduce MythTriage, an efficient triage pipeline that uses a lightweight model for routine cases and defers harder ones to a high-performing, but costlier, large language model (LLM). MythTriage achieves up to 0.86 macro F1-score while estimated to reduce annotation time and financial cost by over 76% compared to experts and full LLM labeling. We analyze 2.9K search results and 343K recommendations, uncovering how myths persist on YouTube and offering actionable insights for public health and platform moderation.",,"Hayoung Jung, Shravika Mittal, Ananya Aatreya, Navreet Kaur, Munmun De Choudhury, Tanushree Mitra",2025-05-30T23:37:10Z,MythTriage: Scalable Detection of Opioid Use Disorder Myths on a   Video-Sharing Platform,MythTriage: Scalable Detection of Opioid Use Disorder Myths on a Video-Sharing Platform,传奇:在视频共享平台上对类鸦片使用混乱神话进行可缩放检测,http://arxiv.org/abs/2506.00308v1
1252,"Existing work on prompt compression for Large Language Models (LLM) focuses on lossy methods that try to maximize the retention of semantic information that is relevant to downstream tasks while significantly reducing the sequence length. In this paper, we introduce a task-agnostic lossless compression technique similar to LZ77 that makes it possible to reduce the input token sequence length on average by 27\% and 18\% for the two evaluation tasks explored here. Given that we use transformer-based LLMs, this equates to 47\% and 33\% less encoding computation, respectively, due to the quadratic nature of attention. The token sequence transformation is trivial to reverse and highlights that no semantic information is lost in the process. We evaluate our proposed approach on two tasks that require strict preservation of semantics/syntax and demonstrate that existing lossy compression methods perform poorly in this setting. We find that our lossless compression technique produces only a small gap in performance compared to using the uncompressed input and posit that larger models and an expanded computing budget would likely erase the gap entirely.",,"John Harvill, Ziwei Fan, Hao Wang, Yizhou Sun, Hao Ding, Luke Huan, Anoop Deoras",2025-05-30T23:32:57Z,Lossless Token Sequence Compression via Meta-Tokens,Lossless Token Sequence Compression via Meta-Tokens,通过 Meta-Tokens 的无损调序压缩,http://arxiv.org/abs/2506.00307v1
1253,"Unvoiced electromyography (EMG) is an effective communication tool for individuals unable to produce vocal speech. However, most prior methods rely on paired voiced and unvoiced EMG signals, along with speech data, for EMG-to-text conversion, which is not practical for such individuals. Given the rise of large language models (LLMs) in speech recognition, we explore their potential to understand unvoiced speech. To this end, we address the challenge of learning from unvoiced EMG alone and propose a novel EMG adaptor module that maps EMG features into an LLM's input space, achieving an average word error rate (WER) of 0.49 on a closed-vocabulary unvoiced EMG-to-text task. Even with a conservative data availability of just six minutes, our approach improves performance over specialized models by nearly 20%. While LLMs have been shown to be extendable to new language modalities -- such as audio -- understanding articulatory biosignals like unvoiced EMG remains more challenging. This work takes a crucial first step toward enabling LLMs to comprehend unvoiced speech using surface EMG.",,"Payal Mohapatra, Akash Pandey, Xiaoyuan Zhang, Qi Zhu",2025-05-30T23:22:44Z,Can LLMs Understand Unvoiced Speech? Exploring EMG-to-Text Conversion   with LLMs,Können LLMs unvoiced Speech verstehen? EMG-zu-Text-Konvertierung mit LLMs erkunden,LLM女士能理解无声讲话吗?探索用LM女士来进行EMG转换成文本转换,http://arxiv.org/abs/2506.00304v1
1254,"Warning: this paper discusses content related, but not limited to, violence, sex, and suicide. Loneliness, or the lack of fulfilling relationships, significantly impacts a person's mental and physical well-being and is prevalent worldwide. Previous research suggests that large language models (LLMs) may help mitigate loneliness. However, we argue that the use of widespread LLMs in services like ChatGPT is more prevalent--and riskier, as they are not designed for this purpose. To explore this, we analysed user interactions with ChatGPT outside of its marketed use as a task-oriented assistant. In dialogues classified as lonely, users frequently (37%) sought advice or validation, and received good engagement. However, ChatGPT failed in sensitive scenarios, like responding appropriately to suicidal ideation or trauma. We also observed a 35% higher incidence of toxic content, with women being 22x more likely to be targeted than men. Our findings underscore ethical and legal questions about this technology, and note risks like radicalisation or further isolation. We conclude with recommendations to research and industry to address loneliness.",,Adrian de Wynter,2025-05-30T23:15:32Z,If Eleanor Rigby Had Met ChatGPT: A Study on Loneliness in a Post-LLM   World,Wenn Eleanor Rigby ChatGPT getroffen hatte: Eine Studie über Einsamkeit in einer Post-LLM Welt,如果Eleanor Rigby曾与Met ChatGPT:LLM后世界的孤独问题研究,http://arxiv.org/abs/2412.01617v2
1255,"Modern large language models (LLMs) are capable of interpreting input strings as instructions, or prompts, and carry out tasks based on them. Unlike traditional learners, LLMs cannot use back-propagation to obtain feedback, and condition their output in situ in a phenomenon known as in-context learning (ICL). Many approaches to prompting and pre-training these models involve the automated generation of these prompts, also known as meta-prompting, or prompting to obtain prompts. However, they do not formally describe the properties and behavior of the LLMs themselves. We propose a theoretical framework based on category theory to generalize and describe ICL and LLM behavior when interacting with users. Our framework allows us to obtain formal results around task agnosticity and equivalence of various meta-prompting approaches. Using our framework and experimental results we argue that meta-prompting is more effective than basic prompting at generating desirable outputs.",,"Adrian de Wynter, Xun Wang, Qilong Gu, Si-Qing Chen",2025-05-30T23:08:20Z,On Meta-Prompting,Auf Meta-Prompting,关于元促进,http://arxiv.org/abs/2312.06562v3
1256,"We perform a critical examination of the scientific methodology behind contemporary large language model (LLM) research. For this we assess over 2,000 research works released between 2020 and 2024 based on criteria typical of what is considered good research (e.g. presence of statistical tests and reproducibility), and cross-validate it with arguments that are at the centre of controversy (e.g., claims of emergent behaviour). We find multiple trends, such as declines in ethics disclaimers, a rise of LLMs as evaluators, and an increase on claims of LLM reasoning abilities without leveraging human evaluation. We note that conference checklists are effective at curtailing some of these issues, but balancing velocity and rigour in research cannot solely rely on these. We tie all these findings to findings from recent meta-reviews and extend recommendations on how to address what does, does not, and should work in LLM research.",,Adrian de Wynter,2025-05-30T22:55:40Z,"Awes, Laws, and Flaws From Today's LLM Research","Ehrfurcht, Gesetze und Fehler aus der heutigen LLM-Forschung",今日法学硕士研究的奥氏、法律和法条,http://arxiv.org/abs/2408.15409v3
1257,"In this work, we compare the domain-specific translation performance of open-source autoregressive decoder-only large language models (LLMs) with task-oriented machine translation (MT) models. Our experiments focus on the medical domain and cover four language directions with varied resource availability: English-to-French, English-to-Portuguese, English-to-Swahili, and Swahili-to-English. Despite recent advancements, LLMs demonstrate a significant quality gap in specialized translation compared to multilingual encoder-decoder MT models such as NLLB-200. Our results indicate that NLLB-200 3.3B outperforms all evaluated LLMs in the 7-8B parameter range across three out of the four language directions. While fine-tuning improves the performance of LLMs such as Mistral and Llama, these models still underperform compared to fine-tuned NLLB-200 3.3B models. Our findings highlight the ongoing need for specialized MT models to achieve high-quality domain-specific translation, especially in medium-resource and low-resource settings. Moreover, the superior performance of larger LLMs over their 8B variants suggests potential value in pre-training domain-specific medium-sized language models, employing targeted data selection and knowledge distillation approaches to enhance both quality and efficiency in specialized translation tasks.",,"Aman Kassahun Wassie, Mahdi Molaei, Yasmin Moslem",2025-05-30T22:50:03Z,Domain-Specific Translation with Open-Source Large Language Models:   Resource-Oriented Analysis,Domainspezifische Übersetzung mit Open-Source-Großsprachenmodellen: Ressourcenorientierte Analyse,使用开放源码大语言模型的具体域名翻译:资源导向分析,http://arxiv.org/abs/2412.05862v4
1258,"This paper introduces DLM-One, a score-distillation-based framework for one-step sequence generation with continuous diffusion language models (DLMs). DLM-One eliminates the need for iterative refinement by aligning the scores of a student model's outputs in the continuous token embedding space with the score function of a pretrained teacher DLM. We investigate whether DLM-One can achieve substantial gains in sampling efficiency for language modeling. Through comprehensive experiments on DiffuSeq -- a representative continuous DLM -- we show that DLM-One achieves up to ~500x speedup in inference time while maintaining competitive performance on benchmark text generation tasks used to evaluate the teacher models. We further analyze the method's empirical behavior across multiple datasets, providing initial insights into its generality and practical applicability. Our findings position one-step diffusion as a promising direction for efficient, high-quality language generation and broader adoption of continuous diffusion models operating in embedding space for natural language processing.",,"Tianqi Chen, Shujian Zhang, Mingyuan Zhou",2025-05-30T22:42:23Z,DLM-One: Diffusion Language Models for One-Step Sequence Generation,DLM-One: Diffusions-Sprachmodelle für die Ein-Schritt-Sequenz-Generierung,DLM-1:单序列生成的传播语言模型,http://arxiv.org/abs/2506.00290v1
1259,"Social biases can manifest in language agency. However, very limited research has investigated such biases in Large Language Model (LLM)-generated content. In addition, previous works often rely on string-matching techniques to identify agentic and communal words within texts, falling short of accurately classifying language agency. We introduce the Language Agency Bias Evaluation (LABE) benchmark, which comprehensively evaluates biases in LLMs by analyzing agency levels attributed to different demographic groups in model generations. LABE tests for gender, racial, and intersectional language agency biases in LLMs on 3 text generation tasks: biographies, professor reviews, and reference letters. Using LABE, we unveil language agency social biases in 3 recent LLMs: ChatGPT, Llama3, and Mistral. We observe that: (1) LLM generations tend to demonstrate greater gender bias than human-written texts; (2) Models demonstrate remarkably higher levels of intersectional bias than the other bias aspects. (3) Prompt-based mitigation is unstable and frequently leads to bias exacerbation. Based on our observations, we propose Mitigation via Selective Rewrite (MSR), a novel bias mitigation strategy that leverages an agency classifier to identify and selectively revise parts of generated texts that demonstrate communal traits. Empirical results prove MSR to be more effective and reliable than prompt-based mitigation method, showing a promising research direction.",,"Yixin Wan, Kai-Wei Chang",2025-05-30T22:39:05Z,"White Men Lead, Black Women Help? Benchmarking and Mitigating Language   Agency Social Biases in LLMs","White Men Lead, Schwarze Frauen Hilfe? Benchmarking und Mitigating Language Agency Social Biases in LLMs",白男性铅、黑人妇女帮助?,http://arxiv.org/abs/2404.10508v5
1260,"Creating lyrics and melodies for the vocal track in a symbolic format, known as song composition, demands expert musical knowledge of melody, an advanced understanding of lyrics, and precise alignment between them. Despite achievements in sub-tasks such as lyric generation, lyric-to-melody, and melody-to-lyric, etc, a unified model for song composition has not yet been achieved. In this paper, we introduce SongComposer, a pioneering step towards a unified song composition model that can readily create symbolic lyrics and melodies following instructions. SongComposer is a music-specialized large language model (LLM) that, for the first time, integrates the capability of simultaneously composing lyrics and melodies into LLMs by leveraging three key innovations: 1) a flexible tuple format for word-level alignment of lyrics and melodies, 2) an extended tokenizer vocabulary for song notes, with scalar initialization based on musical knowledge to capture rhythm, and 3) a multi-stage pipeline that captures musical structure, starting with motif-level melody patterns and progressing to phrase-level structure for improved coherence. Extensive experiments demonstrate that SongComposer outperforms advanced LLMs, including GPT-4, in tasks such as lyric-to-melody generation, melody-to-lyric generation, song continuation, and text-to-song creation. Moreover, we will release SongCompose, a large-scale dataset for training, containing paired lyrics and melodies in Chinese and English.",,"Shuangrui Ding, Zihan Liu, Xiaoyi Dong, Pan Zhang, Rui Qian, Junhao Huang, Conghui He, Dahua Lin, Jiaqi Wang",2025-05-30T22:33:14Z,SongComposer: A Large Language Model for Lyric and Melody Generation in   Song Composition,SongComposer: Ein großes Sprachmodell für Lyrik und Melodie in der Songkomposition,SongComporoser:歌声和美乐一代歌曲组成大语言模型,http://arxiv.org/abs/2402.17645v2
1261,"Contextual large language model embeddings are increasingly utilized for topic modeling and clustering. However, current methods often scale poorly, rely on opaque similarity metrics, and struggle in multilingual settings. In this work, we present a novel, scalable, interpretable, hierarchical, and multilingual approach to clustering news articles and social media data. To do this, we first train multilingual Matryoshka embeddings that can determine story similarity at varying levels of granularity based on which subset of the dimensions of the embeddings is examined. This embedding model achieves state-of-the-art performance on the SemEval 2022 Task 8 test dataset (Pearson $\rho$ = 0.816). Once trained, we develop an efficient hierarchical clustering algorithm that leverages the hierarchical nature of Matryoshka embeddings to identify unique news stories, narratives, and themes. We conclude by illustrating how our approach can identify and cluster stories, narratives, and overarching themes within real-world news datasets.",,"Hans W. A. Hanley, Zakir Durumeric",2025-05-30T22:17:18Z,Hierarchical Level-Wise News Article Clustering via Multilingual   Matryoshka Embeddings,Hierarchische Level-Wise-Nachrichten Artikel Clustering über Mehrsprachige Matryoshka-Einbetten,通过多种语言的 Matryoshka 嵌入式集成文章,http://arxiv.org/abs/2506.00277v1
1262,"Robot co-design, jointly optimizing morphology and control policy, remains a longstanding challenge in the robotics community, where many promising robots have been developed. However, a key limitation lies in its tendency to converge to sub-optimal designs due to the use of fixed reward functions, which fail to explore the diverse motion modes suitable for different morphologies. Here we propose RoboMoRe, a large language model (LLM)-driven framework that integrates morphology and reward shaping for co-optimization within the robot co-design loop. RoboMoRe performs a dual-stage optimization: in the coarse optimization stage, an LLM-based diversity reflection mechanism generates both diverse and high-quality morphology-reward pairs and efficiently explores their distribution. In the fine optimization stage, top candidates are iteratively refined through alternating LLM-guided reward and morphology gradient updates. RoboMoRe can optimize both efficient robot morphologies and their suited motion behaviors through reward shaping. Results demonstrate that without any task-specific prompting or predefined reward/morphology templates, RoboMoRe significantly outperforms human-engineered designs and competing methods across eight different tasks.",,"Jiawei Fang, Yuxuan Sun, Chengtian Ma, Qiuyu Lu, Lining Yao",2025-05-30T22:16:07Z,RoboMoRe: LLM-based Robot Co-design via Joint Optimization of Morphology   and Reward,RoboMoRe: LLM-basiertes Roboter-Co-Design durch gemeinsame Optimierung von Morphologie und Belohnung,"机器人机器人:基于LLM的机器人机器人共同设计,通过联合优化病理学和奖赏",http://arxiv.org/abs/2506.00276v1
1263,"The success of large language models has driven interest in developing similar speech processing capabilities. However, a key challenge is the scarcity of high-quality spontaneous speech data, as most existing datasets contain scripted dialogues. To address this, we present a novel pipeline for eliciting and recording natural dialogues and release our Stage 1 dataset with 200+ hours of spontaneous speech. Our approach fosters fluid, natural conversations while encouraging a diverse range of topics and interactive exchanges. Unlike traditional methods, it facilitates genuine interactions, providing a reproducible framework for future data collection. This paper introduces our dataset and methodology, laying the groundwork for addressing the shortage of spontaneous speech data. We plan to expand this dataset in future stages, offering a growing resource for the research community.",,"Cihan Xiao, Ruixing Liang, Xiangyu Zhang, Mehmet Emre Tiryaki, Veronica Bae, Lavanya Shankar, Rong Yang, Ethan Poon, Emmanuel Dupoux, Sanjeev Khudanpur, Leibny Paola Garcia Perera",2025-05-30T22:03:59Z,CASPER: A Large Scale Spontaneous Speech Dataset,CASPER: Ein großer Spontane-Sprachdatensatz,大规模自应自发语音数据集,http://arxiv.org/abs/2506.00267v1
1264,"Large Language Models (LLMs) have shown impressive capability in language generation and understanding, but their tendency to hallucinate and produce factually incorrect information remains a key limitation. To verify LLM-generated contents and claims from other sources, traditional verification approaches often rely on holistic models that assign a single factuality label to complex claims, potentially obscuring nuanced errors. In this paper, we advocate for a shift towards fine-grained verification, where complex claims are broken down into smaller sub-claims for individual verification, allowing for more precise identification of inaccuracies, improved transparency, and reduced ambiguity in evidence retrieval. However, generating sub-claims poses challenges, such as maintaining context and ensuring semantic equivalence with respect to the original claim. We introduce FactLens, a benchmark for evaluating fine-grained fact verification, with metrics and automated evaluators of sub-claim quality. The benchmark data is manually curated to ensure high-quality ground truth. Our results show alignment between automated FactLens evaluators and human judgments, and we discuss the impact of sub-claim characteristics on the overall verification performance.",,"Kushan Mitra, Dan Zhang, Sajjadur Rahman, Estevam Hruschka",2025-05-30T22:03:36Z,FactLens: Benchmarking Fine-Grained Fact Verification,FactLens: Benchmarking-Feinkörnungs-Faktenverifizierung,事实:确定精细核查事实的基准,http://arxiv.org/abs/2411.05980v3
1265,"A brief, fluent, and relevant summary can be helpful during program comprehension; however, such a summary does require significant human effort to produce. Often, good summaries are unavailable in software projects, which makes maintenance more difficult. There has been a considerable body of research into automated AI-based methods, using Large Language models (LLMs), to generate summaries of code; there also has been quite a bit of work on ways to measure the performance of such summarization methods, with special attention paid to how closely these AI-generated summaries resemble a summary a human might have produced. Measures such as BERTScore and BLEU have been suggested and evaluated with human-subject studies.   However, LLM-generated summaries can be inaccurate, incomplete, etc.: generally, too dissimilar to one that a good developer might write. Given an LLM-generated code summary, how can a user rationally judge if a summary is sufficiently good and reliable? Given just some input source code, and an LLM-generated summary, existing approaches can help judge brevity, fluency and relevance of the summary; however, it's difficult to gauge whether an LLM-generated summary sufficiently resembles what a human might produce, without a ""golden"" human-produced summary to compare against. We study this resemblance question as calibration problem: given just the code & the summary from an LLM, can we compute a confidence measure, that provides a reliable indication of whether the summary sufficiently resembles what a human would have produced in this situation? We examine this question using several LLMs, for several languages, and in several different settings. Our investigation suggests approaches to provide reliable predictions of the likelihood that an LLM-generated summary would sufficiently resemble a summary a human might write for the same code.",,"Yuvraj Virk, Premkumar Devanbu, Toufique Ahmed",2025-05-30T21:55:33Z,Calibration of Large Language Models on Code Summarization,Kalibrierung von großen Sprachmodellen zur Code-Zusammenfassung,校准关于代码汇总的大语言模型,http://arxiv.org/abs/2404.19318v3
1266,"As large language models (LLMs) become increasingly integrated into hiring processes, concerns about fairness have gained prominence. When applying for jobs, companies often request/require demographic information, including gender, race, and disability or veteran status. This data is collected to support diversity and inclusion initiatives, but when provided to LLMs, especially disability-related information, it raises concerns about potential biases in candidate selection outcomes. Many studies have highlighted how disability can impact CV screening, yet little research has explored the specific effect of voluntarily disclosed information on LLM-driven candidate selection. This study seeks to bridge that gap. When candidates shared identical gender, race, qualifications, experience, and backgrounds, and sought jobs with minimal employment rate gaps between individuals with and without disabilities (e.g., Cashier, Software Developer), LLMs consistently favored candidates who disclosed that they had no disability. Even in cases where candidates chose not to disclose their disability status, the LLMs were less likely to select them compared to those who explicitly stated they did not have a disability.",,"Mahammed Kamruzzaman, Gene Louis Kim",2025-05-30T21:44:17Z,The Impact of Disability Disclosure on Fairness and Bias in LLM-Driven   Candidate Selection,Die Auswirkungen der Offenlegung von Behinderung auf Fairness und Bias bei der Auswahl von LLM-getriebenen Kandidaten,残疾披露对LLM-Driven候选人甄选中的公平和偏见的影响,http://arxiv.org/abs/2506.00256v1
1267,"Should LLMs generate language that makes them seem human? Human-like language might improve user experience, but might also lead to deception, overreliance, and stereotyping. Assessing these potential impacts requires a systematic way to measure human-like tone in LLM outputs. We introduce HumT and SocioT, metrics for human-like tone and other dimensions of social perceptions in text data based on relative probabilities from an LLM. By measuring HumT across preference and usage datasets, we find that users prefer less human-like outputs from LLMs in many contexts. HumT also offers insights into the perceptions and impacts of anthropomorphism: human-like LLM outputs are highly correlated with warmth, social closeness, femininity, and low status, which are closely linked to the aforementioned harms. We introduce DumT, a method using HumT to systematically control and reduce the degree of human-like tone while preserving model performance. DumT offers a practical approach for mitigating risks associated with anthropomorphic language generation.",,"Myra Cheng, Sunny Yu, Dan Jurafsky",2025-05-30T21:38:03Z,HumT DumT: Measuring and controlling human-like language in LLMs,HumT DumT: Messung und Kontrolle der menschlichen Sprache in LLMs,HumT DumT:测量和控制LLMM中类似人的语言,http://arxiv.org/abs/2502.13259v2
1268,"There has been a surge of interest in harnessing the reasoning capabilities of Large Language Models (LLMs) to accelerate scientific discovery. While existing approaches rely on grounding the discovery process within the relevant literature, effectiveness varies significantly with the quality and nature of the retrieved literature. We address the challenge of retrieving prior work whose concepts can inspire solutions for a given research problem, a task we define as Methodology Inspiration Retrieval (MIR). We construct a novel dataset tailored for training and evaluating retrievers on MIR, and establish baselines. To address MIR, we build the Methodology Adjacency Graph (MAG); capturing methodological lineage through citation relationships. We leverage MAG to embed an ""intuitive prior"" into dense retrievers for identifying patterns of methodological inspiration beyond superficial semantic similarity. This achieves significant gains of +5.4 in Recall@3 and +7.8 in Mean Average Precision (mAP) over strong baselines. Further, we adapt LLM-based re-ranking strategies to MIR, yielding additional improvements of +4.5 in Recall@3 and +4.8 in mAP. Through extensive ablation studies and qualitative analyses, we exhibit the promise of MIR in enhancing automated scientific discovery and outline avenues for advancing inspiration-driven retrieval.",,"Aniketh Garikaparthi, Manasi Patwardhan, Aditya Sanjiv Kanade, Aman Hassan, Lovekesh Vig, Arman Cohan",2025-05-30T21:33:03Z,MIR: Methodology Inspiration Retrieval for Scientific Research Problems,MIR: Methodologie Inspiration Retrieval für wissenschaftliche Forschungsprobleme,MIR: 科学研究问题求求方法,http://arxiv.org/abs/2506.00249v1
1269,"Hallucination in large language models (LLMs) can be detected by assessing the uncertainty of model outputs, typically measured using entropy. Semantic entropy (SE) enhances traditional entropy estimation by quantifying uncertainty at the semantic cluster level. However, as modern LLMs generate longer one-sentence responses, SE becomes less effective because it overlooks two crucial factors: intra-cluster similarity (the spread within a cluster) and inter-cluster similarity (the distance between clusters). To address these limitations, we propose a simple black-box uncertainty quantification method inspired by nearest neighbor estimates of entropy. Our approach can also be easily extended to white-box settings by incorporating token probabilities. Additionally, we provide theoretical results showing that our method generalizes semantic entropy. Extensive empirical results demonstrate its effectiveness compared to semantic entropy across two recent LLMs (Phi3 and Llama3) and three common text generation tasks: question answering, text summarization, and machine translation. Our code is available at https://github.com/BigML-CS-UCLA/SNNE.",,"Dang Nguyen, Ali Payani, Baharan Mirzasoleiman",2025-05-30T21:21:05Z,Beyond Semantic Entropy: Boosting LLM Uncertainty Quantification with   Pairwise Semantic Similarity,Beyond Semantic Entropy: Steigerung der LLM Uncertainty Quantification mit paarweise semantischer Ähnlichkeit,超越语义 Entrop : 推进 LLM 具有对等语义相似性的不确定性量化,http://arxiv.org/abs/2506.00245v1
1270,"The integration of large language models (LLMs) into global applications necessitates effective cultural alignment for meaningful and culturally-sensitive interactions. Current LLMs often lack the nuanced understanding required for diverse cultural contexts, and adapting them typically involves costly full fine-tuning. To address this, we introduce a novel soft prompt fine-tuning framework that enables efficient and modular cultural alignment. Our method utilizes vectorized prompt tuning to dynamically route queries to a committee of culturally specialized 'expert' LLM configurations, created by optimizing soft prompt embeddings without altering the base model's parameters. Extensive experiments demonstrate that our framework significantly enhances cultural sensitivity and adaptability, improving alignment scores from 0.208 to 0.820, offering a robust solution for culturally-aware LLM deployment. This research paves the way for subsequent investigations into enhanced cultural coverage and dynamic expert adaptation, crucial for realizing autonomous AI with deeply nuanced understanding in a globally interconnected world.",,"Shuai Feng, Wei-Chuang Chan, Srishti Chouhan, Junior Francisco Garcia Ayala, Srujananjali Medicherla, Kyle Clark, Mingwei Shi",2025-05-30T21:16:25Z,Whispers of Many Shores: Cultural Alignment through Collaborative   Cultural Expertise,Whispers of Many Shores: Kulturelle Ausrichtung durch kollaborative kulturelle Expertise,许多海岸的耳语:通过合作文化专门知识实现文化和谐,http://arxiv.org/abs/2506.00242v1
1271,"Natural disasters usually affect vast areas and devastate infrastructures. Performing a timely and efficient response is crucial to minimize the impact on affected communities, and data-driven approaches are the best choice. Visual question answering (VQA) models help management teams to achieve in-depth understanding of damages. However, recently published models do not possess the ability to answer open-ended questions and only select the best answer among a predefined list of answers. If we want to ask questions with new additional possible answers that do not exist in the predefined list, the model needs to be fin-tuned/retrained on a new collected and annotated dataset, which is a time-consuming procedure. In recent years, large-scale Vision-Language Models (VLMs) have earned significant attention. These models are trained on extensive datasets and demonstrate strong performance on both unimodal and multimodal vision/language downstream tasks, often without the need for fine-tuning. In this paper, we propose a VLM-based zero-shot VQA (ZeShot-VQA) method, and investigate the performance of on post-disaster FloodNet dataset. Since the proposed method takes advantage of zero-shot learning, it can be applied on new datasets without fine-tuning. In addition, ZeShot-VQA is able to process and generate answers that has been not seen during the training procedure, which demonstrates its flexibility.",,"Ehsan Karimi, Maryam Rahnemoonfar",2025-05-30T21:15:11Z,ZeShot-VQA: Zero-Shot Visual Question Answering Framework with Answer   Mapping for Natural Disaster Damage Assessment,ZeShot-VQA: Zero-Shot Visual Question Answering Framework mit Antwortmapping für die Bewertung von Naturkatastrophen,ZeShot-VQA: 自然灾害损害评估答案绘图的零热视觉问题解答框架,http://arxiv.org/abs/2506.00238v1
1272,"Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, offer compact and effective alternatives to full model fine-tuning by introducing low-rank updates to pretrained weights. However, most existing approaches rely on global low-rank structures, which can overlook spatial patterns spread across the parameter space. In this work, we propose Localized LoRA, a generalized framework that models weight updates as a composition of low-rank matrices applied to structured blocks of the weight matrix. This formulation enables dense, localized updates throughout the parameter space-without increasing the total number of trainable parameters. We provide a formal comparison between global, diagonal-local, and fully localized low-rank approximations, and show that our method consistently achieves lower approximation error under matched parameter budgets. Experiments on both synthetic and practical settings demonstrate that Localized LoRA offers a more expressive and adaptable alternative to existing methods, enabling efficient fine-tuning with improved performance.",,Babak Barazandeh,2025-05-30T21:13:23Z,Localized LoRA: A Structured Low-Rank Approximation for Efficient   Fine-Tuning,Lokalisierte LoRA: Eine strukturierte Low-Rank-Annäherung für effiziente Feinsteuerung,LoRA 本地化 LoRA: 一种结构化低拉近于低拉差的高效微调,http://arxiv.org/abs/2506.00236v1
1273,"Healthcare decision-making represents one of the most challenging domains for Artificial Intelligence (AI), requiring the integration of diverse knowledge sources, complex reasoning, and various external analytical tools. Current AI systems often rely on either task-specific models, which offer limited adaptability, or general language models without grounding with specialized external knowledge and tools. We introduce MedOrch, a novel framework that orchestrates multiple specialized tools and reasoning agents to provide comprehensive medical decision support. MedOrch employs a modular, agent-based architecture that facilitates the flexible integration of domain-specific tools without altering the core system. Furthermore, it ensures transparent and traceable reasoning processes, enabling clinicians to meticulously verify each intermediate step underlying the system's recommendations. We evaluate MedOrch across three distinct medical applications: Alzheimer's disease diagnosis, chest X-ray interpretation, and medical visual question answering, using authentic clinical datasets. The results demonstrate MedOrch's competitive performance across these diverse medical tasks. Notably, in Alzheimer's disease diagnosis, MedOrch achieves an accuracy of 93.26%, surpassing the state-of-the-art baseline by over four percentage points. For predicting Alzheimer's disease progression, it attains a 50.35% accuracy, marking a significant improvement. In chest X-ray analysis, MedOrch exhibits superior performance with a Macro AUC of 61.2% and a Macro F1-score of 25.5%. Moreover, in complex multimodal visual question answering (Image+Table), MedOrch achieves an accuracy of 54.47%. These findings underscore MedOrch's potential to advance healthcare AI by enabling reasoning-driven tool utilization for multimodal medical data processing and supporting intricate cognitive tasks in clinical decision-making.",,"Yexiao He, Ang Li, Boyi Liu, Zhewei Yao, Yuxiong He",2025-05-30T21:13:12Z,MedOrch: Medical Diagnosis with Tool-Augmented Reasoning Agents for   Flexible Extensibility,MedOrch: Medizinische Diagnose mit Tool-Augmented Reasoning Agents für flexible Erweiterbarkeit,"MedOrch:与工具强化推理剂进行医学诊断,以利灵活扩展",http://arxiv.org/abs/2506.00235v1
1274,"Do autoregressive Transformer language models require explicit positional encodings (PEs)? The answer is 'no' provided they have more than one layer -- they can distinguish sequences with permuted tokens without the need for explicit PEs. This follows from the fact that a cascade of (permutation invariant) set processors can collectively exhibit sequence-sensitive behavior in the autoregressive setting. This property has been known since early efforts (contemporary with GPT-2) adopting the Transformer for language modeling. However, this result does not appear to have been well disseminated, leading to recent rediscoveries. This may be partially due to a sudden growth of the language modeling community after the advent of GPT-2/3, but perhaps also due to the lack of a clear explanation in prior work, despite being commonly understood by practitioners in the past. Here we review the long-forgotten explanation why explicit PEs are nonessential for multi-layer autoregressive Transformers (in contrast, one-layer models require PEs to discern order information of their inputs), as well as the origin of this result, and hope to re-establish it as a common knowledge.",,Kazuki Irie,2025-05-30T21:12:49Z,Why Are Positional Encodings Nonessential for Deep Autoregressive   Transformers? Revisiting a Petroglyph,Warum sind Positionskodierungen für tiefe autoregressive Transformer nicht wesentlich?,为什么定位编码对深自动递减变异器来说不必要?,http://arxiv.org/abs/2501.00659v2
1275,"Retrieval-Augmented Generation (RAG) systems are increasingly diverse, yet many suffer from monolithic designs that tightly couple core functions like query reformulation, retrieval, reasoning, and verification. This limits their interpretability, systematic evaluation, and targeted improvement, especially for complex multi-hop question answering. We introduce ComposeRAG, a novel modular abstraction that decomposes RAG pipelines into atomic, composable modules. Each module, such as Question Decomposition, Query Rewriting, Retrieval Decision, and Answer Verification, acts as a parameterized transformation on structured inputs/outputs, allowing independent implementation, upgrade, and analysis. To enhance robustness against errors in multi-step reasoning, ComposeRAG incorporates a self-reflection mechanism that iteratively revisits and refines earlier steps upon verification failure. Evaluated on four challenging multi-hop QA benchmarks, ComposeRAG consistently outperforms strong baselines in both accuracy and grounding fidelity. Specifically, it achieves up to a 15% accuracy improvement over fine-tuning-based methods and up to a 5% gain over reasoning-specialized pipelines under identical retrieval conditions. Crucially, ComposeRAG significantly enhances grounding: its verification-first design reduces ungrounded answers by over 10% in low-quality retrieval settings, and by approximately 3% even with strong corpora. Comprehensive ablation studies validate the modular architecture, demonstrating distinct and additive contributions from each component. These findings underscore ComposeRAG's capacity to deliver flexible, transparent, scalable, and high-performing multi-hop reasoning with improved grounding and interpretability.",,"Ruofan Wu, Youngwon Lee, Fan Shu, Danmei Xu, Seung-won Hwang, Zhewei Yao, Yuxiong He, Feng Yan",2025-05-30T21:10:30Z,ComposeRAG: A Modular and Composable RAG for Corpus-Grounded Multi-Hop   Question Answering,ComposeRAG: Modulare und komponierbare RAG für Corpus-Grounded Multi-Hop Question Answering,ComposeRAG: 用于 Corpus 四舍五入多组问题解答的模块式和组合式RAG,http://arxiv.org/abs/2506.00232v1
1276,"Language models (LMs) rely on their parametric knowledge augmented with relevant contextual knowledge for certain tasks, such as question answering. However, the contextual knowledge can contain private information that may be leaked when answering queries, and estimating this privacy leakage is not well understood. A straightforward approach of directly comparing an LM's output to the contexts can overestimate the privacy risk, since the LM's parametric knowledge might already contain the augmented contextual knowledge. To this end, we introduce *context influence*, a metric that builds on differential privacy, a widely-adopted privacy notion, to estimate the privacy leakage of contextual knowledge during decoding. Our approach effectively measures how each subset of the context influences an LM's response while separating the specific parametric knowledge of the LM. Using our context influence metric, we demonstrate that context privacy leakage occurs when contextual knowledge is out of distribution with respect to parametric knowledge. Moreover, we experimentally demonstrate how context influence properly attributes the privacy leakage to augmented contexts, and we evaluate how factors -- such as model size, context size, generation position, etc. -- affect context privacy leakage. The practical implications of our results will inform practitioners of the privacy risk associated with augmented contextual knowledge.",,"James Flemings, Bo Jiang, Wanrong Zhang, Zafar Takhirov, Murali Annavaram",2025-05-30T21:01:01Z,Estimating Privacy Leakage of Augmented Contextual Knowledge in Language   Models,Schätzung der Privatsphäre Leckage von Augmented Contextual Knowledge in Language Models,估计语言模型中增加的背景知识的隐私疏漏,http://arxiv.org/abs/2410.03026v2
1277,"Recent English Common Crawl datasets like FineWeb-Edu and DCLM achieved significant benchmark gains via aggressive model-based filtering, but at the cost of removing 90% of data. This limits their suitability for long token horizon training, such as 15T tokens for Llama 3.1. In this paper, we show how to achieve better trade-offs between accuracy and data quantity by a combination of classifier ensembling, synthetic data rephrasing, and reduced reliance on heuristic filters. When training 8B parameter models for 1T tokens, using a high-quality subset of our data improves MMLU by 5.6 over DCLM, demonstrating the efficacy of our methods for boosting accuracies over a relatively short token horizon. Furthermore, our full 6.3T token dataset matches DCLM on MMLU, but contains four times more unique real tokens than DCLM. This unlocks state-of-the-art training over a long token horizon: an 8B parameter model trained for 15T tokens, of which 7.2T came from our dataset, is better than the Llama 3.1 8B model: +5 on MMLU, +3.1 on ARC-Challenge, and +0.5 on average across ten diverse tasks. The dataset is available at https://data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/index.html",,"Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro",2025-05-30T20:49:42Z,Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon   Pretraining Dataset,Nemotron-CC: Umwandlung von Crawl in einen raffinierten Long-Horizon-Vortraining-Datensatz,尼米天-CC:将普通爬行转换成经过精炼的长毛利区预培训数据集,http://arxiv.org/abs/2412.02595v2
1278,"This paper argues that the relationship between lexical identity and prosody -- one well-studied parameter of linguistic variation -- can be characterized using information theory. We predict that languages that use prosody to make lexical distinctions should exhibit a higher mutual information between word identity and prosody, compared to languages that don't. We test this hypothesis in the domain of pitch, which is used to make lexical distinctions in tonal languages, like Cantonese. We use a dataset of speakers reading sentences aloud in ten languages across five language families to estimate the mutual information between the text and their pitch curves. We find that, across languages, pitch curves display similar amounts of entropy. However, these curves are easier to predict given their associated text in the tonal languages, compared to pitch- and stress-accent languages, and thus the mutual information is higher in these languages, supporting our hypothesis. Our results support perspectives that view linguistic typology as gradient, rather than categorical.",,"Ethan Gotlieb Wilcox, Cui Ding, Giovanni Acampa, Tiago Pimentel, Alex Warstadt, Tamar I. Regev",2025-05-30T20:49:17Z,"Using Information Theory to Characterize Prosodic Typology: The Case of   Tone, Pitch-Accent and Stress-Accent","Verwendung von Informationstheorie zur Charakterisierung der prosodischen Typologie: Der Fall von Ton, Pitch-Accent und Stress-Accent",使用信息理论来说明Prosod Tymlogy特征:Tone、Pitch-Accent和压力-Accent的案例,http://arxiv.org/abs/2505.07659v2
1279,"The development of high-performing, robust, and reliable speech technologies depends on large, high-quality datasets. However, African languages -- including our focus, Igbo, Hausa, and Yoruba -- remain under-represented due to insufficient data. Popular voice-enabled technologies do not support any of the 2000+ African languages, limiting accessibility for circa one billion people. While previous dataset efforts exist for the target languages, they lack the scale and diversity needed for robust speech models. To bridge this gap, we introduce the NaijaVoices dataset, a 1,800-hour speech-text dataset with 5,000+ speakers. We outline our unique data collection approach, analyze its acoustic diversity, and demonstrate its impact through finetuning experiments on automatic speech recognition, averagely achieving 75.86% (Whisper), 52.06% (MMS), and 42.33% (XLSR) WER improvements. These results highlight NaijaVoices' potential to advance multilingual speech processing for African languages.",,"Chris Emezue, NaijaVoices Community, Busayo Awobade, Abraham Owodunni, Handel Emezue, Gloria Monica Tobechukwu Emezue, Nefertiti Nneoma Emezue, Sewade Ogun, Bunmi Akinremi, David Ifeoluwa Adelani, Chris Pal",2025-05-30T20:40:58Z,"The NaijaVoices Dataset: Cultivating Large-Scale, High-Quality,   Culturally-Rich Speech Data for African Languages","Der NaijaVoices-Datensatz: Pflege von großformatigen, qualitativ hochwertigen, kulturell-richschen Sprachdaten für afrikanische Sprachen",NaijaVoices数据集:培养非洲语言的大型、高质量、文化-Rich语音数据,http://arxiv.org/abs/2505.20564v2
1280,"Accurate intent classification is critical for efficient routing in customer service, ensuring customers are connected with the most suitable agents while reducing handling times and operational costs. However, as companies expand their product lines, intent classification faces scalability challenges due to the increasing number of intents and variations in taxonomy across different verticals. In this paper, we introduce REIC, a Retrieval-augmented generation Enhanced Intent Classification approach, which addresses these challenges effectively. REIC leverages retrieval-augmented generation (RAG) to dynamically incorporate relevant knowledge, enabling precise classification without the need for frequent retraining. Through extensive experiments on real-world datasets, we demonstrate that REIC outperforms traditional fine-tuning, zero-shot, and few-shot methods in large-scale customer service settings. Our results highlight its effectiveness in both in-domain and out-of-domain scenarios, demonstrating its potential for real-world deployment in adaptive and large-scale intent classification systems.",,"Ziji Zhang, Michael Yang, Zhiyu Chen, Yingying Zhuang, Shu-Ting Pi, Qun Liu, Rajashekar Maragoud, Vy Nguyen, Anurag Beniwal",2025-05-30T20:32:10Z,REIC: RAG-Enhanced Intent Classification at Scale,REIC: RAG-erweiterte Intent-Klassifikation im Maßstab,REIC: 增强的ROAG 规模指数分类,http://arxiv.org/abs/2506.00210v1
1281,"Large Language Models (LLMs), such as the GPT-4 and LLaMA families, have demonstrated considerable success across diverse tasks, including multiple-choice questions (MCQs). However, these models exhibit a positional bias, particularly an even worse anchored bias in the GPT-2 family, where they consistently favour the first choice 'A' in MCQs during inference. This anchored bias challenges the integrity of GPT-2's decision-making process, as it skews performance based on the position rather than the content of the choices in MCQs. In this study, we utilise the mechanistic interpretability approach to identify the internal modules within GPT-2 models responsible for this bias. We focus on the Multi-Layer Perceptron (MLP) layers and attention heads, using the ""logit lens"" method to trace and modify the specific value vectors that contribute to the bias. By updating these vectors within MLP and recalibrating attention patterns to neutralise the preference for the first choice 'A', we effectively mitigate the anchored bias. Our interventions not only mitigate the bias but also improve the overall MCQ prediction accuracy for the GPT-2 family across various datasets. This work represents the first comprehensive mechanistic analysis of anchored bias from the failing cases in MCQs within the GPT-2 models, introducing targeted, minimal-intervention strategies that significantly enhance GPT2 model robustness and accuracy in MCQs. Our code is available at https://github.com/ruizheliUOA/Anchored_Bias_GPT2.",,"Ruizhe Li, Yanjun Gao",2025-05-30T20:24:51Z,Anchored Answers: Unravelling Positional Bias in GPT-2's Multiple-Choice   Questions,Verankerte Antworten: Entwirren von Positional Bias in den Multiple-Choice-Fragen von GPT-2,精心策划的答复:GPT-2多选择问题中未加破坏的定位偏见,http://arxiv.org/abs/2405.03205v3
1282,"Fill-in-the-Middle (FIM) is a common pretraining method for code LLMs, where models complete code segments given surrounding context. However, existing LLMs treat code as plain text and mask random character spans. We propose and evaluate AST-FIM, a pretraining strategy that leverages Abstract Syntax Trees (ASTs) to mask complete syntactic structures at scale, ensuring coherent training examples better aligned with universal code structures and common code editing patterns such as blocks, expressions, or functions. To evaluate real-world fill-in-the-middle (FIM) programming tasks, we introduce Real-FIM-Eval, a benchmark derived from 30,000+ GitHub commits across 12 languages. On infilling tasks, experiments on 1B and 8B parameter models show that AST-FIM is particularly beneficial for real-world code editing as it outperforms standard random-character FIM by up to 5 pts on standard FIM benchmarks. Our code is publicly available at https://github.com/gonglinyuan/ast_fim.",,"Linyuan Gong, Alvin Cheung, Mostafa Elhoushi, Sida Wang",2025-05-30T20:19:39Z,Structure-Aware Fill-in-the-Middle Pretraining for Code,Structure-Aware-Fill-in-the-Middle-Vorschulung für Code,代码结构软件填入中中预科培训,http://arxiv.org/abs/2506.00204v1
1283,"Radiology reports are critical for clinical decision-making but often lack a standardized format, limiting both human interpretability and machine learning (ML) applications. While large language models (LLMs) have shown strong capabilities in reformatting clinical text, their high computational requirements, lack of transparency, and data privacy concerns hinder practical deployment. To address these challenges, we explore lightweight encoder-decoder models (<300M parameters)-specifically T5 and BERT2BERT-for structuring radiology reports from the MIMIC-CXR and CheXpert Plus datasets. We benchmark these models against eight open-source LLMs (1B-70B), adapted using prefix prompting, in-context learning (ICL), and low-rank adaptation (LoRA) finetuning. Our best-performing lightweight model outperforms all LLMs adapted using prompt-based techniques on a human-annotated test set. While some LoRA-finetuned LLMs achieve modest gains over the lightweight model on the Findings section (BLEU 6.4%, ROUGE-L 4.8%, BERTScore 3.6%, F1-RadGraph 1.1%, GREEN 3.6%, and F1-SRR-BERT 4.3%), these improvements come at the cost of substantially greater computational resources. For example, LLaMA-3-70B incurred more than 400 times the inference time, cost, and carbon emissions compared to the lightweight model. These results underscore the potential of lightweight, task-specific models as sustainable and privacy-preserving solutions for structuring clinical text in resource-constrained healthcare settings.",,"Johannes Moll, Louisa Fay, Asfandyar Azhar, Sophie Ostmeier, Tim Lueth, Sergios Gatidis, Curtis Langlotz, Jean-Benoit Delbrouck",2025-05-30T20:12:51Z,Structuring Radiology Reports: Challenging LLMs with Lightweight Models,Structuring Radiology Reports: Herausfordernde LLMs mit Leichtbaumodellen,结构化放射学报告:用轻量级模型对LMS提出挑战,http://arxiv.org/abs/2506.00200v1
1284,"Current LLMs are trained to refuse potentially harmful input queries regardless of whether users actually had harmful intents, causing a tradeoff between safety and user experience. Through a study of 480 participants evaluating 3,840 query-response pairs, we examine how different refusal strategies affect user perceptions across varying motivations. Our findings reveal that response strategy largely shapes user experience, while actual user motivation has negligible impact. Partial compliance -- providing general information without actionable details -- emerges as the optimal strategy, reducing negative user perceptions by over 50% to flat-out refusals. Complementing this, we analyze response patterns of 9 state-of-the-art LLMs and evaluate how 6 reward models score different refusal strategies, demonstrating that models rarely deploy partial compliance naturally and reward models currently undervalue it. This work demonstrates that effective guardrails require focusing on crafting thoughtful refusals rather than detecting intent, offering a path toward AI safety mechanisms that ensure both safety and sustained user engagement.",,"Mingqian Zheng, Wenjia Hu, Patrick Zhao, Motahhare Eslami, Jena D. Hwang, Faeze Brahman, Carolyn Rose, Maarten Sap",2025-05-30T20:07:07Z,Let Them Down Easy! Contextual Effects of LLM Guardrails on User   Perceptions and Preferences,Lassen Sie sie einfach herunter! Kontextuelle Effekte von LLM-Guardrails auf Benutzerwahrnehmungen und Einstellungen,让Them 轻松下来! LLLM 守护器对用户感知和首选项的背景效果,http://arxiv.org/abs/2506.00195v1
1285,"This paper target in addressing the challenges of underthinking and overthinking in long chain-of-thought (CoT) reasoning for Large Reasoning Models (LRMs) by introducing Reasoning Control Fields (RCF)--a novel test-time approach that injects structured control signals to guide reasoning from a tree search perspective. RCF enables models to adjust reasoning effort according to given control conditions when solving complex tasks. Additionally, we present the Control-R-4K dataset, which consists of challenging problems annotated with detailed reasoning processes and corresponding control fields. To further enhance reasoning control, we propose a Conditional Distillation Finetuning (CDF) method, which trains model--particularly Control-R-32B--to effectively adjust reasoning effort during test time. Experimental results on benchmarks such as AIME2024 and MATH500 demonstrate that our approach achieves state-of-the-art performance at the 32B scale while enabling a controllable Long CoT reasoning process (L-CoT). Overall, this work introduces an effective paradigm for controllable test-time scaling reasoning.",,"Di Zhang, Weida Wang, Junxian Li, Xunzhi Wang, Jiatong Li, Jianbo Wu, Jingdi Lei, Haonan He, Peng Ye, Shufei Zhang, Wanli Ouyang, Yuqiang Li, Dongzhan Zhou",2025-05-30T19:59:44Z,Control-R: Towards controllable test-time scaling,Control-R: Zur steuerbaren Testzeitskalierung,Control- R: 迈向可控测试时间缩放,http://arxiv.org/abs/2506.00189v1
1286,"Detecting factual inconsistencies in summarization is critical, yet existing benchmarks lack the necessary challenge and interpretability for robust evaluation. In this paper, we introduce SummExecEdit, a novel pipeline and benchmark leveraging executable edits to assess models on their ability to both detect factual errors and provide accurate explanations. The top-performing model, Claude3-Opus, achieves a joint detection and explanation score of only 0.49 in our benchmark, with individual scores of 0.67 for detection and 0.73 for explanation. We conduct detailed evaluations to assess the current state of models in this field and find that more than half of the 20+ LLMs in our study struggle with over 30% of the SummExecEdit benchmark. Additionally, we identify four primary types of explanation errors, with 45.4% of them involving a focus on completely unrelated parts of the summary.",,"Onkar Thorat, Philippe Laban, Chien-Sheng Wu",2025-05-30T19:47:02Z,SummExecEdit: A Factual Consistency Benchmark in Summarization with   Executable Edits,SummExecEdit: Ein Factual Consistency Benchmark in der Zusammenfassung mit ausführbaren Edits,SummExeced: 带有可执行编辑的汇总中的事实一致性基准,http://arxiv.org/abs/2412.13378v2
1287,"Transducer models have emerged as a promising choice for end-to-end ASR systems, offering a balanced trade-off between recognition accuracy, streaming capabilities, and inference speed in greedy decoding. However, beam search significantly slows down Transducers due to repeated evaluations of key network components, limiting practical applications. This paper introduces a universal method to accelerate beam search for Transducers, enabling the implementation of two optimized algorithms: ALSD++ and AES++. The proposed method utilizes batch operations, a tree-based hypothesis structure, novel blank scoring for enhanced shallow fusion, and CUDA graph execution for efficient GPU inference. This narrows the speed gap between beam and greedy modes to only 10-20% for the whole system, achieves 14-30% relative improvement in WER compared to greedy decoding, and improves shallow fusion for low-resource up to 11% compared to existing implementations. All the algorithms are open sourced.",,"Lilit Grigoryan, Vladimir Bataev, Andrei Andrusenko, Hainan Xu, Vitaly Lavrukhin, Boris Ginsburg",2025-05-30T19:42:48Z,Pushing the Limits of Beam Search Decoding for Transducer-based ASR   models,Drücke die Grenzen der Beam Search Decodierung für Transducer-basierte ASR-Modelle,"推推星束搜索极限,以ASR模型为基础",http://arxiv.org/abs/2506.00185v1
1288,"The development of speech foundation models (SFMs) like Whisper and SeamlessM4T has significantly advanced the field of speech processing. However, their closed nature--with inaccessible training data and code--poses major reproducibility and fair evaluation challenges. While other domains have made substantial progress toward open science by developing fully transparent models trained on open-source (OS) code and data, similar efforts in speech remain limited. To fill this gap, we introduce FAMA, the first family of open science SFMs for English and Italian, trained on 150k+ hours of OS speech data. Moreover, we present a new dataset containing 16k hours of cleaned and pseudo-labeled speech for both languages. Results show that FAMA achieves competitive performance compared to existing SFMs while being up to 8 times faster. All artifacts, including code, datasets, and models, are released under OS-compliant licenses, promoting openness in speech technology research.",,"Sara Papi, Marco Gaido, Luisa Bentivogli, Alessio Brutti, Mauro Cettolo, Roberto Gretter, Marco Matassoni, Mohamed Nabih, Matteo Negri",2025-05-30T19:40:00Z,FAMA: The First Large-Scale Open-Science Speech Foundation Model for   English and Italian,FAMA: Das erste großformatige Open-Science-Sprechstiftungsmodell für Englisch und Italienisch,FAMA:英语和意大利语第一个大型开放科学演讲基金会模型,http://arxiv.org/abs/2505.22759v2
1289,"Existing paradigms for ensuring AI safety, such as guardrail models and alignment training, often compromise either inference efficiency or development flexibility. We introduce Disentangled Safety Adapters (DSA), a novel framework addressing these challenges by decoupling safety-specific computations from a task-optimized base model. DSA utilizes lightweight adapters that leverage the base model's internal representations, enabling diverse and flexible safety functionalities with minimal impact on inference cost. Empirically, DSA-based safety guardrails substantially outperform comparably sized standalone models, notably improving hallucination detection (0.88 vs. 0.61 AUC on Summedits) and also excelling at classifying hate speech (0.98 vs. 0.92 on ToxiGen) and unsafe model inputs and responses (0.93 vs. 0.90 on AEGIS2.0 & BeaverTails). Furthermore, DSA-based safety alignment allows dynamic, inference-time adjustment of alignment strength and a fine-grained trade-off between instruction following performance and model safety. Importantly, combining the DSA safety guardrail with DSA safety alignment facilitates context-dependent alignment strength, boosting safety on StrongReject by 93% while maintaining 98% performance on MTBench -- a total reduction in alignment tax of 8 percentage points compared to standard safety alignment fine-tuning. Overall, DSA presents a promising path towards more modular, efficient, and adaptable AI safety and alignment.",,"Kundan Krishna, Joseph Y Cheng, Charles Maalouf, Leon A Gatys",2025-05-30T19:11:52Z,Disentangled Safety Adapters Enable Efficient Guardrails and Flexible   Inference-Time Alignment,Entwirrte Sicherheitsadapter ermöglichen effiziente Guardrails und flexible Inferenz-Time Alignment,使高效护卫力和灵活推断-时间一致,http://arxiv.org/abs/2506.00166v1
1290,"The growing popularity of social deduction game systems for both business applications and AI research has greatly benefited from the rapid advancements in Large Language Models (LLMs), which now demonstrate stronger reasoning and persuasion capabilities. Especially with the raise of DeepSeek R1 and V3 models, LLMs should enable a more engaging experience for human players in LLM-agent-based social deduction games like Werewolf. Previous works either fine-tuning, advanced prompting engineering, or additional experience pool to achieve engaging text-format Werewolf game experience. We propose a novel yet straightforward LLM-based Werewolf game system with tuned Text-to-Speech(TTS) models designed for enhanced compatibility with various LLM models, and improved user engagement. We argue with ever enhancing LLM reasoning, extra components will be unnecessary in the case of Werewolf.",,"Qihui Fan, Enfu Nan, Wenbo Li, Lei Lu, Pu Zhao, Yanzhi Wang",2025-05-30T18:58:57Z,Werewolf: A Straightforward Game Framework with TTS for Improved User   Engagement,Werwolf: Ein einfaches Game-Framework mit TTS für verbessertes Benutzerengagement,"狼人:与TTS的直向式游戏框架,以改善用户参与",http://arxiv.org/abs/2506.00160v1
1291,"We present SimpleStories, a large synthetic story dataset in simple language, consisting of 2 million samples each in English and Japanese. Through parameterizing prompts at multiple levels of abstraction, we achieve control over story characteristics at scale, inducing syntactic and semantic diversity. Ablations on a newly trained model suite show improved sample efficiency and model interpretability compared to the TinyStories dataset. We open-source all constituent parts of model creation, hoping to enable novel ways to study the end-to-end training process. As a byproduct, we move the frontier regarding the fewest-parameter language model that outputs grammatical natural language.",,"Lennart Finke, Chandan Sreedhara, Thomas Dooms, Mat Allen, Emerald Zhang, Juan Diego Rodriguez, Noa Nabeshima, Thomas Marshall, Dan Braun",2025-05-30T18:50:03Z,Parameterized Synthetic Text Generation with SimpleStories,Parameterisierte Synthetische Textgenerierung mit SimpleStories,具有简单炉灶的 参数化合成文本生成,http://arxiv.org/abs/2504.09184v3
1292,"Sanskrit, an ancient language with a rich linguistic heritage, presents unique challenges for automatic speech recognition (ASR) due to its phonemic complexity and the phonetic transformations that occur at word junctures, similar to the connected speech found in natural conversations. Due to these complexities, there has been limited exploration of ASR in Sanskrit, particularly in the context of its poetic verses, which are characterized by intricate prosodic and rhythmic patterns. This gap in research raises the question: How can we develop an effective ASR system for Sanskrit, particularly one that captures the nuanced features of its poetic form? In this study, we introduce Vedavani, the first comprehensive ASR study focused on Sanskrit Vedic poetry. We present a 54-hour Sanskrit ASR dataset, consisting of 30,779 labelled audio samples from the Rig Veda and Atharva Veda. This dataset captures the precise prosodic and rhythmic features that define the language. We also benchmark the dataset on various state-of-the-art multilingual speech models.$^{1}$ Experimentation revealed that IndicWhisper performed the best among the SOTA models.",,"Sujeet Kumar, Pretam Ray, Abhinay Beerukuri, Shrey Kamoji, Manoj Balaji Jagadeeshan, Pawan Goyal",2025-05-30T18:36:54Z,Vedavani: A Benchmark Corpus for ASR on Vedic Sanskrit Poetry,Vedavani: Ein Benchmark Corpus für ASR auf Vedic Sanskrit Poetry,维达瓦尼:关于Veddic 梵文诗歌的ASR基准体,http://arxiv.org/abs/2506.00145v1
1293,"While personalized recommendations are often desired by users, it can be difficult in practice to distinguish cases of bias from cases of personalization: we find that models generate racially stereotypical recommendations regardless of whether the user revealed their identity intentionally through explicit indications or unintentionally through implicit cues. We demonstrate that when people use large language models (LLMs) to generate recommendations, the LLMs produce responses that reflect both what the user wants and who the user is. We argue that chatbots ought to transparently indicate when recommendations are influenced by a user's revealed identity characteristics, but observe that they currently fail to do so. Our experiments show that even though a user's revealed identity significantly influences model recommendations (p < 0.001), model responses obfuscate this fact in response to user queries. This bias and lack of transparency occurs consistently across multiple popular consumer LLMs and for four American racial groups.",,"Anjali Kantharuban, Jeremiah Milbauer, Maarten Sap, Emma Strubell, Graham Neubig",2025-05-30T18:25:09Z,Stereotype or Personalization? User Identity Biases Chatbot   Recommendations,Stereotyp oder Personalisierung? User Identity Biases Chatbot Empfehlungen,陈规定型观念或个性化?,http://arxiv.org/abs/2410.05613v2
1294,"Large language models (LLMs) have been extensively evaluated on medical question answering tasks based on licensing exams. However, real-world evaluations often depend on costly human annotators, and existing benchmarks tend to focus on isolated tasks that rarely capture the clinical reasoning or full workflow underlying medical decisions. In this paper, we introduce ER-Reason, a benchmark designed to evaluate LLM-based clinical reasoning and decision-making in the emergency room (ER)--a high-stakes setting where clinicians make rapid, consequential decisions across diverse patient presentations and medical specialties under time pressure. ER-Reason includes data from 3,984 patients, encompassing 25,174 de-identified longitudinal clinical notes spanning discharge summaries, progress notes, history and physical exams, consults, echocardiography reports, imaging notes, and ER provider documentation. The benchmark includes evaluation tasks that span key stages of the ER workflow: triage intake, initial assessment, treatment selection, disposition planning, and final diagnosis--each structured to reflect core clinical reasoning processes such as differential diagnosis via rule-out reasoning. We also collected 72 full physician-authored rationales explaining reasoning processes that mimic the teaching process used in residency training, and are typically absent from ER documentation. Evaluations of state-of-the-art LLMs on ER-Reason reveal a gap between LLM-generated and clinician-authored clinical reasoning for ER decisions, highlighting the need for future research to bridge this divide.",,"Nikita Mehandru, Niloufar Golchini, David Bamman, Travis Zack, Melanie F. Molina, Ahmed Alaa",2025-05-30T18:23:56Z,ER-REASON: A Benchmark Dataset for LLM-Based Clinical Reasoning in the   Emergency Room,ER-REASON: Ein Benchmark-Datensatz für LLM-basierte klinische Vernunft in der Notaufnahme,ER-REASON:应急室以LLM为基础的临床原因基准数据集,http://arxiv.org/abs/2505.22919v2
1295,"Personalization is essential for question answering systems that are user-centric. Despite its importance, personalization in answer generation has been relatively underexplored. This is mainly due to lack of resources for training and evaluating personalized question answering systems. We address this gap by introducing LaMP-QA -- a benchmark designed for evaluating personalized long-form answer generation. The benchmark covers questions from three major categories: (1) Arts & Entertainment, (2) Lifestyle & Personal Development, and (3) Society & Culture, encompassing over 45 subcategories in total. To assess the quality and potential impact of the LaMP-QA benchmark for personalized question answering, we conduct comprehensive human and automatic evaluations, to compare multiple evaluation strategies for evaluating generated personalized responses and measure their alignment with human preferences. Furthermore, we benchmark a number of non-personalized and personalized approaches based on open-source and proprietary large language models (LLMs). Our results show that incorporating the personalized context provided leads to performance improvements of up to 39%. The benchmark is publicly released to support future research in this area.",,"Alireza Salemi, Hamed Zamani",2025-05-30T18:16:03Z,LaMP-QA: A Benchmark for Personalized Long-form Question Answering,LaMP-QA: Ein Benchmark für die personalisierte Langform-Fragebeantwortung,LaMP-QA:个性化长方回答问题基准,http://arxiv.org/abs/2506.00137v1
1296,"Social determinants of health (SDOH) extraction from clinical text is critical for downstream healthcare analytics. Although large language models (LLMs) have shown promise, they may rely on superficial cues leading to spurious predictions. Using the MIMIC portion of the SHAC (Social History Annotation Corpus) dataset and focusing on drug status extraction as a case study, we demonstrate that mentions of alcohol or smoking can falsely induce models to predict current/past drug use where none is present, while also uncovering concerning gender disparities in model performance. We further evaluate mitigation strategies - such as prompt engineering and chain-of-thought reasoning - to reduce these false positives, providing insights into enhancing LLM reliability in health domains.",,"Fardin Ahsan Sakib, Ziwei Zhu, Karen Trister Grace, Meliha Yetisgen, Ozlem Uzuner",2025-05-30T18:11:33Z,Spurious Correlations and Beyond: Understanding and Mitigating Shortcut   Learning in SDOH Extraction with Large Language Models,Puriöse Correlationen und darüber hinaus: Verstehen und Abmildern von Shortcut-Lernen in der SDOH-Extraktion mit großen Sprachmodellen,纯洁的拼贴及其他:理解并减缓在采用大语言模型的SDOH抽取中快速学习,http://arxiv.org/abs/2506.00134v1
1297,"Variable binding -- the ability to associate variables with values -- is fundamental to symbolic computation and cognition. Although classical architectures typically implement variable binding via addressable memory, it is not well understood how modern neural networks lacking built-in binding operations may acquire this capacity. We investigate this by training a Transformer to dereference queried variables in symbolic programs where variables are assigned either numerical constants or other variables. Each program requires following chains of variable assignments up to four steps deep to find the queried value, and also contains irrelevant chains of assignments acting as distractors. Our analysis reveals a developmental trajectory with three distinct phases during training: (1) random prediction of numerical constants, (2) a shallow heuristic prioritizing early variable assignments, and (3) the emergence of a systematic mechanism for dereferencing assignment chains. Using causal interventions, we find that the model learns to exploit the residual stream as an addressable memory space, with specialized attention heads routing information across token positions. This mechanism allows the model to dynamically track variable bindings across layers, resulting in accurate dereferencing. Our results show how Transformer models can learn to implement systematic variable binding without explicit architectural support, bridging connectionist and symbolic approaches. To facilitate reproducible research, we developed Variable Scope, an interactive web platform for exploring our findings at https://variablescope.org",,"Yiwei Wu, Atticus Geiger, Raphaël Millière",2025-05-30T18:08:50Z,How Do Transformers Learn Variable Binding in Symbolic Programs?,Wie lernen Transformer variable Bindungen in Symbolischen Programmen?,变换者如何在符号程序中学习变数绑定 ?,http://arxiv.org/abs/2505.20896v2
1298,"In this work, we establish a novel theoretical connection between supervised fine-tuning and offline reinforcement learning under the token-level Markov decision process, revealing that large language models indeed learn an implicit $Q$-function for inference. Through this theoretical lens, we demonstrate that the widely used beam search method suffers from unacceptable over-optimism, where inference errors are inevitably amplified due to inflated $Q$-value estimations of suboptimal steps. To address this limitation, we propose Supervised Optimism Correction(SOC), which introduces a simple yet effective auxiliary loss for token-level $Q$-value estimations during supervised fine-tuning. Specifically, the auxiliary loss employs implicit value regularization to boost model confidence in expert-demonstrated responses, thereby suppressing over-optimism toward insufficiently supervised responses. Extensive experiments on mathematical reasoning benchmarks, including GSM8K, MATH, and GAOKAO, showcase the superiority of the proposed SOC with beam search across a series of open-source models.",,"Junjie Zhang, Rushuai Yang, Shunyu Liu, Ting-En Lin, Fei Huang, Yi Chen, Yongbin Li, Dacheng Tao",2025-05-30T18:05:02Z,Supervised Optimism Correction: Be Confident When LLMs Are Sure,"Überwachte Optimismuskorrektur: Seien Sie sicher, wenn LLMs sicher sind",受监督的乐观主义纠正:当LLMS确定时要有信心,http://arxiv.org/abs/2504.07527v2
1299,"CAPTCHAs have been a critical bottleneck for deploying web agents in real-world applications, often blocking them from completing end-to-end automation tasks. While modern multimodal LLM agents have demonstrated impressive performance in static perception tasks, their ability to handle interactive, multi-step reasoning challenges like CAPTCHAs is largely untested. To address this gap, we introduce Open CaptchaWorld, the first web-based benchmark and platform specifically designed to evaluate the visual reasoning and interaction capabilities of MLLM-powered agents through diverse and dynamic CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225 CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth, which quantifies the number of cognitive and motor steps required to solve each puzzle. Experimental results show that humans consistently achieve near-perfect scores, state-of-the-art MLLM agents struggle significantly, with success rates at most 40.0% by Browser-Use Openai-o3, far below human-level performance, 93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing the limits of current multimodal agents and guiding the development of more robust multimodal reasoning systems. Code and Data are available at this https URL.",,"Yaxin Luo, Zhaoyi Li, Jiacheng Liu, Jiacheng Cui, Xiaohan Zhao, Zhiqiang Shen",2025-05-30T17:59:55Z,Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and   Benchmarking Multimodal LLM Agents,Open CaptchaWorld: Eine umfassende Web-basierte Plattform für Tests und Benchmarking multimodaler LLM-Agenten,"Open CaptchaWorld:一个基于网络的综合平台,用于测试和基准确定多式联运LLM代理商",http://arxiv.org/abs/2505.24878v1
1300,"Deep reasoning is fundamental for solving complex tasks, especially in vision-centric scenarios that demand sequential, multimodal understanding. However, existing benchmarks typically evaluate agents with fully synthetic, single-turn queries, limited visual modalities, and lack a framework to assess reasoning quality over multiple steps as required in real-world settings. To address this, we introduce Agent-X, a large-scale benchmark for evaluating vision-centric agents multi-step and deep reasoning capabilities in real-world, multimodal settings. Agent- X features 828 agentic tasks with authentic visual contexts, including images, multi-image comparisons, videos, and instructional text. These tasks span six major agentic environments: general visual reasoning, web browsing, security and surveillance, autonomous driving, sports, and math reasoning. Our benchmark requires agents to integrate tool use with explicit, stepwise decision-making in these diverse settings. In addition, we propose a fine-grained, step-level evaluation framework that assesses the correctness and logical coherence of each reasoning step and the effectiveness of tool usage throughout the task. Our results reveal that even the best-performing models, including GPT, Gemini, and Qwen families, struggle to solve multi-step vision tasks, achieving less than 50% full-chain success. These findings highlight key bottlenecks in current LMM reasoning and tool-use capabilities and identify future research directions in vision-centric agentic reasoning models. Our data and code are publicly available at https://github.com/mbzuai-oryx/Agent-X",,"Tajamul Ashraf, Amal Saqib, Hanan Ghani, Muhra AlMahri, Yuhao Li, Noor Ahsan, Umair Nawaz, Jean Lahoud, Hisham Cholakkal, Mubarak Shah, Philip Torr, Fahad Shahbaz Khan, Rao Muhammad Anwer, Salman Khan",2025-05-30T17:59:53Z,Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic   Tasks,Agent-X: Bewertung tiefer multimodaler Vernunft in Vision-Centric Agentic Aufgaben,Agent-X: 评估远景中心制剂任务中的深度多模式理由,http://arxiv.org/abs/2505.24876v1
1301,"Recent advancements in reinforcement learning with verifiable rewards have pushed the boundaries of the visual reasoning capabilities in large vision-language models (LVLMs). However, training LVLMs with reinforcement fine-tuning (RFT) is computationally expensive, posing a significant challenge to scaling model size. In this work, we propose ProxyThinker, an inference-time technique that enables large models to inherit the visual reasoning capabilities from small, slow-thinking visual reasoners without any training. By subtracting the output distributions of base models from those of RFT reasoners, ProxyThinker modifies the decoding dynamics and successfully elicits the slow-thinking reasoning demonstrated by the emerged sophisticated behaviors such as self-verification and self-correction. ProxyThinker consistently boosts performance on challenging visual benchmarks on spatial, mathematical, and multi-disciplinary reasoning, enabling untuned base models to compete with the performance of their full-scale RFT counterparts. Furthermore, our implementation efficiently coordinates multiple language models with parallelism techniques and achieves up to 38 $\times$ faster inference compared to previous decoding-time methods, paving the way for the practical deployment of ProxyThinker. Code is available at https://github.com/MrZilinXiao/ProxyThinker.",,"Zilin Xiao, Jaywon Koo, Siru Ouyang, Jefferson Hernandez, Yu Meng, Vicente Ordonez",2025-05-30T17:59:43Z,ProxyThinker: Test-Time Guidance through Small Visual Reasoners,ProxyThinker: Test-Zeit-Anleitung durch kleine visuelle Reasoner,代理服务器:通过小型视觉理由提供测试时间指导,http://arxiv.org/abs/2505.24872v1
1302,"Recent advances in reasoning-centric language models have highlighted reinforcement learning (RL) as a promising method for aligning models with verifiable rewards. However, it remains contentious whether RL truly expands a model's reasoning capabilities or merely amplifies high-reward outputs already latent in the base model's distribution, and whether continually scaling up RL compute reliably leads to improved reasoning performance. In this work, we challenge prevailing assumptions by demonstrating that prolonged RL (ProRL) training can uncover novel reasoning strategies that are inaccessible to base models, even under extensive sampling. We introduce ProRL, a novel training methodology that incorporates KL divergence control, reference policy resetting, and a diverse suite of tasks. Our empirical analysis reveals that RL-trained models consistently outperform base models across a wide range of pass@k evaluations, including scenarios where base models fail entirely regardless of the number of attempts. We further show that reasoning boundary improvements correlates strongly with task competence of base model and training duration, suggesting that RL can explore and populate new regions of solution space over time. These findings offer new insights into the conditions under which RL meaningfully expands reasoning boundaries in language models and establish a foundation for future work on long-horizon RL for reasoning. We release model weights to support further research: https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B",,"Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, Yi Dong",2025-05-30T17:59:01Z,ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in   Large Language Models,ProRL: Verlängertes Stärkungslernen erweitert aufschlussreiche Grenzen in großen Sprachmodellen,ProRL:长期加强学习扩大大语言模式中的理由界限,http://arxiv.org/abs/2505.24864v1
1303,"This paper presents AlphaOne ($\alpha$1), a universal framework for modulating reasoning progress in large reasoning models (LRMs) at test time. $\alpha$1 first introduces $\alpha$ moment, which represents the scaled thinking phase with a universal parameter $\alpha$. Within this scaled pre-$\alpha$ moment phase, it dynamically schedules slow thinking transitions by modeling the insertion of reasoning transition tokens as a Bernoulli stochastic process. After the $\alpha$ moment, $\alpha$1 deterministically terminates slow thinking with the end-of-thinking token, thereby fostering fast reasoning and efficient answer generation. This approach unifies and generalizes existing monotonic scaling methods by enabling flexible and dense slow-to-fast reasoning modulation. Extensive empirical studies on various challenging benchmarks across mathematical, coding, and scientific domains demonstrate $\alpha$1's superior reasoning capability and efficiency. Project page: https://alphaone-project.github.io/",,"Junyu Zhang, Runpei Dong, Han Wang, Xuying Ning, Haoran Geng, Peihao Li, Xialin He, Yutong Bai, Jitendra Malik, Saurabh Gupta, Huan Zhang",2025-05-30T17:58:36Z,AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time,AlphaOne: Denke langsam und schnell in der Testzeit,阿尔法一:在测试时间思考慢速和快速的理性模型,http://arxiv.org/abs/2505.24863v1
1304,"Steering vectors are a lightweight method for controlling text properties by adding a learned bias to language model activations at inference time. So far, steering vectors have predominantly been evaluated in multiple-choice settings, while their effectiveness in free-form generation tasks remains understudied. Moving ""Beyond Multiple Choice,"" we thoroughly evaluate the effectiveness of steering vectors in adaptively controlling topical focus, sentiment, toxicity, and readability in abstractive summaries of the NEWTS dataset. We find that steering effectively controls the targeted summary properties, but high steering strengths consistently degrade both intrinsic and extrinsic text quality. Compared to steering, prompting offers weaker control, while preserving text quality. Combining steering and prompting yields the strongest control over text properties and offers the most favorable efficacy-quality trade-off at moderate steering strengths. Our results underscore the practical trade-off between control strength and text quality preservation when applying steering vectors to free-form generation tasks.",,"Joschka Braun, Carsten Eickhoff, Seyed Ali Bahrainian",2025-05-30T17:57:15Z,Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive   Free-Form Summarization,Beyond Multiple Choice: Bewertung von Steuerungsvektoren für adaptive Freiform-Zusammenfassung,超越多重选择:评估适应性自由形式总结指导矢量,http://arxiv.org/abs/2505.24859v1
1305,"A critical component in the trustworthiness of LLMs is reliable uncertainty communication, yet LLMs often use assertive language when conveying false claims, leading to over-reliance and eroded trust. We present the first systematic study of $\textit{faithful confidence calibration}$ of LLMs, benchmarking models' ability to use linguistic expressions of uncertainty that $\textit{faithfully reflect}$ their intrinsic uncertainty, across a comprehensive array of models, datasets, and prompting strategies. Our results demonstrate that LLMs largely fail at this task, and that existing interventions are insufficient: standard prompt approaches provide only marginal gains, and existing, factuality-based calibration techniques can even harm faithful calibration. To address this critical gap, we introduce MetaFaith, a novel prompt-based calibration approach inspired by human metacognition. We show that MetaFaith robustly improves faithful calibration across diverse models and task domains, enabling up to 61% improvement in faithfulness and achieving an 83% win rate over original generations as judged by humans.",,"Gabrielle Kaili-May Liu, Gal Yona, Avi Caciularu, Idan Szpektor, Tim G. J. Rudner, Arman Cohan",2025-05-30T17:54:08Z,MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs,MetaFaith: Treue natürliche Sprache Ungewissheitsausdruck in LLMs,Meta Faith:LLMM中忠实自然语言不确定性表达法,http://arxiv.org/abs/2505.24858v1
1306,"Multimodal large language models excel across diverse domains but struggle with complex visual reasoning tasks. Current approaches aim to incorporate structured thinking via two strategies: explicit search methods and post-training techniques. However, both approaches face significant limitations: Search-based methods suffer from computational inefficiency due to extensive solution space exploration, while post-training methods require substantial data, computational resources, and often encounter training instability. To address these limitations, we propose AStar, an \textbf{A}utomated \textbf{S}tructured \textbf{t}hinking paradigm for multimod\textbf{a}l \textbf{r}easoning. Our method introduces ""thought cards"", a lightweight library of high-level reasoning patterns abstracted from 500 prior samples using Monte Carlo Tree Search. For each test problem, AStar adaptively retrieves the optimal thought cards and seamlessly integrates these external explicit guidelines with the model's internal implicit reasoning capabilities. Extensive experiments demonstrate AStar's effectiveness and efficiency: using only 500 prior samples and a 7B backbone, our training-free framework achieves 53.9$\%$ accuracy on MathVerse (surpassing GPT-4o's 50.2%) and 32.7% on MathVision (versus GPT-4o's 30.4%). Further analysis reveals that AStar generalizes beyond multimodal reasoning to visual perception and understanding domains, and serves as a plug-and-play test-time inference method compatible with mainstream post-training techniques like GRPO.",,"Jinyang Wu, Mingkuan Feng, Shuai Zhang, Fangrui Lv, Ruihan Jin, Feihu Che, Zengqi Wen, Jianhua Tao",2025-05-30T17:53:06Z,Boosting Multimodal Reasoning with Automated Structured Thinking,Multimodale Vernunft durch automatisiertes strukturiertes Denken fördern,以自动结构思考促进多模式理由,http://arxiv.org/abs/2502.02339v3
1307,"Recent advances in model distillation demonstrate that data from advanced reasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer complex reasoning abilities to smaller, efficient student models. However, standard practices employ rejection sampling, discarding incorrect reasoning examples -- valuable, yet often underutilized data. This paper addresses the critical question: How can both positive and negative distilled reasoning traces be effectively leveraged to maximize LLM reasoning performance in an offline setting? To this end, We propose Reinforcement Distillation (REDI), a two-stage framework. Stage 1 learns from positive traces via Supervised Fine-Tuning (SFT). Stage 2 further refines the model using both positive and negative traces through our proposed REDI objective. This novel objective is a simple, reference-free loss function that outperforms established methods like DPO and SimPO in this distillation context. Our empirical evaluations demonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT combined with DPO/SimPO on mathematical reasoning tasks. Notably, the Qwen-REDI-1.5B model, post-trained on just 131k positive and negative examples from the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1). Its performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a model post-trained on 800k proprietary data) across various mathematical reasoning benchmarks, establishing a new state-of-the-art for 1.5B models post-trained offline with openly available data.",,"Shuyao Xu, Cheng Peng, Jiangxuan Long, Weidi Xu, Wei Chu, Yuan Qi",2025-05-30T17:47:17Z,Harnessing Negative Signals: Reinforcement Distillation from Teacher   Data for LLM Reasoning,Negative Signale nutzen: Verstärkte Destillation aus Lehrerdaten für LLM-Reasoning,利用负负信号:从师资数据中为LLM理由进行强化蒸馏,http://arxiv.org/abs/2505.24850v1
1308,"Reward modeling is a key step in building safe foundation models when applying reinforcement learning from human feedback (RLHF) to align Large Language Models (LLMs). However, reward modeling based on the Bradley-Terry (BT) model assumes a global reward function, failing to capture the inherently diverse and heterogeneous human preferences. Hence, such oversimplification limits LLMs from supporting personalization and pluralistic alignment. Theoretically, we show that when human preferences follow a mixture distribution of diverse subgroups, a single BT model has an irreducible error. While existing solutions, such as multi-objective learning with fine-grained annotations, help address this issue, they are costly and constrained by predefined attributes, failing to fully capture the richness of human values. In this work, we introduce MiCRo, a two-stage framework that enhances personalized preference learning by leveraging large-scale binary preference datasets without requiring explicit fine-grained annotations. In the first stage, MiCRo introduces context-aware mixture modeling approach to capture diverse human preferences. In the second stage, MiCRo integrates an online routing strategy that dynamically adapts mixture weights based on specific context to resolve ambiguity, allowing for efficient and scalable preference adaptation with minimal additional supervision. Experiments on multiple preference datasets demonstrate that MiCRo effectively captures diverse human preferences and significantly improves downstream personalization.",,"Jingyan Shen, Jiarui Yao, Rui Yang, Yifan Sun, Feng Luo, Rui Pan, Tong Zhang, Han Zhao",2025-05-30T17:44:28Z,MiCRo: Mixture Modeling and Context-aware Routing for Personalized   Preference Learning,MiCRo: Mixture Modeling und Context-aware Routing für personalisiertes Preference-Lernen,MiCRO: 个性化首选学习混合建模和背景认知分流,http://arxiv.org/abs/2505.24846v1
1309,"This paper introduces RuleArena, a novel and challenging benchmark designed to evaluate the ability of large language models (LLMs) to follow complex, real-world rules in reasoning. Covering three practical domains -- airline baggage fees, NBA transactions, and tax regulations -- RuleArena assesses LLMs' proficiency in handling intricate natural language instructions that demand long-context understanding, logical reasoning, and accurate mathematical computation. Two key attributes distinguish RuleArena from traditional rule-based reasoning benchmarks: (1) it extends beyond standard first-order logic representations, and (2) it is grounded in authentic, practical scenarios, providing insights into the suitability and reliability of LLMs for real-world applications. Our findings reveal several notable limitations in LLMs: (1) they struggle to identify and apply the appropriate rules, frequently becoming confused by similar but distinct regulations, (2) they cannot consistently perform accurate mathematical computations, even when they correctly identify the relevant rules, and (3) in general, they perform poorly in the benchmark. We also observe a significant performance boost when LLMs are provided with external tools for oracle math and logic operations. These results highlight significant challenges and promising research directions in advancing LLMs' rule-guided reasoning capabilities in real-life applications. Our codes and data are publicly available on https://github.com/skyriver-2000/RuleArena.",,"Ruiwen Zhou, Wenyue Hua, Liangming Pan, Sitao Cheng, Xiaobao Wu, En Yu, William Yang Wang",2025-05-30T17:43:10Z,RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World   Scenarios,RuleArena: Ein Benchmark für regelgeführte Vernunft mit LLMs in realen Szenarien,规则阿雷纳:在现实世界情景中与LLMs进行有章可循的理据基准,http://arxiv.org/abs/2412.08972v2
1310,"Training data mixtures greatly impact the generalization performance of large language models. Existing domain reweighting methods often rely on costly weight computations and require retraining when new data is introduced. To this end, we introduce a flexible and efficient data mixing framework, Chameleon, that employs leverage scores to quantify domain importance within a learned embedding space. We first construct a domain affinity matrix over domain embeddings. The induced leverage scores determine a mixture that upweights domains sharing common representations in embedding space. This formulation allows direct transfer to new data by computing the new domain embeddings. In experiments, we demonstrate improvements over three key scenarios: (i) our computed weights improve performance on pretraining domains with a fraction of the compute of existing methods; (ii) Chameleon can adapt to data changes without proxy retraining, boosting few-shot reasoning accuracies when transferred to new data; (iii) our method enables efficient domain reweighting in finetuning, consistently improving test perplexity on all finetuning domains over uniform mixture. Our code is available at https://github.com/LIONS-EPFL/Chameleon.",,"Wanyun Xie, Francesco Tonin, Volkan Cevher",2025-05-30T17:43:10Z,Chameleon: A Flexible Data-mixing Framework for Language Model   Pretraining and Finetuning,Chameleon: Ein flexibles Daten-Mixing-Framework für Sprachmodellvor- und Feinsteuerung,变色变色:语文示范预培训和微调前程和微调灵活数据混合框架,http://arxiv.org/abs/2505.24844v1
1311,"This paper reveals that many state-of-the-art large language models (LLMs) lack hierarchical knowledge about our visual world, unaware of even well-established biology taxonomies. This shortcoming makes LLMs a bottleneck for vision LLMs' hierarchical visual understanding (e.g., recognizing Anemone Fish but not Vertebrate). We arrive at these findings using about one million four-choice visual question answering (VQA) tasks constructed from six taxonomies and four image datasets. Interestingly, finetuning a vision LLM using our VQA tasks reaffirms LLMs' bottleneck effect to some extent because the VQA tasks improve the LLM's hierarchical consistency more than the vision LLM's. We conjecture that one cannot make vision LLMs understand visual concepts fully hierarchical until LLMs possess corresponding taxonomy knowledge.",,"Yuwen Tan, Yuan Qing, Boqing Gong",2025-05-30T17:40:46Z,"Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are   the Bottleneck","Vision LLMs sind schlecht im Hierarchical Visual Understanding, und LLMs sind der Engpass","愿景LLL女士在等级视觉理解方面很差,而LLLM女士是瓶颈",http://arxiv.org/abs/2505.24840v1
1312,"Training high-quality CLIP models typically requires enormous datasets, which limits the development of domain-specific models -- especially in areas that even the largest CLIP models do not cover well -- and drives up training costs. This poses challenges for scientific research that needs fine-grained control over the training procedure of CLIP models. In this work, we show that by employing smart web search strategies enhanced with knowledge graphs, a robust CLIP model can be trained from scratch with considerably less data. Specifically, we demonstrate that an expert foundation model for living organisms can be built using just 10M images. Moreover, we introduce EntityNet, a dataset comprising 33M images paired with 46M text descriptions, which enables the training of a generic CLIP model in significantly reduced time.",,"Simon Ging, Sebastian Walter, Jelena Bratulić, Johannes Dienert, Hannah Bast, Thomas Brox",2025-05-30T17:39:41Z,Using Knowledge Graphs to harvest datasets for efficient CLIP model   training,Verwendung von Wissensgrafiken zur Ernte von Datensätzen für ein effizientes CLIP-Modelltraining,"利用知识图收集数据集,以高效的CLIP模式培训",http://arxiv.org/abs/2505.02746v2
1313,"Cross-lingual transfer allows models to perform tasks in languages unseen during training and is often assumed to benefit from increased multilinguality. In this work, we challenge this assumption in the context of two underexplored, sense-aware tasks: polysemy disambiguation and lexical semantic change. Through a large-scale analysis across 28 languages, we show that multilingual training is neither necessary nor inherently beneficial for effective transfer. Instead, we find that confounding factors - such as fine-tuning data composition and evaluation artifacts - better account for the perceived advantages of multilinguality. Our findings call for more rigorous evaluations in multilingual NLP. We release fine-tuned models and benchmarks to support further research, with implications extending to low-resource and typologically diverse languages.",,"Roksana Goworek, Haim Dubossarsky",2025-05-30T17:36:20Z,Multilinguality Does not Make Sense: Investigating Factors Behind   Zero-Shot Transfer in Sense-Aware Tasks,Mehrsprachigkeit macht keinen Sinn: Untersuchungsfaktoren hinter Null-Schuss-Übertragung in Sense-Aware-Aufgaben,多质量不使人产生感知:在Sense-Aware任务中零热传输背后的调查因素,http://arxiv.org/abs/2505.24834v1
1314,"Large language models (LLMs) exhibit extensive medical knowledge but are prone to hallucinations and inaccurate citations, which pose a challenge to their clinical adoption and regulatory compliance. Current methods, such as Retrieval Augmented Generation, partially address these issues by grounding answers in source documents, but hallucinations and low fact-level explainability persist. In this work, we introduce a novel atomic fact-checking framework designed to enhance the reliability and explainability of LLMs used in medical long-form question answering. This method decomposes LLM-generated responses into discrete, verifiable units called atomic facts, each of which is independently verified against an authoritative knowledge base of medical guidelines. This approach enables targeted correction of errors and direct tracing to source literature, thereby improving the factual accuracy and explainability of medical Q&A. Extensive evaluation using multi-reader assessments by medical experts and an automated open Q&A benchmark demonstrated significant improvements in factual accuracy and explainability. Our framework achieved up to a 40% overall answer improvement and a 50% hallucination detection rate. The ability to trace each atomic fact back to the most relevant chunks from the database provides a granular, transparent explanation of the generated responses, addressing a major gap in current medical AI applications. This work represents a crucial step towards more trustworthy and reliable clinical applications of LLMs, addressing key prerequisites for clinical application and fostering greater confidence in AI-assisted healthcare.",,"Juraj Vladika, Annika Domres, Mai Nguyen, Rebecca Moser, Jana Nano, Felix Busch, Lisa C. Adams, Keno K. Bressem, Denise Bernhardt, Stephanie E. Combs, Kai J. Borm, Florian Matthes, Jan C. Peeken",2025-05-30T17:33:07Z,Improving Reliability and Explainability of Medical Question Answering   through Atomic Fact Checking in Retrieval-Augmented LLMs,Verbesserung der Zuverlässigkeit und Erklärbarkeit medizinischer Fragen durch Atomic Fact Checking in LLMs mit Retrieval-Augmented,通过在收回贷款的LMM中进行原子事实检查提高医疗问题答案的可靠性和可解释性,http://arxiv.org/abs/2505.24830v1
1315,"With the growing influence of Large Language Models (LLMs), there is increasing interest in integrating speech representations with them to enable more seamless multi-modal processing and speech understanding. This study introduces a novel approach that combines self-supervised speech representations with instruction-tuned LLMs for speech-to-text translation. The proposed approach leverages a modality adapter to align extracted speech features with instruction-tuned LLMs using English speech data. Our experiments demonstrate that this method effectively preserves the semantic content of the input speech and serves as an effective bridge between self-supervised speech models and instruction-tuned LLMs, offering a promising approach for various speech understanding applications.",,"Amirbek Djanibekov, Hanan Aldarmaki",2025-05-30T17:30:40Z,SparQLe: Speech Queries to Text Translation Through LLMs,SparQLe: Sprachfragen zur Textübersetzung durch LLMs,SparQLe:通过LLM 询问文本翻译的语音查询,http://arxiv.org/abs/2502.09284v3
1316,"As large language models (LLMs) are increasingly used in legal applications, current evaluation benchmarks tend to focus mainly on factual accuracy while largely neglecting important linguistic quality aspects such as clarity, coherence, and terminology. To address this gap, we propose three steps: First, we develop a regression model to evaluate the quality of legal texts based on clarity, coherence, and terminology. Second, we create a specialized set of legal questions. Third, we analyze 49 LLMs using this evaluation framework.   Our analysis identifies three key findings: First, model quality levels off at 14 billion parameters, with only a marginal improvement of $2.7\%$ noted at 72 billion parameters. Second, engineering choices such as quantization and context length have a negligible impact, as indicated by statistical significance thresholds above 0.016. Third, reasoning models consistently outperform base architectures. A significant outcome of our research is the release of a ranking list and Pareto analysis, which highlight the Qwen3 series as the optimal choice for cost-performance tradeoffs. This work not only establishes standardized evaluation protocols for legal LLMs but also uncovers fundamental limitations in current training data refinement approaches. Code and models are available at: https://github.com/lyxx3rd/LegalEval-Q.",,"Li yunhan, Wu gengshen",2025-05-30T17:30:18Z,LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated   Legal Text,LegalEval-Q: Ein neuer Benchmark für die Qualitätsbewertung von LLM-generiertem Rechtstext,法律Val-Q:对LLM创制法律文本进行质量评价的新基准,http://arxiv.org/abs/2505.24826v1
1317,"Large language models (LLMs) have rapidly advanced and are increasingly capable of tackling complex scientific problems, including those in physics. Despite this progress, current LLMs often fail to emulate the concise, principle-based reasoning characteristic of human experts, instead generating lengthy and opaque solutions. This discrepancy highlights a crucial gap in their ability to apply core physical principles for efficient and interpretable problem solving. To systematically investigate this limitation, we introduce PhySense, a novel principle-based physics reasoning benchmark designed to be easily solvable by experts using guiding principles, yet deceptively difficult for LLMs without principle-first reasoning. Our evaluation across multiple state-of-the-art LLMs and prompt types reveals a consistent failure to align with expert-like reasoning paths, providing insights for developing AI systems with efficient, robust and interpretable principle-based scientific reasoning.",,"Yinggan Xu, Yue Liu, Zhiqiang Gao, Changnan Peng, Di Luo",2025-05-30T17:25:20Z,PhySense: Principle-Based Physics Reasoning Benchmarking for Large   Language Models,PhySense: Prinzipbasierte Physik-Reasoning-Benchmarking für große Sprachmodelle,物理常识:大语言模式的基于原则的物理理由基准,http://arxiv.org/abs/2505.24823v1
1318,"Meeting summarization suffers from limited high-quality data, mainly due to privacy restrictions and expensive collection processes. We address this gap with FAME, a dataset of 500 meetings in English and 300 in German produced by MIMIC, our new multi-agent meeting synthesis framework that generates meeting transcripts on a given knowledge source by defining psychologically grounded participant profiles, outlining the conversation, and orchestrating a large language model (LLM) debate. A modular post-processing step refines these outputs, mitigating potential repetitiveness and overly formal tones, ensuring coherent, credible dialogues at scale. We also propose a psychologically grounded evaluation framework assessing naturalness, social behavior authenticity, and transcript difficulties. Human assessments show that FAME approximates real-meeting spontaneity (4.5/5 in naturalness), preserves speaker-centric challenges (3/5 in spoken language), and introduces richer information-oriented difficulty (4/5 in difficulty). These findings highlight that FAME is a good and scalable proxy for real-world meeting conditions. It enables new test scenarios for meeting summarization research and other conversation-centric applications in tasks requiring conversation data or simulating social scenarios under behavioral constraints.",,"Frederic Kirstein, Muneeb Khan, Jan Philip Wahle, Terry Ruas, Bela Gipp",2025-05-30T17:21:32Z,You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with   a Multi-Agent Conversations,"Sie müssen MIMIC, um FAME: Lösen Meeting Text Scarcity mit einem Multi-Agent Gespräche","您需要MIMIC 来获得 FAME: 解决会议定线的缺口, 与多功能对话",http://arxiv.org/abs/2502.13001v2
1319,"Detecting disinformation that blends manipulated text and images has become increasingly challenging, as AI tools make synthetic content easy to generate and disseminate. While most existing AI safety benchmarks focus on single modality misinformation (i.e., false content shared without intent to deceive), intentional multimodal disinformation, such as propaganda or conspiracy theories that imitate credible news, remains largely unaddressed. We introduce the Vision-Language Disinformation Detection Benchmark (VLDBench), the first large-scale resource supporting both unimodal (text-only) and multimodal (text + image) disinformation detection. VLDBench comprises approximately 62,000 labeled text-image pairs across 13 categories, curated from 58 news outlets. Using a semi-automated pipeline followed by expert review, 22 domain experts invested over 500 hours to produce high-quality annotations with substantial inter-annotator agreement. Evaluations of state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs) on VLDBench show that incorporating visual cues improves detection accuracy by 5 to 35 percentage points over text-only models. VLDBench provides data and code for evaluation, fine-tuning, and robustness testing to support disinformation analysis. Developed in alignment with AI governance frameworks (e.g., the MIT AI Risk Repository), VLDBench offers a principled foundation for advancing trustworthy disinformation detection in multimodal media.   Project: https://vectorinstitute.github.io/VLDBench/ Dataset: https://huggingface.co/datasets/vector-institute/VLDBench Code: https://github.com/VectorInstitute/VLDBench",,"Shaina Raza, Ashmal Vayani, Aditya Jain, Aravind Narayanan, Vahid Reza Khazaie, Syed Raza Bashir, Elham Dolatabadi, Gias Uddin, Christos Emmanouilidis, Rizwan Qureshi, Mubarak Shah",2025-05-30T17:17:11Z,VLDBench Evaluating Multimodal Disinformation with Regulatory Alignment,VLDBench Bewertung multimodaler Desinformation mit regulatorischer Ausrichtung,VLDBENCH 以监管协调方式评价多式联运错误信息,http://arxiv.org/abs/2502.11361v3
1320,"While subgroup disparities and performance bias are increasingly studied in computational research, fairness in categorical Speech Emotion Recognition (SER) remains underexplored. Existing methods often rely on explicit demographic labels, which are difficult to obtain due to privacy concerns. To address this limitation, we introduce an Implicit Demography Inference (IDI) module that leverages pseudo-labeling from a pre-trained model and unsupervised learning using k-means clustering to mitigate bias in SER. Our experiments show that pseudo-labeling IDI reduces subgroup disparities, improving fairness metrics by over 28% with less than a 2% decrease in SER accuracy. Also, the unsupervised IDI yields more than a 4.6% improvement in fairness metrics with a drop of less than 3.6% in SER performance. Further analyses reveal that the unsupervised IDI consistently mitigates race and age disparities, demonstrating its potential when explicit demographic information is unavailable.",,"Yi-Cheng Lin, Huang-Cheng Chou, Hung-yi Lee",2025-05-30T17:10:08Z,Mitigating Subgroup Disparities in Multi-Label Speech Emotion   Recognition: A Pseudo-Labeling and Unsupervised Learning Approach,Untergruppendisparitäten in der multi-Label Sprachemotionserkennung abmildern: Ein Pseudo-Labeling und unüberwachter Lernansatz,减轻多标签言论情感认识中的分分组差异:优多助学和不受监督的学习方法,http://arxiv.org/abs/2505.14449v3
1321,"Evaluating Video Language Models (VLMs) is a challenging task. Due to its transparency, Multiple-Choice Question Answering (MCQA) is widely used to measure the performance of these models through accuracy. However, existing MCQA benchmarks fail to capture the full reasoning capabilities of VLMs due to selection bias, when models disproportionately favor certain answer options based on positional patterns observed during training. In this work, we conduct a comprehensive empirical analysis of several VLM architectures across major datasets designed to assess complex video-focused reasoning. We identify where the bias is most pronounced and demonstrate to what extent model responses reflect genuine understanding of video content and related questions, as opposed to reliance on arbitrary patterns or superficial cues, such as answer position. By decomposing the MCQA task and adapting fairness bias metrics to VLMs, we introduce a post-processing calibration technique BOLD to balance this bias. Our results show that reducing selection bias improves not only debiasing metrics but also overall model performance, including Accuracy and F1 Mean score. Our method, by suppressing ""blind guessing"", offers a more cost- and time-effective approach to mitigating selection bias compared to existing techniques. This study represents the first focused investigation of selection bias in video-to-text LLM-powered models.",,"Olga Loginova, Oleksandr Bezrukov, Ravi Shekhar, Alexey Kravets",2025-05-30T17:01:57Z,Addressing Blind Guessing: Calibration of Selection Bias in   Multiple-Choice Question Answering by Video Language Models,Addressing Blind Guessing: Kalibrierung der Auswahl Bias in Multiple-Choice-Frage Beantwortung von Video Language Models,解决盲人猜测问题:校准视频语言模式在多选择问题解答中选择的偏见,http://arxiv.org/abs/2410.14248v2
1322,"Despite dropout's ubiquity in machine learning, its effectiveness as a form of data augmentation remains under-explored. We address two key questions: (i) When is dropout effective as an augmentation strategy? (ii) Is dropout uniquely effective under these conditions? To explore these questions, we propose Deep Augmentation, a network- and modality-agnostic method that applies dropout or PCA transformations to targeted layers in neural networks. Through extensive experiments on contrastive learning tasks in NLP, computer vision, and graph learning, we find that uniformly applying dropout across layers does not consistently improve performance. Instead, dropout proves most beneficial in deeper layers and can be matched by alternative augmentations (e.g., PCA). We also show that a stop-gradient operation is critical for ensuring dropout functions effectively as an augmentation, and that performance trends invert when moving from contrastive tasks to supervised tasks. Our analysis suggests that Deep Augmentation helps mitigate inter-layer co-adaptation -- a notable issue in self-supervised learning due to the absence of labeled data. Drawing on these insights, we outline a procedure for selecting the optimal augmentation layer and demonstrate that Deep Augmentation can outperform traditional input-level augmentations. This simple yet powerful approach can be seamlessly integrated into a wide range of architectures and modalities, yielding notable gains in both performance and generalization.",,"Rickard Brüel-Gabrielsson, Tongzhou Wang, Manel Baradad, Justin Solomon",2025-05-30T17:01:15Z,Deep Augmentation: Dropout as Augmentation for Self-Supervised Learning,Deep Augmentation: Dropout als Augmentation für selbstüberwachtes Lernen,深层增强:辍学作为自学强化,http://arxiv.org/abs/2303.14537v5
1323,"Tool learning has emerged as a crucial capability for large language models (LLMs) to solve complex real-world tasks through interaction with external tools. Existing approaches face significant challenges, including reliance on hand-crafted prompts, difficulty in multi-step planning, and lack of precise error diagnosis and reflection mechanisms. We propose ToolCoder, a novel framework that reformulates tool learning as a code generation task. Inspired by software engineering principles, ToolCoder transforms natural language queries into structured Python function scaffold and systematically breaks down tasks with descriptive comments, enabling LLMs to leverage coding paradigms for complex reasoning and planning. It then generates and executes function implementations to obtain final responses. Additionally, ToolCoder stores successfully executed functions in a repository to promote code reuse, while leveraging error traceback mechanisms for systematic debugging, optimizing both execution efficiency and robustness. Experiments demonstrate that ToolCoder achieves superior performance in task completion accuracy and execution reliability compared to existing approaches, establishing the effectiveness of code-centric approaches in tool learning.",,"Hanxing Ding, Shuchang Tao, Liang Pang, Zihao Wei, Jinyang Gao, Bolin Ding, Huawei Shen, Xueqi Cheng",2025-05-30T16:59:23Z,ToolCoder: A Systematic Code-Empowered Tool Learning Framework for Large   Language Models,ToolCoder: Ein Systematisches Code-Empowered Tool Learning Framework für große Sprachmodelle,工具编码器:大型语言模式系统代码动力工具学习框架,http://arxiv.org/abs/2502.11404v2
1324,"Originally, dropout was seen as a breakthrough regularization technique that reduced overfitting and improved performance in almost all applications of deep learning by reducing overfitting. Yet, single-epoch pretraining tasks common to modern LLMs yield minimal overfitting, leading to dropout not being used for large LLMs. Nevertheless, no thorough empirical investigation has been done on the role of dropout in LM pretraining. Through experiments in single-epoch pretraining of both masked (BERT) and autoregressive (Pythia 160M and 1.4B) LMs with varying levels of dropout, we find that downstream performance in language modeling, morpho-syntax (BLiMP), question answering (SQuAD), and natural-language inference (MNLI) improves when dropout is not applied during pretraining. We additionally find that the recently-introduced ""early dropout"" also degrades performance over applying no dropout at all. We further investigate the models' editability, and find that models trained without dropout are more successful in gradient-based model editing (MEND) and equivalent in representation-based model editing (ReFT). Therefore, we advocate to drop dropout during single-epoch pretraining.",,"Houjun Liu, John Bauer, Christopher D. Manning",2025-05-30T16:48:38Z,Drop Dropout on Single-Epoch Language Model Pretraining,Drop Dropout auf Single-Epoch Sprachmodell Vorschulung,单埃波语示范语言预培训前辍学,http://arxiv.org/abs/2505.24788v1
1325,"Recent advancements in text-to-image (T2I) generation have enabled models to produce high-quality images from textual descriptions. However, these models often struggle with complex instructions involving multiple objects, attributes, and spatial relationships. Existing benchmarks for evaluating T2I models primarily focus on general text-image alignment and fail to capture the nuanced requirements of complex, multi-faceted prompts. Given this gap, we introduce LongBench-T2I, a comprehensive benchmark specifically designed to evaluate T2I models under complex instructions. LongBench-T2I consists of 500 intricately designed prompts spanning nine diverse visual evaluation dimensions, enabling a thorough assessment of a model's ability to follow complex instructions. Beyond benchmarking, we propose an agent framework (Plan2Gen) that facilitates complex instruction-driven image generation without requiring additional model training. This framework integrates seamlessly with existing T2I models, using large language models to interpret and decompose complex prompts, thereby guiding the generation process more effectively. As existing evaluation metrics, such as CLIPScore, fail to adequately capture the nuances of complex instructions, we introduce an evaluation toolkit that automates the quality assessment of generated images using a set of multi-dimensional metrics. The data and code are released at https://github.com/yczhou001/LongBench-T2I.",,"Yucheng Zhou, Jiahao Yuan, Qianning Wang",2025-05-30T16:48:14Z,Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for   Complex Instruction-based Image Generation,Zeichnen Sie ALL Ihre Vorstellung: Ein ganzheitliches Benchmark- und Agent-Framework für komplexe instruction-basierte Bildgenerierung,绘制您所有的想象力: 复杂的基于指令的图像生成综合基准和代理框架,http://arxiv.org/abs/2505.24787v1
1326,"Inference-Time Scaling has been critical to the success of recent models such as OpenAI o1 and DeepSeek R1. However, many techniques used to train models for inference-time scaling require tasks to have answers that can be verified, limiting their application to domains such as math, coding and logical reasoning. We take inspiration from how humans make first attempts, ask for detailed feedback from others and make improvements based on such feedback across a wide spectrum of open-ended endeavors. To this end, we collect HelpSteer3 data to train dedicated Feedback and Edit Models that are capable of performing inference-time scaling for open-ended general-domain tasks. In our setup, one model generates an initial response, which are given feedback by a second model, that are then used by a third model to edit the response. We show that performance on Arena Hard, a benchmark strongly predictive of Chatbot Arena Elo can be boosted by scaling the number of initial response drafts, effective feedback and edited responses. When scaled optimally, our setup based on 70B models from the Llama 3 family can reach SoTA performance on Arena Hard at 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and DeepSeek R1 with 92.3.",,"Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Daniel Egert, Ellie Evans, Hoo-Chang Shin, Felipe Soares, Yi Dong, Oleksii Kuchaiev",2025-05-30T16:42:57Z,HelpSteer3: Human-Annotated Feedback and Edit Data to Empower   Inference-Time Scaling in Open-Ended General-Domain Tasks,HilfeSteer3: Human-Annotiertes Feedback und Bearbeiten von Daten zur Empower Inferenz-Zeit-Skalierung in offenen allgemeinen Domain-Aufgaben,"HelpSteer3: 人类附加说明的反馈和编辑数据,以在不限成员名额的一般领域任务中增强推断力和时间尺度",http://arxiv.org/abs/2503.04378v2
1327,"As large language models (LLMs) are increasingly used in high-stakes domains, accurately assessing their confidence is crucial. Humans typically express confidence through epistemic markers (e.g., ""fairly confident"") instead of numerical values. However, it remains unclear whether LLMs consistently use these markers to reflect their intrinsic confidence due to the difficulty of quantifying uncertainty associated with various markers. To address this gap, we first define marker confidence as the observed accuracy when a model employs an epistemic marker. We evaluate its stability across multiple question-answering datasets in both in-distribution and out-of-distribution settings for open-source and proprietary LLMs. Our results show that while markers generalize well within the same distribution, their confidence is inconsistent in out-of-distribution scenarios. These findings raise significant concerns about the reliability of epistemic markers for confidence estimation, underscoring the need for improved alignment between marker based confidence and actual model uncertainty. Our code is available at https://github.com/HKUST-KnowComp/MarCon.",,"Jiayu Liu, Qing Zong, Weiqi Wang, Yangqiu Song",2025-05-30T16:41:24Z,Revisiting Epistemic Markers in Confidence Estimation: Can Markers   Accurately Reflect Large Language Models' Uncertainty?,Epistemische Marker in der Einschätzung von Vertrauen wiedersehen: Können Marker die Ungewissheit großer Sprachmodelle genau widerspiegeln?,重新审视信心估计中的亮点标记:标记能否准确地反映大语言模型的不确定性?,http://arxiv.org/abs/2505.24778v1
1328,"Large language models (LLMs) excel in natural language processing but adapting these LLMs to speech processing tasks efficiently is not straightforward. Direct task-specific fine-tuning is limited by overfitting risks, data requirements, and computational costs. To address these challenges, we propose a scalable, two-stage training approach: (1) A task-independent speech pretraining stage using contrastive learning to align text and speech representations over all layers, followed by (2) a task-specific fine-tuning stage requiring minimal data. This approach outperforms traditional ASR pretraining and enables the model to surpass models specialized on speech translation and question answering while being trained on only 10% of the task-specific data.",,"Maike Züfle, Jan Niehues",2025-05-30T16:35:44Z,Contrastive Learning for Task-Independent SpeechLLM-Pretraining,Kontrastives Lernen für aufgabenunabhängige Sprach-LLM-Vorschulung,独立任务独立语言交流交流学习LLLLM-培训前,http://arxiv.org/abs/2412.15712v2
1329,"Visual Question Answering (VQA) requires reasoning across visual and textual modalities, yet Large Vision-Language Models (LVLMs) often lack integrated commonsense knowledge, limiting their robustness in real-world scenarios. To address this, we introduce MAGIC-VQA, a novel framework that enhances VQA by systematically integrating commonsense knowledge with LVLMs. MAGIC-VQA employs a three-stage process: (1) Explicit Knowledge Integration from external sources, (2) By-Type Post-Processing for contextual refinement, and (3) Implicit Knowledge Augmentation using a Graph Neural Network (GNN) for structured reasoning. While GNNs bring greater depth to structured inference, they enable superior relational inference beyond LVLMs. MAGIC-VQA bridges a key gap by unifying commonsensse knowledge with LVLM-driven reasoning, eliminating the need for extensive pre-training or complex prompt tuning. Our framework achieves state-of-the-art performance on benchmark datasets, significantly improving commonsense reasoning in VQA.",,"Shuo Yang, Siwen Luo, Soyeon Caren Han, Eduard Hovy",2025-05-30T16:34:32Z,MAGIC-VQA: Multimodal And Grounded Inference with Commonsense Knowledge   for Visual Question Answering,MAGIC-VQA: Multimodale und begründete Schlussfolgerungen mit Commonsense-Wissen für visuelle Fragenbeantwortung,MAGIC-VQA:视觉问题解答使用常识知识的多式和有根据的推断,http://arxiv.org/abs/2503.18491v2
1330,"Dataset diversity plays a pivotal role for the successful training of many machine learning models, particularly in the supervised fine-tuning (SFT) stage of large language model (LLM) development. Despite increasing recognition of its importance, systematic analyses of dataset diversity still remain underexplored. To address this gap, this work presents a systematic taxonomy of existing diversity-control strategies, which primarily focus on the instruction component, operating at either macroscopic (entire instruction semantics) or mesoscopic levels (instruction units), and furthermore introduces a novel analysis of microscopic diversity within the response component, specifically analyzing the statistical distribution of tokens in SFT training samples. In the experimental evaluation, we construct fixed-size datasets (e.g., 10,000 samples each) from a corpus of 117,000 open-source SFT samples, incorporating six distinct diversity-control strategies spanning macro-, meso-, and microscopic levels applied to both instructions and responses. We then fine-tune LLMs on these datasets to assess the six diversity-control strategies. Results reveal that while macroscopic and mesoscopic strategies lead to higher performance with increasing diversity, the microscopic strategy in responses exhibits both a stronger correlation between model performance and the degree of diversity and superior performance with maximum diversity across all strategies. These findings offer actionable insights for constructing high-performance SFT datasets.",,"Haoyu Li, Xuhong Li, Yiming Dong, Kun Liu",2025-05-30T16:31:05Z,From Macro to Micro: Probing Dataset Diversity in Language Model   Fine-Tuning,Vom Makro zum Mikro: Probing Dataset Diversity im Sprachmodell Feintuning,从宏观到微观:在语言模型微调中检验数据集多样性,http://arxiv.org/abs/2505.24768v1
1331,"We introduce Reasoning Gym (RG), a library of reasoning environments for reinforcement learning with verifiable rewards. It provides over 100 data generators and verifiers spanning multiple domains including algebra, arithmetic, computation, cognition, geometry, graph theory, logic, and various common games. Its key innovation is the ability to generate virtually infinite training data with adjustable complexity, unlike most previous reasoning datasets, which are typically fixed. This procedural generation approach allows for continuous evaluation across varying difficulty levels. Our experimental results demonstrate the efficacy of RG in both evaluating and reinforcement learning of reasoning models.",,"Zafir Stojanovski, Oliver Stanley, Joe Sharratt, Richard Jones, Abdulhakeem Adefioye, Jean Kaddour, Andreas Köpf",2025-05-30T16:20:18Z,REASONING GYM: Reasoning Environments for Reinforcement Learning with   Verifiable Rewards,BEGRÜNDUNG VON GYM: Reasoning Environments for Reinforcement Learning with Verifizierbare Rewards,GYM:用可核实的奖励加强学习的合理环境,http://arxiv.org/abs/2505.24760v1
1332,"The scientific literature is growing rapidly, making it hard to keep track of the state-of-the-art. Systematic literature reviews (SLRs) aim to identify and evaluate all relevant papers on a topic. After retrieving a set of candidate papers, the abstract screening phase determines initial relevance. To date, abstract screening methods using large language models (LLMs) focus on binary classification settings; existing question answering (QA) based ranking approaches suffer from error propagation. LLMs offer a unique opportunity to evaluate the SLR's inclusion and exclusion criteria, yet, existing benchmarks do not provide them exhaustively. We manually extract these criteria as well as research questions for 57 SLRs, mostly in the medical domain, enabling principled comparisons between approaches. Moreover, we propose LGAR, a zero-shot LLM Guided Abstract Ranker composed of an LLM based graded relevance scorer and a dense re-ranker. Our extensive experiments show that LGAR outperforms existing QA-based methods by 5-10 pp. in mean average precision. Our code and data is publicly available.",,"Christian Jaumann, Andreas Wiedholz, Annemarie Friedrich",2025-05-30T16:18:50Z,LGAR: Zero-Shot LLM-Guided Neural Ranking for Abstract Screening in   Systematic Literature Reviews,LGAR: Zero-Shot LLM-geführtes Neural Ranking für Abstract Screening in Systematic Literature Reviews,LGAR: 系统文学评论中抽象筛选的零热热LLM-指导神经神经定级,http://arxiv.org/abs/2505.24757v1
1333,"In this work, we investigate an important task named instruction-following text embedding, which generates dynamic text embeddings that adapt to user instructions, highlighting specific attributes of text. Despite recent advancements, existing approaches suffer from significant computational overhead, as they require re-encoding the entire corpus for each new instruction. To address this challenge, we propose GSTransform, a novel instruction-following text embedding framework based on Guided Space Transformation. Our key observation is that instruction-relevant information is inherently encoded in generic embeddings but remains underutilized. Instead of repeatedly encoding the corpus for each instruction, GSTransform is a lightweight transformation mechanism that adapts pre-computed embeddings in real time to align with user instructions, guided by a small amount of text data with instruction-focused label annotation. We conduct extensive experiments on three instruction-awareness downstream tasks across nine real-world datasets, demonstrating that GSTransform improves instruction-following text embedding quality over state-of-the-art methods while achieving dramatic speedups of 6~300x in real-time processing on large-scale datasets. The source code is available at https://github.com/YingchaojieFeng/GSTransform.",,"Yingchaojie Feng, Yiqun Sun, Yandong Sun, Minfeng Zhu, Qiang Huang, Anthony K. H. Tung, Wei Chen",2025-05-30T16:16:22Z,Don't Reinvent the Wheel: Efficient Instruction-Following Text Embedding   based on Guided Space Transformation,Nicht das Rad neu erfinden: Effizientes Instruktions-Following Text-Embedding auf Basis der Geführten Raumtransformation,不要重新发明轮子:基于引导空间转换的高效教学- 遵循的文本嵌入,http://arxiv.org/abs/2505.24754v1
1334,"Low-rank gradient-based optimization methods have significantly improved memory efficiency during the training of large language models (LLMs), enabling operations within constrained hardware without sacrificing performance. However, these methods primarily emphasize memory savings, often overlooking potential acceleration in convergence due to their reliance on standard isotropic steepest descent techniques, which can perform suboptimally in the highly anisotropic landscapes typical of deep networks, particularly LLMs. In this paper, we propose SUMO (Subspace-Aware Moment-Orthogonalization), an optimizer that employs exact singular value decomposition (SVD) for moment orthogonalization within a dynamically adapted low-dimensional subspace, enabling norm-inducing steepest descent optimization steps. By explicitly aligning optimization steps with the spectral characteristics of the loss landscape, SUMO effectively mitigates approximation errors associated with commonly used methods like Newton-Schulz orthogonalization approximation. We theoretically establish an upper bound on these approximation errors, proving their dependence on the condition numbers of moments, conditions we analytically demonstrate are encountered during LLM training. Furthermore, we both theoretically and empirically illustrate that exact orthogonalization via SVD substantially improves convergence rates while reducing overall complexity. Empirical evaluations confirm that SUMO accelerates convergence, enhances stability, improves performance, and reduces memory requirements by up to 20% compared to state-of-the-art methods.",,"Yehonathan Refael, Guy Smorodinsky, Tom Tirer, Ofir Lindenbaum",2025-05-30T16:08:40Z,SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating   Memory-Efficient LLM Training,SUMO: Subspace-Aware Moment-Orthogonalisierung zur Beschleunigung des Speicher-Effizienten LLM-Trainings,SUMO: 用于加速内存效率LLM培训的辅助空间器件移动-正向式,http://arxiv.org/abs/2505.24749v1
1335,"Turn-taking in dialogue follows universal constraints but also varies significantly. This study examines how demographic (sex, age, education) and individual factors shape turn-taking using a large dataset of US English conversations (Fisher). We analyze Transition Floor Offset (TFO) and find notable interspeaker variation. Sex and age have small but significant effects female speakers and older individuals exhibit slightly shorter offsets - while education shows no effect. Lighter topics correlate with shorter TFOs. However, individual differences have a greater impact, driven by a strong idiosyncratic and an even stronger ""dyadosyncratic"" component - speakers in a dyad resemble each other more than they resemble themselves in different dyads. This suggests that the dyadic relationship and joint activity are the strongest determinants of TFO, outweighing demographic influences.",,"Julio Cesar Cavalcanti, Gabriel Skantze",2025-05-30T15:55:47Z,"""Dyadosyncrasy"", Idiosyncrasy and Demographic Factors in Turn-Taking","""Dyadosyncrasy"", Idiosyncrasy und demographische Faktoren beim Turn-Taking",“黄麻、破烂和人口因素”,http://arxiv.org/abs/2505.24736v1
1336,"Extensively evaluating the capabilities of (large) language models is difficult. Rapid development of state-of-the-art models induce benchmark saturation, while creating more challenging datasets is labor-intensive. Inspired by the recent developments in mechanistic interpretability, we introduce circuit stability as a new way to assess model performance. Circuit stability refers to a model's ability to apply a consistent reasoning process-its circuit-across various inputs. We mathematically formalize circuit stability and circuit equivalence. Then, through three case studies, we empirically show that circuit stability and the lack thereof can characterize and predict different aspects of generalization. Our proposed methods offer a step towards rigorously relating the generality of models to their interpretability.",,Alan Sun,2025-05-30T15:53:56Z,Circuit Stability Characterizes Language Model Generalization,Circuit Stabilität charakterisiert Sprachmodell Verallgemeinerung,语言模型通用化,http://arxiv.org/abs/2505.24731v1
1337,"We explore a method for improving the performance of large language models through self-reflection and reinforcement learning. By incentivizing the model to generate better self-reflections when it answers incorrectly, we demonstrate that a model's ability to solve complex, verifiable tasks can be enhanced even when generating synthetic data is infeasible and only binary feedback is available. Our framework operates in two stages: first, upon failing a given task, the model generates a self-reflective commentary analyzing its previous attempt; second, the model is given another attempt at the task with the self-reflection in context. If the subsequent attempt succeeds, the tokens generated during the self-reflection phase are rewarded. Our experimental results show substantial performance gains across a variety of model architectures, as high as 34.7% improvement at math equation writing and 18.1% improvement at function calling. Notably, smaller fine-tuned models (1.5 billion to 7 billion parameters) outperform models in the same family that are 10 times larger. Our novel paradigm is thus an exciting pathway to more useful and reliable language models that can self-improve on challenging tasks with limited external feedback.",,"Shelly Bensal, Umar Jamil, Christopher Bryant, Melisa Russak, Kiran Kamble, Dmytro Mozolevskyi, Muayad Ali, Waseem AlShikh",2025-05-30T15:49:42Z,"Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning","Reflektieren, erneut versuchen, Belohnung: Selbstverbessernde LLMs durch Verstärkungslernen",反射、回溯、回报:通过强化学习自我改进LLMs,http://arxiv.org/abs/2505.24726v1
1338,"We introduce Llama-Krikri-8B, a cutting-edge Large Language Model tailored for the Greek language, built on Meta's Llama 3.1-8B. Llama-Krikri-8B has been extensively trained on high-quality Greek data to ensure superior adaptation to linguistic nuances. With 8 billion parameters, it offers advanced capabilities while maintaining efficient computational performance. Llama-Krikri-8B supports both Modern Greek and English, and is also equipped to handle polytonic text and Ancient Greek. The chat version of Llama-Krikri-8B features a multi-stage post-training pipeline, utilizing both human and synthetic instruction and preference data, by applying techniques such as MAGPIE. In addition, for evaluation, we propose three novel public benchmarks for Greek. Our evaluation on existing as well as the proposed benchmarks shows notable improvements over comparable Greek and multilingual LLMs in both natural language understanding and generation as well as code generation.",,"Dimitris Roussis, Leon Voukoutis, Georgios Paraskevopoulos, Sokratis Sofianopoulos, Prokopis Prokopidis, Vassilis Papavasileiou, Athanasios Katsamanis, Stelios Piperidis, Vassilis Katsouros",2025-05-30T15:44:32Z,Krikri: Advancing Open Large Language Models for Greek,"Krikri: Offene, große Sprachmodelle für Griechisch",Krikri:促进希腊语的开放大语言模式,http://arxiv.org/abs/2505.13772v2
1339,"Large Language Models (LLMs) have emerged as powerful tools for generating coherent text, understanding context, and performing reasoning tasks. However, they struggle with temporal reasoning, which requires processing time-related information such as event sequencing, durations, and inter-temporal relationships. These capabilities are critical for applications including question answering, scheduling, and historical analysis. In this paper, we introduce TISER, a novel framework that enhances the temporal reasoning abilities of LLMs through a multi-stage process that combines timeline construction with iterative self-reflection. Our approach leverages test-time scaling to extend the length of reasoning traces, enabling models to capture complex temporal dependencies more effectively. This strategy not only boosts reasoning accuracy but also improves the traceability of the inference process. Experimental results demonstrate state-of-the-art performance across multiple benchmarks, including out-of-distribution test sets, and reveal that TISER enables smaller open-source models to surpass larger closed-weight models on challenging temporal reasoning tasks.",,"Adrián Bazaga, Rexhina Blloshmi, Bill Byrne, Adrià de Gispert",2025-05-30T15:37:19Z,Learning to Reason Over Time: Timeline Self-Reflection for Improved   Temporal Reasoning in Language Models,Mit der Zeit zur Vernunft lernen: Zeitlinien-Selbstreflexion für verbesserte zeitliche Vernunft in Sprachmodellen,学习时间推理:改进语文模式中时间上的理由自评,http://arxiv.org/abs/2504.05258v2
1340,"In this paper, we introduce CoRet, a dense retrieval model designed for code-editing tasks that integrates code semantics, repository structure, and call graph dependencies. The model focuses on retrieving relevant portions of a code repository based on natural language queries such as requests to implement new features or fix bugs. These retrieved code chunks can then be presented to a user or to a second code-editing model or agent. To train CoRet, we propose a loss function explicitly designed for repository-level retrieval. On SWE-bench and Long Code Arena's bug localisation datasets, we show that our model substantially improves retrieval recall by at least 15 percentage points over existing models, and ablate the design choices to show their importance in achieving these results.",,"Fabio Fehr, Prabhu Teja Sivaprasad, Luca Franceschi, Giovanni Zappella",2025-05-30T15:36:37Z,CoRet: Improved Retriever for Code Editing,AdRET: Verbesserter Retriever für Code-Editing,Coret: 改进代码编辑的搜索,http://arxiv.org/abs/2505.24715v1
1341,"Multimodal Large Language Models (MLLMs) have experienced rapid development in recent years. However, in the financial domain, there is a notable lack of effective and specialized multimodal evaluation datasets. To advance the development of MLLMs in the finance domain, we introduce FinMME, encompassing more than 11,000 high-quality financial research samples across 18 financial domains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We ensure data quality through 20 annotators and carefully designed validation mechanisms. Additionally, we develop FinScore, an evaluation system incorporating hallucination penalties and multi-dimensional capability assessment to provide an unbiased evaluation. Extensive experimental results demonstrate that even state-of-the-art models like GPT-4o exhibit unsatisfactory performance on FinMME, highlighting its challenging nature. The benchmark exhibits high robustness with prediction variations under different prompts remaining below 1%, demonstrating superior reliability compared to existing datasets. Our dataset and evaluation protocol are available at https://huggingface.co/datasets/luojunyu/FinMME and https://github.com/luo-junyu/FinMME.",,"Junyu Luo, Zhizhuo Kou, Liming Yang, Xiao Luo, Jinsheng Huang, Zhiping Xiao, Jingshu Peng, Chengzhong Liu, Jiaming Ji, Xuanzhe Liu, Sirui Han, Ming Zhang, Yike Guo",2025-05-30T15:36:19Z,FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation,FinMME: Benchmark-Datensatz für die finanzielle multi-Modal-Reasoning-Bewertung,FinMME:金融多模式定价评估基准数据集,http://arxiv.org/abs/2505.24714v1
1342,"Arabic dialect identification (ADI) systems are essential for large-scale data collection pipelines that enable the development of inclusive speech technologies for Arabic language varieties. However, the reliability of current ADI systems is limited by poor generalization to out-of-domain speech. In this paper, we present an effective approach based on voice conversion for training ADI models that achieves state-of-the-art performance and significantly improves robustness in cross-domain scenarios. Evaluated on a newly collected real-world test set spanning four different domains, our approach yields consistent improvements of up to +34.1% in accuracy across domains. Furthermore, we present an analysis of our approach and demonstrate that voice conversion helps mitigate the speaker bias in the ADI dataset. We release our robust ADI model and cross-domain evaluation dataset to support the development of inclusive speech technologies for Arabic.",,"Badr M. Abdullah, Matthew Baas, Bernd Möbius, Dietrich Klakow",2025-05-30T15:36:08Z,Voice Conversion Improves Cross-Domain Robustness for Spoken Arabic   Dialect Identification,Sprachumwandlung verbessert Cross-Domain Robustheit für gesprochene arabische Dialekt-Identifikation,语音转换改善口音阿拉伯语语音识别的跨域强度,http://arxiv.org/abs/2505.24713v1
1343,"Most resources for evaluating social biases in Large Language Models are developed without co-design from the communities affected by these biases, and rarely involve participatory approaches. We introduce HESEIA, a dataset of 46,499 sentences created in a professional development course. The course involved 370 high-school teachers and 5,370 students from 189 Latin-American schools. Unlike existing benchmarks, HESEIA captures intersectional biases across multiple demographic axes and school subjects. It reflects local contexts through the lived experience and pedagogical expertise of educators. Teachers used minimal pairs to create sentences that express stereotypes relevant to their school subjects and communities. We show the dataset diversity in term of demographic axes represented and also in terms of the knowledge areas included. We demonstrate that the dataset contains more stereotypes unrecognized by current LLMs than previous datasets. HESEIA is available to support bias assessments grounded in educational communities.",,"Guido Ivetta, Marcos J. Gomez, Sofía Martinelli, Pietro Palombini, M. Emilia Echeveste, Nair Carolina Mazzeo, Beatriz Busaniche, Luciana Benotti",2025-05-30T15:32:48Z,"HESEIA: A community-based dataset for evaluating social biases in large   language models, co-designed in real school settings in Latin America","HESEIA: Ein gemeinschaftsbasierter Datensatz zur Bewertung sozialer Voreingenommenheiten in großen Sprachmodellen, mitgestaltet in realen Schuleinstellungen in Lateinamerika","HESEIA:一个社区数据集,用于评价大型语言模式的社会偏见,在拉丁美洲实际学校环境中共同设计",http://arxiv.org/abs/2505.24712v1
1344,"Large language models (LLMs) have shown great potential in decision-making due to the vast amount of knowledge stored within the models. However, these pre-trained models are prone to lack reasoning abilities and are difficult to adapt to new environments, further hindering their application to complex real-world tasks. To address these challenges, inspired by the human cognitive process, we propose Causal-aware LLMs, which integrate the structural causal model (SCM) into the decision-making process to model, update, and utilize structured knowledge of the environment in a ``learning-adapting-acting"" paradigm. Specifically, in the learning stage, we first utilize an LLM to extract the environment-specific causal entities and their causal relations to initialize a structured causal model of the environment. Subsequently,in the adapting stage, we update the structured causal model through external feedback about the environment, via an idea of causal intervention. Finally, in the acting stage, Causal-aware LLMs exploit structured causal knowledge for more efficient policy-making through the reinforcement learning agent. The above processes are performed iteratively to learn causal knowledge, ultimately enabling the causal-aware LLMs to achieve a more accurate understanding of the environment and make more efficient decisions. Experimental results across 22 diverse tasks within the open-world game ``Crafter"" validate the effectiveness of our proposed method.",,"Wei Chen, Jiahao Zhang, Haipeng Zhu, Boyan Xu, Zhifeng Hao, Keli Zhang, Junjian Ye, Ruichu Cai",2025-05-30T15:30:44Z,"Causal-aware Large Language Models: Enhancing Decision-Making Through   Learning, Adapting and Acting","Causal-aware Große Sprachmodelle: Entscheidungsfindung durch Lernen, Anpassen und Handeln verbessern",Causal-awal大语言模式:通过学习、适应和行动加强决策,http://arxiv.org/abs/2505.24710v1
1345,"Aspect-Based Sentiment Analysis (ABSA) offers granular insights into opinions but often suffers from the scarcity of diverse, labeled datasets that reflect real-world conversational nuances. This paper presents an approach for generating synthetic ABSA data using Large Language Models (LLMs) to address this gap. We detail the generation process aimed at producing data with consistent topic and sentiment distributions across multiple domains using GPT-4o. The quality and utility of the generated data were evaluated by assessing the performance of three state-of-the-art LLMs (Gemini 1.5 Pro, Claude 3.5 Sonnet, and DeepSeek-R1) on topic and sentiment classification tasks. Our results demonstrate the effectiveness of the synthetic data, revealing distinct performance trade-offs among the models: DeepSeekR1 showed higher precision, Gemini 1.5 Pro and Claude 3.5 Sonnet exhibited strong recall, and Gemini 1.5 Pro offered significantly faster inference. We conclude that LLM-based synthetic data generation is a viable and flexible method for creating valuable ABSA resources, facilitating research and model evaluation without reliance on limited or inaccessible real-world labeled data.",,"Tejul Pandit, Meet Raval, Dhvani Upadhyay",2025-05-30T15:24:17Z,Multi-Domain ABSA Conversation Dataset Generation via LLMs for   Real-World Evaluation and Model Comparison,Multi-Domain ABSA Conversation Dataset Generierung über LLMs für Real-World Evaluation und Modellvergleich,通过真实世界评价和示范比较LLMLM公司进行多界交流,http://arxiv.org/abs/2505.24701v1
1346,"When making predictions, a language model must trade off how much it relies on its context vs. its prior knowledge. Choosing how sensitive the model is to its context is a fundamental functionality, as it enables the model to excel at tasks like retrieval-augmented generation and question-answering. In this paper, we search for a knob which controls this sensitivity, determining whether language models answer from the context or their prior knowledge. To guide this search, we design a task for controllable context sensitivity. In this task, we first feed the model a context (Paris is in England) and a question (Where is Paris?); we then instruct the model to either use its prior or contextual knowledge and evaluate whether it generates the correct answer for both intents (either France or England). When fine-tuned on this task, instruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it with high accuracy (85-95%). Analyzing these high-performing models, we narrow down which layers may be important to context sensitivity using a novel linear time algorithm. Then, in each model, we identify a 1-D subspace in a single layer that encodes whether the model follows context or prior knowledge. Interestingly, while we identify this subspace in a fine-tuned model, we find that the exact same subspace serves as an effective knob in not only that model but also non-fine-tuned instruct and base models of that model family. Finally, we show a strong correlation between a model's performance and how distinctly it separates context-agreeing from context-ignoring answers in this subspace. These results suggest a single subspace facilitates how the model chooses between context and prior knowledge, hinting at a simple fundamental mechanism that controls this behavior.",,"Julian Minder, Kevin Du, Niklas Stoehr, Giovanni Monea, Chris Wendler, Robert West, Ryan Cotterell",2025-05-30T15:21:51Z,Controllable Context Sensitivity and the Knob Behind It,Kontrollierbarer Kontext Empfindlichkeit und der Knob dahinter,控制环境的感应度及其背后的Knob,http://arxiv.org/abs/2411.07404v4
1347,"Document retrieval techniques are essential for developing large-scale information systems. The common approach involves using a bi-encoder to compute the semantic similarity between a query and documents. However, the scalar similarity often fail to reflect enough information, hindering the interpretation of retrieval results. In addition, this process primarily focuses on global semantics, overlooking the finer-grained semantic relationships between the query and the document's content. In this paper, we introduce a novel method, $\textbf{Ge}$neration $\textbf{A}$ugmented $\textbf{R}$etrieval ($\textbf{GeAR}$), which not only improves the global document-query similarity through contrastive learning, but also integrates well-designed fusion and decoding modules. This enables GeAR to generate relevant context within the documents based on a given query, facilitating learning to retrieve local fine-grained information. Furthermore, when used as a retriever, GeAR does not incur any additional computational cost over bi-encoders. GeAR exhibits competitive retrieval performance across diverse scenarios and tasks. Moreover, qualitative analysis and the results generated by GeAR provide novel insights into the interpretation of retrieval results. The code, data, and models will be released at \href{https://github.com/microsoft/LMOps}{https://github.com/microsoft/LMOps}.",,"Haoyu Liu, Shaohan Huang, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Weiwei Deng, Feng Sun, Furu Wei, Qi Zhang",2025-05-30T15:15:19Z,GeAR: Generation Augmented Retrieval,GeAR: Generation Augmented Retrieval,GeAR: 代际递增检索,http://arxiv.org/abs/2501.02772v2
1348,"We propose a Speech-to-Text Translation (S2TT) approach that integrates phoneme representations into a Chain-of-Thought (CoT) framework to improve translation in low-resource and zero-resource settings. By introducing phoneme recognition as an intermediate step, we enhance cross-lingual transfer, enabling translation even for languages with no labeled speech data. Our system builds on a multilingual LLM, which we extend to process speech and phonemes. Training follows a curriculum learning strategy that progressively introduces more complex tasks. Experiments on multilingual S2TT benchmarks show that phoneme-augmented CoT improves translation quality in low-resource conditions and enables zero-resource translation, while slightly impacting high-resource performance. Despite this trade-off, our findings demonstrate that phoneme-based CoT is a promising step toward making S2TT more accessible across diverse languages.",,"Gerard I. Gállego, Oriol Pareras, Martí Cortada Garcia, Lucas Takanori, Javier Hernando",2025-05-30T15:15:00Z,Speech-to-Text Translation with Phoneme-Augmented CoT: Enhancing   Cross-Lingual Transfer in Low-Resource Scenarios,Speech-to-Text-Übersetzung mit Phoneme-Augmented CoT: Verbesserung der Cross-Lingual-Übertragung in Low-Resource-Szenarien,语音对文字翻译:在低资源情景中加强跨语言转让,http://arxiv.org/abs/2505.24691v1
1349,"Byte Pair Encoding (BPE) tokenizers, widely used in Large Language Models, face challenges in multilingual settings, including penalization of non-Western scripts and the creation of tokens with partial UTF-8 sequences. Pretokenization, often reliant on complex regular expressions, can also introduce fragility and unexpected edge cases. We propose SCRIPT (Script Category Representation in PreTokenization), a novel encoding scheme that bypasses UTF-8 byte conversion by using initial tokens based on Unicode script and category properties. This approach enables a simple, rule-based pretokenization strategy that respects script boundaries, offering a robust alternative to pretokenization strategies based on regular expressions. We also introduce and validate a constrained BPE merging strategy that enforces character integrity, applicable to both SCRIPT-BPE and byte-based BPE. Our experiments demonstrate that SCRIPT-BPE achieves competitive compression while eliminating encoding-based penalties for non-Latin-script languages.",,"Sander Land, Catherine Arnett",2025-05-30T15:12:41Z,BPE Stays on SCRIPT: Structured Encoding for Robust Multilingual   Pretokenization,BPE bleibt auf SCRIPT: Strukturierte Kodierung für robuste Mehrsprachige Pretokenisierung,BPE 停留在SCRIPT: 强力多语种优先语言结构化编码,http://arxiv.org/abs/2505.24689v1
1350,"Language is a form of symbolic capital that affects people's lives in many ways (Bourdieu1977,1991). As a powerful means of communication, it reflects identities, cultures, traditions, and societies more broadly. Therefore, data in a given language should be regarded as more than just a collection of tokens. Rigorous data collection and labeling practices are essential for developing more human-centered and socially aware technologies. Although there has been growing interest in under-resourced languages within the NLP community, work in this area faces unique challenges, such as data scarcity and limited access to qualified annotators.   In this paper, we collect feedback from individuals directly involved in and impacted by NLP artefacts for medium- and low-resource languages. We conduct both quantitative and qualitative analyses of their responses and highlight key issues related to: (1) data quality, including linguistic and cultural appropriateness; and (2) the ethics of common annotation practices, such as the misuse of participatory research. Based on these findings, we make several recommendations for creating high-quality language artefacts that reflect the cultural milieu of their speakers, while also respecting the dignity and labor of data workers.",,"Nedjma Ousidhoum, Meriem Beloucif, Saif M. Mohammad",2025-05-30T15:10:13Z,Building Better: Avoiding Pitfalls in Developing Language Resources when   Data is Scarce,"Besser bauen: Pitfalls vermeiden bei der Entwicklung von Sprachressourcen, wenn Daten knapp sind",建设更好:在数据缺乏时避免开发语文资源方面出现空洞,http://arxiv.org/abs/2410.12691v6
1351,"As people increasingly use AI systems in work and daily life, feedback mechanisms that help them use AI responsibly are urgently needed, particularly in settings where users are not equipped to assess the quality of AI predictions. We study a realistic Machine Translation (MT) scenario where monolingual users decide whether to share an MT output, first without and then with quality feedback. We compare four types of quality feedback: explicit feedback that directly give users an assessment of translation quality using 1) error highlights and 2) LLM explanations, and implicit feedback that helps users compare MT inputs and outputs through 3) backtranslation and 4) question-answer (QA) tables. We find that all feedback types, except error highlights, significantly improve both decision accuracy and appropriate reliance. Notably, implicit feedback, especially QA tables, yields significantly greater gains than explicit feedback in terms of decision accuracy, appropriate reliance, and user perceptions, receiving the highest ratings for helpfulness and trust, and the lowest for mental burden.",,"Dayeon Ki, Kevin Duh, Marine Carpuat",2025-05-30T15:08:10Z,Should I Share this Translation? Evaluating Quality Feedback for User   Reliance on Machine Translation,Soll ich diese Übersetzung teilen? Qualitätsfeedback für Benutzer-Zuverlässigkeit auf maschinelle Übersetzung bewerten,评估用户对机器翻译依赖的质量反馈,http://arxiv.org/abs/2505.24683v1
1352,"Layer pruning has become a popular technique for compressing large language models (LLMs) due to its simplicity. However, existing layer pruning methods often suffer from significant performance drops. We identify that this degradation stems from the mismatch of activation magnitudes across layers and tokens at the pruning interface. To address this, we propose LinearPatch, a simple plug-and-play technique to revive the layer-pruned LLMs. The proposed method adopts Hadamard transformation to suppress massive outliers in particular tokens, and channel-wise scaling to align the activation magnitudes. These operations can be fused into a single matrix, which functions as a patch to bridge the pruning interface with negligible inference overhead. LinearPatch retains up to 94.15% performance of the original model when pruning 5 layers of LLaMA-3-8B on the question answering benchmark, surpassing existing state-of-the-art methods by 4%. In addition, the patch matrix can be further optimized with memory efficient offline knowledge distillation. With only 5K samples, the retained performance of LinearPatch can be further boosted to 95.16% within 30 minutes on a single computing card.",,"Xinrui Chen, Haoli Bai, Tao Yuan, Ruikang Liu, Kang Zhao, Xianzhi Yu, Lu Hou, Tian Guan, Yonghong He, Chun Yuan",2025-05-30T15:06:08Z,A Simple Linear Patch Revives Layer-Pruned Large Language Models,Ein einfacher linearer Patch revives Layer-Prented Large Language Models,简单线性补补补整图层驱动大语言模型,http://arxiv.org/abs/2505.24680v1
1353,"Large Language Models (LLMs) excel in various natural language processing tasks but remain vulnerable to generating harmful content or being exploited for malicious purposes. Although safety alignment datasets have been introduced to mitigate such risks through supervised fine-tuning (SFT), these datasets often lack comprehensive risk coverage. Most existing datasets focus primarily on lexical diversity while neglecting other critical dimensions. To address this limitation, we propose a novel analysis framework to systematically measure the risk coverage of alignment datasets across three essential dimensions: Lexical Diversity, Malicious Intent, and Jailbreak Tactics. We further introduce TRIDENT, an automated pipeline that leverages persona-based, zero-shot LLM generation to produce diverse and comprehensive instructions spanning these dimensions. Each harmful instruction is paired with an ethically aligned response, resulting in two datasets: TRIDENT-Core, comprising 26,311 examples, and TRIDENT-Edge, with 18,773 examples. Fine-tuning Llama 3.1-8B on TRIDENT-Edge demonstrates substantial improvements, achieving an average 14.29% reduction in Harm Score, and a 20% decrease in Attack Success Rate compared to the best-performing baseline model fine-tuned on the WildBreak dataset.",,"Xiaorui Wu, Xiaofeng Mao, Fei Li, Xin Zhang, Xuanhong Li, Chong Teng, Donghong Ji, Zhuang Li",2025-05-30T15:02:21Z,TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional   Diversified Red-Teaming Data Synthesis,"TRIDENT: Verbesserung der Sicherheit von großen Sprachmodellen mit dreidimensionaler, diversifizierter Red-Teaming-Datensynthese","TRIDENT:用三维多样化的红色组合数据合成,加强大语言模式安全,加强大语言模式安全",http://arxiv.org/abs/2505.24672v1
1354,"Large Language Models (LLMs) need to adapt their predictions to diverse cultural contexts to benefit diverse communities across the world. While previous efforts have focused on single-LLM, single-turn approaches, we propose to exploit the complementary strengths of multiple LLMs to promote cultural adaptability. We introduce a Multi-Agent Debate framework, where two LLM-based agents debate over a cultural scenario and collaboratively reach a final decision. We propose two variants: one where either LLM agents exclusively debate and another where they dynamically choose between self-reflection and debate during their turns. We evaluate these approaches on 7 open-weight LLMs (and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette norms in 75 countries. Experiments show that debate improves both overall accuracy and cultural group parity over single-LLM baselines. Notably, multi-agent debate enables relatively small LLMs (7-9B) to achieve accuracies comparable to that of a much larger model (27B parameters).",,"Dayeon Ki, Rachel Rudinger, Tianyi Zhou, Marine Carpuat",2025-05-30T15:01:52Z,Multiple LLM Agents Debate for Equitable Cultural Alignment,Mehrere LLM-Agenten diskutieren für eine gleichberechtigte kulturelle Ausrichtung,公平文化协调辩论,http://arxiv.org/abs/2505.24671v1
1355,"Large Language Models (LLMs) frequently generate hallucinated content, posing significant challenges for applications where factuality is crucial. While existing hallucination detection methods typically operate at the sentence level or passage level, we propose FactSelfCheck, a novel black-box sampling-based method that enables fine-grained fact-level detection. Our approach represents text as knowledge graphs consisting of facts in the form of triples. Through analyzing factual consistency across multiple LLM responses, we compute fine-grained hallucination scores without requiring external resources or training data. Our evaluation demonstrates that FactSelfCheck performs competitively with leading sentence-level sampling-based methods while providing more detailed insights. Most notably, our fact-level approach significantly improves hallucination correction, achieving a 35.5% increase in factual content compared to the baseline, while sentence-level SelfCheckGPT yields only a 10.6% improvement. The granular nature of our detection enables more precise identification and correction of hallucinated content. Additionally, we contribute a new dataset for evaluating sampling-based methods - FavaMultiSamples.",,"Albert Sawczyn, Jakub Binkowski, Denis Janiak, Bogdan Gabrys, Tomasz Kajdanowicz",2025-05-30T14:59:56Z,FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs,FactSelfCheck: Fact-Level Black-Box Halluzination Detection für LLMs,事实水平的黑色-毒素LLM探明,http://arxiv.org/abs/2503.17229v2
1356,"Vision-language models (VLMs) have achieved strong results on coding and math benchmarks that are challenging for humans, yet their ability to perform tasks that come naturally to humans--such as perception, spatial navigation, and memory management--remains understudied. Real video games are crafted to be intuitive for humans to learn and master by leveraging innate inductive biases, making them an ideal testbed for evaluating such capabilities in VLMs. To this end, we introduce VideoGameBench, a benchmark consisting of 10 popular video games from the 1990s that VLMs directly interact with in real-time. VideoGameBench challenges models to complete entire games with access to only raw visual inputs and a high-level description of objectives and controls, a significant departure from existing setups that rely on game-specific scaffolding and auxiliary information. We keep three of the games secret to encourage solutions that generalize to unseen environments. Our experiments show that frontier vision-language models struggle to progress beyond the beginning of each game. We find inference latency to be a major limitation of frontier models in the real-time setting; therefore, we introduce VideoGameBench Lite, a setting where the game pauses while waiting for the LM's next action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of VideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization of the human skills mentioned above into this benchmark motivates progress in these research directions.",,"Alex L. Zhang, Thomas L. Griffiths, Karthik R. Narasimhan, Ofir Press",2025-05-30T14:50:48Z,VideoGameBench: Can Vision-Language Models complete popular video games?,VideoGameBench: Können Vision-Language-Modelle beliebte Videospiele komplettieren?,视频GameBench:视觉语言模型能否完成流行电子游戏?,http://arxiv.org/abs/2505.18134v2
1357,"Equipping large language models (LLMs) with latent-space memory has attracted increasing attention as they can extend the context window of existing language models. However, retaining information from the distant past remains a challenge. For example, MemoryLLM (Wang et al., 2024a), as a representative work with latent-space memory, compresses past information into hidden states across all layers, forming a memory pool of 1B parameters. While effective for sequence lengths up to 16k tokens, it struggles to retain knowledge beyond 20k tokens. In this work, we address this limitation by introducing M+, a memory-augmented model based on MemoryLLM that significantly enhances long-term information retention. M+ integrates a long-term memory mechanism with a co-trained retriever, dynamically retrieving relevant information during text generation. We evaluate M+ on diverse benchmarks, including long-context understanding and knowledge retention tasks. Experimental results show that M+ significantly outperforms MemoryLLM and recent strong baselines, extending knowledge retention from under 20k to over 160k tokens with similar GPU memory overhead. We open-source our code at https://github.com/wangyu-ustc/MemoryLLM",,"Yu Wang, Dmitry Krotov, Yuanzhe Hu, Yifan Gao, Wangchunshu Zhou, Julian McAuley, Dan Gutfreund, Rogerio Feris, Zexue He",2025-05-30T14:40:56Z,M+: Extending MemoryLLM with Scalable Long-Term Memory,M+: Erweitern von MemoryLLM mit skalierbarem Langzeitspeicher,M+:用可缩放的长期内存扩展内存LLLM,http://arxiv.org/abs/2502.00592v2
1358,"The reliability of large language models (LLMs) is greatly compromised by their tendency to hallucinate, underscoring the need for precise identification of knowledge gaps within LLMs. Various methods for probing such gaps exist, ranging from calibration-based to prompting-based methods. To evaluate these probing methods, in this paper, we propose a new process based on using input variations and quantitative metrics. Through this, we expose two dimensions of inconsistency in knowledge gap probing. (1) Intra-method inconsistency: Minimal non-semantic perturbations in prompts lead to considerable variance in detected knowledge gaps within the same probing method; e.g., the simple variation of shuffling answer options can decrease agreement to around 40%. (2) Cross-method inconsistency: Probing methods contradict each other on whether a model knows the answer. Methods are highly inconsistent -- with decision consistency across methods being as low as 7% -- even though the model, dataset, and prompt are all the same. These findings challenge existing probing methods and highlight the urgent need for perturbation-robust probing frameworks.",,"Raoyuan Zhao, Abdullatif Köksal, Ali Modarressi, Michael A. Hedderich, Hinrich Schütze",2025-05-30T14:39:34Z,Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge   Probing,"Wissen wir, was LLMs nicht wissen? Eine Studie der Konsistenz in der Wissensprobe",我们知道什么是不知道的LLLM不知道的吗?关于知识检验的一致性的研究。,http://arxiv.org/abs/2505.21701v2
1359,"Large language models (LLMs) are used globally across many languages, but their English-centric pretraining raises concerns about cross-lingual disparities for cultural awareness, often resulting in biased outputs. However, comprehensive multilingual evaluation remains challenging due to limited benchmarks and questionable translation quality. To better assess these disparities, we introduce MAKIEval, an automatic multilingual framework for evaluating cultural awareness in LLMs across languages, regions, and topics. MAKIEval evaluates open-ended text generation, capturing how models express culturally grounded knowledge in natural language. Leveraging Wikidata's multilingual structure as a cross-lingual anchor, it automatically identifies cultural entities in model outputs and links them to structured knowledge, enabling scalable, language-agnostic evaluation without manual annotation or translation. We then introduce four metrics that capture complementary dimensions of cultural awareness: granularity, diversity, cultural specificity, and consensus across languages. We assess 7 LLMs developed from different parts of the world, encompassing both open-source and proprietary systems, across 13 languages, 19 countries and regions, and 6 culturally salient topics (e.g., food, clothing). Notably, we find that models tend to exhibit stronger cultural awareness in English, suggesting that English prompts more effectively activate culturally grounded knowledge. We publicly release our code and data.",,"Raoyuan Zhao, Beiduo Chen, Barbara Plank, Michael A. Hedderich",2025-05-30T14:37:57Z,MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural   Awareness Evaluation for LLMs,"MAKIEval: Ein multilingualer, automatischer WiKidata-basierter Rahmen für die Bewertung des kulturellen Bewusstseins für LLMs",MAKIEval:以多种语言自动维基数据为基础的LLMs文化认识评价框架,http://arxiv.org/abs/2505.21693v2
1360,"Reinforcement learning with verifiable rewards (RLVR) has enabled large language models (LLMs) to achieve remarkable breakthroughs in reasoning tasks with objective ground-truth answers, such as mathematics and code generation. However, a significant gap remains for non-verifiable tasks, like creative writing and open-ended dialogue, where quality assessment is inherently subjective and lacks definitive references. Existing approaches for these domains often rely on scalar reward models trained with human preferences, which suffer from limited generalization and are prone to reward hacking, such as over-explanation and length bias. In this work, we propose a unified RLVR-based training paradigm that bridges the gap between non-verifiable tasks and verifiable rewards. We introduce a writing-principle-based pairwise Generative Reward Model (GenRM) and a novel Bootstrapped Relative Policy Optimization (BRPO) algorithm. The pairwise writing GenRM leverages self-principled critique to transform subjective assessments into reliable, verifiable rewards, while BRPO enables dynamic, reference-free pairwise comparison by leveraging a bootstrapped response as temporary reference from within group rollouts during RL training. Our approach empowers LLMs to develop robust writing capabilities without supervised fine-tuning, as demonstrated by Writing-Zero, which shows consistent improvement and strong resistance to reward hacking compared to scalar reward baselines. Furthermore, our method achieves competitive results on both in-house and open-source writing benchmarks. Our findings suggest the potential to unify rule-based, reference-based, and reference-free reward modeling under the RLVR framework, thus paving the way for a comprehensive and scalable RL training paradigm applicable across all language tasks.",,Xun Lu,2025-05-30T14:34:57Z,Writing-Zero: Bridge the Gap Between Non-verifiable Problems and   Verifiable Rewards,Writing-Zero: Brücke zwischen nicht überprüfbaren Problemen und überprüfbaren Belohnungen,零书写:弥合非可核实问题与可核实的奖励之间的差距,http://arxiv.org/abs/2506.00103v1
1361,"Semantic Text Embedding is a fundamental NLP task that encodes textual content into vector representations, where proximity in the embedding space reflects semantic similarity. While existing embedding models excel at capturing general meaning, they often overlook ideological nuances, limiting their effectiveness in tasks that require an understanding of political bias. To address this gap, we introduce PRISM, the first framework designed to Produce inteRpretable polItical biaS eMbeddings. PRISM operates in two key stages: (1) Controversial Topic Bias Indicator Mining, which systematically extracts fine-grained political topics and their corresponding bias indicators from weakly labeled news data, and (2) Cross-Encoder Political Bias Embedding, which assigns structured bias scores to news articles based on their alignment with these indicators. This approach ensures that embeddings are explicitly tied to bias-revealing dimensions, enhancing both interpretability and predictive power. Through extensive experiments on two large-scale datasets, we demonstrate that PRISM outperforms state-of-the-art text embedding models in political bias classification while offering highly interpretable representations that facilitate diversified retrieval and ideological analysis. The source code is available at https://github.com/dukesun99/ACL-PRISM.",,"Yiqun Sun, Qiang Huang, Anthony K. H. Tung, Jun Yu",2025-05-30T14:31:53Z,PRISM: A Framework for Producing Interpretable Political Bias Embeddings   with Political-Aware Cross-Encoder,PRISM: Ein Rahmen für die Herstellung von interpretierbaren politischen Bias Einbettungen mit politikbewusstem Cross-Encoder,PRISM: 与政治软件交叉编码者建立可解释的政治偏见框架,http://arxiv.org/abs/2505.24646v1
1362,"We introduce a novel framework for analyzing sorting algorithms in pairwise ranking prompting (PRP), re-centering the cost model around LLM inferences rather than traditional pairwise comparisons. While classical metrics based on comparison counts have traditionally been used to gauge efficiency, our analysis reveals that expensive LLM inferences overturn these predictions; accordingly, our framework encourages strategies such as batching and caching to mitigate inference costs. We show that algorithms optimal in the classical setting can lose efficiency when LLM inferences dominate the cost under certain optimizations.",,"Juan Wisznia, Cecilia Bolaños, Juan Tollo, Giovanni Marraffini, Agustín Gianolini, Noe Hsueh, Luciano Del Corro",2025-05-30T14:29:55Z,Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based   Pairwise Ranking with Batching and Caching,Sind Optimale Algorithmen immer noch Optimal? Sortieren im LLM-basierten Pairwise-Ranking mit Batching und Caching,最佳算法是否仍然最佳 ? 重新思考以 LLM 为基础与 Batching 和 Caching 相配的 LLM 排序,http://arxiv.org/abs/2505.24643v1
1363,"Labor market analysis relies on extracting insights from job advertisements, which provide valuable yet unstructured information on job titles and corresponding skill requirements. While state-of-the-art methods for skill extraction achieve strong performance, they depend on large language models (LLMs), which are computationally expensive and slow. In this paper, we propose \textbf{ConTeXT-match}, a novel contrastive learning approach with token-level attention that is well-suited for the extreme multi-label classification task of skill classification. \textbf{ConTeXT-match} significantly improves skill extraction efficiency and performance, achieving state-of-the-art results with a lightweight bi-encoder model. To support robust evaluation, we introduce \textbf{Skill-XL}, a new benchmark with exhaustive, sentence-level skill annotations that explicitly address the redundancy in the large label space. Finally, we present \textbf{JobBERT V2}, an improved job title normalization model that leverages extracted skills to produce high-quality job title representations. Experiments demonstrate that our models are efficient, accurate, and scalable, making them ideal for large-scale, real-time labor market analysis.",,"Jens-Joris Decorte, Jeroen Van Hautte, Chris Develder, Thomas Demeester",2025-05-30T14:27:25Z,Efficient Text Encoders for Labor Market Analysis,Effiziente Text-Encoder für die Arbeitsmarktanalyse,劳动力市场分析高效文本编码器,http://arxiv.org/abs/2505.24640v1
1364,"This paper introduces a Dual Evaluation Framework to comprehensively assess the multilingual capabilities of LLMs. By decomposing the evaluation along the dimensions of linguistic medium and cultural context, this framework enables a nuanced analysis of LLMs' ability to process questions within both native and cross-cultural contexts cross-lingually. Extensive evaluations are conducted on a wide range of models, revealing a notable ""CulturalLinguistic Synergy"" phenomenon, where models exhibit better performance when questions are culturally aligned with the language. This phenomenon is further explored through interpretability probing, which shows that a higher proportion of specific neurons are activated in a language's cultural context. This activation proportion could serve as a potential indicator for evaluating multilingual performance during model training. Our findings challenge the prevailing notion that LLMs, primarily trained on English data, perform uniformly across languages and highlight the necessity of culturally and linguistically model evaluations. Our code can be found at https://yingjiahao14. github.io/Dual-Evaluation/.",,"Jiahao Ying, Wei Tang, Yiran Zhao, Yixin Cao, Yu Rong, Wenxuan Zhang",2025-05-30T14:25:45Z,Disentangling Language and Culture for Evaluating Multilingual Large   Language Models,Entwirren von Sprache und Kultur zur Bewertung mehrsprachiger Großsprachenmodelle,"拆分语言和文化,以评价多语言大语言模式",http://arxiv.org/abs/2505.24635v1
1365,"Large language models (LLMs) have significantly advanced in reasoning tasks through reinforcement learning (RL) optimization, achieving impressive capabilities across various challenging benchmarks. However, our empirical analysis reveals a critical drawback: reasoning-oriented RL fine-tuning significantly increases the prevalence of hallucinations. We theoretically analyze the RL training dynamics, identifying high-variance gradient, entropy-induced randomness, and susceptibility to spurious local optima as key factors leading to hallucinations. To address this drawback, we propose Factuality-aware Step-wise Policy Optimization (FSPO), an innovative RL fine-tuning algorithm incorporating explicit factuality verification at each reasoning step. FSPO leverages automated verification against given evidence to dynamically adjust token-level advantage values, incentivizing factual correctness throughout the reasoning process. Experiments across mathematical reasoning and hallucination benchmarks using Qwen2.5 and Llama models demonstrate that FSPO effectively reduces hallucinations while enhancing reasoning accuracy, substantially improving both reliability and performance.",,"Junyi Li, Hwee Tou Ng",2025-05-30T14:23:32Z,The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for   Large Reasoning Models,Das Halluzinations-Dilemma: Factuality-Aware-Verstärkungs-Lernen für große Vernunftmodelle,《幻觉困境:大理由模型的实用-软件强化学习》,http://arxiv.org/abs/2505.24630v1
1366,"Model diffing is the study of how fine-tuning changes a model's representations and internal algorithms. Many behaviors of interest are introduced during fine-tuning, and model diffing offers a promising lens to interpret such behaviors. Crosscoders are a recent model diffing method that learns a shared dictionary of interpretable concepts represented as latent directions in both the base and fine-tuned models, allowing us to track how concepts shift or emerge during fine-tuning. Notably, prior work has observed concepts with no direction in the base model, and it was hypothesized that these model-specific latents were concepts introduced during fine-tuning. However, we identify two issues which stem from the crosscoders L1 training loss that can misattribute concepts as unique to the fine-tuned model, when they really exist in both models. We develop Latent Scaling to flag these issues by more accurately measuring each latent's presence across models. In experiments comparing Gemma 2 2B base and chat models, we observe that the standard crosscoder suffers heavily from these issues. Building on these insights, we train a crosscoder with BatchTopK loss and show that it substantially mitigates these issues, finding more genuinely chat-specific and highly interpretable concepts. We recommend practitioners adopt similar techniques. Using the BatchTopK crosscoder, we successfully identify a set of chat-specific latents that are both interpretable and causally effective, representing concepts such as $\textit{false information}$ and $\textit{personal question}$, along with multiple refusal-related latents that show nuanced preferences for different refusal triggers. Overall, our work advances best practices for the crosscoder-based methodology for model diffing and demonstrates that it can provide concrete insights into how chat-tuning modifies model behavior.",,"Julian Minder, Clément Dumas, Caden Juang, Bilal Chugtai, Neel Nanda",2025-05-30T14:15:11Z,Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning,"Überwindung von Sparsity-Artefakten in Crosscodern, um Chat-Tuning zu interpretieren",克服交叉器中用于翻译聊天聊天时的分性手工艺,http://arxiv.org/abs/2504.02922v2
1367,"Universal goal hijacking is a kind of prompt injection attack that forces LLMs to return a target malicious response for arbitrary normal user prompts. The previous methods achieve high attack performance while being too cumbersome and time-consuming. Also, they have concentrated solely on optimization algorithms, overlooking the crucial role of the prompt. To this end, we propose a method called POUGH that incorporates an efficient optimization algorithm and two semantics-guided prompt organization strategies. Specifically, our method starts with a sampling strategy to select representative prompts from a candidate pool, followed by a ranking strategy that prioritizes them. Given the sequentially ranked prompts, our method employs an iterative optimization algorithm to generate a fixed suffix that can concatenate to arbitrary user prompts for universal goal hijacking. Experiments conducted on four popular LLMs and ten types of target responses verified the effectiveness.",,"Yihao Huang, Chong Wang, Xiaojun Jia, Qing Guo, Felix Juefei-Xu, Jian Zhang, Geguang Pu, Yang Liu",2025-05-30T14:12:54Z,Efficient Universal Goal Hijacking with Semantics-guided Prompt   Organization,Effizientes Universal Goal Hijacking mit Semantik-geführter Prompt-Organisation,与以语义为指南的迅速组织一起劫持,http://arxiv.org/abs/2405.14189v2
1368,"Recent advancements in Large Language Models (LLMs) have transformed natural language understanding and generation, leading to extensive benchmarking across diverse tasks. However, cryptanalysis a critical area for data security and encryption has not yet been thoroughly explored in LLM evaluations. To address this gap, we evaluate cryptanalytic potential of state of the art LLMs on encrypted texts generated using a range of cryptographic algorithms. We introduce a novel benchmark dataset comprising diverse plain texts spanning various domains, lengths, writing styles, and topics paired with their encrypted versions. Using zero-shot and few shot settings, we assess multiple LLMs for decryption accuracy and semantic comprehension across different encryption schemes. Our findings reveal key insights into the strengths and limitations of LLMs in side-channel communication while raising concerns about their susceptibility to jailbreaking attacks. This research highlights the dual-use nature of LLMs in security contexts and contributes to the ongoing discussion on AI safety and security.",,"Utsav Maskey, Chencheng Zhu, Usman Naseem",2025-05-30T14:12:07Z,Benchmarking Large Language Models for Cryptanalysis and   Mismatched-Generalization,Benchmarking von großen Sprachmodellen für Kryptanalyse und falscher Generalisierung,确定用于加密分析和不匹配的通用化的大语言模式基准,http://arxiv.org/abs/2505.24621v1
1369,"Objective: Heart failure (HF) patients present with diverse phenotypes affecting treatment and prognosis. This study evaluates models for phenotyping HF patients based on left ventricular ejection fraction (LVEF) classes, using structured and unstructured data, assessing performance and interpretability.   Materials and Methods: The study analyzes all HF hospitalizations at both Amsterdam UMC hospitals (AMC and VUmc) from 2015 to 2023 (33,105 hospitalizations, 16,334 patients). Data from AMC were used for model training, and from VUmc for external validation. The dataset was unlabelled and included tabular clinical measurements and discharge letters. Silver labels for LVEF classes were generated by combining diagnosis codes, echocardiography results, and textual mentions. Gold labels were manually annotated for 300 patients for testing. Multiple Transformer-based (black-box) and Aug-Linear (white-box) models were trained and compared with baselines on structured and unstructured data. To evaluate interpretability, two clinicians annotated 20 discharge letters by highlighting information they considered relevant for LVEF classification. These were compared to SHAP and LIME explanations from black-box models and the inherent explanations of Aug-Linear models.   Results: BERT-based and Aug-Linear models, using discharge letters alone, achieved the highest classification results (AUC=0.84 for BERT, 0.81 for Aug-Linear on external validation), outperforming baselines. Aug-Linear explanations aligned more closely with clinicians' explanations than post-hoc explanations on black-box models.   Conclusions: Discharge letters emerged as the most informative source for phenotyping HF patients. Aug-Linear models matched black-box performance while providing clinician-aligned interpretability, supporting their use in transparent clinical decision-making.",,"Vittorio Torri, Machteld J. Boonstra, Marielle C. van de Veerdonk, Deborah N. Kalkman, Alicia Uijl, Francesca Ieva, Ameen Abu-Hanna, Folkert W. Asselbergs, Iacer Calixto",2025-05-30T14:11:32Z,Interpretable phenotyping of Heart Failure patients with Dutch discharge   letters,Interpretierbare Phänotypisierung von Herzinsuffizienz-Patienten mit niederländischen Entladungsbuchstaben,使用荷兰释放信解释心衰竭病人的情况,http://arxiv.org/abs/2505.24619v1
1370,"The evaluation of machine-generated image captions is a complex and evolving challenge. With the advent of Multimodal Large Language Models (MLLMs), image captioning has become a core task, increasing the need for robust and reliable evaluation metrics. This survey provides a comprehensive overview of advancements in image captioning evaluation, analyzing the evolution, strengths, and limitations of existing metrics. We assess these metrics across multiple dimensions, including correlation with human judgment, ranking accuracy, and sensitivity to hallucinations. Additionally, we explore the challenges posed by the longer and more detailed captions generated by MLLMs and examine the adaptability of current metrics to these stylistic variations. Our analysis highlights some limitations of standard evaluation approaches and suggests promising directions for future research in image captioning assessment.",,"Sara Sarto, Marcella Cornia, Rita Cucchiara",2025-05-30T14:11:25Z,Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges   and Future Perspectives,Bildunterschrift Evaluierung im Zeitalter multimodaler LLMs: Herausforderungen und Zukunftsperspektiven,多模式LLMs时代的图像描述评价:挑战和未来展望,http://arxiv.org/abs/2503.14604v2
1371,"We introduce POLLUX, a comprehensive open-source benchmark designed to evaluate the generative capabilities of large language models (LLMs) in Russian. Our main contribution is a novel evaluation methodology that enhances the interpretability of LLM assessment. For each task type, we define a set of detailed criteria and develop a scoring protocol where models evaluate responses and provide justifications for their ratings. This enables transparent, criteria-driven evaluation beyond traditional resource-consuming, side-by-side human comparisons. POLLUX includes a detailed, fine-grained taxonomy of 35 task types covering diverse generative domains such as code generation, creative writing, and practical assistant use cases, totaling 2,100 manually crafted and professionally authored prompts. Each task is categorized by difficulty (easy/medium/hard), with experts constructing the dataset entirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B) evaluators trained for nuanced assessment of generative outputs. This approach provides scalable, interpretable evaluation and annotation tools for model development, effectively replacing costly and less precise human judgments.",,"Nikita Martynov, Anastasia Mordasheva, Dmitriy Gorbetskiy, Danil Astafurov, Ulyana Isaeva, Elina Basyrova, Sergey Skachkov, Victoria Berestova, Nikolay Ivanov, Valeriia Zanina, Alena Fenogenova",2025-05-30T14:08:17Z,Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs   with POLLUX,Auge des Urteils: Die Bewertung der russischsprachigen LLMs mit POLLUX,判断之眼:用POLLUX对讲俄语的LLMs的评价进行分解,http://arxiv.org/abs/2505.24616v1
1372,"In an era of exponential scientific growth, identifying novel research ideas is crucial and challenging in academia. Despite potential, the lack of an appropriate benchmark dataset hinders the research of novelty detection. More importantly, simply adopting existing NLP technologies, e.g., retrieving and then cross-checking, is not a one-size-fits-all solution due to the gap between textual similarity and idea conception. In this paper, we propose to harness large language models (LLMs) for scientific novelty detection (ND), associated with two new datasets in marketing and NLP domains. To construct the considerate datasets for ND, we propose to extract closure sets of papers based on their relationship, and then summarize their main ideas based on LLMs. To capture idea conception, we propose to train a lightweight retriever by distilling the idea-level knowledge from LLMs to align ideas with similar conception, enabling efficient and accurate idea retrieval for LLM novelty detection. Experiments show our method consistently outperforms others on the proposed benchmark datasets for idea retrieval and ND tasks. Codes and data are available at https://anonymous.4open.science/r/NoveltyDetection-10FB/.",,"Yan Liu, Zonglin Yang, Soujanya Poria, Thanh-Son Nguyen, Erik Cambria",2025-05-30T14:08:13Z,Harnessing Large Language Models for Scientific Novelty Detection,Große Sprachmodelle für wissenschaftliche Neuheitserkennung nutzen,利用大语言模型进行科学创新探测,http://arxiv.org/abs/2505.24615v1
1373,"Natural Question Answering (QA) datasets play a crucial role in evaluating the capabilities of large language models (LLMs), ensuring their effectiveness in real-world applications. Despite the numerous QA datasets that have been developed and some work has been done in parallel, there is a notable lack of a framework and large scale region-specific datasets queried by native users in their own languages. This gap hinders the effective benchmarking and the development of fine-tuned models for regional and cultural specificities. In this study, we propose a scalable, language-independent framework, NativQA, to seamlessly construct culturally and regionally aligned QA datasets in native languages, for LLM evaluation and tuning. We demonstrate the efficacy of the proposed framework by designing a multilingual natural QA dataset, MultiNativQA, consisting of ~64k manually annotated QA pairs in seven languages, ranging from high to extremely low resource, based on queries from native speakers from 9 regions covering 18 topics. We benchmark open- and closed-source LLMs with the MultiNativQA dataset. We made the MultiNativQA dataset(https://huggingface.co/datasets/QCRI/MultiNativQA), and other experimental scripts(https://gitlab.com/nativqa/multinativqa) publicly available for the community.",,"Md. Arid Hasan, Maram Hasanain, Fatema Ahmad, Sahinur Rahman Laskar, Sunaya Upadhyay, Vrunda N Sukhadia, Mucahid Kutlu, Shammur Absar Chowdhury, Firoj Alam",2025-05-30T14:06:34Z,NativQA: Multilingual Culturally-Aligned Natural Query for LLMs,"NativQA: Mehrsprachige, kulturell ausgerichtete natürliche Abfrage für LLMs",NativQA: 多语种文化和谐的自然搜索LLMs,http://arxiv.org/abs/2407.09823v3
1374,"Endowing dialogue agents with persona information has proven to significantly improve the consistency and diversity of their generations. While much focus has been placed on aligning dialogues with provided personas, the adaptation to the interlocutor's profile remains largely underexplored. In this work, we investigate three key aspects: (1) a model's ability to align responses with both the provided persona and the interlocutor's; (2) its robustness when dealing with familiar versus unfamiliar interlocutors and topics, and (3) the impact of additional fine-tuning on specific persona-based dialogues. We evaluate dialogues generated with diverse speaker pairings and topics, framing the evaluation as an author identification task and employing both LLM-as-a-judge and human evaluations. By systematically masking or disclosing information about the interlocutor, we assess its impact on dialogue generation. Results show that access to the interlocutor's persona improves the recognition of the target speaker, while masking it does the opposite. Although models generalise well across topics, they struggle with unfamiliar interlocutors. Finally, we found that in zero-shot settings, LLMs often copy biographical details, facilitating identification but trivialising the task.",,"Daniela Occhipinti, Marco Guerini, Malvina Nissim",2025-05-30T14:04:30Z,When Harry Meets Superman: The Role of The Interlocutor in Persona-Based   Dialogue Generation,Wenn Harry Superman trifft: Die Rolle des Gesprächspartners in Persona-Based Dialogue Generation,当哈里与超人相遇时:对话者在人造对话一代中的作用,http://arxiv.org/abs/2505.24613v1
1375,"In recent years, the need for natural language interfaces to knowledge graphs has become increasingly important since they enable easy and efficient access to the information contained in them. In particular, property graphs (PGs) have seen increased adoption as a means of representing complex structured information. Despite their growing popularity in industry, PGs remain relatively underrepresented in semantic parsing research with a lack of resources for evaluation. To address this gap, we introduce ZOGRASCOPE, a benchmark designed specifically for PGs and queries written in Cypher. Our benchmark includes a diverse set of manually annotated queries of varying complexity and is organized into three partitions: iid, compositional and length. We complement this paper with a set of experiments that test the performance of different LLMs in a variety of learning settings.",,"Francesco Cazzaro, Justin Kleindienst, Sofia Marquez Gomez, Ariadna Quattoni",2025-05-30T14:02:52Z,ZOGRASCOPE: A New Benchmark for Semantic Parsing over Property Graphs,ZOGRASCOPE: Ein neuer Benchmark für semantische Parsing über Immobiliengraphen,ZOGRASCOPE:财产图的语义分析新基准,http://arxiv.org/abs/2503.05268v2
1376,"Adapting pre-trained models has become an effective strategy in artificial intelligence, offering a scalable and efficient alternative to training models from scratch. In the context of remote sensing (RS), where visual grounding(VG) remains underexplored, this approach enables the deployment of powerful vision-language models to achieve robust cross-modal understanding while significantly reducing computational overhead. To address this, we applied Parameter Efficient Fine Tuning (PEFT) techniques to adapt these models for RS-specific VG tasks. Specifically, we evaluated LoRA placement across different modules in Grounding DINO and used BitFit and adapters to fine-tune the OFA foundation model pre-trained on general-purpose VG datasets. This approach achieved performance comparable to or surpassing current State Of The Art (SOTA) models while significantly reducing computational costs. This study highlights the potential of PEFT techniques to advance efficient and precise multi-modal analysis in RS, offering a practical and cost-effective alternative to full model training.",,"Hasan Moughnieh, Mohamad Chalhoub, Hasan Nasrallah, Cristiano Nattero, Paolo Campanella, Giovanni Nico, Ali J. Ghandour",2025-05-30T14:02:07Z,Efficient Adaptation For Remote Sensing Visual Grounding,Effiziente Anpassung für die Fernerkundung visueller Erdung,用于遥感视觉定位的高效适应,http://arxiv.org/abs/2503.23083v3
1377,"This paper addresses the critical need for improved explainability in text-based depression detection. While offering predictive outcomes, current solutions often overlook the understanding of model predictions which can hinder trust in the system. We propose the use of Masked Hard Instance Mining (MHIM) to enhance the explainability in the depression detection task. MHIM strategically masks attention weights within the model, compelling it to distribute attention across a wider range of salient features. We evaluate MHIM on two datasets representing distinct languages: Thai (Thai-Maywe) and English (DAIC-WOZ). Our results demonstrate that MHIM significantly improves performance in terms of both prediction accuracy and explainability metrics.",,"Patawee Prakrankamanant, Shinji Watanabe, Ekapol Chuangsuwanich",2025-05-30T14:01:20Z,Explainable Depression Detection using Masked Hard Instance Mining,Erklärbare Depression Detection mit Masked Hard Instance Mining,利用隐蔽硬金矿开采法进行可解释的抑郁症检测,http://arxiv.org/abs/2505.24609v1
1378,"The speech tokenizer plays a crucial role in recent speech tasks, generally serving as a bridge between speech signals and language models. While low-frame-rate codecs are widely employed as speech tokenizers, the impact of frame rates on speech tokens remains underexplored. In this study, we investigate how varying frame rates affect speech tokenization by examining Mandarin and English, two typologically distinct languages. We encode speech at different frame rates and evaluate the resulting semantic tokens in the speech recognition task. Our findings reveal that frame rate variations influence speech tokenization differently for each language, highlighting the interplay between frame rates, phonetic density, and language-specific acoustic features. The results provide insights into optimizing frame rate selection for speech tokenizers, with implications for automatic speech recognition, text-to-speech, and other speech-related applications.",,"Haoyang Zhang, Hexin Liu, Xiangyu Zhang, Qiquan Zhang, Yuchen Hu, Junqi Zhao, Fei Tian, Xuerui Yang, Eng Siong Chng",2025-05-30T14:00:46Z,Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and   English,Auswirkungen der Rahmensätze auf Sprachtokenizer: Eine Fallstudie zu Mandarin und Englisch,《框架率对语言控制器的影响:普通话和英语案例研究》,http://arxiv.org/abs/2505.17076v2
1379,"We introduce MAIA (Multimodal AI Assessment), a native-Italian benchmark designed for fine-grained investigation of the reasoning abilities of visual language models on videos. MAIA differs from other available video benchmarks for its design, its reasoning categories, the metric it uses, and the language and culture of the videos. MAIA evaluates Vision Language Models (VLMs) on two aligned tasks: a visual statement verification task and an open-ended visual question-answering task, both on the same set of video-related questions. It considers twelve reasoning categories that aim to disentangle language and vision relations by highlighting the role of the visual input. Thanks to its carefully taught design, it evaluates VLMs' consistency and visually grounded natural language comprehension and generation simultaneously through an aggregated metric revealing low results that highlight models' fragility. Last but not least, the video collection has been carefully selected to reflect the Italian culture, and the language data are produced by native-speakers.",,"Davide Testa, Giovanni Bonetta, Raffaella Bernardi, Alessandro Bondielli, Alessandro Lenci, Alessio Miaschi, Lucia Passaro, Bernardo Magnini",2025-05-30T13:57:45Z,All-in-one: Understanding and Generation in Multimodal Reasoning with   the MAIA Benchmark,All-in-One: Verständnis und Generierung in multimodaler Vernunft mit dem MAIA-Benchmark,All-in-one:根据MAIA基准的多模式理由理解和创造,http://arxiv.org/abs/2502.16989v2
1380,"Many court systems are overwhelmed all over the world, leading to huge backlogs of pending cases. Effective triage systems, like those in emergency rooms, could ensure proper prioritization of open cases, optimizing time and resource allocation in the court system. In this work, we introduce the Criticality Prediction dataset, a novel resource for evaluating case prioritization. Our dataset features a two-tier labeling system: (1) the binary LD-Label, identifying cases published as Leading Decisions (LD), and (2) the more granular Citation-Label, ranking cases by their citation frequency and recency, allowing for a more nuanced evaluation. Unlike existing approaches that rely on resource-intensive manual annotations, we algorithmically derive labels leading to a much larger dataset than otherwise possible. We evaluate several multilingual models, including both smaller fine-tuned models and large language models in a zero-shot setting. Our results show that the fine-tuned models consistently outperform their larger counterparts, thanks to our large training set. Our results highlight that for highly domain-specific tasks like ours, large training sets are still valuable.",,"Ronja Stern, Ken Kawamura, Matthias Stürmer, Ilias Chalkidis, Joel Niklaus",2025-05-30T13:57:28Z,From Citations to Criticality: Predicting Legal Decision Influence in   the Multilingual Swiss Jurisprudence,Von Zitationen zur Kritikalität: Vorhersagen von Rechtsentscheidungen Einfluss in die Mehrsprachige Schweizer Jurisprudenz,《从引言到临界:预测法律决定对多种语言的瑞士判例的影响》,http://arxiv.org/abs/2410.13460v2
1381,"We present a novel reasoning approach called Flow-of-Options (FoO), designed to address intrinsic biases in Large Language Models (LLMs). Flow-of-Options enables LLMs to systematically explore a diverse range of possibilities in their reasoning, as demonstrated by an FoO-based agentic framework developed for autonomously solving Machine Learning (ML) tasks. FoO enforces diversity in LLM solutions through compressed and interpretable task representations, resulting in improvements of 38.2% - 69.2% on standard data science tasks, and 37.4% - 47.9% on therapeutic chemistry tasks, as compared to state-of-the-art baselines. With an overall operation cost under $1 per task, our framework is well-suited for cost-sensitive applications. Going beyond tabular classification and regression, we show the broader applicability of our FoO-based agentic system to tasks such as reinforcement learning and image generation. Our code is open-sourced at: https://github.com/flagshippioneering/Flow-of-Options.",,"Lakshmi Nair, Ian Trase, Mark Kim",2025-05-30T13:56:47Z,Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking   Through Options,Flow-of-Options: Diversifizierte und verbesserte LLM-Gründung durch das Durchdenken von Optionen,选择流动:通过思考选择来解释多样化和改进的LLM,http://arxiv.org/abs/2502.12929v2
1382,"In Switzerland legal translation is uniquely important due to the country's four official languages and requirements for multilingual legal documentation. However, this process traditionally relies on professionals who must be both legal experts and skilled translators -- creating bottlenecks and impacting effective access to justice. To address this challenge, we introduce SwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned Swiss legal translation pairs comprising laws, headnotes, and press releases across all Swiss languages along with English, designed to evaluate LLM-based translation systems. Our systematic evaluation reveals that frontier models achieve superior translation performance across all document types, while specialized translation systems excel specifically in laws but under-perform in headnotes. Through rigorous testing and human expert validation, we demonstrate that while fine-tuning open SLMs significantly improves their translation quality, they still lag behind the best zero-shot prompted frontier models such as Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM evaluation system that aligns best with human expert assessments.",,"Joel Niklaus, Jakob Merane, Luka Nenadic, Sina Ahmadi, Yingqiang Gao, Cyrill A. H. Chevalley, Claude Humbel, Christophe Gösken, Lorenzo Tanzi, Thomas Lüthi, Stefan Palombo, Spencer Poff, Boling Yang, Nan Wu, Matthew Guillod, Robin Mamié, Daniel Brunner, Julio Pereyra, Niko Grupen",2025-05-30T13:48:42Z,SwiLTra-Bench: The Swiss Legal Translation Benchmark,SwiLTra-Bench: Der Schweizer Benchmark für Rechtsübersetzungen,SwiLTra-Bunch:瑞士法律翻译基准,http://arxiv.org/abs/2503.01372v2
1383,"The interpretability of Mixture-of-Experts (MoE) models, especially those with heterogeneous designs, remains underexplored. Existing attribution methods for dense models fail to capture dynamic routing-expert interactions in sparse MoE architectures. To address this issue, we propose a cross-level attribution algorithm to analyze sparse MoE architectures (Qwen 1.5-MoE, OLMoE, Mixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mixtral-7B). Results show MoE models achieve 37% higher per-layer efficiency via a ""mid-activation, late-amplification"" pattern: early layers screen experts, while late layers refine knowledge collaboratively. Ablation studies reveal a ""basic-refinement"" framework--shared experts handle general tasks (entity recognition), while routed experts specialize in domain-specific processing (geographic attributes). Semantic-driven routing is evidenced by strong correlations between attention heads and experts (r=0.68), enabling task-aware coordination. Notably, architectural depth dictates robustness: deep Qwen 1.5-MoE mitigates expert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10 experts) through shared expert redundancy, whereas shallow OLMoE suffers severe degradation (76% drop). Task sensitivity further guides design: core-sensitive tasks (geography) require concentrated expertise, while distributed-tolerant tasks (object attributes) leverage broader participation. These insights advance MoE interpretability, offering principles to balance efficiency, specialization, and robustness.",,"Junzhuo Li, Bo Wang, Xiuze Zhou, Peijie Jiang, Jia Liu, Xuming Hu",2025-05-30T13:40:51Z,Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of   Basic-Refinement Collaboration and Efficiency Analysis,Decoding Knowledge Attribution in Mixture-of-Experts: Ein Rahmenwerk der Basic-Refinement-Kollaboration und Effizienzanalyse,在混合专家中解码知识的配置:基础改进协作和效率分析框架,http://arxiv.org/abs/2505.24593v1
1384,"Recent advances in LLMs, particularly in language reasoning and tool integration, have rapidly sparked the \emph{Language Agents} for real-world development. Among these, travel planning represents a prominent domain, combining complex multi-objective planning challenges with practical deployment demands. However, existing benchmarks often oversimplify real-world requirements by focusing on synthetic queries and limited constraints. We address the gap of evaluating language agents in multi-day, multi-POI travel planning scenarios with diverse and open human needs. Specifically, we introduce \emph{ChinaTravel}, the first open-ended benchmark grounded in authentic Chinese travel requirements collected from 1,154 human participants. We design a compositionally generalizable domain-specific language (DSL) for scalable evaluation, covering feasibility, constraint satisfaction, and preference comparison. Empirical studies reveal the potential of neuro-symbolic agents in travel planning, achieving a 37.0\% constraint satisfaction rate on human queries, a 10\times improvement over purely neural models. These findings highlight ChinaTravel as a pivotal milestone for advancing language agents in complex, real-world planning scenarios.",,"Jie-Jing Shao, Bo-Wen Zhang, Xiao-Wen Yang, Baizhi Chen, Si-Yu Han, Wen-Da Wei, Guohao Cai, Zhenhua Dong, Lan-Zhe Guo, Yu-feng Li",2025-05-30T13:35:50Z,ChinaTravel: An Open-Ended Benchmark for Language Agents in Chinese   Travel Planning,ChinaTravel: Ein offener Benchmark für Sprachagenten in der chinesischen Reiseplanung,《华人旅行:中国旅行规划中语文代表的开放式基准》,http://arxiv.org/abs/2412.13682v3
1385,"Semantic textual similarity (STS) is a critical task in natural language processing (NLP), enabling applications in retrieval, clustering, and understanding semantic relationships between texts. However, research in this area for the Arabic language remains limited due to the lack of high-quality datasets and pre-trained models. This scarcity of resources has restricted the accurate evaluation and advance of semantic similarity in Arabic text. This paper introduces General Arabic Text Embedding (GATE) models that achieve state-of-the-art performance on the Semantic Textual Similarity task within the MTEB benchmark. GATE leverages Matryoshka Representation Learning and a hybrid loss training approach with Arabic triplet datasets for Natural Language Inference, which are essential for enhancing model performance in tasks that demand fine-grained semantic understanding. GATE outperforms larger models, including OpenAI, with a 20-25% performance improvement on STS benchmarks, effectively capturing the unique semantic nuances of Arabic.",,"Omer Nacar, Anis Koubaa, Serry Sibaee, Yasser Al-Habashi, Adel Ammar, Wadii Boulila",2025-05-30T13:29:03Z,GATE: General Arabic Text Embedding for Enhanced Semantic Textual   Similarity with Matryoshka Representation Learning and Hybrid Loss Training,GATE: Allgemeines Arabisches Text-Embedding für verbesserte semantische Text-Ähnlichkeit mit Matryoshka Representative Learning und Hybrid Loss Training,GATE:关于与Matryoshka代表制学习和混合损失培训增强语义文字相似性的一般阿拉伯文本,http://arxiv.org/abs/2505.24581v1
1386,"Summarizing long-form narratives--such as books, movies, and TV scripts--requires capturing intricate plotlines, character interactions, and thematic coherence, a task that remains challenging for existing LLMs. We introduce NexusSum, a multi-agent LLM framework for narrative summarization that processes long-form text through a structured, sequential pipeline--without requiring fine-tuning. Our approach introduces two key innovations: (1) Dialogue-to-Description Transformation: A narrative-specific preprocessing method that standardizes character dialogue and descriptive text into a unified format, improving coherence. (2) Hierarchical Multi-LLM Summarization: A structured summarization pipeline that optimizes chunk processing and controls output length for accurate, high-quality summaries. Our method establishes a new state-of-the-art in narrative summarization, achieving up to a 30.0% improvement in BERTScore (F1) across books, movies, and TV scripts. These results demonstrate the effectiveness of multi-agent LLMs in handling long-form content, offering a scalable approach for structured summarization in diverse storytelling domains.",,"Hyuntak Kim, Byung-Hak Kim",2025-05-30T13:26:23Z,NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization,NexusSum: Hierarchische LLM-Agenten für langformige Narrative Zusammenfassung,NexusSum: 长式叙述式描述的等级级LLM代理物,http://arxiv.org/abs/2505.24575v1
1387,"Automating primary stress identification has been an active research field due to the role of stress in encoding meaning and aiding speech comprehension. Previous studies relied mainly on traditional acoustic features and English datasets. In this paper, we investigate the approach of fine-tuning a pre-trained transformer model with an audio frame classification head. Our experiments use a new Croatian training dataset, with test sets in Croatian, Serbian, the Chakavian dialect, and Slovenian. By comparing an SVM classifier using traditional acoustic features with the fine-tuned speech transformer, we demonstrate the transformer's superiority across the board, achieving near-perfect results for Croatian and Serbian, with a 10-point performance drop for the more distant Chakavian and Slovenian. Finally, we show that only a few hundred multi-syllabic training words suffice for strong performance. We release our datasets and model under permissive licenses.",,"Nikola Ljubešić, Ivan Porupski, Peter Rupnik",2025-05-30T13:23:46Z,Identifying Primary Stress Across Related Languages and Dialects with   Transformer-based Speech Encoder Models,Identifizierung primärer Stress in verwandten Sprachen und Dialekten mit transformerbasierten Sprach-Encoder-Modellen,与基于变换语音编码器模型的变换语音编码器模型查明相关语言和方言的主要压力,http://arxiv.org/abs/2505.24571v1
1388,"Large language models (LLMs) exhibit excellent performance in natural language processing (NLP), but remain highly sensitive to the quality of input queries, especially when these queries contain misleading or inaccurate information. Existing methods focus on correcting the output, but they often overlook the potential of improving the ability of LLMs to detect and correct misleading content in the input itself. In this paper, we propose a novel three-stage fine-tuning method that enhances the ability of LLMs to detect and correct misleading information in the input, further improving response accuracy and reducing hallucinations. Specifically, the three stages include (1) training LLMs to identify misleading information, (2) training LLMs to correct the misleading information using built-in or external knowledge, and (3) training LLMs to generate accurate answers based on the corrected queries. To evaluate our method, we conducted experiments on three datasets for the hallucination detection task and the question answering~(QA) task, as well as two datasets containing misleading information that we constructed. The experimental results demonstrate that our method significantly improves the accuracy and factuality of LLM responses, while also enhancing the ability to detect hallucinations and reducing the generation of hallucinations in the output, particularly when the query contains misleading information.",,"Guocong Li, Weize Liu, Yihang Wu, Ping Wang, Shuaihan Huang, Hongxia Xu, Jian Wu",2025-05-30T13:16:10Z,From Misleading Queries to Accurate Answers: A Three-Stage Fine-Tuning   Method for LLMs,Von irreführenden Abfragen zu präzisen Antworten: Eine dreistufige Feinsteuerungsmethode für LLMs,从错误引导询问到准确答案:LLMM的三步精准调试方法,http://arxiv.org/abs/2504.11277v2
1389,"Current translation systems, despite being highly multilingual, cover only 5% of the world's languages. Expanding language coverage to the long-tail of low-resource languages requires data-efficient methods that rely on cross-lingual and cross-modal knowledge transfer. To this end, we propose a character-based approach to improve adaptability to new languages and modalities. Our method leverages SONAR, a multilingual fixed-size embedding space with different modules for encoding and decoding. We use a teacher-student approach with parallel translation data to obtain a character-level encoder. Then, using ASR data, we train a lightweight adapter to connect a massively multilingual CTC ASR model (MMS), to the character-level encoder, potentially enabling speech translation from 1,000+ languages. Experimental results in text translation for 75 languages on FLORES+ demonstrate that our character-based approach can achieve better language transfer than traditional subword-based models, especially outperforming them in low-resource settings, and demonstrating better zero-shot generalizability to unseen languages. Our speech adaptation, maximizing knowledge transfer from the text modality, achieves state-of-the-art results in speech-to-text translation on the FLEURS benchmark on 33 languages, surpassing previous supervised and cascade models, albeit being a zero-shot model with minimal supervision from ASR data.",,"Ioannis Tsiamas, David Dale, Marta R. Costa-jussà",2025-05-30T13:16:08Z,Improving Language and Modality Transfer in Translation by   Character-level Modeling,Verbesserung der Sprach- und Modalitätsübertragung in der Übersetzung durch Charaktermodellierung,改进按品级建模方式翻译的语言和方式方式转让,http://arxiv.org/abs/2505.24561v1
1390,"In the quest for artificial general intelligence, Multi-modal Large Language Models (MLLMs) have emerged as a focal point in recent advancements. However, the predominant focus remains on developing their capabilities in static image understanding. The potential of MLLMs in processing sequential visual data is still insufficiently explored, highlighting the absence of a comprehensive, high-quality assessment of their performance. In this paper, we introduce Video-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of MLLMs in Video analysis. Our work distinguishes from existing benchmarks through four key features: 1) Diversity in video types, spanning 6 primary visual domains with 30 subfields to ensure broad scenario generalizability; 2) Duration in temporal dimension, encompassing both short-, medium-, and long-term videos, ranging from 11 seconds to 1 hour, for robust contextual dynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides video frames, including subtitles and audios, to unveil the all-round capabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual labeling by expert annotators to facilitate precise and reliable model assessment. 900 videos with a total of 254 hours are manually selected and annotated by repeatedly viewing all the video content, resulting in 2,700 question-answer pairs. With Video-MME, we extensively evaluate various state-of-the-art MLLMs, including GPT-4 series and Gemini 1.5 Pro, as well as open-source image models like InternVL-Chat-V1.5 and video models like LLaVA-NeXT-Video. Our experiments reveal that Gemini 1.5 Pro is the best-performing commercial model, significantly outperforming the open-source models. Our dataset along with these findings underscores the need for further improvements in handling longer sequences and multi-modal data. Project Page: https://video-mme.github.io",,"Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Caifeng Shan, Ran He, Xing Sun",2025-05-30T13:08:27Z,Video-MME: The First-Ever Comprehensive Evaluation Benchmark of   Multi-modal LLMs in Video Analysis,Video-MME: Der erste umfassende Evaluierungs-Benchmark für multimodale LLMs in der Videoanalyse,视频-MME:视频分析中多模式LMS第一期综合评价基准,http://arxiv.org/abs/2405.21075v3
1391,"Cross-modal entity linking refers to the ability to align entities and their attributes across different modalities. While cross-modal entity linking is a fundamental skill needed for real-world applications such as multimodal code generation, fake news detection, or scene understanding, it has not been thoroughly studied in the literature. In this paper, we introduce a new task and benchmark to address this gap. Our benchmark, MATE, consists of 5.5k evaluation instances featuring visual scenes aligned with their textual representations. To evaluate cross-modal entity linking performance, we design a question-answering task that involves retrieving one attribute of an object in one modality based on a unique attribute of that object in another modality. We evaluate state-of-the-art Vision-Language Models (VLMs) and humans on this task, and find that VLMs struggle significantly compared to humans, particularly as the number of objects in the scene increases. Our analysis also shows that, while chain-of-thought prompting can improve VLM performance, models remain far from achieving human-level proficiency. These findings highlight the need for further research in cross-modal entity linking and show that MATE is a strong benchmark to support that progress.",,"Iñigo Alonso, Gorka Azkune, Ander Salaberria, Jeremy Barnes, Oier Lopez de Lacalle",2025-05-30T13:04:40Z,Vision-Language Models Struggle to Align Entities across Modalities,"Vision-Language-Modelle kämpfen, um Entitäten über Modalitäten hinweg auszurichten",不同模式实体对齐的愿景-语言模型,http://arxiv.org/abs/2503.03854v2
1392,"Understanding complex character relations is crucial for narrative analysis and efficient script evaluation, yet existing extraction methods often fail to handle long-form narratives with nuanced interactions. To address this challenge, we present CREFT, a novel sequential framework leveraging specialized Large Language Model (LLM) agents. First, CREFT builds a base character graph through knowledge distillation, then iteratively refines character composition, relation extraction, role identification, and group assignments. Experiments on a curated Korean drama dataset demonstrate that CREFT significantly outperforms single-agent LLM baselines in both accuracy and completeness. By systematically visualizing character networks, CREFT streamlines narrative comprehension and accelerates script review -- offering substantial benefits to the entertainment, publishing, and educational sectors.",,"Ye Eun Chun, Taeyoon Hwang, Seung-won Hwang, Byung-Hak Kim",2025-05-30T13:01:36Z,CREFT: Sequential Multi-Agent LLM for Character Relation Extraction,CREFT: Sequentielles Multi-Agent LLM für Zeichenrelation-Extraktion,CREFT:,http://arxiv.org/abs/2505.24553v1
1393,"Reasoning-enabled large language models (LLMs) excel in logical tasks, yet their utility for evaluating natural language generation remains unexplored. This study systematically compares reasoning LLMs with non-reasoning counterparts across machine translation and text summarization evaluation tasks. We evaluate eight models spanning state-of-the-art reasoning models (DeepSeek-R1, OpenAI o3), their distilled variants (8B-70B parameters), and equivalent non-reasoning LLMs. Experiments on WMT23 and SummEval benchmarks reveal architecture and task-dependent benefits: OpenAI o3-mini models show improved performance with increased reasoning on MT, while DeepSeek-R1 and generally underperforms compared to its non-reasoning variant except in summarization consistency evaluation. Correlation analysis demonstrates that reasoning token usage correlates with evaluation quality only in specific models, while almost all models generally allocate more reasoning tokens when identifying more quality issues. Distillation maintains reasonable performance up to 32B parameter models but degrades substantially at 8B scale. This work provides the first assessment of reasoning LLMs for NLG evaluation and comparison to non-reasoning models. We share our code to facilitate further research: https://github.com/NL2G/reasoning-eval.",,"Daniil Larionov, Sotaro Takeshita, Ran Zhang, Yanran Chen, Christoph Leiter, Zhipin Wang, Christian Greisinger, Steffen Eger",2025-05-30T12:59:36Z,DeepSeek-R1 vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and   Summarization?,DeepSeek-R1 vs. o3-mini: Wie gut kann LLMs mit Vernunft bewerten MT und Zusammenfassung?,DeepSeek-R1诉O3-mini:如何合理解释LLMs评估MT和总结?,http://arxiv.org/abs/2504.08120v3
1394,"Large Reasoning Models (LRMs) achieve superior performance by extending the thought length. However, a lengthy thinking trajectory leads to reduced efficiency. Most of the existing methods are stuck in the assumption of overthinking and attempt to reason efficiently by compressing the Chain-of-Thought, but this often leads to performance degradation. To address this problem, we introduce A*-Thought, an efficient tree search-based unified framework designed to identify and isolate the most essential thoughts from the extensive reasoning chains produced by these models. It formulates the reasoning process of LRMs as a search tree, where each node represents a reasoning span in the giant reasoning space. By combining the A* search algorithm with a cost function specific to the reasoning path, it can efficiently compress the chain of thought and determine a reasoning path with high information density and low cost. In addition, we also propose a bidirectional importance estimation mechanism, which further refines this search process and enhances its efficiency beyond uniform sampling. Extensive experiments on several advanced math tasks show that A*-Thought effectively balances performance and efficiency over a huge search space. Specifically, A*-Thought can improve the performance of QwQ-32B by 2.39$\times$ with low-budget and reduce the length of the output token by nearly 50% with high-budget. The proposed method is also compatible with several other LRMs, demonstrating its generalization capability. The code can be accessed at: https://github.com/AI9Stars/AStar-Thought.",,"Xiaoang Xu, Shuo Wang, Xu Han, Zhenghao Liu, Huijia Wu, Peipei Li, Zhiyuan Liu, Maosong Sun, Zhaofeng He",2025-05-30T12:58:34Z,A*-Thought: Efficient Reasoning via Bidirectional Compression for   Low-Resource Settings,A*-Thought: Effizientes Reasoning über bidirektionale Kompression für Low-Resource-Einstellungen,A* - 考虑:通过对低资源设置的双向压缩来有效说明理由,http://arxiv.org/abs/2505.24550v1
1395,"The remarkable multimodal capabilities and interactive experience of GPT-4o underscore their necessity in practical applications, yet open-source models rarely excel in both areas. In this paper, we introduce VITA, the first-ever open-source Multimodal Large Language Model (MLLM) adept at simultaneous processing and analysis of Video, Image, Text, and Audio modalities, and meanwhile has an advanced multimodal interactive experience. Starting from Mixtral 8x7B as a language foundation, we expand its Chinese vocabulary followed by bilingual instruction tuning. We further endow the language model with visual and audio capabilities through two-stage multi-task learning of multimodal alignment and instruction tuning. VITA demonstrates robust foundational capabilities of multilingual, vision, and audio understanding, as evidenced by its strong performance across a range of both unimodal and multimodal benchmarks. Beyond foundational capabilities, we have made considerable progress in enhancing the natural multimodal human-computer interaction experience. VITA is the first step for the open-source community to explore the seamless integration of multimodal understanding and interaction. While there is still lots of work to be done on VITA to get close to close-source counterparts, we hope that its role as a pioneer can serve as a cornerstone for subsequent research. Project Page: https://vita-home.github.io.",,"Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Yuhang Dai, Meng Zhao, Yi-Fan Zhang, Shaoqi Dong, Yangze Li, Xiong Wang, Haoyu Cao, Di Yin, Long Ma, Xiawu Zheng, Rongrong Ji, Yunsheng Wu, Ran He, Caifeng Shan, Xing Sun",2025-05-30T12:57:16Z,VITA: Towards Open-Source Interactive Omni Multimodal LLM,VITA: Auf dem Weg zu Open Source Interactive Omni Multimodal LLM,VITA: 走向开放源码交互式全小型多模式LM,http://arxiv.org/abs/2408.05211v3
1396,"Disagreement in human labeling is ubiquitous, and can be captured in human judgment distributions (HJDs). Recent research has shown that explanations provide valuable information for understanding human label variation (HLV) and large language models (LLMs) can approximate HJD from a few human-provided label-explanation pairs. However, collecting explanations for every label is still time-consuming. This paper examines whether LLMs can be used to replace humans in generating explanations for approximating HJD. Specifically, we use LLMs as annotators to generate model explanations for a few given human labels. We test ways to obtain and combine these label-explanations with the goal to approximate human judgment distributions. We further compare the resulting human with model-generated explanations, and test automatic and human explanation selection. Our experiments show that LLM explanations are promising for NLI: to estimate HJDs, generated explanations yield comparable results to human's when provided with human labels. Importantly, our results generalize from datasets with human explanations to i) datasets where they are not available and ii) challenging out-of-distribution test sets.",,"Beiduo Chen, Siyao Peng, Anna Korhonen, Barbara Plank",2025-05-30T12:57:08Z,A Rose by Any Other Name: LLM-Generated Explanations Are Good Proxies   for Human Explanations to Collect Label Distributions on NLI,"Eine Rose von jedem anderen Namen: LLM-erzeugte Erklärungen sind gute Proxies für menschliche Erklärungen, um Etikettenverteilungen auf NLI zu sammeln","以任何其他名称命名的玫瑰:LLM - 创世解释是人类解释的好替代物,以收集NLI上的标签分布。",http://arxiv.org/abs/2412.13942v2
1397,"Speculative decoding (SD) is a widely adopted approach for accelerating inference in large language models (LLMs), particularly when the draft and target models are well aligned. However, state-of-the-art SD methods typically rely on tightly coupled, self-attention-based Transformer decoders, often augmented with auxiliary pooling or fusion layers. This coupling makes them increasingly complex and harder to generalize across different models. We present Budget EAGLE (Beagle), the first, to our knowledge, cross-attention-based Transformer decoder SD model that achieves performance on par with leading self-attention SD models (EAGLE-v2) while eliminating the need for pooling or auxiliary components, simplifying the architecture, improving training efficiency, and maintaining stable memory usage during training-time simulation. To enable effective training of this novel architecture, we propose Two-Stage Block-Attention Training, a new method that achieves training stability and convergence efficiency in block-level attention scenarios. Extensive experiments across multiple LLMs and datasets show that Beagle achieves competitive inference speedups and higher training efficiency than EAGLE-v2, offering a strong alternative for architectures in speculative decoding.",,"Wei Zhong, Manasa Bharadwaj, Yixiao Wang, Nikhil Verma, Yipeng Ji, Chul Lee",2025-05-30T12:52:35Z,Cross-Attention Speculative Decoding,Spekulative Dekodierung mit Queraufmerksamkeit,跨目的投机性下限,http://arxiv.org/abs/2505.24544v1
1398,"The advent of Large Language Models (LLMs) has revolutionized product recommenders, yet their susceptibility to adversarial manipulation poses critical challenges, particularly in real-world commercial applications. Our approach is the first one to tap into human psychological principles, seamlessly modifying product descriptions, making such manipulations hard to detect. In this work, we investigate cognitive biases as black-box adversarial strategies, drawing parallels between their effects on LLMs and human purchasing behavior. Through extensive evaluation across models of varying scale, we find that certain biases, such as social proof, consistently boost product recommendation rate and ranking, while others, like scarcity and exclusivity, surprisingly reduce visibility. Our results demonstrate that cognitive biases are deeply embedded in state-of-the-art LLMs, leading to highly unpredictable behavior in product recommendations and posing significant challenges for effective mitigation.",,"Giorgos Filandrianos, Angeliki Dimitriou, Maria Lymperaiou, Konstantinos Thomas, Giorgos Stamou",2025-05-30T12:51:36Z,Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product   Recommendations,Bias Vorsicht: Die Auswirkungen kognitiver Biasen auf LLM-getriebene Produktempfehlungen,注意:认知分量对LLM - 开发产品建议的影响,http://arxiv.org/abs/2502.01349v2
1399,"Model merging aims to integrate multiple task-specific models into a unified model that inherits the capabilities of the task-specific models, without additional training. Existing model merging methods often lack consideration of the varying contribution ratios of different task-specific models to the final merged model. In this paper, we propose Mixup Model Merge (M3), a simple yet effective method inspired by the randomized linear interpolation strategy from the Mixup data augmentation technique. M3 performs randomized linear interpolation in parameter space between two task-specific LLMs, where interpolation coefficients are sampled from a Beta distribution to explore diverse contribution ratios. This controllable randomness allows M3 to outperform standard equal-ratio merging by discovering better contribution ratio combinations. Extensive experiments show that M3 significantly (1) improves merged LLM performance across tasks, (2) enhances out-of-distribution and adversarial robustness, and (3) outperforms the positive effects of the sparsification method DARE on model merging and can be further combined with DARE to achieve superior results. By tuning the Beta distribution's shape parameters, (4) M3 balances exploration efficiency and diversity in contribution ratios. The code is available at: https://github.com/MLGroupJLU/MixupModelMerge",,"Yue Zhou, Yi Chang, Yuan Wu",2025-05-30T12:49:16Z,Mixup Model Merge: Enhancing Model Merging Performance through   Randomized Linear Interpolation,Mixup Model Merge: Verbesserung der Modellzusammenführung durch Randomized Linear Interpolation,混合模型合并:通过随机化线性线性内插加强模型合并性能,http://arxiv.org/abs/2502.15434v2
1400,"Cultural Heritage (CH) data hold invaluable knowledge, reflecting the history, traditions, and identities of societies, and shaping our understanding of the past and present. However, many CH collections contain outdated or offensive descriptions that reflect historical biases. CH Institutions (CHIs) face significant challenges in curating these data due to the vast scale and complexity of the task. To address this, we develop an AI-powered tool that detects offensive terms in CH metadata and provides contextual insights into their historical background and contemporary perception. We leverage a multilingual vocabulary co-created with marginalized communities, researchers, and CH professionals, along with traditional NLP techniques and Large Language Models (LLMs). Available as a standalone web app and integrated with major CH platforms, the tool has processed over 7.9 million records, contextualizing the contentious terms detected in their metadata. Rather than erasing these terms, our approach seeks to inform, making biases visible and providing actionable insights for creating more inclusive and accessible CH collections.",,"Orfeas Menis Mastromichalakis, Jason Liartis, Kristina Rose, Antoine Isaac, Giorgos Stamou",2025-05-30T12:44:54Z,"Don't Erase, Inform! Detecting and Contextualizing Harmful Language in   Cultural Heritage Collections","Nicht löschen, informieren! Aufspüren und Kontextualisieren schädlicher Sprache in Sammlungen des Kulturerbes",在文化遗产收藏中发现有害语言并将其背景化,http://arxiv.org/abs/2505.24538v1
1401,"Controlling multiple behavioral attributes in large language models (LLMs) at inference time is a challenging problem due to interference between attributes and the limitations of linear steering methods, which assume additive behavior in activation space and require per-attribute tuning. We introduce K-Steering, a unified and flexible approach that trains a single non-linear multi-label classifier on hidden activations and computes intervention directions via gradients at inference time. This avoids linearity assumptions, removes the need for storing and tuning separate attribute vectors, and allows dynamic composition of behaviors without retraining. To evaluate our method, we propose two new benchmarks, ToneBank and DebateMix, targeting compositional behavioral control. Empirical results across 3 model families, validated by both activation-based classifiers and LLM-based judges, demonstrate that K-Steering outperforms strong baselines in accurately steering multiple behaviors.",,"Narmeen Oozeer, Luke Marks, Fazl Barez, Amirali Abdullah",2025-05-30T12:41:19Z,Beyond Linear Steering: Unified Multi-Attribute Control for Language   Models,Beyond Linear Steering: Unified Multi-Attribute Control für Sprachmodelle,在线性指导:语言模式统一多属性控制,http://arxiv.org/abs/2505.24535v1
1402,"LLMs often excel on standard benchmarks but falter on real-world tasks. We introduce DeepQuestion, a scalable automated framework that augments existing datasets based on Bloom's taxonomy and creates novel questions that trace original solution paths to probe evaluative and creative skills. Extensive experiments across ten open-source and proprietary models, covering both general-purpose and reasoning LLMs, reveal substantial performance drops (even up to 70% accuracy loss) on higher-order tasks, underscoring persistent gaps in deep reasoning. Our work highlights the need for cognitively diverse benchmarks to advance LLM progress. DeepQuestion and related datasets will be released upon acceptance of the paper.",,"Ali Khoramfar, Ali Ramezani, Mohammad Mahdi Mohajeri, Mohammad Javad Dousti, Majid Nili Ahmadabadi, Heshaam Faili",2025-05-30T12:39:42Z,DEEPQUESTION: Systematic Generation of Real-World Challenges for   Evaluating LLMs Performance,DEEPQUESTION: Systematische Generierung von realen Herausforderungen für die Bewertung der Leistung von LLMs,千年发展目标:系统地产生评价LLLMs绩效的现实世界挑战,http://arxiv.org/abs/2505.24532v1
1403,"Cross-lingual transfer from related high-resource languages is a well-established strategy to enhance low-resource language technologies. Prior work has shown that adapters show promise for, e.g., improving low-resource machine translation (MT). In this work, we investigate an adapter souping method combined with cross-attention fine-tuning of a pre-trained MT model to leverage language transfer for three low-resource Creole languages, which exhibit relatedness to different language groups across distinct linguistic dimensions. Our approach improves performance substantially over baselines. However, we find that linguistic relatedness -- or even a lack thereof -- does not covary meaningfully with adapter performance. Surprisingly, our cross-attention fine-tuning approach appears equally effective with randomly initialized adapters, implying that the benefit of adapters in this setting lies in parameter regularization, and not in meaningful information transfer. We provide analysis supporting this regularization hypothesis. Our findings underscore the reality that neural language processing involves many success factors, and that not all neural methods leverage linguistic knowledge in intuitive ways.",,"Marcell Fekete, Nathaniel R. Robinson, Ernests Lavrinovics, E. Djeride Jean-Baptiste, Raj Dabre, Johannes Bjerva, Heather Lent",2025-05-30T12:34:28Z,"Limited-Resource Adapters Are Regularizers, Not Linguists","Begrenzte Ressourcenadapter sind Regularisierer, keine Linguisten","有限资源适应器是正规化器,不是语言学家",http://arxiv.org/abs/2505.24525v1
1404,"Recent advancements in Generative AI and Large Language Models (LLMs) have enabled the creation of highly realistic synthetic content, raising concerns about the potential for malicious use, such as misinformation and manipulation. Moreover, detecting Machine-Generated Text (MGT) remains challenging due to the lack of robust benchmarks that assess generalization to real-world scenarios. In this work, we present a pipeline to test the resilience of state-of-the-art MGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed adversarial attacks. To challenge the detectors, we fine-tune language models using Direct Preference Optimization (DPO) to shift the MGT style toward human-written text (HWT). This exploits the detectors' reliance on stylistic clues, making new generations more challenging to detect. Additionally, we analyze the linguistic shifts induced by the alignment and which features are used by detectors to detect MGT texts. Our results show that detectors can be easily fooled with relatively few examples, resulting in a significant drop in detection performance. This highlights the importance of improving detection methods and making them robust to unseen in-domain texts.",,"Andrea Pedrotti, Michele Papucci, Cristiano Ciaccio, Alessio Miaschi, Giovanni Puccetti, Felice Dell'Orletta, Andrea Esuli",2025-05-30T12:33:30Z,Stress-testing Machine Generated Text Detection: Shifting Language   Models Writing Style to Fool Detectors,Stresstest-Maschine Generierte Texterkennung: Verschieben von Sprachmodellen Schreibstil auf Narren-Detektoren,压力测试 机器生成的文本检测:将语言模式书写风格转换为愚人探测器,http://arxiv.org/abs/2505.24523v1
1405,"We introduce AMIA, a lightweight, inference-only defense for Large Vision-Language Models (LVLMs) that (1) Automatically Masks a small set of text-irrelevant image patches to disrupt adversarial perturbations, and (2) conducts joint Intention Analysis to uncover and mitigate hidden harmful intents before response generation. Without any retraining, AMIA improves defense success rates across diverse LVLMs and jailbreak benchmarks from an average of 52.4% to 81.7%, preserves general utility with only a 2% average accuracy drop, and incurs only modest inference overhead. Ablation confirms both masking and intention analysis are essential for a robust safety-utility trade-off.",,"Yuqi Zhang, Yuchun Miao, Zuchao Li, Liang Ding",2025-05-30T12:30:50Z,AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust   Jailbreak Defenders,AMIA: Automatische Masken- und gemeinsame Intentionsanalyse macht LVLMs robust Jailbreak Defenders,AMIA: 自动防护和联合意图分析使劳改监狱维护者LVLMs Robust监狱,http://arxiv.org/abs/2505.24519v1
1406,"Complex instruction-following with elaborate constraints is imperative for Large Language Models (LLMs). While existing methods have constructed data for complex instruction alignment, they all rely on a more advanced model, especially GPT-4, limiting their application. In this paper, we propose a Multi-granularity Self-Contrastive Training (MuSC) framework, to improve the complex instruction alignment without relying on a stronger model. Our method is conducted on both coarse and fine granularity. On coarse-granularity, we construct constraint-aware preference data based on instruction decomposition and recombination. On fine-granularity, we perform token-aware preference optimization with dynamic token-level supervision. Our method is evaluated on open-sourced models, and experiment results show our method achieves significant improvement on both complex and general instruction-following benchmarks, surpassing previous self-alignment methods.",,"Hui Huang, Jiaheng Liu, Yancheng He, Shilong Li, Bing Xu, Conghui Zhu, Muyun Yang, Tiejun Zhao",2025-05-30T12:13:42Z,MuSC: Improving Complex Instruction Following with Multi-granularity   Self-Contrastive Training,MuSC: Komplexe Anleitung nach Multi-Granularität Selbst-Kontrastives Training verbessern,MuSC:在开展多岛屿自我协调培训后改进复杂教学,http://arxiv.org/abs/2502.11541v3
1407,"Extracting scientific evidence from biomedical studies for clinical research questions (e.g., Does stem cell transplantation improve quality of life in patients with medically refractory Crohn's disease compared to placebo?) is a crucial step in synthesising biomedical evidence. In this paper, we focus on the task of document-level scientific evidence extraction for clinical questions with conflicting evidence. To support this task, we create a dataset called CochraneForest, leveraging forest plots from Cochrane systematic reviews. It comprises 202 annotated forest plots, associated clinical research questions, full texts of studies, and study-specific conclusions. Building on CochraneForest, we propose URCA (Uniform Retrieval Clustered Augmentation), a retrieval-augmented generation framework designed to tackle the unique challenges of evidence extraction. Our experiments show that URCA outperforms the best existing methods by up to 10.3% in F1 score on this task. However, the results also underscore the complexity of CochraneForest, establishing it as a challenging testbed for advancing automated evidence synthesis systems.",,"Massimiliano Pronesti, Joao Bettencourt-Silva, Paul Flanagan, Alessandra Pascale, Oisin Redmond, Anya Belz, Yufang Hou",2025-05-30T12:10:09Z,Query-driven Document-level Scientific Evidence Extraction from   Biomedical Studies,Abfrage-getriebene Dokument-Ebene wissenschaftliche Evidenz-Extraktion aus biomedizinischen Studien,由查询驱动的文件级科学证据从生物医学研究中提取的科学证据,http://arxiv.org/abs/2505.06186v3
1408,"Recently, Large Language Models (LLMs) have made significant progress in IQ-related domains that require careful thinking, such as mathematics and coding. However, enhancing LLMs' cognitive development in social domains, particularly from a post-training perspective, remains underexplored. Recognizing that the social world follows a distinct timeline and requires a richer blend of cognitive modes (from intuitive reactions (System 1) and surface-level thinking to deliberate thinking (System 2)) than mathematics, which primarily relies on System 2 cognition (careful, step-by-step reasoning), we introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we systematically explore improving LLMs' social intelligence and validate the effectiveness of the TimeHC-RL method, through five other post-training paradigms and two test-time intervention paradigms on eight datasets with diverse data patterns. Experimental results reveal the superiority of our proposed TimeHC-RL method compared to the widely adopted System 2 RL method. It gives the 7B backbone model wings, enabling it to rival the performance of advanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic exploration from post-training and test-time interventions perspectives to improve LLMs' social intelligence has uncovered several valuable insights.",,"Guiyang Hou, Xing Gao, Yuchuan Wu, Xiang Huang, Wenqi Zhang, Zhe Zheng, Yongliang Shen, Jialu Du, Fei Huang, Yongbin Li, Weiming Lu",2025-05-30T12:01:06Z,TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning   for Enhancing LLMs' Social Intelligence,TimeHC-RL: Temporal-aware Hierarchisches Kognitives Verstärkungslernen zur Verbesserung der sozialen Intelligenz von LLMs,时间HC-RL:加强LLMS社会智能的时觉认知等级认知强化学习,http://arxiv.org/abs/2505.24500v1
1409,"Recently, there has been a growing trend of utilizing Large Language Model (LLM) to evaluate the quality of other LLMs. Many studies have fine-tuned judge models based on open-source LLMs for evaluation. While the fine-tuned judge models are claimed to achieve comparable evaluation capability with GPT-4, in this work, we conduct an empirical study of LLM-as-a-Judge. Our findings indicate that although the fine-tuned judge models achieve high performance on in-domain test sets, even surpassing GPT-4, they underperform GPT-4 across several dimensions, including generalizability, fairness and adaptability. We also reveal that the fine-tuned judge model inherently operates as a task-specific classifier, consequently imposing the limitations.",,"Hui Huang, Xingyuan Bu, Hongli Zhou, Yingqi Qu, Jing Liu, Muyun Yang, Bing Xu, Tiejun Zhao",2025-05-30T12:01:03Z,An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned   Judge Model is not a General Substitute for GPT-4,Eine empirische Studie des LLM-as-a-Richters für die LLM-Bewertung: Feinabstimmung des Richtermodells ist kein allgemeiner Ersatz für GPT-4,法学硕士(LLM)评价法官的经验研究:微调法官模型不是GPT-4通用替代工具,http://arxiv.org/abs/2403.02839v4
1410,"Hypothesis ranking is a crucial component of automated scientific discovery, particularly in natural sciences where wet-lab experiments are costly and throughput-limited. Existing approaches focus on pre-experiment ranking, relying solely on large language model's internal reasoning without incorporating empirical outcomes from experiments. We introduce the task of experiment-guided ranking, which aims to prioritize candidate hypotheses based on the results of previously tested ones. However, developing such strategies is challenging due to the impracticality of repeatedly conducting real experiments in natural science domains. To address this, we propose a simulator grounded in three domain-informed assumptions, modeling hypothesis performance as a function of similarity to a known ground truth hypothesis, perturbed by noise. We curate a dataset of 124 chemistry hypotheses with experimentally reported outcomes to validate the simulator. Building on this simulator, we develop a pseudo experiment-guided ranking method that clusters hypotheses by shared functional characteristics and prioritizes candidates based on insights derived from simulated experimental feedback. Experiments show that our method outperforms pre-experiment baselines and strong ablations.",,"Wanhao Liu, Zonglin Yang, Jue Wang, Lidong Bing, Di Zhang, Dongzhan Zhou, Yuqiang Li, Houqiang Li, Erik Cambria, Wanli Ouyang",2025-05-30T11:59:51Z,MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated   Experimental Feedback,MOOSE-Chem3: Auf dem Weg zu experimentgeführter Hypothesen-Ranking durch simuliertes Experimentelles Feedback,MOOSE-Chem3:通过模拟实验反馈实现实验引导假设的排名,http://arxiv.org/abs/2505.17873v2
1411,"Neural machine translation (NMT) systems amplify lexical biases present in their training data, leading to artificially impoverished language in output translations. These language-level characteristics render automatic translations different from text originally written in a language and human translations, which hinders their usefulness in for example creating evaluation datasets. Attempts to increase naturalness in NMT can fall short in terms of content preservation, where increased lexical diversity comes at the cost of translation accuracy. Inspired by the reinforcement learning from human feedback framework, we introduce a novel method that rewards both naturalness and content preservation. We experiment with multiple perspectives to produce more natural translations, aiming at reducing machine and human translationese. We evaluate our method on English-to-Dutch literary translation, and find that our best model produces translations that are lexically richer and exhibit more properties of human-written language, without loss in translation accuracy.",,"Huiyuan Lai, Esther Ploeger, Rik van Noord, Antonio Toral",2025-05-30T11:47:35Z,Multi-perspective Alignment for Increasing Naturalness in Neural Machine   Translation,Multi-Perspektive Ausrichtung für die Erhöhung der Natürlichkeit in neuralen maschinellen Übersetzung,神经机器翻译中增加自然特性的多视角一致,http://arxiv.org/abs/2412.08473v2
1412,"Large Vision and Language Models have enabled significant advances in fully supervised and zero-shot visual tasks. These large architectures serve as the baseline to what is currently known as Instruction Tuning Large Vision and Language models (IT-LVLMs). IT-LVLMs are general-purpose multi-modal assistants whose responses are modulated by natural language instructions and visual data. Despite this versatility, IT-LVLM effectiveness in fundamental computer vision problems remains unclear, primarily due to the absence of a standardized evaluation benchmark. This paper introduces a Multi-modal Evaluation Benchmark named MERLIM, a scalable test-bed to assess the capabilities of IT-LVLMs on fundamental computer vision tasks. MERLIM contains over 300K image-question pairs and has a strong focus on detecting cross-modal ""hallucination"" events in IT-LVLMs. Our results bring important insights on the performance of state-of-the-art IT-LVLMs including limitations at identifying fine-grained visual concepts, object hallucinations across tasks, and biases towards the language query. Our findings also suggest that these models have weak visual grounding, but manage to make adequate guesses from global visual patterns or language biases contained in the LLM component. We name this phenomenon of correct answers with no visual grounding as hidden hallucinations.",,"Andrés Villa, Juan Carlos León Alcázar, Alvaro Soto, Bernard Ghanem",2025-05-30T11:40:41Z,"Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large   Image-Language Models","Hinter der Magie, MERLIM: Multimodaler Evaluations-Benchmark für große Image-Language-Modelle",魔法背后的MERLIM:大型图像语言模型的多模式评价基准,http://arxiv.org/abs/2312.02219v3
1413,"In this paper, we investigate code-integrated reasoning, where models generate code when necessary and integrate feedback by executing it through a code interpreter. To acquire this capability, models must learn when and how to use external code tools effectively, which is supported by tool-augmented reinforcement learning (RL) through interactive learning. Despite its benefits, tool-augmented RL can still suffer from potential instability in the learning dynamics. In light of this challenge, we present a systematic approach to improving the training effectiveness and stability of tool-augmented RL for code-integrated reasoning. Specifically, we develop enhanced training strategies that balance exploration and stability, progressively building tool-use capabilities while improving reasoning performance. Through extensive experiments on five mainstream mathematical reasoning benchmarks, our model demonstrates significant performance improvements over multiple competitive baselines. Furthermore, we conduct an in-depth analysis of the mechanism and effect of code-integrated reasoning, revealing several key insights, such as the extension of model's capability boundaries and the simultaneous improvement of reasoning efficiency through code integration. All data and code for reproducing this work are available at: https://github.com/RUCAIBox/CIR.",,"Fei Bai, Yingqian Min, Beichen Zhang, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, Zheng Liu, Zhongyuan Wang, Ji-Rong Wen",2025-05-30T11:30:18Z,Towards Effective Code-Integrated Reasoning,Auf dem Weg zu einer wirksamen code-integrierten Vernunft,实现有效的守则综合理由,http://arxiv.org/abs/2505.24480v1
1414,"The rapid spread of misinformation, further amplified by recent advances in generative AI, poses significant threats to society, impacting public opinion, democratic stability, and national security. Understanding and proactively assessing these threats requires exploring methodologies that enable structured and scalable misinformation generation. In this paper, we propose a novel approach that leverages knowledge graphs (KGs) as structured semantic resources to systematically generate fake triplets. By analyzing the structural properties of KGs, such as the distance between entities and their predicates, we identify plausibly false relationships. These triplets are then used to guide large language models (LLMs) in generating misinformation statements with varying degrees of credibility. By utilizing structured semantic relationships, our deterministic approach produces misinformation inherently challenging for humans to detect, drawing exclusively upon publicly available KGs (e.g., WikiGraphs).   Additionally, we investigate the effectiveness of LLMs in distinguishing between genuine and artificially generated misinformation. Our analysis highlights significant limitations in current LLM-based detection methods, underscoring the necessity for enhanced detection strategies and a deeper exploration of inherent biases in generative models.",,"Sania Nayab, Marco Simoni, Giulio Rossolini",2025-05-30T11:29:10Z,Leveraging Knowledge Graphs and LLMs for Structured Generation of   Misinformation,Nutzung von Wissensgraphen und LLMs für strukturierte Erzeugung von Fehlinformationen,利用知识图和LLMs 利用知识图和LLM 进行有结构生成错误信息,http://arxiv.org/abs/2505.24479v1
1415,"Integrating Large Language Models (LLMs) with Knowledge Graphs (KGs) results in complex systems with numerous hyperparameters that directly affect performance. While such systems are increasingly common in retrieval-augmented generation, the role of systematic hyperparameter optimization remains underexplored. In this paper, we study this problem in the context of Cognee, a modular framework for end-to-end KG construction and retrieval. Using three multi-hop QA benchmarks (HotPotQA, TwoWikiMultiHop, and MuSiQue) we optimize parameters related to chunking, graph construction, retrieval, and prompting. Each configuration is scored using established metrics (exact match, F1, and DeepEval's LLM-based correctness metric). Our results demonstrate that meaningful gains can be achieved through targeted tuning. While the gains are consistent, they are not uniform, with performance varying across datasets and metrics. This variability highlights both the value of tuning and the limitations of standard evaluation measures. While demonstrating the immediate potential of hyperparameter tuning, we argue that future progress will depend not only on architectural advances but also on clearer frameworks for optimization and evaluation in complex, modular systems.",,"Vasilije Markovic, Lazar Obradovic, Laszlo Hajdu, Jovan Pavlovic",2025-05-30T11:27:59Z,Optimizing the Interface Between Knowledge Graphs and LLMs for Complex   Reasoning,Optimierung der Schnittstelle zwischen Wissensgraphen und LLMs für komplexe Vernunft,"优化知识图和LLMs之间的接口,以找出复杂的理由",http://arxiv.org/abs/2505.24478v1
1416,"Machine translation systems fail when processing code-mixed inputs for low-resource languages. We address this challenge by curating VietMix, a parallel corpus of naturally occurring code-mixed Vietnamese text paired with expert English translations. Augmenting this resource, we developed a complementary synthetic data generation pipeline. This pipeline incorporates filtering mechanisms to ensure syntactic plausibility and pragmatic appropriateness in code-mixing patterns. Experimental validation shows our naturalistic and complementary synthetic data boost models' performance, measured by translation quality estimation scores, of up to 71.84 on COMETkiwi and 81.77 on XCOMET. Triangulating positive results with LLM-based assessments, augmented models are favored over seed fine-tuned counterparts in approximately 49% of judgments (54-56% excluding ties). VietMix and our augmentation methodology advance ecological validity in neural MT evaluations and establish a framework for addressing code-mixed translation challenges across other low-resource pairs.",,"Hieu Tran, Phuong-Anh Nguyen-Le, Huy Nghiem, Quang-Nhan Nguyen, Wei Ai, Marine Carpuat",2025-05-30T11:18:10Z,VietMix: A Naturally Occurring Vietnamese-English Code-Mixed Corpus with   Iterative Augmentation for Machine Translation,VietMix: Ein natürlich vorkommender vietnamesisch-englischer Code-Mixed Corpus mit iterativer Augmentation für maschinelle Übersetzung,"越南混合:一个自然操作的越南-英语编码混合体,用于机器翻译的循环增强体",http://arxiv.org/abs/2505.24472v1
1417,"Large language models (LLMs) have demonstrated impressive capabilities in a wide range of downstream natural language processing tasks. Nevertheless, their considerable sizes and memory demands hinder practical deployment, underscoring the importance of developing efficient compression strategies. Singular value decomposition (SVD) decomposes a matrix into orthogonal components, enabling efficient low-rank approximation. This is particularly suitable for LLM compression, where weight matrices often exhibit significant redundancy. However, current SVD-based methods neglect the residual matrix from truncation, resulting in significant truncation loss. Additionally, compressing all layers of the model results in severe performance degradation. To overcome these limitations, we propose ResSVD, a new post-training SVD-based LLM compression method. Specifically, we leverage the residual matrix generated during the truncation process to reduce truncation loss. Moreover, under a fixed overall compression ratio, we selectively compress the last few layers of the model, which mitigates error propagation and significantly improves the performance of compressed models. Comprehensive evaluations of ResSVD on diverse LLM families and multiple benchmark datasets indicate that ResSVD consistently achieves superior performance over existing counterpart methods, demonstrating its practical effectiveness.",,"Haolei Bai, Siyong Jian, Tuo Liang, Yu Yin, Huan Wang",2025-05-30T11:11:24Z,ResSVD: Residual Compensated SVD for Large Language Model Compression,ResSVD: Residual Compensated SVD für großsprachliche Modellkompression,ResSVD: 大语言模型压缩剩余补偿SVD,http://arxiv.org/abs/2505.20112v2
1418,"Factual hallucinations are a major challenge for Large Language Models (LLMs). They undermine reliability and user trust by generating inaccurate or fabricated content. Recent studies suggest that when generating false statements, the internal states of LLMs encode information about truthfulness. However, these studies often rely on synthetic datasets that lack realism, which limits generalization when evaluating the factual accuracy of text generated by the model itself. In this paper, we challenge the findings of previous work by investigating truthfulness encoding capabilities, leading to the generation of a more realistic and challenging dataset. Specifically, we extend previous work by introducing: (1) a strategy for sampling plausible true-false factoid sentences from tabular data and (2) a procedure for generating realistic, LLM-dependent true-false datasets from Question Answering collections. Our analysis of two open-source LLMs reveals that while the findings from previous studies are partially validated, generalization to LLM-generated datasets remains challenging. This study lays the groundwork for future research on factuality in LLMs and offers practical guidelines for more effective evaluation.",,"Giovanni Servedio, Alessandro De Bellis, Dario Di Palma, Vito Walter Anelli, Tommaso Di Noia",2025-05-30T10:53:48Z,Are the Hidden States Hiding Something? Testing the Limits of   Factuality-Encoding Capabilities in LLMs,Sind die versteckten Staaten etwas verbergen? Testen Sie die Grenzen der Faktizität-Encoding Fähigkeiten in LLMs,隐秘国是否隐藏着什么?测试LLMM中实际质量-编码能力限度。,http://arxiv.org/abs/2505.16520v3
1419,"Large language models (LLMs) using in-context learning (ICL) excel in many tasks without task-specific fine-tuning. However, demonstration selection and ordering greatly impact ICL effectiveness. To address this, we propose DemoShapley and Beta-DemoShapley, inspired by Data Shapley and Beta Shapley, to assess the influence of individual demonstrations. DemoShapley captures how each example influences performance in different contexts, unlike other influence-based methods that rely on a fixed number of demonstrations. Beta-DemoShapley further enhances this framework by incorporating the Beta distribution, allowing users to assign higher weights to smaller cardinalities, which aligns with ICL's prompt length and computational constraints. Our findings show that the proposed algorithms improve model performance by selecting quality demonstrations, and enhancing generalization to out-of-distribution tasks. It also identifies noise-compromised data and promotes fairness in LLMs, protecting model performance and ensuring robustness across various scenarios.",,"Shan Xie, Man Luo, Chadly Daniel Stern, Mengnan Du, Lu Cheng",2025-05-30T10:48:42Z,DemoShapley: Valuation of Demonstrations for In-Context Learning,DemoShapley: Bewertung von Demonstrationen für das In-Context-Lernen,Demoshapley:评估内文学习示范活动,http://arxiv.org/abs/2410.07523v2
1420,"Cultural content poses challenges for machine translation systems due to the differences in conceptualizations between cultures, where language alone may fail to convey sufficient context to capture region-specific meanings. In this work, we investigate whether images can act as cultural context in multimodal translation. We introduce CaMMT, a human-curated benchmark of over 5,800 triples of images along with parallel captions in English and regional languages. Using this dataset, we evaluate five Vision Language Models (VLMs) in text-only and text+image settings. Through automatic and human evaluations, we find that visual context generally improves translation quality, especially in handling Culturally-Specific Items (CSIs), disambiguation, and correct gender usage. By releasing CaMMT, we aim to support broader efforts in building and evaluating multimodal translation systems that are better aligned with cultural nuance and regional variation.",,"Emilio Villa-Cueva, Sholpan Bolatzhanova, Diana Turmakhan, Kareem Elzeky, Henok Biadglign Ademtew, Alham Fikri Aji, Israel Abebe Azime, Jinheon Baek, Frederico Belcavello, Fermin Cristobal, Jan Christian Blaise Cruz, Mary Dabre, Raj Dabre, Toqeer Ehsan, Naome A Etori, Fauzan Farooqui, Jiahui Geng, Guido Ivetta, Thanmay Jayakumar, Soyeong Jeong, Zheng Wei Lim, Aishik Mandal, Sofia Martinelli, Mihail Minkov Mihaylov, Daniil Orel, Aniket Pramanick, Sukannya Purkayastha, Israfel Salazar, Haiyue Song, Tiago Timponi Torrent, Debela Desalegn Yadeta, Injy Hamed, Atnafu Lambebo Tonja, Thamar Solorio",2025-05-30T10:42:44Z,CaMMT: Benchmarking Culturally Aware Multimodal Machine Translation,CaMMT: Benchmarking kulturbewusster multimodaler maschineller Übersetzung,CAMMT: 确定具有文化意识的多式机器翻译基准,http://arxiv.org/abs/2505.24456v1
1421,"This empirical study analyzes the effects of the pre-training corpus on the quality of learned transformer representations. We focus on the representation quality induced solely through pre-training. Our experiments show that pre-training on a small, specialized corpus can yield effective representations, and that the success of combining a generic and a specialized corpus depends on the distributional similarity between the target task and the specialized corpus.",,"Cesar Gonzalez-Gutierrez, Ariadna Quattoni",2025-05-30T10:42:43Z,Domain Pre-training Impact on Representations,Domain-Vorschulung Auswirkungen auf Repräsentationen,培训前培训对代表人数的影响,http://arxiv.org/abs/2505.24455v1
1422,"Large language/multimodal models (LLMs/LMMs) store extensive pre-trained knowledge but struggle to maintain consistency with real-world updates, making it difficult to avoid catastrophic forgetting while acquiring evolving knowledge. Previous work focused on constructing textual knowledge datasets and exploring knowledge injection in LLMs, lacking exploration of multimodal evolving knowledge injection in LMMs. To address this, we propose the EVOKE benchmark to evaluate LMMs' ability to inject multimodal evolving knowledge in real-world scenarios. Meanwhile, a comprehensive evaluation of multimodal evolving knowledge injection revealed two challenges: (1) Existing knowledge injection methods perform terribly on evolving knowledge. (2) Supervised fine-tuning causes catastrophic forgetting, particularly instruction following ability is severely compromised. Additionally, we provide pathways and find that: (1) Text knowledge augmentation during the training phase improves performance, while image augmentation cannot achieve it. (2) Continual learning methods, especially Replay and MoELoRA, effectively mitigate forgetting. Our findings indicate that current knowledge injection methods have many limitations on evolving knowledge, which motivates further research on more efficient and stable knowledge injection methods.",,"Kailin Jiang, Yuntao Du, Yukai Ding, Yuchen Ren, Ning Jiang, Zhi Gao, Zilong Zheng, Lei Liu, Bin Li, Qing Li",2025-05-30T10:36:19Z,When Large Multimodal Models Confront Evolving Knowledge:Challenges and   Pathways,Wenn große multimodale Modelle sich dem Wissen annähern: Herausforderungen und Wege,当大型多模式模式对抗不断演变的知识:挑战和途径,http://arxiv.org/abs/2505.24449v1
1423,"Recent studies on personas have improved the way Large Language Models (LLMs) interact with users. However, the effect of personas on domain-specific question-answering (QA) tasks remains a subject of debate. This study analyzes whether personas enhance specialized QA performance by introducing two types of persona: Profession-Based Personas (PBPs) (e.g., scientist), which directly relate to domain expertise, and Occupational Personality-Based Personas (OPBPs) (e.g., scientific person), which reflect cognitive tendencies rather than explicit expertise. Through empirical evaluations across multiple scientific domains, we demonstrate that while PBPs can slightly improve accuracy, OPBPs often degrade performance, even when semantically related to the task. Our findings suggest that persona relevance alone does not guarantee effective knowledge utilization and that they may impose cognitive constraints that hinder optimal knowledge application. Future research can explore how nuanced distinctions in persona representations guide LLMs, potentially contributing to reasoning and knowledge retrieval that more closely mirror human social conceptualization.",,"Eojin Kang, Jaehyuk Yu, Juae Kim",2025-05-30T10:35:39Z,Exploring the Impact of Occupational Personas on Domain-Specific QA,Untersuchung der Auswirkungen berufsbezogener Personen auf die bereichsspezifische Qualitätssicherung,探索职业人员对特定领域质量保证的影响,http://arxiv.org/abs/2505.24448v1
1424,"Transformers have theoretical limitations in modeling certain sequence-to-sequence tasks, yet it remains largely unclear if these limitations play a role in large-scale pretrained LLMs, or whether LLMs might effectively overcome these constraints in practice due to the scale of both the models themselves and their pretraining data. We explore how these architectural constraints manifest after pretraining, by studying a family of $\textit{retrieval}$ and $\textit{copying}$ tasks inspired by Liu et al. [2024a]. We use a recently proposed framework for studying length generalization [Huang et al., 2025] to provide guarantees for each of our settings. Empirically, we observe an $\textit{induction-versus-anti-induction}$ asymmetry, where pretrained models are better at retrieving tokens to the right (induction) rather than the left (anti-induction) of a query token. This asymmetry disappears upon targeted fine-tuning if length-generalization is guaranteed by theory. Mechanistic analysis reveals that this asymmetry is connected to the differences in the strength of induction versus anti-induction circuits within pretrained transformers. We validate our findings through practical experiments on real-world tasks demonstrating reliability risks. Our results highlight that pretraining selectively enhances certain transformer capabilities, but does not overcome fundamental length-generalization limits.",,"Yana Veitsman, Mayank Jobanputra, Yash Sarrof, Aleksandra Bakalova, Vera Demberg, Ellie Pavlick, Michael Hahn",2025-05-30T10:27:46Z,Born a Transformer -- Always a Transformer?,Geboren ein Transformer - immer ein Transformer?,天生的变形人 - - 总是变形人?,http://arxiv.org/abs/2505.21785v2
1425,"The scarcity of high-quality and multi-task singing datasets significantly hinders the development of diverse controllable and personalized singing tasks, as existing singing datasets suffer from low quality, limited diversity of languages and singers, absence of multi-technique information and realistic music scores, and poor task suitability. To tackle these problems, we present GTSinger, a large global, multi-technique, free-to-use, high-quality singing corpus with realistic music scores, designed for all singing tasks, along with its benchmarks. Particularly, (1) we collect 80.59 hours of high-quality singing voices, forming the largest recorded singing dataset; (2) 20 professional singers across nine widely spoken languages offer diverse timbres and styles; (3) we provide controlled comparison and phoneme-level annotations of six commonly used singing techniques, helping technique modeling and control; (4) GTSinger offers realistic music scores, assisting real-world musical composition; (5) singing voices are accompanied by manual phoneme-to-audio alignments, global style labels, and 16.16 hours of paired speech for various singing tasks. Moreover, to facilitate the use of GTSinger, we conduct four benchmark experiments: technique-controllable singing voice synthesis, technique recognition, style transfer, and speech-to-singing conversion. The corpus and demos can be found at http://aaronz345.github.io/GTSingerDemo/. We provide the dataset and the code for processing data and conducting benchmarks at https://huggingface.co/datasets/AaronZ345/GTSinger and https://github.com/AaronZ345/GTSinger.",,"Yu Zhang, Changhao Pan, Wenxiang Guo, Ruiqi Li, Zhiyuan Zhu, Jialei Wang, Wenhao Xu, Jingyu Lu, Zhiqing Hong, Chuxin Wang, LiChao Zhang, Jinzheng He, Ziyue Jiang, Yuxin Chen, Chen Yang, Jiecheng Zhou, Xinyu Cheng, Zhou Zhao",2025-05-30T10:24:13Z,GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music   Scores for All Singing Tasks,GTSinger: Globales Multi-Technique Singen Corpus mit realistischen Noten für alle Singaufgaben,GTSinger:一个拥有现实音乐分数的全唱任务全球多技术多技术歌唱公司,http://arxiv.org/abs/2409.13832v7
1426,"Prompt engineering for large language models is challenging, as even small prompt perturbations or model changes can significantly impact the generated output texts. Existing evaluation methods of LLM outputs, either automated metrics or human evaluation, have limitations, such as providing limited insights or being labor-intensive. We propose Spotlight, a new approach that combines both automation and human analysis. Based on data mining techniques, we automatically distinguish between random (decoding) variations and systematic differences in language model outputs. This process provides token patterns that describe the systematic differences and guide the user in manually analyzing the effects of their prompts and changes in models efficiently. We create three benchmarks to quantitatively test the reliability of token pattern extraction methods and demonstrate that our approach provides new insights into established prompt data. From a human-centric perspective, through demonstration studies and a user study, we show that our token pattern approach helps users understand the systematic differences of language model outputs. We are further able to discover relevant differences caused by prompt and model changes (e.g. related to gender or culture), thus supporting the prompt engineering process and human-centric model behavior research.",,"Michael A. Hedderich, Anyi Wang, Raoyuan Zhao, Florian Eichin, Jonas Fischer, Barbara Plank",2025-05-30T10:23:44Z,What's the Difference? Supporting Users in Identifying the Effects of   Prompt and Model Changes Through Token Patterns,Was ist der Unterschied? Unterstützung der Benutzer bei der Identifizierung der Auswirkungen von Prompt- und Modelländerungen durch Token-Muster,区别是什么? 支持用户识别即时和模式改变的影响,http://arxiv.org/abs/2504.15815v2
1427,"Customizable multilingual zero-shot singing voice synthesis (SVS) has various potential applications in music composition and short video dubbing. However, existing SVS models overly depend on phoneme and note boundary annotations, limiting their robustness in zero-shot scenarios and producing poor transitions between phonemes and notes. Moreover, they also lack effective multi-level style control via diverse prompts. To overcome these challenges, we introduce TCSinger 2, a multi-task multilingual zero-shot SVS model with style transfer and style control based on various prompts. TCSinger 2 mainly includes three key modules: 1) Blurred Boundary Content (BBC) Encoder, predicts duration, extends content embedding, and applies masking to the boundaries to enable smooth transitions. 2) Custom Audio Encoder, uses contrastive learning to extract aligned representations from singing, speech, and textual prompts. 3) Flow-based Custom Transformer, leverages Cus-MOE, with F0 supervision, enhancing both the synthesis quality and style modeling of the generated singing voice. Experimental results show that TCSinger 2 outperforms baseline models in both subjective and objective metrics across multiple related tasks. Singing voice samples are available at https://aaronz345.github.io/TCSinger2Demo/.",,"Yu Zhang, Wenxiang Guo, Changhao Pan, Dongyu Yao, Zhiyuan Zhu, Ziyue Jiang, Yuhan Wang, Tao Jin, Zhou Zhao",2025-05-30T10:20:59Z,TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis,TCSinger 2: Anpassbare Mehrsprachige Null-Shot-Singen-Stimme-Synthese,TCSinger 2:可定制的多语种零弹唱声合成,http://arxiv.org/abs/2505.14910v3
1428,"Large language model (LLM) research has grown rapidly, along with increasing concern about their limitations such as failures in reasoning, hallucinations, and limited multilingual capability. While prior reviews have addressed these issues, they often focus on individual limitations or consider them within the broader context of evaluating overall model performance. This survey addresses the gap by presenting a data-driven, semi-automated review of research on limitations of LLMs (LLLMs) from 2022 to 2025, using a bottom-up approach. From a corpus of 250,000 ACL and arXiv papers, we extract 14,648 relevant limitation papers using keyword filtering and LLM-based classification, validated against expert labels. Using topic clustering (via two approaches, HDBSCAN+BERTopic and LlooM), we identify between 7 and 15 prominent types of limitations discussed in recent LLM research across the ACL and arXiv datasets. We find that LLM-related research increases nearly sixfold in ACL and nearly fifteenfold in arXiv between 2022 and 2025, while LLLMs research grows even faster, by a factor of over 12 in ACL and nearly 28 in arXiv. Reasoning remains the most studied limitation, followed by generalization, hallucination, bias, and security. The distribution of topics in the ACL dataset stays relatively stable over time, while arXiv shifts toward safety and controllability (with topics like security risks, alignment, hallucinations, knowledge editing), and multimodality between 2022 and 2025. We offer a quantitative view of trends in LLM limitations research and release a dataset of annotated abstracts and a validated methodology, available at: https://github.com/a-kostikova/LLLMs-Survey.",,"Aida Kostikova, Zhipin Wang, Deidamea Bajri, Ole Pütz, Benjamin Paaßen, Steffen Eger",2025-05-30T10:19:31Z,LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large   Language Models,LLLMs: Eine datengestützte Untersuchung der sich entwickelnden Forschung über Grenzen großer Sprachmodelle,LLLMs:关于大语言模式限制的不断发展的研究数据驱动调查,http://arxiv.org/abs/2505.19240v2
1429,"Multilingual sentence encoders (MSEs) are commonly obtained by training multilingual language models to map sentences from different languages into a shared semantic space. As such, they are subject to curse of multilinguality, a loss of monolingual representational accuracy due to parameter sharing. Another limitation of MSEs is the trade-off between different task performance: cross-lingual alignment training distorts the optimal monolingual structure of semantic spaces of individual languages, harming the utility of sentence embeddings in monolingual tasks; cross-lingual tasks, such as cross-lingual semantic similarity and zero-shot transfer for sentence classification, may also require conflicting cross-lingual alignment strategies. In this work, we address both issues by means of modular training of sentence encoders. We first train language-specific monolingual modules to mitigate negative interference between languages (i.e., the curse). We then align all non-English sentence embeddings to the English by training cross-lingual alignment adapters, preventing interference with monolingual specialization from the first step. We train the cross-lingual adapters with two different types of data to resolve the conflicting requirements of different cross-lingual tasks. Monolingual and cross-lingual results on semantic text similarity and relatedness, bitext mining and sentence classification show that our modular solution achieves better and more balanced performance across all the tasks compared to full-parameter training of monolithic multilingual sentence encoders, especially benefiting low-resource languages.",,"Yongxin Huang, Kexin Wang, Goran Glavaš, Iryna Gurevych",2025-05-30T10:19:22Z,Modular Sentence Encoders: Separating Language Specialization from   Cross-Lingual Alignment,Modular Sentence Encoder: Trennung der Sprachspezialisierung von der Cross-Lingual Alignment,模块句句编码器:将语言专业与跨语言对齐分开,http://arxiv.org/abs/2407.14878v2
1430,"Song generation focuses on producing controllable high-quality songs based on various prompts. However, existing methods struggle to generate vocals and accompaniments with prompt-based control and proper alignment. Additionally, they fall short in supporting various tasks. To address these challenges, we introduce VersBand, a multi-task song generation framework for synthesizing high-quality, aligned songs with prompt-based control. VersBand comprises these primary models: 1) VocalBand, a decoupled model, leverages the flow-matching method for generating singing styles, pitches, and mel-spectrograms, allowing fast, high-quality vocal generation with style control. 2) AccompBand, a flow-based transformer model, incorporates the Band-MOE, selecting suitable experts for enhanced quality, alignment, and control. This model allows for generating controllable, high-quality accompaniments aligned with vocals. 3) Two generation models, LyricBand for lyrics and MelodyBand for melodies, contribute to the comprehensive multi-task song generation system, allowing for extensive control based on multiple prompts. Experimental results demonstrate that VersBand performs better over baseline models across multiple song generation tasks using objective and subjective metrics. Audio samples are available at https://aaronz345.github.io/VersBandDemo.",,"Yu Zhang, Wenxiang Guo, Changhao Pan, Zhiyuan Zhu, Ruiqi Li, Jingyu Lu, Rongjie Huang, Ruiyuan Zhang, Zhiqing Hong, Ziyue Jiang, Zhou Zhao",2025-05-30T10:15:41Z,Versatile Framework for Song Generation with Prompt-based Control,Vielseitiges Framework für Songgenerierung mit Prompt-based Control,具有即时控制制控的歌曲制作VVersatile框架,http://arxiv.org/abs/2504.19062v3
1431,"Large Language Models (LLMs) have rapidly become central to NLP, demonstrating their ability to adapt to various tasks through prompting techniques, including sentiment analysis. However, we still have a limited understanding of how these models capture sentiment-related information. This study probes the hidden layers of Llama models to pinpoint where sentiment features are most represented and to assess how this affects sentiment analysis.   Using probe classifiers, we analyze sentiment encoding across layers and scales, identifying the layers and pooling methods that best capture sentiment signals. Our results show that sentiment information is most concentrated in mid-layers for binary polarity tasks, with detection accuracy increasing up to 14% over prompting techniques. Additionally, we find that in decoder-only models, the last token is not consistently the most informative for sentiment encoding. Finally, this approach enables sentiment tasks to be performed with memory requirements reduced by an average of 57%.   These insights contribute to a broader understanding of sentiment in LLMs, suggesting layer-specific probing as an effective approach for sentiment tasks beyond prompting, with potential to enhance model utility and reduce memory requirements.",,"Dario Di Palma, Alessandro De Bellis, Giovanni Servedio, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia",2025-05-30T10:15:03Z,LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion   Representations in LLaMA Models Through Probing,LLaMAs haben auch Gefühle: Enthüllen von Sentiment- und Emotionsdarstellungen in LLaMA-Modellen durch Probing,LLARMAs也有感觉:通过试探在LLARMA模型中不懈的情感和情感表现,http://arxiv.org/abs/2505.16491v2
1432,"Zero-shot singing voice synthesis (SVS) with style transfer and style control aims to generate high-quality singing voices with unseen timbres and styles (including singing method, emotion, rhythm, technique, and pronunciation) from audio and text prompts. However, the multifaceted nature of singing styles poses a significant challenge for effective modeling, transfer, and control. Furthermore, current SVS models often fail to generate singing voices rich in stylistic nuances for unseen singers. To address these challenges, we introduce TCSinger, the first zero-shot SVS model for style transfer across cross-lingual speech and singing styles, along with multi-level style control. Specifically, TCSinger proposes three primary modules: 1) the clustering style encoder employs a clustering vector quantization model to stably condense style information into a compact latent space; 2) the Style and Duration Language Model (S\&D-LM) concurrently predicts style information and phoneme duration, which benefits both; 3) the style adaptive decoder uses a novel mel-style adaptive normalization method to generate singing voices with enhanced details. Experimental results show that TCSinger outperforms all baseline models in synthesis quality, singer similarity, and style controllability across various tasks, including zero-shot style transfer, multi-level style control, cross-lingual style transfer, and speech-to-singing style transfer. Singing voice samples can be accessed at https://aaronz345.github.io/TCSingerDemo/.",,"Yu Zhang, Ziyue Jiang, Ruiqi Li, Changhao Pan, Jinzheng He, Rongjie Huang, Chuxin Wang, Zhou Zhao",2025-05-30T10:11:27Z,TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and   Multi-Level Style Control,TCSinger: Zero-Shot Singing Voice Synthesis mit Style Transfer und Multi-Level Style Control,"TCSinger: 零点点唱语音合成,带有样式传输和多级样式控制",http://arxiv.org/abs/2409.15977v6
1433,"Style transfer for out-of-domain (OOD) singing voice synthesis (SVS) focuses on generating high-quality singing voices with unseen styles (such as timbre, emotion, pronunciation, and articulation skills) derived from reference singing voice samples. However, the endeavor to model the intricate nuances of singing voice styles is an arduous task, as singing voices possess a remarkable degree of expressiveness. Moreover, existing SVS methods encounter a decline in the quality of synthesized singing voices in OOD scenarios, as they rest upon the assumption that the target vocal attributes are discernible during the training phase. To overcome these challenges, we propose StyleSinger, the first singing voice synthesis model for zero-shot style transfer of out-of-domain reference singing voice samples. StyleSinger incorporates two critical approaches for enhanced effectiveness: 1) the Residual Style Adaptor (RSA) which employs a residual quantization module to capture diverse style characteristics in singing voices, and 2) the Uncertainty Modeling Layer Normalization (UMLN) to perturb the style attributes within the content representation during the training phase and thus improve the model generalization. Our extensive evaluations in zero-shot style transfer undeniably establish that StyleSinger outperforms baseline models in both audio quality and similarity to the reference singing voice samples. Access to singing voice samples can be found at https://aaronz345.github.io/StyleSingerDemo/.",,"Yu Zhang, Rongjie Huang, Ruiqi Li, JinZheng He, Yan Xia, Feiyang Chen, Xinyu Duan, Baoxing Huai, Zhou Zhao",2025-05-30T10:09:30Z,StyleSinger: Style Transfer for Out-of-Domain Singing Voice Synthesis,StyleSinger: Style-Übertragung für die Out-of-Domain-Singing Voice-Synthese,StyleSinger: 外庭歌曲合成的样式传输,http://arxiv.org/abs/2312.10741v5
1434,"Large language models (LLMs) store vast amounts of information, making them powerful yet raising privacy and safety concerns when selective knowledge removal is required. Existing unlearning strategies, ranging from gradient-based fine-tuning and model editing to sparse autoencoder (SAE) steering, either lack interpretability or fail to provide a robust defense against adversarial prompts. We propose SAE-Guided Subspace Projection Unlearning (SSPU), a novel framework that leverages SAE features to drive targeted updates in the model's parameter space, enabling precise, interpretable, and robust unlearning. SSPU's three-stage pipeline performs data-driven layer and feature selection, subspace construction via QR decomposition, and constrained optimization that controls activations into an ""irrelevant"" subspace while preserving retained knowledge. Overall, we use SAE features to construct a subspace that supervises unlearning, refining the loss and adding a regularization term to guide interpretable parameter updates. In experiments on the WMDP-Cyber forget set and three utility benchmarks (MMLU, TruthfulQA, GSM8K), SSPU reduces harmful knowledge accuracy by 3.22% compared to the strongest baseline. It also improves adversarial robustness, lowering malicious accuracy under jailbreak prompts compared to baselines. Our findings expose the limitations of prior unlearning methods and demonstrate how interpretable subspace-guided optimization can achieve robust, controllable model behavior.",,"Xu Wang, Zihao Li, Benyou Wang, Yan Hu, Difan Zou",2025-05-30T10:07:52Z,Model Unlearning via Sparse Autoencoder Subspace Guided Projections,Modell Entlernen über Sparse Autoencoder Subspace Geführte Projektionen,通过 Sparass Autoencoder 子空间向导预测退出学习模型,http://arxiv.org/abs/2505.24428v1
1435,"Accurate modeling of subjective phenomena such as emotion expression requires data annotated with authors' intentions. Commonly such data is collected by asking study participants to donate and label genuine content produced in the real world, or create content fitting particular labels during the study. Asking participants to create content is often simpler to implement and presents fewer risks to participant privacy than data donation. However, it is unclear if and how study-created content may differ from genuine content, and how differences may impact models. We collect study-created and genuine multimodal social media posts labeled for emotion and compare them on several dimensions, including model performance. We find that compared to genuine posts, study-created posts are longer, rely more on their text and less on their images for emotion expression, and focus more on emotion-prototypical events. The samples of participants willing to donate versus create posts are demographically different. Study-created data is valuable to train models that generalize well to genuine data, but realistic effectiveness estimates require genuine data.",,"Christopher Bagdon, Aidan Combs, Carina Silberer, Roman Klinger",2025-05-30T10:07:34Z,Donate or Create? Comparing Data Collection Strategies for   Emotion-labeled Multimodal Social Media Posts,Spenden oder Erstellen? Vergleichende Datenerhebungsstrategien für emotional markierte multimodale Social Media-Posts,捐赠还是创建? 比较情感标签的多模式社会媒体职位的数据收集战略,http://arxiv.org/abs/2505.24427v1
1436,"Large language models and vision-language models (which we jointly call LMs) have transformed NLP and CV, demonstrating remarkable potential across various fields. However, their capabilities in affective analysis (i.e. sentiment analysis and emotion detection) remain underexplored. This gap is largely due to the absence of comprehensive evaluation benchmarks, and the inherent complexity of affective analysis tasks. In this paper, we introduce MMAFFBen, the first extensive open-source benchmark for multilingual multimodal affective analysis. MMAFFBen encompasses text, image, and video modalities across 35 languages, covering four key affective analysis tasks: sentiment polarity, sentiment intensity, emotion classification, and emotion intensity. Moreover, we construct the MMAFFIn dataset for fine-tuning LMs on affective analysis tasks, and further develop MMAFFLM-3b and MMAFFLM-7b based on it. We evaluate various representative LMs, including GPT-4o-mini, providing a systematic comparison of their affective understanding capabilities. This project is available at https://github.com/lzw108/MMAFFBen.",,"Zhiwei Liu, Lingfei Qian, Qianqian Xie, Jimin Huang, Kailai Yang, Sophia Ananiadou",2025-05-30T10:02:15Z,MMAFFBen: A Multilingual and Multimodal Affective Analysis Benchmark for   Evaluating LLMs and VLMs,MMAFFBen: Multilingualer und multimodaler Analyse-Benchmark für die Bewertung von LLMs und VLMs,MMAFF Ben: 用于评价LMM和VLM的多语种和多模式情感分析基准,http://arxiv.org/abs/2505.24423v1
1437,"Multilingual large language models (LLMs) open up new possibilities for leveraging information across languages, but their factual knowledge recall remains inconsistent depending on the input language. While previous studies have attempted to address this issue through English-based prompting and evaluation, we explore non-English to English transfer via Language and Thought Theory. This perspective allows us to examine language-thought binding in LLMs and uncover why factual knowledge often fails to transfer effectively. We propose the Language-to-Thought (L2T) prompting strategy, which analyzes the relationship between input language, internal cognitive processes, and knowledge. Experimental results challenge the assumption that English-based approaches consistently outperform other languages and offer a novel insight that aligning the model's internal thought with the knowledge required for the task is critical for successful cross-lingual transfer. Furthermore, we show that applying L2T during training can alleviate LLMs' reliance on the input language and facilitate cross-linguistic knowledge integration without translation-based learning. Code and datasets will be available.",,"Eojin Kang, Juae Kim",2025-05-30T09:47:25Z,LLMs Are Globally Multilingual Yet Locally Monolingual: Exploring   Knowledge Transfer via Language and Thought Theory,LLMs sind weltweit mehrsprachig und doch lokal einsprachig: Wissenstransfer über Sprache und Gedankentheorie erforschen,LLMs是全球多语言但当地独语:探索通过语言和思想理论转让知识,http://arxiv.org/abs/2505.24409v1
1438,"Large language models have recently pushed open domain question answering (ODQA) to new frontiers. However, prevailing retriever-reader pipelines often depend on multiple rounds of prompt level instructions, leading to high computational overhead, instability, and suboptimal retrieval coverage. In this paper, we propose EmbQA, an embedding-level framework that alleviates these shortcomings by enhancing both the retriever and the reader. Specifically, we refine query representations via lightweight linear layers under an unsupervised contrastive learning objective, thereby reordering retrieved passages to highlight those most likely to contain correct answers. Additionally, we introduce an exploratory embedding that broadens the model's latent semantic space to diversify candidate generation and employs an entropy-based selection mechanism to choose the most confident answer automatically. Extensive experiments across three open-source LLMs, three retrieval methods, and four ODQA benchmarks demonstrate that EmbQA substantially outperforms recent baselines in both accuracy and efficiency.",,"Zhanghao Hu, Hanqi Yan, Qinglin Zhu, Zhenyi Shen, Yulan He, Lin Gui",2025-05-30T09:44:30Z,Beyond Prompting: An Efficient Embedding Framework for Open-Domain   Question Answering,Beyond Prompting: Ein effizientes Embedding-Framework für Open-Domain-Fragenbeantwortung,超越提示:开放域问答的有效嵌入框架,http://arxiv.org/abs/2503.01606v2
1439,"Training safe LLMs remains a critical challenge. The most widely used method, Refusal Training (RT), struggles to generalize against various Out-of-Distribution (OOD) jailbreaking attacks. Although various advanced methods have been proposed to address this issue, we instead question whether OOD attacks inherently surpass the capability of vanilla RT. Evaluations using Best-of-N (BoN) reveal significant safety improvements as N increases, indicating models possess adequate latent safety knowledge but RT fails to consistently elicit it under OOD scenarios. Further domain adaptation analysis reveals that direct RT causes reliance on superficial shortcuts, resulting in non-generalizable representation mappings. Inspired by our findings, we propose training model to perform safety reasoning for each query. Specifically, we synthesize reasoning supervision aligned with specified guidelines that reflect diverse perspectives on safety knowledge. This encourages model to engage in deeper reasoning, explicitly eliciting and utilizing latent safety knowledge for each query. Extensive experiments show that our method significantly improves model generalization against OOD attacks.",,"Haoyu Wang, Zeyu Qin, Li Shen, Xueqian Wang, Dacheng Tao, Minhao Cheng",2025-05-30T09:43:42Z,Safety Reasoning with Guidelines,Sicherheitsbegründeung mit Leitlinien,符合《准则》的安全理由,http://arxiv.org/abs/2502.04040v2
1440,"The generation of toxic content by large language models (LLMs) remains a critical challenge for the safe deployment of language technology. We propose a novel framework for implicit knowledge editing and controlled text generation by fine-tuning LLMs with a prototype-based contrastive perplexity objective. Central to our method is the construction of hard negatives - toxic outputs that are generated through adversarial paraphrasing to be semantically similar and model probability to their non-toxic counterparts. By training on these challenging and realistic pairs, our approach ensures robust and stable contrastive optimization. Experimental results in the domain of detoxification demonstrate that our method significantly reduces toxic generation while maintaining strong performance on downstream tasks such as commonsense reasoning and reading comprehension. Our findings highlight the effectiveness of exploiting hard negatives for attribute-aware fine-tuning.",,"Tassilo Klein, Moin Nabi",2025-05-30T09:37:59Z,Contrastive Perplexity for Controlled Generation: An Application in   Detoxifying Large Language Models,Kontrastive Komplexität für kontrollierte Generation: Eine Anwendung bei der Entgiftung großer Sprachmodelle,受控生成的矛盾性:在解毒大语言模型中的应用,http://arxiv.org/abs/2401.08491v3
1441,"The evaluation of factual accuracy in large vision language models (LVLMs) has lagged behind their rapid development, making it challenging to fully reflect these models' knowledge capacity and reliability. In this paper, we introduce the first factuality-based visual question-answering benchmark in Chinese, named ChineseSimpleVQA, aimed at assessing the visual factuality of LVLMs across 8 major topics and 56 subtopics. The key features of this benchmark include a focus on the Chinese language, diverse knowledge types, a multi-hop question construction, high-quality data, static consistency, and easy-to-evaluate through short answers. Moreover, we contribute a rigorous data construction pipeline and decouple the visual factuality into two parts: seeing the world (i.e., object recognition) and discovering knowledge. This decoupling allows us to analyze the capability boundaries and execution mechanisms of LVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source models, revealing critical performance gaps within this field. Our evaluation-friendly code and data have already been open-sourced.",,"Jihao Gu, Yingyao Wang, Pi Bu, Chen Wang, Ziming Wang, Tengtao Song, Donglai Wei, Jiale Yuan, Yingxiu Zhao, Yancheng He, Shilong Li, Jiaheng Liu, Meng Cao, Jun Song, Yingshui Tan, Xiang Li, Wenbo Su, Zhicheng Zheng, Xiaoyong Zhu, Bo Zheng",2025-05-30T09:27:22Z,"""See the World, Discover Knowledge"": A Chinese Factuality Evaluation for   Large Vision Language Models","""See the World, Discover Knowledge"": Eine chinesische Wahrheitsbewertung für große Visions-Sprachmodelle","“见世界,发现知识”:中国大视野语言模式事实评估",http://arxiv.org/abs/2502.11718v4
1442,"Retrieval-Augmented Generation (RAG) augments Large Language Models (LLMs) with external knowledge to improve factuality. However, existing RAG systems frequently underutilize the retrieved documents, failing to extract and integrate the key clues needed to support faithful and interpretable reasoning, especially in cases where relevant evidence is implicit, scattered, or obscured by noise. To address this issue, we propose ClueAnchor, a novel framework for enhancing RAG via clue-anchored reasoning exploration and optimization. ClueAnchor extracts key clues from retrieved content and generates multiple reasoning paths based on different knowledge configurations, optimizing the model by selecting the most effective one through reward-based preference optimization. Experiments show that ClueAnchor significantly outperforms prior RAG baselines in reasoning completeness and robustness. Further analysis confirms its strong resilience to noisy or partially relevant retrieved content, as well as its capability to identify supporting evidence even in the absence of explicit clue supervision during inference.",,"Hao Chen, Yukun Yan, Sen Mei, Wanxiang Che, Zhenghao Liu, Qi Shi, Xinze Li, Yuchun Fan, Pengcheng Huang, Qiushi Xiong, Zhiyuan Liu, Maosong Sun",2025-05-30T09:18:08Z,ClueAnchor: Clue-Anchored Knowledge Reasoning Exploration and   Optimization for Retrieval-Augmented Generation,ClueAnchor: Clue-Anchored Knowledge Reasoning Exploration and Optimization for Retrieval-Augmented Generation,ClueAnchor:为回溯性回溯性回溯性回溯性一代人提供克隆知识理由探索和优化,http://arxiv.org/abs/2505.24388v1
1443,"Continual learning (CL) is crucial for deploying large language models (LLMs) in dynamic real-world environments without costly retraining. While recent model ensemble and model merging methods guided by parameter importance have gained popularity, they often struggle to balance knowledge transfer and forgetting, mainly due to the reliance on static importance estimates during sequential training. In this paper, we present Recurrent-KIF, a novel CL framework for Recurrent Knowledge Identification and Fusion, which enables dynamic estimation of parameter importance distributions to enhance knowledge transfer. Inspired by human continual learning, Recurrent-KIF employs an inner loop that rapidly adapts to new tasks while identifying important parameters, coupled with an outer loop that globally manages the fusion of new and historical knowledge through redundant knowledge pruning and key knowledge merging. These inner-outer loops iteratively perform multiple rounds of fusion, allowing Recurrent-KIF to leverage intermediate training information and adaptively adjust fusion strategies based on evolving importance distributions. Extensive experiments on two CL benchmarks with various model sizes (from 770M to 13B) demonstrate that Recurrent-KIF effectively mitigates catastrophic forgetting and enhances knowledge transfer.",,"Yujie Feng, Xujia Wang, Zexin Lu, Shenghong Fu, Guangyuan Shi, Yongxin Xu, Yasha Wang, Philip S. Yu, Xu Chu, Xiao-Ming Wu",2025-05-30T09:15:45Z,Recurrent Knowledge Identification and Fusion for Language Model   Continual Learning,Recurrent Knowledge Identification and Fusion for Language Model Continual Learning,语文模式持续学习知识识别和融合,http://arxiv.org/abs/2502.17510v2
1444,"Large language models are typically trained on datasets collected from the web, which may inadvertently contain harmful or sensitive personal information. To address growing privacy concerns, unlearning methods have been proposed to remove the influence of specific data from trained models. Of these, exact unlearning -- which retrains the model from scratch without the target data -- is widely regarded the gold standard, believed to be robust against privacy-related attacks. In this paper, we challenge this assumption by introducing a novel data extraction attack that compromises even exact unlearning. Our method leverages both the pre- and post-unlearning models: by guiding the post-unlearning model using signals from the pre-unlearning model, we uncover patterns that reflect the removed data distribution. Combining model guidance with a token filtering strategy, our attack significantly improves extraction success rates -- doubling performance in some cases -- across common benchmarks such as MUSE, TOFU, and WMDP. Furthermore, we demonstrate our attack's effectiveness on a simulated medical diagnosis dataset to highlight real-world privacy risks associated with exact unlearning. In light of our findings, which suggest that unlearning may, in a contradictory way, increase the risk of privacy leakage, we advocate for evaluation of unlearning methods to consider broader threat models that account not only for post-unlearning models but also for adversarial access to prior checkpoints.",,"Xiaoyu Wu, Yifei Pang, Terrance Liu, Zhiwei Steven Wu",2025-05-30T09:09:33Z,Breaking the Gold Standard: Extracting Forgotten Data under Exact   Unlearning in Large Language Models,Brechen des Gold-Standards: Extrahieren vergessener Daten unter exaktem Lernen in großen Sprachmodellen,打破黄金标准:根据在大语言模型中精确解学提取被遗忘的数据,http://arxiv.org/abs/2505.24379v1
1445,"Recent advancements in large language models (LLMs) have enhanced natural-language reasoning. However, their limited parametric memory and susceptibility to hallucination present persistent challenges for tasks requiring accurate, context-based inference. To overcome these limitations, an increasing number of studies have proposed leveraging external knowledge to enhance LLMs. This study offers a systematic exploration of strategies for using external knowledge to enhance LLMs, beginning with a taxonomy that categorizes external knowledge into unstructured and structured data. We then focus on structured knowledge, presenting distinct taxonomies for tables and knowledge graphs (KGs), detailing their integration paradigms with LLMs, and reviewing representative methods. Our comparative analysis further highlights the trade-offs among interpretability, scalability, and performance, providing insights for developing trustworthy and generalizable knowledge-enhanced LLMs.",,"Yu-Hsuan Lin, Qian-Hui Chen, Yi-Jie Cheng, Jia-Ren Zhang, Yi-Hung Liu, Liang-Yu Hsia, Yun-Nung Chen",2025-05-30T09:08:51Z,LLM Inference Enhanced by External Knowledge: A Survey,LLM-Inferenz durch externes Wissen verbessert: Eine Umfrage,LLM 外部知识增强的推论:调查,http://arxiv.org/abs/2505.24377v1
1446,"Sign Language Translation (SLT) aims to convert sign language (SL) videos into spoken language text, thereby bridging the communication gap between the sign and the spoken community. While most existing works focus on translating a single sign language into a single spoken language (one-to-one SLT), leveraging multilingual resources could mitigate low-resource issues and enhance accessibility. However, multilingual SLT (MLSLT) remains unexplored due to language conflicts and alignment difficulties across SLs and spoken languages. To address these challenges, we propose a multilingual gloss-free model with dual CTC objectives for token-level SL identification and spoken text generation. Our model supports 10 SLs and handles one-to-one, many-to-one, and many-to-many SLT tasks, achieving competitive performance compared to state-of-the-art methods on three widely adopted benchmarks: multilingual SP-10, PHOENIX14T, and CSL-Daily.",,"Sihan Tan, Taro Miyazaki, Kazuhiro Nakadai",2025-05-30T08:47:44Z,Multilingual Gloss-free Sign Language Translation: Towards Building a   Sign Language Foundation Model,"Mehrsprachige, glänzende Sign Language Übersetzung: Auf dem Weg zum Aufbau eines Sign Language Foundation Modells",多种语言无粗粗粗手语翻译:努力建立一个手语基金会模式,http://arxiv.org/abs/2505.24355v1
1447,"LLMs have achieved remarkable fluency and coherence in text generation, yet their widespread adoption has raised concerns about content reliability and accountability. In high-stakes domains, it is crucial to understand where and how the content is created. To address this, we introduce the Text pROVEnance (TROVE) challenge, designed to trace each sentence of a target text back to specific source sentences within potentially lengthy or multi-document inputs. Beyond identifying sources, TROVE annotates the fine-grained relationships (quotation, compression, inference, and others), providing a deep understanding of how each target sentence is formed. To benchmark TROVE, we construct our dataset by leveraging three public datasets covering 11 diverse scenarios (e.g., QA and summarization) in English and Chinese, spanning source texts of varying lengths (0-5k, 5-10k, 10k+), emphasizing the multi-document and long-document settings essential for provenance. To ensure high-quality data, we employ a three-stage annotation process: sentence retrieval, GPT-4o provenance, and human provenance. We evaluate 11 LLMs under direct prompting and retrieval-augmented paradigms, revealing that retrieval is essential for robust performance, larger models perform better in complex relationship classification, and closed-source models often lead, yet open-source models show significant promise, particularly with retrieval augmentation. We make our dataset available here: https://github.com/ZNLP/ZNLP-Dataset.",,"Junnan Zhu, Min Xiao, Yining Wang, Feifei Zhai, Yu Zhou, Chengqing Zong",2025-05-30T08:47:17Z,TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence   Tracing and Relationship Classification,TROVE: Eine Herausforderung für die feinkörnige Textprovenz mittels Source Sentence Tracing und Relationship Classification,TROVE:通过来源判决追踪和关系分类对通过来源判决追踪和关系分类获得精细文本的挑战,http://arxiv.org/abs/2503.15289v3
1448,"As AI capabilities increasingly surpass human proficiency in complex tasks, current alignment techniques including SFT and RLHF face fundamental challenges in ensuring reliable oversight. These methods rely on direct human assessment and become untenable when AI outputs exceed human cognitive thresholds. In response to this challenge, we explore two hypotheses: (1) \textit{Critique of critique can be easier than critique itself}, extending the widely-accepted observation that verification is easier than generation to the critique domain, as critique itself is a specialized form of generation; (2) \textit{This difficulty relationship is recursively held}, suggesting that when direct evaluation is infeasible, performing high-order critiques (e.g., critique of critique of critique) offers a more tractable supervision pathway. We further conduct Human-AI and AI-AI experiments to investigate the potential of utilizing recursive self-critiquing for AI supervision. Our results highlight recursive critique as a promising approach for scalable AI oversight.",,"Xueru Wen, Jie Lou, Xinyu Lu, Junjie Yang, Yanjiang Liu, Yaojie Lu, Debing Zhang, Xing Yu",2025-05-30T08:46:59Z,Scalable Oversight for Superhuman AI via Recursive Self-Critiquing,Skalierbare Aufsicht für übermenschliche KI über rekursive Selbstbeherrschung,通过递归自裁量法对超人AI进行可缩放的监督,http://arxiv.org/abs/2502.04675v3
1449,"Language agents powered by large language models (LLMs) have demonstrated remarkable capabilities in understanding, reasoning, and executing complex tasks. However, developing robust agents presents significant challenges: substantial engineering overhead, lack of standardized components, and insufficient evaluation frameworks for fair comparison. We introduce Agent Graph-based Orchestration for Reasoning and Assessment (AGORA), a flexible and extensible framework that addresses these challenges through three key contributions: (1) a modular architecture with a graph-based workflow engine, efficient memory management, and clean component abstraction; (2) a comprehensive suite of reusable agent algorithms implementing state-of-the-art reasoning approaches; and (3) a rigorous evaluation framework enabling systematic comparison across multiple dimensions. Through extensive experiments on mathematical reasoning and multimodal tasks, we evaluate various agent algorithms across different LLMs, revealing important insights about their relative strengths and applicability. Our results demonstrate that while sophisticated reasoning approaches can enhance agent capabilities, simpler methods like Chain-of-Thought often exhibit robust performance with significantly lower computational overhead. AGORA not only simplifies language agent development but also establishes a foundation for reproducible agent research through standardized evaluation protocols.",,"Qianqian Zhang, Jiajia Liao, Heting Ying, Yibo Ma, Haozhan Shen, Jingcheng Li, Peng Liu, Lu Zhang, Chunxin Fang, Kyusong Lee, Ruochen Xu, Tiancheng Zhao",2025-05-30T08:46:23Z,Unifying Language Agent Algorithms with Graph-based Orchestration Engine   for Reproducible Agent Research,Gemeinsamer Sprachagent Algorithmen mit Graph-basierter Orchestrierungs-Engine für reproduzierbare Agent-Forschung,使用基于图表的可复制剂研究管弦化引擎统一语言代理方算法,http://arxiv.org/abs/2505.24354v1
1450,"As Large Language Models (LLMs) are widely applied in various domains, the safety of LLMs is increasingly attracting attention to avoid their powerful capabilities being misused. Existing jailbreak methods create a forced instruction-following scenario, or search adversarial prompts with prefix or suffix tokens to achieve a specific representation manually or automatically. However, they suffer from low efficiency and explicit jailbreak patterns, far from the real deployment of mass attacks to LLMs. In this paper, we point out that simply rewriting the original instruction can achieve a jailbreak, and we find that this rewriting approach is learnable and transferable. We propose the Rewrite to Jailbreak (R2J) approach, a transferable black-box jailbreak method to attack LLMs by iteratively exploring the weakness of the LLMs and automatically improving the attacking strategy. The jailbreak is more efficient and hard to identify since no additional features are introduced. Extensive experiments and analysis demonstrate the effectiveness of R2J, and we find that the jailbreak is also transferable to multiple datasets and various types of models with only a few queries. We hope our work motivates further investigation of LLM safety. The code can be found at https://github.com/ythuang02/R2J/.",,"Yuting Huang, Chengyuan Liu, Yifeng Feng, Yiquan Wu, Chao Wu, Fei Wu, Kun Kuang",2025-05-30T08:36:46Z,Rewrite to Jailbreak: Discover Learnable and Transferable Implicit   Harmfulness Instruction,Umschreiben zu Jailbreak: Entdecken Sie erlernbare und übertragbare Implizite Harmfulness Instruction,重写侵入监狱:发现可学习和可转移的隐性伤害指示,http://arxiv.org/abs/2502.11084v2
1451,"Understanding what emotions images evoke in their viewers is a foundational goal in human-centric visual computing. While recent advances in vision-language models (VLMs) have shown promise for visual emotion analysis (VEA), several key challenges remain unresolved. Emotional cues in images are often abstract, overlapping, and entangled, making them difficult to model and interpret. Moreover, VLMs struggle to align these complex visual patterns with emotional semantics due to limited supervision and sparse emotional grounding. Finally, existing approaches lack structured affective knowledge to resolve ambiguity and ensure consistent emotional reasoning across diverse visual domains.   To address these limitations, we propose \textbf{K-EVER\textsuperscript{2}}, a knowledge-enhanced framework for emotion reasoning and retrieval. Our approach introduces a semantically structured formulation of visual emotion cues and integrates external affective knowledge through multimodal alignment. Without relying on handcrafted labels or direct emotion supervision, K-EVER\textsuperscript{2} achieves robust and interpretable emotion predictions across heterogeneous image types.   We validate our framework on three representative benchmarks, Emotion6, EmoSet, and M-Disaster, covering social media imagery, human-centric scenes, and disaster contexts. K-EVER\textsuperscript{2} consistently outperforms strong CNN and VLM baselines, achieving up to a \textbf{19\% accuracy gain} for specific emotions and a \textbf{12.3\% average accuracy gain} across all emotion categories. Our results demonstrate a scalable and generalizable solution for advancing emotional understanding of visual content.",,"Fanhang Man, Xiaoyue Chen, Huandong Wang, Baining Zhao, Han Li, Xinlei Chen, Yong Li",2025-05-30T08:33:32Z,KEVER^2: Knowledge-Enhanced Visual Emotion Reasoning and Retrieval,KEVER^2: Wissensverstärkte visuelle Emotionsveranlagung und Retrieval,KEWL 2: 知识强化的视觉情感理性和检索,http://arxiv.org/abs/2505.24342v1
1452,"Detecting toxic content using language models is important but challenging. While large language models (LLMs) have demonstrated strong performance in understanding Chinese, recent studies show that simple character substitutions in toxic Chinese text can easily confuse the state-of-the-art (SOTA) LLMs. In this paper, we highlight the multimodal nature of Chinese language as a key challenge for deploying LLMs in toxic Chinese detection. First, we propose a taxonomy of 3 perturbation strategies and 8 specific approaches in toxic Chinese content. Then, we curate a dataset based on this taxonomy, and benchmark 9 SOTA LLMs (from both the US and China) to assess if they can detect perturbed toxic Chinese text. Additionally, we explore cost-effective enhancement solutions like in-context learning (ICL) and supervised fine-tuning (SFT). Our results reveal two important findings. (1) LLMs are less capable of detecting perturbed multimodal Chinese toxic contents. (2) ICL or SFT with a small number of perturbed examples may cause the LLMs ""overcorrect'': misidentify many normal Chinese contents as toxic.",,"Shujian Yang, Shiyao Cui, Chuanrui Hu, Haicheng Wang, Tianwei Zhang, Minlie Huang, Jialiang Lu, Han Qiu",2025-05-30T08:32:45Z,"Exploring Multimodal Challenges in Toxic Chinese Detection: Taxonomy,   Benchmark, and Findings","Erforschung multimodaler Herausforderungen bei der toxischen chinesischen Erkennung: Taxonomie, Benchmark und Befunde",探索中国毒物检测的多模式挑战:分类学、基准和调查结果,http://arxiv.org/abs/2505.24341v1
1453,"Classifying geospatial imagery remains a major bottleneck for applications such as disaster response and land-use monitoring-particularly in regions where annotated data is scarce or unavailable. Existing tools (e.g., RS-CLIP) that claim zero-shot classification capabilities for satellite imagery nonetheless rely on task-specific pretraining and adaptation to reach competitive performance. We introduce GeoVision Labeler (GVL), a strictly zero-shot classification framework: a vision Large Language Model (vLLM) generates rich, human-readable image descriptions, which are then mapped to user-defined classes by a conventional Large Language Model (LLM). This modular, and interpretable pipeline enables flexible image classification for a large range of use cases. We evaluated GVL across three benchmarks-SpaceNet v7, UC Merced, and RESISC45. It achieves up to 93.2% zero-shot accuracy on the binary Buildings vs. No Buildings task on SpaceNet v7. For complex multi-class classification tasks (UC Merced, RESISC45), we implemented a recursive LLM-driven clustering to form meta-classes at successive depths, followed by hierarchical classification-first resolving coarse groups, then finer distinctions-to deliver competitive zero-shot performance. GVL is open-sourced at https://github.com/microsoft/geo-vision-labeler to catalyze adoption in real-world geospatial workflows.",,"Gilles Quentin Hacheme, Girmaw Abebe Tadesse, Caleb Robinson, Akram Zaytar, Rahul Dodhia, Juan M. Lavista Ferres",2025-05-30T08:32:37Z,GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and   Language Models,GeoVision Labeler: Zero-Shot Geospatial Classification mit Vision und Sprachmodellen,GeoVision Labeler:带有愿景和语言模型的零热地理空间分类,http://arxiv.org/abs/2505.24340v1
1454,"Faithfulness hallucinations are claims generated by a Large Language Model (LLM) not supported by contexts provided to the LLM. Lacking assessment standards, existing benchmarks focus on ""factual statements"" that rephrase source materials while overlooking ""cognitive statements"" that involve making inferences from the given context. Consequently, evaluating and detecting the hallucination of cognitive statements remains challenging. Inspired by how evidence is assessed in the legal domain, we design a rigorous framework to assess different levels of faithfulness of cognitive statements and introduce the CogniBench dataset where we reveal insightful statistics. To keep pace with rapidly evolving LLMs, we further develop an automatic annotation pipeline that scales easily across different models. This results in a large-scale CogniBench-L dataset, which facilitates training accurate detectors for both factual and cognitive hallucinations. We release our model and datasets at: https://github.com/FUTUREEEEEE/CogniBench",,"Xiaqiang Tang, Jian Li, Keyu Hu, Du Nan, Xiaolong Li, Xi Zhang, Weigao Sun, Sihong Xie",2025-05-30T08:16:51Z,CogniBench: A Legal-inspired Framework and Dataset for Assessing   Cognitive Faithfulness of Large Language Models,CogniBench: Ein gesetzlich inspirierter Rahmen und Datensatz zur Bewertung der kognitiven Treue großer Sprachmodelle,CogniBench:评估大语言模型认知性信仰的受法律启发的框架和数据集,http://arxiv.org/abs/2505.20767v3
1455,"Information seeking demands iterative evidence gathering and reflective reasoning, yet large language models (LLMs) still struggle with it in open-web question answering. Existing methods rely on static prompting rules or training with Wikipedia-based corpora and retrieval environments, limiting adaptability to the real-world web environment where ambiguity, conflicting evidence, and noise are prevalent. These constrained training settings hinder LLMs from learning to dynamically decide when and where to search, and how to adjust search depth and frequency based on informational demands. We define this missing capacity as Search Intensity Scaling (SIS)--the emergent skill to intensify search efforts under ambiguous or conflicting conditions, rather than settling on overconfident, under-verification answers.   To study SIS, we introduce WebPuzzle, the first dataset designed to foster information-seeking behavior in open-world internet environments. WebPuzzle consists of 24K training instances and 275 test questions spanning both wiki-based and open-web queries. Building on this dataset, we propose DeepDiver, a Reinforcement Learning (RL) framework that promotes SIS by encouraging adaptive search policies through exploration under a real-world open-web environment. Experimental results show that Pangu-7B-Reasoner empowered by DeepDiver achieve performance on real-web tasks comparable to the 671B-parameter DeepSeek-R1. We detail DeepDiver's training curriculum from cold-start supervised fine-tuning to a carefully designed RL phase, and present that its capability of SIS generalizes from closed-form QA to open-ended tasks such as long-form writing. Our contributions advance adaptive information seeking in LLMs and provide a valuable benchmark and dataset for future research.",,"Wenxuan Shi, Haochen Tan, Chuqiao Kuang, Xiaoguang Li, Xiaozhe Ren, Chen Zhang, Hanting Chen, Yasheng Wang, Lifeng Shang, Fisher Yu, Yunhe Wang",2025-05-30T08:15:39Z,Pangu DeepDiver: Adaptive Search Intensity Scaling via Open-Web   Reinforcement Learning,Pangu DeepDiver: Adaptive Search Intensity Scaling über Open-Web-Verstärkungslernen,Pangu 深didiver:通过开放网络强化学习使适应性搜索强度缩放,http://arxiv.org/abs/2505.24332v1
1456,"User sentiment on social media reveals the underlying social trends, crises, and needs. Researchers have analyzed users' past messages to trace the evolution of sentiments and reconstruct sentiment dynamics. However, predicting the imminent sentiment of an ongoing event is rarely studied. In this paper, we address the problem of \textbf{sentiment forecasting} on social media to predict the user's future sentiment in response to the development of the event. We extract sentiment-related features to enhance the modeling skill and propose a multi-perspective role-playing framework to simulate the process of human response. Our preliminary results show significant improvement in sentiment forecasting on both microscopic and macroscopic levels.",,"Fanhang Man, Huandong Wang, Jianjie Fang, Zhaoyi Deng, Baining Zhao, Xinlei Chen, Yong Li",2025-05-30T08:13:33Z,Context-Aware Sentiment Forecasting via LLM-based Multi-Perspective   Role-Playing Agents,Kontext-Bewusst-Sentiment-Prognose über LLM-basierte multiperspektive Rollenspiel-Agenten,"通过基于LLM的多展望角色布局代理物预测,通过基于LLM的多展望角色布局代理物预测",http://arxiv.org/abs/2505.24331v1
1457,"Does the prior knowledge of the vision encoder constrain the capability boundary of Multi-modal Large Language Models (MLLMs)? While most existing research treats MLLMs as unified systems optimized through end-to-end training, the impact of vision encoder's prior knowledge is seldom investigated. In this work, we introduce a novel metric, $Rank_e$, to quantify the effect of prior knowledge of the vision encoder on MLLM performance. Our analysis reveals a positive correlation between prior knowledge and MLLM performance. Moreover, we find that domain-specific fine-tuning using solely end-to-end visual question answering (VQA) data is insufficient, particularly for entities with low inherent visual prior knowledge. To address this issue, we propose VisPRE (Vision Prior Remediation), a two-stage training framework that explicitly incorporates prior knowledge at the vision encoder level. Experimental results demonstrate that augmenting vision encoder's prior knowledge substantially boosts the visual understanding capabilities of MLLMs, offering a novel and effective strategy for improving performance, especially in scenarios involving uncommon visual entities.",,"Qiao Liang, Yanjiang Liu, Weixiang Zhou, Ben He, Yaojie Lu, Hongyu Lin, Jia Zheng, Xianpei Han, Le Sun, Yingfei Sun",2025-05-30T08:06:52Z,Expanding the Boundaries of Vision Prior Knowledge in Multi-modal Large   Language Models,Erweiterung der Grenzen von Vision Prior Knowledge in multimodalen großen Sprachmodellen,扩大多模式大语言模式中愿景先见先见知识的界限,http://arxiv.org/abs/2503.18034v2
1458,"In recent years, large language models (LLMs) have showcased significant advancements in code generation. However, most evaluation benchmarks are primarily oriented towards Python, making it difficult to evaluate other programming languages, such as Swift, with high quality. By examining widely established multilingual benchmarks like HumanEval-XL and MultiPL-E, we identified critical issues specific to their Swift components, making them insufficient or even irrelevant for assessing LLM coding capabilities on Swift. Unlike these existing approaches, which prioritize rapid scaling and generalization by automatically translating Python-centric benchmarks with LLMs, we adopt a quality-over-quantity methodology. We present SwiftEval, the first Swift-oriented benchmark consisting of 28 carefully hand-crafted problems, and evaluate 44 popular Code LLMs on it. Our results show significant LLM scores drop for problems requiring language-specific features, most noticeable in the models of smaller sizes.",,"Ivan Petrukha, Yana Kurliak, Nataliia Stulova",2025-05-30T08:06:30Z,SwiftEval: Developing a Language-Specific Benchmark for LLM-generated   Code Evaluation,SwiftEval: Entwicklung eines sprachspezifischen Benchmarks für die LLM-generierte Code-Bewertung,SwiftEval:为LLM产生的守则评价制定语言特定基准,http://arxiv.org/abs/2505.24324v1
1459,"Knowledge Base Question Answering (KBQA) aims to answer natural language questions with a large-scale structured knowledge base (KB). Despite advancements with large language models (LLMs), KBQA still faces challenges in weak KB awareness, imbalance between effectiveness and efficiency, and high reliance on annotated data. To address these challenges, we propose KBQA-o1, a novel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a ReAct-based agent process for stepwise logical form generation with KB environment exploration. Moreover, it employs MCTS, a heuristic search method driven by policy and reward models, to balance agentic exploration's performance and search space. With heuristic exploration, KBQA-o1 generates high-quality annotations for further improvement by incremental fine-tuning. Experimental results show that KBQA-o1 outperforms previous low-resource KBQA methods with limited annotated data, boosting Llama-3.1-8B model's GrailQA F1 performance to 78.5% compared to 48.5% of the previous sota method with GPT-3.5-turbo. Our code is publicly available.",,"Haoran Luo, Haihong E, Yikai Guo, Qika Lin, Xiaobao Wu, Xinyu Mu, Wenhao Liu, Meina Song, Yifan Zhu, Luu Anh Tuan",2025-05-30T08:04:19Z,KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree   Search,KBQA-o1: Agentische Wissensdatenbank Frage beantworten mit Monte Carlo Baumsuche,KBQA- o1: 用于蒙特卡洛树搜索的代理知识库问题解答,http://arxiv.org/abs/2501.18922v4
1460,"While Chain of Thought (CoT) prompting approaches have significantly consolidated the reasoning capabilities of large language models (LLMs), they still face limitations that require extensive human effort or have performance needs to be improved. Existing endeavors have focused on bridging these gaps; however, these approaches either hinge on external data and cannot completely eliminate manual effort, or they fall short in effectively directing LLMs to generate high-quality exemplary prompts. To address the said pitfalls, we propose a novel prompt approach for automatic reasoning named \textbf{LBS3}, inspired by curriculum learning which better reflects human learning habits. Specifically, LBS3 initially steers LLMs to recall easy-to-hard proxy queries that are pertinent to the target query. Following this, it invokes a progressive strategy that utilizes exemplary prompts stemmed from easy-proxy queries to direct LLMs in solving hard-proxy queries, enabling the high-quality of the proxy solutions. Finally, our extensive experiments in various reasoning-intensive tasks with varying open- and closed-source LLMs show that LBS3 achieves strongly competitive performance compared to the SOTA baselines.",,"Kangyang Luo, Zichen Ding, Zhenmin Weng, Lingfeng Qiao, Meng Zhao, Xiang Li, Di Yin, Jinlong Shu",2025-05-30T08:02:49Z,Let's Be Self-generated via Step by Step: A Curriculum Learning Approach   to Automated Reasoning with Large Language Models,Lassen Sie uns Schritt für Schritt selbsterzeugen: Ein Curriculum-Lernansatz zur automatisierten Vernunft mit großen Sprachmodellen,"让我们一步一步地自我产生:学习课程的方法,用大语言模式自动说明理由",http://arxiv.org/abs/2410.21728v4
1461,"Large Language Models (LLMs) have achieved remarkable success in various domains. However, when handling long-form text modification tasks, they still face two major problems: (1) producing undesired modifications by inappropriately altering or summarizing irrelevant content, and (2) missing necessary modifications to implicitly related passages that are crucial for maintaining document coherence. To address these issues, we propose HiCaM, a Hierarchical-Causal Modification framework that operates through a hierarchical summary tree and a causal graph. Furthermore, to evaluate HiCaM, we derive a multi-domain dataset from various benchmarks, providing a resource for assessing its effectiveness. Comprehensive evaluations on the dataset demonstrate significant improvements over strong LLMs, with our method achieving up to a 79.50\% win rate. These results highlight the comprehensiveness of our approach, showing consistent performance improvements across multiple models and domains.",,"Yuntao Shi, Yi Luo, Yeyun Gong, Chen Lin",2025-05-30T08:02:48Z,HiCaM: A Hierarchical-Causal Modification Framework for Long-Form Text   Modification,HiCaM: Ein Hierarchisch-Kausal-Änderungsrahmen für langformige Text-Änderungen,HICAM: 长期文本修改的等级-横向修改框架,http://arxiv.org/abs/2505.24319v1
1462,"Knowledge Graph Completion (KGC), which aims to infer missing or incomplete facts, is a crucial task for KGs. However, integrating the vital structural information of KGs into Large Language Models (LLMs) and outputting predictions deterministically remains challenging. To address this, we propose a new method called GLTW, which encodes the structural information of KGs and merges it with LLMs to enhance KGC performance. Specifically, we introduce an improved Graph Transformer (iGT) that effectively encodes subgraphs with both local and global structural information and inherits the characteristics of language model, bypassing training from scratch. Also, we develop a subgraph-based multi-classification training objective, using all entities within KG as classification objects, to boost learning efficiency.Importantly, we combine iGT with an LLM that takes KG language prompts as input.Our extensive experiments on various KG datasets show that GLTW achieves significant performance gains compared to SOTA baselines.",,"Kangyang Luo, Yuzhuo Bai, Cheng Gao, Shuzheng Si, Yingli Shen, Zhu Liu, Zhitong Wang, Cunliang Kong, Wenhao Li, Yufei Huang, Ye Tian, Xuantang Xiong, Lei Han, Maosong Sun",2025-05-30T07:59:38Z,GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language   for Knowledge Graph Completion,GLTW: Gemeinsamer verbesserter Graph Transformer und LLM über Drei-Word-Sprache für Wissensgraphenvervollständigung,GLTW:通过三种语文知识图完成的改进图变器和LLM联合改进图变器和LLM,http://arxiv.org/abs/2502.11471v4
1463,"Relying on human experts to evaluate CEFR speaking assessments in an e-learning environment creates scalability challenges, as it limits how quickly and widely assessments can be conducted. We aim to automate the evaluation of CEFR B2 English speaking assessments in e-learning environments from conversation transcripts. First, we evaluate the capability of leading open source and commercial Large Language Models (LLMs) to score a candidate's performance across various criteria in the CEFR B2 speaking exam in both global and India-specific contexts. Next, we create a new expert-validated, CEFR-aligned synthetic conversational dataset with transcripts that are rated at different assessment scores. In addition, new instruction-tuned datasets are developed from the English Vocabulary Profile (up to CEFR B2 level) and the CEFR-SP WikiAuto datasets. Finally, using these new datasets, we perform parameter efficient instruction tuning of Mistral Instruct 7B v0.2 to develop a family of models called EvalYaks. Four models in this family are for assessing the four sections of the CEFR B2 speaking exam, one for identifying the CEFR level of vocabulary and generating level-specific vocabulary, and another for detecting the CEFR level of text and generating level-specific text. EvalYaks achieved an average acceptable accuracy of 96%, a degree of variation of 0.35 levels, and performed 3 times better than the next best model. This demonstrates that a 7B parameter LLM instruction tuned with high-quality CEFR-aligned assessment data can effectively evaluate and score CEFR B2 English speaking assessments, offering a promising solution for scalable, automated language proficiency evaluation.",,"Nicy Scaria, Silvester John Joseph Kennedy, Thomas Latinovich, Deepak Subramani",2025-05-30T07:41:36Z,EvalYaks: Instruction Tuning Datasets and LoRA Fine-tuned Models for   Automated Scoring of CEFR B2 Speaking Assessment Transcripts,EvalYaks: Instruction Tuning Datasets und LoRA Feinabstimmungsmodelle für die automatisierte Bewertung von CEFR B2 Speaking Assessment Transcripts,EvalYaks:CEFR B2语声评估脚本自动扫描指示图示数据集和LORA精调模型,http://arxiv.org/abs/2408.12226v2
1464,"Advancements in Large Language Models (LLMs) drive interest in scientific applications, necessitating specialized benchmarks such as Earth science. Existing benchmarks either present a general science focus devoid of Earth science specificity or cover isolated subdomains, lacking holistic evaluation. Furthermore, current benchmarks typically neglect the assessment of LLMs' capabilities in open-ended scientific exploration. In this paper, we present a comprehensive and professional benchmark for the Earth sciences, designed to evaluate the capabilities of LLMs in scientific exploration within this domain, spanning from fundamental to advanced levels. Leveraging a corpus of 100,000 research papers, we first construct two Question Answering (QA) datasets: Earth-Iron, which offers extensive question coverage for broad assessment, and Earth-Silver, which features a higher level of difficulty to evaluate professional depth. These datasets encompass five Earth spheres, 114 disciplines, and 11 task categories, assessing foundational knowledge crucial for scientific exploration. Most notably, we introduce Earth-Gold with new metrics, a dataset comprising open-ended multi-turn dialogues specifically designed to evaluate the advanced capabilities of LLMs in scientific exploration, including methodology induction, limitation analysis, and concept proposal. Extensive experiments reveal limitations in 11 leading LLMs across different domains and tasks, highlighting considerable room for improvement in their scientific exploration capabilities. The benchmark is available on https://huggingface.co/ai-earth .",,"Wanghan Xu, Xiangyu Zhao, Yuhao Zhou, Xiaoyu Yue, Ben Fei, Fenghua Ling, Wenlong Zhang, Lei Bai",2025-05-30T07:31:07Z,EarthSE: A Benchmark for Evaluating Earth Scientific Exploration   Capability of LLMs,EarthSE: Ein Benchmark für die Bewertung der wissenschaftlichen Explorationsfähigkeit von LLMs,EarthSE:评估LLMs在地球科学探索能力的基准,http://arxiv.org/abs/2505.17139v3
1465,"Utilizing Graphic User Interface (GUI) for human-computer interaction is essential for accessing a wide range of digital tools. Recent advancements in Vision Language Models (VLMs) highlight the compelling potential to develop versatile agents to help humans finish GUI navigation tasks. However, current VLMs are challenged in terms of fundamental abilities (OCR and grounding) and GUI knowledge (the functions and control methods of GUI elements), preventing them from becoming practical GUI agents. To solve these challenges, we contribute GUICourse, a suite of datasets to train visual-based GUI agents from general VLMs. First, we introduce the GUIEnv dataset to strengthen the OCR and grounding capabilities of VLMs. Then, we introduce the GUIAct and GUIChat datasets to enrich their knowledge of GUI components and interactions. Experiments demonstrate that our GUI agents have better performance on common GUI tasks than their baseline VLMs. Even the small-size GUI agent (with 3.1B parameters) can still work well on single-step and multi-step GUI tasks. Finally, we analyze the different varieties in the training stage of this agent by ablation study. Our source codes and datasets are released at https://github.com/yiye3/GUICourse.",,"Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, Yuan Yao, Yankai Lin, Zhiyuan Liu, Maosong Sun",2025-05-30T07:30:07Z,GUICourse: From General Vision Language Models to Versatile GUI Agents,GUICourse: Von allgemeinen Vision-Sprachenmodellen zu vielseitigen GUI-Agenten,指南:从一般愿景语言模式到通用指南代表,http://arxiv.org/abs/2406.11317v2
1466,"Large language models (LLMs) enabled dialogue systems have become one of the central modes in human-machine interaction, which bring about vast amounts of conversation logs and increasing demand for dialogue generation. The dialogue's life-cycle spans from $\textit{Prelude}$ through $\textit{Interlocution}$ to $\textit{Epilogue}$, encompassing rich dialogue elements. Despite large volumes of dialogue-related studies, there is a lack of systematic investigation into the dialogue stages to frame benchmark construction that covers comprehensive dialogue elements. This hinders the precise modeling, generation and assessment of LLMs-based dialogue systems. To bridge this gap, in this paper, we introduce a new research task--$\textbf{D}$ialogue $\textbf{E}$lement $\textbf{MO}$deling, including $\textit{Element Awareness}$ and $\textit{Dialogue Agent Interaction}$, and propose a novel benchmark, $\textbf{DEMO}$, designed for a comprehensive dialogue modeling and assessment. On this basis, we further build the DEMO agent with the adept ability to model dialogue elements via imitation learning. Extensive experiments on DEMO indicate that current representative LLMs still have considerable potential for enhancement, and our DEMO agent performs well in both dialogue element modeling and out-of-domain tasks.",,"Minzheng Wang, Xinghua Zhang, Kun Chen, Nan Xu, Haiyang Yu, Fei Huang, Wenji Mao, Yongbin Li",2025-05-30T07:29:20Z,DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling,DEMO: Widerspenstige Dialog-Interaktion mit feinkörniger Element-Modellierung,DEMO: 重建对话与精精美成形要素建模的互动关系,http://arxiv.org/abs/2412.04905v4
1467,"Large Language Models (LLMs) are increasingly used to support scientific research, but their knowledge of scientific advancements can quickly become outdated. We introduce ScienceMeter, a new framework for evaluating scientific knowledge update methods over scientific knowledge spanning the past, present, and future. ScienceMeter defines three metrics: knowledge preservation, the extent to which models' understanding of previously learned papers are preserved; knowledge acquisition, how well scientific claims from newly introduced papers are acquired; and knowledge projection, the ability of the updated model to anticipate or generalize to related scientific claims that may emerge in the future. Using ScienceMeter, we examine the scientific knowledge of LLMs on claim judgment and generation tasks across a curated dataset of 15,444 scientific papers and 30,888 scientific claims from ten domains including medicine, biology, materials science, and computer science. We evaluate five representative knowledge update approaches including training- and inference-time methods. With extensive experiments, we find that the best-performing knowledge update methods can preserve only 85.9% of existing knowledge, acquire 71.7% of new knowledge, and project 37.7% of future knowledge. Inference-based methods work for larger models, whereas smaller models require training to achieve comparable performance. Cross-domain analysis reveals that performance on these objectives is correlated. Even when applying on specialized scientific LLMs, existing knowledge update methods fail to achieve these objectives collectively, underscoring that developing robust scientific knowledge update mechanisms is both crucial and challenging.",,"Yike Wang, Shangbin Feng, Yulia Tsvetkov, Hannaneh Hajishirzi",2025-05-30T07:28:20Z,ScienceMeter: Tracking Scientific Knowledge Updates in Language Models,ScienceMeter: Nachvollziehen wissenschaftlicher Wissensaktualisierungen in Sprachmodellen,ScienceMeter: 语言模式科学知识最新跟踪,http://arxiv.org/abs/2505.24302v1
1468,"Hallucination issues continue to affect multimodal large language models (MLLMs), with existing research mainly addressing object-level or attribute-level hallucinations, neglecting the more complex relation hallucinations that require advanced reasoning. Current benchmarks for relation hallucinations lack detailed evaluation and effective mitigation, and their datasets often suffer from biases due to systematic annotation processes. To address these challenges, we introduce Reefknot, a comprehensive benchmark targeting relation hallucinations, comprising over 20,000 real-world samples. We provide a systematic definition of relation hallucinations, integrating perceptive and cognitive perspectives, and construct a relation-based corpus using the Visual Genome scene graph dataset. Our comparative evaluation reveals significant limitations in current MLLMs' ability to handle relation hallucinations. Additionally, we propose a novel confidence-based mitigation strategy, which reduces the hallucination rate by an average of 9.75% across three datasets, including Reefknot. Our work offers valuable insights for achieving trustworthy multimodal intelligence.",,"Kening Zheng, Junkai Chen, Yibo Yan, Xin Zou, Xuming Hu",2025-05-30T07:27:55Z,"Reefknot: A Comprehensive Benchmark for Relation Hallucination   Evaluation, Analysis and Mitigation in Multimodal Large Language Models","Reefknot: Ein umfassender Benchmark für die Beziehung Halluzination Evaluation, Analyse und Mitigation in multimodalen großen Sprachmodellen",Reefeknot:多模式大语言模型中有关幻觉评价、分析和减轻影响的综合基准,http://arxiv.org/abs/2408.09429v3
1469,"Automating scientific research is considered the final frontier of science. Recently, several papers claim autonomous research agents can generate novel research ideas. Amidst the prevailing optimism, we document a critical concern: a considerable fraction of such research documents are smartly plagiarized. Unlike past efforts where experts evaluate the novelty and feasibility of research ideas, we request $13$ experts to operate under a different situational logic: to identify similarities between LLM-generated research documents and existing work. Concerningly, the experts identify $24\%$ of the $50$ evaluated research documents to be either paraphrased (with one-to-one methodological mapping), or significantly borrowed from existing work. These reported instances are cross-verified by authors of the source papers. Experts find an additional $32\%$ ideas to partially overlap with prior work, and a small fraction to be completely original. Problematically, these LLM-generated research documents do not acknowledge original sources, and bypass inbuilt plagiarism detectors. Lastly, through controlled experiments we show that automated plagiarism detectors are inadequate at catching plagiarized ideas from such systems. We recommend a careful assessment of LLM-generated research, and discuss the implications of our findings on academic publishing.",,"Tarun Gupta, Danish Pruthi",2025-05-30T07:17:39Z,All That Glitters is Not Novel: Plagiarism in AI Generated Research,"Alles, was Glitters ist nicht neu: Plagiat in AI Generated Research",所有这些闪光并非新奇:在AI创创研究中具有的虚幻形象。,http://arxiv.org/abs/2502.16487v2
1470,"Human-AI conversation frequently relies on quoting earlier text-""check it with the formula I just highlighted""-yet today's large language models (LLMs) lack an explicit mechanism for locating and exploiting such spans. We formalise the challenge as span-conditioned generation, decomposing each turn into the dialogue history, a set of token-offset quotation spans, and an intent utterance. Building on this abstraction, we introduce a quotation-centric data pipeline that automatically synthesises task-specific dialogues, verifies answer correctness through multi-stage consistency checks, and yields both a heterogeneous training corpus and the first benchmark covering five representative scenarios. To meet the benchmark's zero-overhead and parameter-efficiency requirements, we propose QuAda, a lightweight training-based method that attaches two bottleneck projections to every attention head, dynamically amplifying or suppressing attention to quoted spans at inference time while leaving the prompt unchanged and updating < 2.8% of backbone weights. Experiments across models show that QuAda is suitable for all scenarios and generalises to unseen topics, offering an effective, plug-and-play solution for quotation-aware dialogue.",,"Yueqi Zhang, Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li",2025-05-30T07:06:11Z,Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via   Plug-and-Play Modules,Zitat beachten: Quotation-Aware Dialog in LLMs über Plug-and-Play-Module aktivieren,Mind the Quote: 通过插件和插件模块在LLMs中启动引号- 孔径对话,http://arxiv.org/abs/2505.24292v1
1471,"We propose TETRIS, a novel method that optimizes the total throughput of batch speculative decoding in multi-request settings. Unlike existing methods that optimize for a single request or a group of requests as a whole, TETRIS actively selects the most promising draft tokens (for every request in a batch) to be accepted when verified in parallel, resulting in fewer rejected tokens and hence less wasted computing resources. Such an effective resource utilization to achieve fast inference in large language models (LLMs) is especially important to service providers with limited inference capacity. Compared to baseline speculative decoding, TETRIS yields a consistently higher acceptance rate and more effective utilization of the limited inference capacity. We show theoretically and empirically that TETRIS outperforms baseline speculative decoding and existing methods that dynamically select draft tokens, leading to a more efficient batch inference in LLMs.",,"Zhaoxuan Wu, Zijian Zhou, Arun Verma, Alok Prakash, Daniela Rus, Bryan Kian Hsiang Low",2025-05-30T06:40:23Z,TETRIS: Optimal Draft Token Selection for Batch Speculative Decoding,TETRIS: Optimale Entwurfs-Tokenauswahl für Batch-Spekulative Dekodierung,TETRIS: 批量投机性代号最佳选择标本草稿,http://arxiv.org/abs/2502.15197v2
1472,"Natural language explanations play a fundamental role in Natural Language Inference (NLI) by revealing how premises logically entail hypotheses. Recent work has shown that the interaction of large language models (LLMs) with theorem provers (TPs) can help verify and improve the validity of NLI explanations. However, TPs require translating natural language into machine-verifiable formal representations, a process that introduces the risk of semantic information loss and unfaithful interpretation, an issue compounded by LLMs' challenges in capturing critical logical structures with sufficient precision. Moreover, LLMs are still limited in their capacity for rigorous and robust proof construction within formal verification frameworks. To mitigate issues related to faithfulness and robustness, this paper investigates strategies to (1) alleviate semantic loss during autoformalisation, (2) efficiently identify and correct syntactic errors in logical representations, (3) explicitly use logical expressions to guide LLMs in generating structured proof sketches, and (4) increase LLMs' capacity of interpreting TP's feedback for iterative refinement. Our empirical results on e-SNLI, QASC and WorldTree using different LLMs demonstrate that the proposed strategies yield significant improvements in autoformalisation (+18.46%, +34.2%, +39.77%) and explanation refinement (+29.5%, +51.5%, +41.25%) over the state-of-the-art model. Moreover, we show that specific interventions on the hybrid LLM-TP architecture can substantially improve efficiency, drastically reducing the number of iterations required for successful verification.",,"Xin Quan, Marco Valentino, Louise A. Dennis, André Freitas",2025-05-30T06:38:39Z,Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations,Treues und robustes LLM-getriebenes Theorem für NLI-Erklärungen,忠实和坚固的LLM-Driven理论为NLI解释证明,http://arxiv.org/abs/2505.24264v1
1473,"The performance of large language models (LLMs) continues to improve, as reflected in rising scores on standard benchmarks. However, the lack of transparency around training data raises concerns about potential overlap with evaluation sets and the fairness of reported results. Although prior work has proposed methods for detecting data leakage, these approaches primarily focus on identifying outliers and have not been evaluated under controlled simulated leakage conditions. In this work, we compare existing leakage detection techniques, namely permutation and n-gram-based methods, under a continual pretraining setup that simulates real-world leakage scenarios, and additionally explore a lightweight method we call semi-half question. Although semi-half offers a low-cost alternative, our analysis shows that the n-gram method consistently achieves the highest F1-score. We also refine these techniques to support instance-level detection and reduce computational overhead. Leveraging the best-performing method, we create cleaned versions of MMLU and HellaSwag, and re-evaluate several LLMs. Our findings present a practical path toward more reliable and transparent evaluations, and we recommend contamination checks as a standard step before releasing benchmark results.",,"Naila Shafirni Hidayat, Muhammad Dehan Al Kautsar, Alfan Farizki Wicaksono, Fajri Koto",2025-05-30T06:37:39Z,Simulating Training Data Leakage in Multiple-Choice Benchmarks for LLM   Evaluation,Simulieren von Trainingsdaten-Leakage in Mehrzweck-Benchmarks für LLM-Evaluierung,为LLM评价模拟多窗口基准中的培训数据漏漏,http://arxiv.org/abs/2505.24263v1
1474,"Aligning general-purpose large language models (LLMs) to downstream tasks often incurs significant training adjustment costs. Prior research has explored various avenues to enhance alignment efficiency, primarily through minimal-data training or data-driven activations to identify key attention heads. However, these approaches inherently introduce data dependency, which hinders generalization and reusability. To address this issue and enhance model alignment efficiency, we propose the \textit{\textbf{A}ttention \textbf{L}ocalization and \textbf{P}runing \textbf{S}trategy (\textbf{ALPS})}, an efficient algorithm that localizes the most task-sensitive attention heads and prunes by restricting attention training updates to these heads, thereby reducing alignment costs. Experimental results demonstrate that our method activates only \textbf{10\%} of attention parameters during fine-tuning while achieving a \textbf{2\%} performance improvement over baselines on three tasks. Moreover, the identified task-specific heads are transferable across datasets and mitigate knowledge forgetting. Our work and findings provide a novel perspective on efficient LLM alignment. The code is available at https://github.com/VoiceBeer/ALPS.",,"Hao Chen, Haoze Li, Zhiqing Xiao, Lirong Gao, Qi Zhang, Xiaomeng Hu, Ningtao Wang, Xing Fu, Junbo Zhao",2025-05-30T06:30:20Z,ALPS: Attention Localization and Pruning Strategy for Efficient   Alignment of Large Language Models,ALPS: Aufmerksamkeit Lokalisierung und Pruning-Strategie zur effizienten Ausrichtung großer Sprachmodelle,ALPS: 高效统一大语言模式的注意地方化和审慎战略,http://arxiv.org/abs/2505.18799v3
1475,"Scaling up executable code data is significant for improving language models' software engineering capability. The intricate nature of the process makes it labor-intensive, time-consuming and expert-knowledge-dependent to build a large number of executable code repositories, limiting the scalability of existing work based on running tests. The primary bottleneck lies in the automated building of test environments for different repositories, which is an essential yet underexplored task. To mitigate the gap, we introduce Repo2Run, the first LLM-based agent aiming at automating the building of executable test environments for any repositories at scale. Specifically, given a code repository, Repo2Run iteratively builds the Docker image, runs unit tests based on the feedback of the building, and synthesizes the Dockerfile until the entire pipeline is executed successfully. The resulting Dockerfile can then be used to create Docker container environments for running code and tests. We created a benchmark containing 420 Python repositories with unit tests for evaluation. The results illustrate that Repo2Run achieves an 86.0% success rate, outperforming SWE-agent by 77.0%. The resources of Repo2Run are available at https://github.com/bytedance/Repo2Run.",,"Ruida Hu, Chao Peng, Xinchen Wang, Junjielong Xu, Cuiyun Gao",2025-05-30T06:25:20Z,Repo2Run: Automated Building Executable Environment for Code Repository   at Scale,Repo2Run: Automatisiertes Gebäude ausführbare Umgebung für Code-Repository auf Scale,Repo2Run: 用于标准代码仓库的自动建设可执行环境,http://arxiv.org/abs/2502.13681v3
1476,"Large Language Models (LLMs) have shown potential in simulating human behaviors and performing theory-of-mind (ToM) reasoning, a crucial skill for complex social interactions. In this study, we investigate the role of ToM reasoning in aligning agentic behaviors with human norms in negotiation tasks, using the ultimatum game as a controlled environment. We initialized LLM agents with different prosocial beliefs (including Greedy, Fair, and Selfless) and reasoning methods like chain-of-thought (CoT) and varying ToM levels, and examined their decision-making processes across diverse LLMs, including reasoning models like o3-mini and DeepSeek-R1 Distilled Qwen 32B. Results from 2,700 simulations indicated that ToM reasoning enhances behavior alignment, decision-making consistency, and negotiation outcomes. Consistent with previous findings, reasoning models exhibit limited capability compared to models with ToM reasoning, different roles of the game benefits with different orders of ToM reasoning. Our findings contribute to the understanding of ToM's role in enhancing human-AI interaction and cooperative decision-making. The code used for our experiments can be found at https://github.com/Stealth-py/UltimatumToM.",,"Neemesh Yadav, Palakorn Achananuparp, Jing Jiang, Ee-Peng Lim",2025-05-30T06:23:52Z,Effects of Theory of Mind and Prosocial Beliefs on Steering   Human-Aligned Behaviors of LLMs in Ultimatum Games,Auswirkungen der Theorie des Geistes und prosozialer Glaube auf die Steuerung von Menschen ausgerichteten Verhaltens von LLMs in Ultimatum Games,思想和主张社会信仰的理论对最后通牒运动会中LLMLM女士的人类-容忍行为指导者的影响,http://arxiv.org/abs/2505.24255v1
1477,"In recent years, large language models (LLMs) have made remarkable advancements, yet hallucination, where models produce inaccurate or non-factual statements, remains a significant challenge for real-world deployment. Although current classification-based methods, such as SAPLMA, are highly efficient in mitigating hallucinations, they struggle when non-factual information arises in the early or mid-sequence of outputs, reducing their reliability. To address these issues, we propose Hallucination Detection-Neural Differential Equations (HD-NDEs), a novel method that systematically assesses the truthfulness of statements by capturing the full dynamics of LLMs within their latent space. Our approaches apply neural differential equations (Neural DEs) to model the dynamic system in the latent space of LLMs. Then, the sequence in the latent space is mapped to the classification space for truth assessment. The extensive experiments across five datasets and six widely used LLMs demonstrate the effectiveness of HD-NDEs, especially, achieving over 14% improvement in AUC-ROC on the True-False dataset compared to state-of-the-art techniques.",,"Qing Li, Jiahui Geng, Zongxiong Chen, Derui Zhu, Yuxia Wang, Congbo Ma, Chenyang Lyu, Fakhri Karray",2025-05-30T06:19:49Z,HD-NDEs: Neural Differential Equations for Hallucination Detection in   LLMs,HD-NDES: Neurale Differentialgleichungen zur Halluzinationserkennung in LLMs,HD-NDEs: 用于测空的测空晶体的神经差异等量,http://arxiv.org/abs/2506.00088v1
1478,"This work investigates the ability of open Large Language Models (LLMs) to predict citation intent through in-context learning and fine-tuning. Unlike traditional approaches relying on domain-specific pre-trained models like SciBERT, we demonstrate that general-purpose LLMs can be adapted to this task with minimal task-specific data. We evaluate twelve model variations across five prominent open LLM families using zero-, one-, few-, and many-shot prompting. Our experimental study identifies the top-performing model and prompting parameters through extensive in-context learning experiments. We then demonstrate the significant impact of task-specific adaptation by fine-tuning this model, achieving a relative F1-score improvement of 8% on the SciCite dataset and 4.3% on the ACL-ARC dataset compared to the instruction-tuned baseline. These findings provide valuable insights for model selection and prompt engineering. Additionally, we make our end-to-end evaluation framework and models openly available for future use.",,"Paris Koloveas, Serafeim Chatzopoulos, Thanasis Vergoulis, Christos Tryfonopoulos",2025-05-30T06:17:22Z,Can LLMs Predict Citation Intent? An Experimental Analysis of In-context   Learning and Fine-tuning on Open LLMs,Kann LLMs Citation Intent voraussagen? Eine experimentelle Analyse des In-Context-Lernens und Feinabstimmungens auf offenen LLMs,LLMs 预测引文意图:对开放式LMs的内文学习和微调的实验分析,http://arxiv.org/abs/2502.14561v2
1479,"The evolution of Large Language Models (LLMs) has significantly advanced multi-turn conversation systems, emphasizing the need for proactive guidance to enhance users' interactions. However, these systems face challenges in dynamically adapting to shifts in users' goals and maintaining low latency for real-time interactions. In the Baidu Search AI assistant, an industrial-scale multi-turn search system, we propose a novel two-phase framework to provide proactive guidance. The first phase, Goal-adaptive Supervised Fine-Tuning (G-SFT), employs a goal adaptation agent that dynamically adapts to user goal shifts and provides goal-relevant contextual information. G-SFT also incorporates scalable knowledge transfer to distill insights from LLMs into a lightweight model for real-time interaction. The second phase, Click-oriented Reinforcement Learning (C-RL), adopts a generate-rank paradigm, systematically constructs preference pairs from user click signals, and proactively improves click-through rates through more engaging guidance. This dual-phase architecture achieves complementary objectives: G-SFT ensures accurate goal tracking, while C-RL optimizes interaction quality through click signal-driven reinforcement learning. Extensive experiments demonstrate that our framework achieves 86.10% accuracy in offline evaluation (+23.95% over baseline) and 25.28% CTR in online deployment (149.06% relative improvement), while reducing inference latency by 69.55% through scalable knowledge distillation.",,"Xiaoyu Li, Xiao Li, Li Gao, Yiding Liu, Xiaoyang Wang, Shuaiqiang Wang, Junfeng Wang, Dawei Yin",2025-05-30T06:16:30Z,Proactive Guidance of Multi-Turn Conversation in Industrial Search,Proaktive Führung des Multi-Turn-Gesprächs in der industriellen Suche,工业搜索多轮对话前瞻性指导,http://arxiv.org/abs/2505.24251v1
1480,"This paper investigates the flow of factual information in Mamba State-Space Model (SSM)-based language models. We rely on theoretical and empirical connections to Transformer-based architectures and their attention mechanisms. Exploiting this relationship, we adapt attentional interpretability techniques originally developed for Transformers--specifically, the Attention Knockout methodology--to both Mamba-1 and Mamba-2. Using them we trace how information is transmitted and localized across tokens and layers, revealing patterns of subject-token information emergence and layer-wise dynamics. Notably, some phenomena vary between mamba models and Transformer based models, while others appear universally across all models inspected--hinting that these may be inherent to LLMs in general. By further leveraging Mamba's structured factorization, we disentangle how distinct ""features"" either enable token-to-token information exchange or enrich individual tokens, thus offering a unified lens to understand Mamba internal operations.",,"Nir Endy, Idan Daniel Grosbard, Yuval Ran-Milo, Yonatan Slutzky, Itay Tshuva, Raja Giryes",2025-05-30T06:08:36Z,Mamba Knockout for Unraveling Factual Information Flow,Mamba Knockout für die Enthüllung des tatsächlichen Informationsflusses,异常事实信息流动 Mamba 击倒 Mamba 事实信息流,http://arxiv.org/abs/2505.24244v1
1481,"Although scaling up the number of trainable parameters in both pre-training and fine-tuning can effectively improve the performance of large language models, it also leads to increased computational overhead. When delving into the parameter difference, we find that a subset of parameters, termed advantageous parameters, plays a crucial role in determining model performance. Further analysis reveals that stronger models tend to possess more such parameters. In this paper, we propose Advantageous Parameter EXpansion Training (APEX), a method that progressively expands advantageous parameters into the space of disadvantageous ones, thereby increasing their proportion and enhancing training effectiveness. Further theoretical analysis from the perspective of matrix effective rank explains the performance gains of APEX. Extensive experiments on both instruction tuning and continued pre-training demonstrate that, in instruction tuning, APEX outperforms full-parameter tuning while using only 52% of the trainable parameters. In continued pre-training, APEX achieves the same perplexity level as conventional training with just 33% of the training data, and yields significant improvements on downstream tasks.",,"Naibin Gu, Yilong Chen, Zhenyu Zhang, Peng Fu, Zheng Lin, Shuohuan Wang, Yu Sun, Hua Wu, Weiping Wang, Haifeng Wang",2025-05-30T06:06:23Z,Advantageous Parameter Expansion Training Makes Better Large Language   Models,Vorteilhaftes Parameter-Erweiterungstraining macht große Sprachmodelle besser,扩大培训使大语言模式更加完善,http://arxiv.org/abs/2505.24241v1
1482,"To create culturally inclusive vision-language models (VLMs), developing a benchmark that tests their ability to address culturally relevant questions is essential. Existing approaches typically rely on human annotators, making the process labor-intensive and creating a cognitive burden in generating diverse questions. To address this, we propose a semi-automated framework for constructing cultural VLM benchmarks, specifically targeting multiple-choice QA. This framework combines human-VLM collaboration, where VLMs generate questions based on guidelines, a small set of annotated examples, and relevant knowledge, followed by a verification process by native speakers. We demonstrate the effectiveness of this framework through the creation of \texttt{K-Viscuit}, a dataset focused on Korean culture. Our experiments on this dataset reveal that open-source models lag behind proprietary ones in understanding Korean culture, highlighting key areas for improvement. We also present a series of further analyses, including human evaluation, augmenting VLMs with external knowledge, and the evaluation beyond multiple-choice QA. Our dataset is available at https://huggingface.co/datasets/ddehun/k-viscuit.",,"ChaeHun Park, Yujin Baek, Jaeseok Kim, Yu-Jung Heo, Du-Seong Chang, Jaegul Choo",2025-05-30T05:58:03Z,Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark   with Human-VLM Collaboration,Bewertung visueller und kultureller Interpretation: Der K-Viscuit-Benchmark mit Mensch-VLM-Kollaboration,评价视觉和文化解释:与人-VLM合作的K-VV-VLM基准,http://arxiv.org/abs/2406.16469v3
1483,"While multi-agent LLM systems show strong capabilities in various domains, they are highly vulnerable to adversarial and low-performing agents. To resolve this issue, in this paper, we introduce a general and adversary-resistant multi-agent LLM framework based on credibility scoring. We model the collaborative query-answering process as an iterative game, where the agents communicate and contribute to a final system output. Our system associates a credibility score that is used when aggregating the team outputs. The credibility scores are learned gradually based on the past contributions of each agent in query answering. Our experiments across multiple tasks and settings demonstrate our system's effectiveness in mitigating adversarial influence and enhancing the resilience of multi-agent cooperation, even in the adversary-majority settings.",,"Sana Ebrahimi, Mohsen Dehghankar, Abolfazl Asudeh",2025-05-30T05:57:37Z,An Adversary-Resistant Multi-Agent LLM System via Credibility Scoring,Ein gegnerisch-beständiges Multi-Agent-LLM-System über die Bewertung der Glaubwürdigkeit,通过信用度测分系统建立逆向-相对性多业务性多业务性LLM系统,http://arxiv.org/abs/2505.24239v1
1484,"Recent studies on the safety alignment of large language models (LLMs) have revealed that existing approaches often operate superficially, leaving models vulnerable to various adversarial attacks. Despite their significance, these studies generally fail to offer actionable solutions beyond data augmentation for achieving more robust safety mechanisms. This paper identifies a fundamental cause of this superficiality: existing alignment approaches often presume that models can implicitly learn a safety-related reasoning task during the alignment process, enabling them to refuse harmful requests. However, the learned safety signals are often diluted by other competing objectives, leading models to struggle with drawing a firm safety-conscious decision boundary when confronted with adversarial attacks. Based on this observation, by explicitly introducing a safety-related binary classification task and integrating its signals with our attention and decoding strategies, we eliminate this ambiguity and allow models to respond more responsibly to malicious queries. We emphasize that, with less than 0.2x overhead cost, our approach enables LLMs to assess the safety of both the query and the previously generated tokens at each necessary generating step. Extensive experiments demonstrate that our method significantly improves the resilience of LLMs against various adversarial attacks, offering a promising pathway toward more robust generative AI systems.",,"Jianwei Li, Jung-Eun Kim",2025-05-30T05:54:56Z,Safety Alignment Can Be Not Superficial With Explicit Safety Signals,Sicherheitsausrichtung kann mit expliziten Sicherheitssignalen nicht übertreffend sein,安全对齐不能带有明确安全信号的超度,http://arxiv.org/abs/2505.17072v2
1485,"Code-switching (CS) is the alternating use of two or more languages within a conversation or utterance, often influenced by social context and speaker identity. This linguistic phenomenon poses challenges for Automatic Speech Recognition (ASR) systems, which are typically designed for a single language and struggle to handle multilingual inputs. The growing global demand for multilingual applications, including Code-Switching ASR (CSASR), Text-to-Speech (CSTTS), and Cross-Lingual Information Retrieval (CLIR), highlights the inadequacy of existing monolingual datasets.   Although some code-switching datasets exist, most are limited to bilingual mixing within homogeneous ethnic groups, leaving a critical need for a large-scale, diverse benchmark akin to ImageNet in computer vision.   To bridge this gap, we introduce \textbf{LinguaMaster}, a multi-agent collaboration framework specifically designed for efficient and scalable multilingual data synthesis. Leveraging this framework, we curate \textbf{SwitchLingua}, the first large-scale multilingual and multi-ethnic code-switching dataset, including: (1) 420K CS textual samples across 12 languages, and (2) over 80 hours of audio recordings from 174 speakers representing 18 countries/regions and 63 racial/ethnic backgrounds, based on the textual data. This dataset captures rich linguistic and cultural diversity, offering a foundational resource for advancing multilingual and multicultural research. Furthermore, to address the issue that existing ASR evaluation metrics lack sensitivity to code-switching scenarios, we propose the \textbf{Semantic-Aware Error Rate (SAER)}, a novel evaluation metric that incorporates semantic information, providing a more accurate and context-aware assessment of system performance.",,"Peng Xie, Xingyuan Liu, Tsz Wai Chan, Yequan Bie, Yangqiu Song, Yang Wang, Hao Chen, Kani Chen",2025-05-30T05:54:46Z,SwitchLingua: The First Large-Scale Multilingual and Multi-Ethnic   Code-Switching Dataset,SwitchLingua: Der erste großformatige multilinguale und multiethnische Code-Schaltdatensatz,SwitchLingua: 首个大型多语言和多种族多语言代码抽动数据集,http://arxiv.org/abs/2506.00087v1
1486,"Although Large Language Models (LLMs) can generate coherent text, they often struggle to recognise user intent behind queries. In contrast, Natural Language Understanding (NLU) models interpret the purpose and key information of user input for responsive interactions. Existing NLU models typically map utterances to a dual-level semantic frame, involving sentence-level intent (SI) and word-level slot (WS) labels. However, real-life conversations primarily consist of multi-turn dialogues, requiring the interpretation of complex and extended exchanges. Researchers encounter challenges in addressing all facets of multi-turn dialogue using a unified NLU model. This paper introduces MIDAS, a novel approach leveraging multi-level intent, domain, and slot knowledge distillation for multi-turn NLU. We construct distinct teachers for SI detection, WS filling, and conversation-level domain (CD) classification, each fine-tuned for specific knowledge. A multi-teacher loss is proposed to facilitate the integration of these teachers, guiding a student model in multi-turn dialogue tasks. Results demonstrate the efficacy of our model in improving multi-turn conversation understanding, showcasing the potential for advancements in NLU through multi-level dialogue knowledge distillation. Our implementation is open-sourced on https://github.com/adlnlp/Midas.",,"Yan Li, So-Eon Kim, Seong-Bae Park, Soyeon Caren Han",2025-05-30T05:51:10Z,"MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for   Multi-turn NLU","MIDAS: Multi-Level-Intent, Domain und Slot Knowledge Destillation für Multi-Turn NLU",MIDAS:多层次意图、域域和用于多方向国家实验室的空置知识蒸馏,http://arxiv.org/abs/2408.08144v3
1487,"Large foundation models (LFMs) are susceptible to two distinct vulnerabilities: hallucinations and jailbreak attacks. While typically studied in isolation, we observe that defenses targeting one often affect the other, hinting at a deeper connection.   We propose a unified theoretical framework that models jailbreaks as token-level optimization and hallucinations as attention-level optimization. Within this framework, we establish two key propositions: (1) \textit{Similar Loss Convergence} - the loss functions for both vulnerabilities converge similarly when optimizing for target-specific outputs; and (2) \textit{Gradient Consistency in Attention Redistribution} - both exhibit consistent gradient behavior driven by shared attention dynamics.   We validate these propositions empirically on LLaVA-1.5 and MiniGPT-4, showing consistent optimization trends and aligned gradients. Leveraging this connection, we demonstrate that mitigation techniques for hallucinations can reduce jailbreak success rates, and vice versa. Our findings reveal a shared failure mode in LFMs and suggest that robustness strategies should jointly address both vulnerabilities.",,"Haibo Jin, Peiyan Zhang, Peiran Wang, Man Luo, Haohan Wang",2025-05-30T05:48:50Z,From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large   Foundation Models,Von Halluzinationen zu Jailbreaks: Die Vulnerabilität großer Stiftungsmodelle neu denken,从幻觉到破狱:反思大型基金会模型的脆弱性,http://arxiv.org/abs/2505.24232v1
1488,"Transformer-based Large Language Models (LLMs) struggle with inputs exceeding their training context window due to positional out-of-distribution (O.O.D.) issues that disrupt attention. Existing solutions, including fine-tuning and training-free methods, face challenges like inefficiency, redundant interpolation, logit outliers, or loss of local positional information. We propose Greedy Attention Logit Interpolation (GALI), a training-free method that improves length extrapolation by greedily reusing pretrained positional intervals and interpolating attention logit to eliminate outliers. GALI achieves stable and superior performance across a wide range of long-context tasks without requiring input-length-specific tuning. Our analysis further reveals that LLMs interpret positional intervals unevenly and that restricting interpolation to narrower ranges improves performance, even on short-context tasks. GALI represents a step toward more robust and generalizable long-text processing in LLMs. Our implementation of GALI, along with the experiments from our paper, is open-sourced at https://github.com/adlnlp/Gali.",,"Yan Li, Tianyi Zhang, Zechuan Li, Soyeon Caren Han",2025-05-30T05:42:35Z,A Training-Free Length Extrapolation Approach for LLMs: Greedy Attention   Logit Interpolation (GALI),Ein Training-Free Länge Extrapolation Ansatz für LLMs: Gierige Aufmerksamkeit Logit Interpolation (GALI),对LLLM女士的无培训期限外推法:贪婪注意力登录国际刑警(GALI),http://arxiv.org/abs/2502.02659v2
1489,"Inverse Text Normalization (ITN) is crucial for converting spoken Automatic Speech Recognition (ASR) outputs into well-formatted written text, enhancing both readability and usability. Despite its importance, the integration of streaming ITN within streaming ASR remains largely unexplored due to challenges in accuracy, efficiency, and adaptability, particularly in low-resource and limited-context scenarios. In this paper, we introduce a streaming pretrained language model for ITN, leveraging pretrained linguistic representations for improved robustness. To address streaming constraints, we propose Dynamic Context-Aware during training and inference, enabling adaptive chunk size adjustments and the integration of right-context information. Experimental results demonstrate that our method achieves accuracy comparable to non-streaming ITN and surpasses existing streaming ITN models on a Vietnamese dataset, all while maintaining low latency, ensuring seamless integration into ASR systems.",,"Luong Ho, Khanh Le, Vinh Pham, Bao Nguyen, Tan Tran, Duc Chau",2025-05-30T05:41:03Z,Dynamic Context-Aware Streaming Pretrained Language Model For Inverse   Text Normalization,Dynamisches Kontext-Bewusst-Streaming vorgebildetes Sprachmodell für die Inverse Text-Normalisierung,反文字正常化的动态内容软件流预训练语言模型,http://arxiv.org/abs/2505.24229v1
1490,"Transformer-based models have achieved remarkable success in various Natural Language Processing (NLP) tasks, yet their ability to handle long documents is constrained by computational limitations. Traditional approaches, such as truncating inputs, sparse self-attention, and chunking, attempt to mitigate these issues, but they often lead to information loss and hinder the model's ability to capture long-range dependencies. In this paper, we introduce ChuLo, a novel chunk representation method for long document understanding that addresses these limitations. Our ChuLo groups input tokens using unsupervised keyphrase extraction, emphasizing semantically important keyphrase based chunks to retain core document content while reducing input length. This approach minimizes information loss and improves the efficiency of Transformer-based models. Preserving all tokens in long document understanding, especially token classification tasks, is important to ensure that fine-grained annotations, which depend on the entire sequence context, are not lost. We evaluate our method on multiple long document classification tasks and long document token classification tasks, demonstrating its effectiveness through comprehensive qualitative and quantitative analysis. Our implementation is open-sourced on https://github.com/adlnlp/Chulo.",,"Yan Li, Soyeon Caren Han, Yue Dai, Feiqi Cao",2025-05-30T05:34:49Z,ChuLo: Chunk-Level Key Information Representation for Long Document   Processing,ChuLo: Chunk-Level-Hauptinformationsdarstellung für die lange Dokumentenverarbeitung,Chu Loo: 长文件处理时的整排级密钥信息代表,http://arxiv.org/abs/2410.11119v4
1491,"Large Language Models (LLMs) have shown remarkable progress across domains, yet their ability to perform inductive reasoning - inferring latent rules from sparse examples - remains limited. It is often assumed that chain-of-thought (CoT) prompting, as used in Large Reasoning Models (LRMs), enhances such reasoning. We investigate this assumption with creating four controlled, diagnostic game-based tasks - chess, Texas Hold'em, dice games, and blackjack - with hidden human-defined rules. We find that CoT reasoning can degrade inductive performance, with LRMs often underperforming their non-reasoning counterparts.   To explain this, we present a theoretical framework that reveals how reasoning steps can amplify error through three failure modes: incorrect sub-task decomposition, incorrect sub-task solving, and incorrect final answer summarization. Based on our theoretical and empirical analysis, we introduce structured interventions that adapt CoT generation according to our identified failure types. These interventions improve inductive accuracy without retraining. Our findings suggest that effective (CoT) reasoning depends not only on taking more steps but also on ensuring those steps are well-structured.",,"Haibo Jin, Peiyan Zhang, Man Luo, Haohan Wang",2025-05-30T05:24:21Z,Reasoning Can Hurt the Inductive Abilities of Large Language Models,Vernunft kann die induktiven Fähigkeiten großer Sprachmodelle verletzen,大语言模型的感应理由会损害感应能力,http://arxiv.org/abs/2505.24225v1
1492,"Unsupervised keyphrase prediction has gained growing interest in recent years. However, existing methods typically rely on heuristically defined importance scores, which may lead to inaccurate informativeness estimation. In addition, they lack consideration for time efficiency. To solve these problems, we propose ERU-KG, an unsupervised keyphrase generation (UKG) model that consists of an informativeness and a phraseness module. The former estimates the relevance of keyphrase candidates, while the latter generate those candidates. The informativeness module innovates by learning to model informativeness through references (e.g., queries, citation contexts, and titles) and at the term-level, thereby 1) capturing how the key concepts of documents are perceived in different contexts and 2) estimating informativeness of phrases more efficiently by aggregating term informativeness, removing the need for explicit modeling of the candidates. ERU-KG demonstrates its effectiveness on keyphrase generation benchmarks by outperforming unsupervised baselines and achieving on average 89\% of the performance of a supervised model for top 10 predictions. Additionally, to highlight its practical utility, we evaluate the model on text retrieval tasks and show that keyphrases generated by ERU-KG are effective when employed as query and document expansions. Furthermore, inference speed tests reveal that ERU-KG is the fastest among baselines of similar model sizes. Finally, our proposed model can switch between keyphrase generation and extraction by adjusting hyperparameters, catering to diverse application requirements.",,"Lam Thanh Do, Aaditya Bodke, Pritom Saha Akash, Kevin Chen-Chuan Chang",2025-05-30T05:09:53Z,ERU-KG: Efficient Reference-aligned Unsupervised Keyphrase Generation,ERU-KG: Effiziente referenzorientierte unüberwachte Keyphrase-Generierung,ERU-KG: 高效的参考比对未受监督的关键词生成,http://arxiv.org/abs/2505.24219v1
1493,"As Large Language Models (LLMs) become increasingly capable at reasoning, the problem of ""faithfulness"" persists: LLM ""reasoning traces"" can contain errors and omissions that are difficult to detect, and may obscure biases in model outputs. To address these limitations, we introduce Semi-Structured Reasoning Models (SSRMs), which internalize a semi-structured Chain-of-Thought (CoT) reasoning format within the model. Our SSRMs generate reasoning traces in a Pythonic syntax. While SSRM traces are not executable, they adopt a restricted, task-specific vocabulary to name distinct reasoning steps, and to mark each step's inputs and outputs. Through extensive evaluation on ten benchmarks, SSRMs demonstrate strong performance and generality: they outperform comparably sized baselines by nearly ten percentage points on in-domain tasks while remaining competitive with specialized models on out-of-domain medical benchmarks. Furthermore, we show that semi-structured reasoning is more amenable to analysis: in particular, they can be automatically audited to identify reasoning flaws. We explore both hand-crafted structured audits, which detect task-specific problematic reasoning patterns, and learned typicality audits, which apply probabilistic models over reasoning patterns, and show that both audits can be used to effectively flag probable reasoning errors.",,"Jixuan Leng, Cassandra A. Cohen, Zhixian Zhang, Chenyan Xiong, William W. Cohen",2025-05-30T05:06:10Z,Semi-structured LLM Reasoners Can Be Rigorously Audited,Halbstrukturierte LLM-Reasoner können streng geprüft werden,可严格审计的半结构LLM 理由,http://arxiv.org/abs/2505.24217v1
1494,"Given the high computational cost of preference alignment training of large language models (LLMs), exploring efficient methods to reduce the training overhead remains an important and compelling research problem. Motivated by the observation that alignment training typically involves only small parameter changes without injecting new knowledge into models, we propose a straightforward method called ExPO (model extrapolation) to expedite LLMs' alignment with human preferences. Given a partially-trained model and its initial SFT checkpoint, ExPO improves the implicit optimization objective of alignment training by simply amplifying the parameter change based on a first-order approximation, without any additional training overhead. Through controlled experiments, we demonstrate that ExPO boosts a DPO model trained with only 20% steps to outperform the fully-trained one. Moreover, we show that ExPO notably improves existing open-source LLMs (ranging from 1.8B to 70B parameters) on the leading AlpacaEval 2.0 and MT-Bench benchmarks, which highlights ExPO's broader utility in efficiently enhancing LLM alignment.",,"Chujie Zheng, Ziqi Wang, Heng Ji, Minlie Huang, Nanyun Peng",2025-05-30T04:58:07Z,Model Extrapolation Expedites Alignment,Modell Extrapolation Expeditionen Ausrichtung,模型外推快速调整,http://arxiv.org/abs/2404.16792v5
1495,"Reinforcement Learning with Human Feedback (RLHF) and its variants have made huge strides toward the effective alignment of large language models (LLMs) to follow instructions and reflect human values. More recently, Direct Alignment Algorithms (DAAs) have emerged in which the reward modeling stage of RLHF is skipped by characterizing the reward directly as a function of the policy being learned. Some popular examples of DAAs include Direct Preference Optimization (DPO) and Simple Preference Optimization (SimPO). These methods often suffer from likelihood displacement, a phenomenon by which the probabilities of preferred responses are often reduced undesirably. In this paper, we argue that, for DAAs the reward (function) shape matters. We introduce \textbf{AlphaPO}, a new DAA method that leverages an $\alpha$-parameter to help change the shape of the reward function beyond the standard log reward. AlphaPO helps maintain fine-grained control over likelihood displacement and over-optimization. Compared to SimPO, one of the best performing DAAs, AlphaPO leads to about 7\% to 10\% relative improvement in alignment performance for the instruct versions of Mistral-7B and Llama3-8B while achieving 15\% to 50\% relative improvement over DPO on the same models. The analysis and results presented highlight the importance of the reward shape and how one can systematically change it to affect training dynamics, as well as improve alignment performance.",,"Aman Gupta, Shao Tang, Qingquan Song, Sirou Zhu, Jiwoo Hong, Ankan Saha, Viral Gupta, Noah Lee, Eunki Kim, Siyu Zhu, Parag Agrawal, Natesh Pillai, S. Sathiya Keerthi",2025-05-30T04:56:02Z,AlphaPO: Reward Shape Matters for LLM Alignment,AlphaPO: Reward Shape Matters für LLM Alignment,AlphapO:LLM对齐的奖励形状事项,http://arxiv.org/abs/2501.03884v4
1496,"Large Language Models (LLMs) encode behaviors such as refusal within their activation space, yet identifying these behaviors remains a significant challenge. Existing methods often rely on predefined refusal templates detectable in output tokens or require manual analysis. We introduce \textbf{COSMIC} (Cosine Similarity Metrics for Inversion of Concepts), an automated framework for direction selection that identifies viable steering directions and target layers using cosine similarity - entirely independent of model outputs. COSMIC achieves steering performance comparable to prior methods without requiring assumptions about a model's refusal behavior, such as the presence of specific refusal tokens. It reliably identifies refusal directions in adversarial settings and weakly aligned models, and is capable of steering such models toward safer behavior with minimal increase in false refusals, demonstrating robustness across a wide range of alignment conditions.",,"Vincent Siu, Nicholas Crispino, Zihao Yu, Sam Pan, Zhun Wang, Yang Liu, Dawn Song, Chenguang Wang",2025-05-30T04:54:18Z,COSMIC: Generalized Refusal Direction Identification in LLM Activations,COSMIC: Generalisierte Identifizierung der Verweigerungsrichtung in LLM-Aktivierungen,COSMIC: LLM 活动一般拒绝指示识别,http://arxiv.org/abs/2506.00085v1
1497,"Any-to-any generative models aim to enable seamless interpretation and generation across multiple modalities within a unified framework, yet their ability to preserve relationships across modalities remains uncertain. Do unified models truly achieve cross-modal coherence, or is this coherence merely perceived? To explore this, we introduce ACON, a dataset of 1,000 images (500 newly contributed) paired with captions, editing instructions, and Q&A pairs to evaluate cross-modal transfers rigorously. Using three consistency criteria-cyclic consistency, forward equivariance, and conjugated equivariance-our experiments reveal that any-to-any models do not consistently demonstrate greater cross-modal consistency than specialized models in pointwise evaluations such as cyclic consistency. However, equivariance evaluations uncover weak but observable consistency through structured analyses of the intermediate latent space enabled by multiple editing operations. We release our code and data at https://github.com/JiwanChung/ACON.",,"Jiwan Chung, Janghan Yoon, Junhyeong Park, Sangeyl Lee, Joowon Yang, Sooyeon Park, Youngjae Yu",2025-05-30T04:51:54Z,Are Any-to-Any Models More Consistent Across Modality Transfers Than   Specialists?,Sind Any-to-Any Modelle konsistenter Across Modalitätstransfers als Spezialisten?,各种模式的转让是否比专家更加一致?,http://arxiv.org/abs/2505.24211v1
1498,"Despite the success of distillation in large language models (LLMs), most prior work applies identical loss functions to both teacher- and student-generated data. These strategies overlook the synergy between loss formulations and data types, leading to a suboptimal performance boost in student models. To address this, we propose DistiLLM-2, a contrastive approach that simultaneously increases the likelihood of teacher responses and decreases that of student responses by harnessing this synergy. Our extensive experiments show that DistiLLM-2 not only builds high-performing student models across a wide range of tasks, including instruction-following and code generation, but also supports diverse applications, such as preference alignment and vision-language extensions. These findings highlight the potential of a contrastive approach to enhance the efficacy of LLM distillation by effectively aligning teacher and student models across varied data types.",,"Jongwoo Ko, Tianyi Chen, Sungnyun Kim, Tianyu Ding, Luming Liang, Ilya Zharkov, Se-Young Yun",2025-05-30T04:41:12Z,DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs,DistiLLM-2: Ein kontrastiver Ansatz steigert die Destillation von LLMs,distillLLM-2:一种反竞争做法促进LLMM的蒸馏,http://arxiv.org/abs/2503.07067v2
1499,"Automatic speech recognition (ASR) for dysarthric speech remains challenging due to data scarcity, particularly in non-English languages. To address this, we fine-tune a voice conversion model on English dysarthric speech (UASpeech) to encode both speaker characteristics and prosodic distortions, then apply it to convert healthy non-English speech (FLEURS) into non-English dysarthric-like speech. The generated data is then used to fine-tune a multilingual ASR model, Massively Multilingual Speech (MMS), for improved dysarthric speech recognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE (Tamil) demonstrates that VC with both speaker and prosody conversion significantly outperforms the off-the-shelf MMS performance and conventional augmentation techniques such as speed and tempo perturbation. Objective and subjective analyses of the generated data further confirm that the generated speech simulates dysarthric characteristics.",,"Chin-Jou Li, Eunjung Yeo, Kwanghee Choi, Paula Andrea Pérez-Toro, Masao Someki, Rohan Kumar Das, Zhengjun Yue, Juan Rafael Orozco-Arroyave, Elmar Nöth, David R. Mortensen",2025-05-30T04:39:27Z,Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric   Speech Recognition in Low-Resource Languages,Towards Inclusive ASR: Untersuchung der Sprachumwandlung für Dysarthric Speech Recognition in Low-Resource Sprachen,努力实现包容性的ASR:低资源语言中承认代谢语言语音转换调查,http://arxiv.org/abs/2505.14874v3
1500,"In-context learning (ICL) can significantly enhance the complex reasoning capabilities of large language models (LLMs), with the key lying in the selection and ordering of demonstration examples. Previous methods typically relied on simple features to measure the relevance between examples. We argue that these features are not sufficient to reflect the intrinsic connections between examples. In this study, we propose a curriculum ICL strategy guided by problem-solving logic. We select demonstration examples by analyzing the problem-solving logic and order them based on curriculum learning. Specifically, we constructed a problem-solving logic instruction set based on the BREAK dataset and fine-tuned a language model to analyze the problem-solving logic of examples. Subsequently, we selected appropriate demonstration examples based on problem-solving logic and assessed their difficulty according to the number of problem-solving steps. In accordance with the principles of curriculum learning, we ordered the examples from easy to hard to serve as contextual prompts. Experimental results on multiple benchmarks indicate that our method outperforms previous ICL approaches in terms of performance and efficiency, effectively enhancing the complex reasoning capabilities of LLMs. Our project will be released at https://github.com/maxuetao/CurriculumICL",,"Xuetao Ma, Wenbin Jiang, Hua Huang",2025-05-30T04:28:52Z,Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs   Complex Reasoning,Problemlösende Logik Geführtes Curriculum In-Context-Lernen für LLMs komplexe Begründung,为LLLMs 复杂原因的理论学习,http://arxiv.org/abs/2502.15401v2
1501,"In the instruction fine-tuning of large language models (LLMs), it is widely recognized that a few high-quality instructions are superior to a large number of low-quality instructions. At present, many instruction selection methods have been proposed, but most of these methods select instruction based on heuristic quality metrics, and only consider data selection before training. These designs lead to insufficient optimization of instruction fine-tuning, and fixed heuristic indicators are often difficult to optimize for specific tasks. Therefore, we design a dynamic, task-objective-driven instruction selection framework RAISE(Reinforced Adaptive Instruction SElection), which incorporates the entire instruction fine-tuning process into optimization, selecting instructions at each step based on the expected impact of each instruction on model performance improvement. Our approach is well interpretable and has strong task-specific optimization capabilities. By modeling dynamic instruction selection as a sequential decision-making process, we use RL to train our selection strategy. Extensive experiments and result analysis prove the superiority of our method compared with other instruction selection methods. Notably, RAISE achieves superior performance by updating only 1% of the training steps compared to full-data training, demonstrating its efficiency and effectiveness.",,"Lv Qingsong, Yangning Li, Zihua Lan, Zishan Xu, Jiwei Tang, Yinghui Li, Wenhao Jiang, Hai-Tao Zheng, Philip S. Yu",2025-05-30T04:23:57Z,RAISE: Reinforced Adaptive Instruction Selection For Large Language   Models,RAISE: Verstärkte adaptive Instruktionsauswahl für große Sprachmodelle,AISE: 强化大语言模式适应性教学选择,http://arxiv.org/abs/2504.07282v3
1502,"Multi-turn instruction following capability constitutes a core competency of large language models (LLMs) in real-world applications. Existing evaluation benchmarks predominantly focus on fine-grained constraint satisfaction and domain-specific capability assessment, yet overlook the crucial structural dependencies between dialogue turns that distinguish multi-turn from single-turn interactions. These structural dependencies not only reflect user intent but also establish an essential second dimension for the instruction following evaluation beyond constraint satisfaction. To address this gap, we propose StructFlowBench, a multi-turn instruction following benchmark with structural flow modeling. The benchmark defines an innovative structural flow framework with six fundamental inter-turn relationships. These relationships introduce novel structural constraints for model evaluation and also serve as generation parameters for creating customized dialogue flows tailored to specific scenarios. Adopting established LLM-based automatic evaluation methodologies, we conduct systematic evaluations of 13 leading open-source and closed-source LLMs. Experimental results reveal significant deficiencies in current models' comprehension of multi-turn dialogue structures. The code is available at https://github.com/MLGroupJLU/StructFlowBench.",,"Jinnan Li, Jinzhe Li, Yue Wang, Yi Chang, Yuan Wu",2025-05-30T04:20:31Z,StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction   Following,StructFlowBench: Ein strukturierter Flow Benchmark für Multiturn-Anleitung,StructFlowBenench: 多转指令的结构化流程基准,http://arxiv.org/abs/2502.14494v2
1503,"The quality of human preference data is crucial for training and evaluating large language models (LLMs), particularly in reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) scenarios. Traditional side-by-side (SBS) annotation approaches often struggle with inherent uncertainty, annotator disagreement, and the complexity of preference judgments. This paper introduces a novel framework based on intuitionistic fuzzy sets (IFS) for modeling and aggregating human preferences in LLM data annotation tasks. Our approach captures not only the degree of preference but also the uncertainty and hesitation inherent in human judgment through membership, non-membership, and hesitation degrees. We propose an IFS-based annotation protocol that enables more nuanced preference modeling, develops aggregation methods for handling annotator disagreement, and introduces quality metrics for preference data assessment. Experimental validation on multiple datasets demonstrates that our IFS-based approach significantly improves annotation consistency, reduces annotator fatigue, and produces higher-quality preference data compared to traditional binary and Likert-scale methods. The resulting preference datasets lead to improved model performance in downstream tasks, with 12.3\% improvement in win-rate against baseline models and 15.7\% reduction in annotation time. Our framework provides a principled approach to handling uncertainty in human preference annotation and offers practical benefits for large-scale LLM training.",,Yimin Du,2025-05-30T04:20:00Z,Intuitionistic Fuzzy Sets for Large Language Model Data Annotation: A   Novel Approach to Side-by-Side Preference Labeling,Intuitionistische Fuzzy-Sets für großsprachige Modelldaten-Annotation: Ein neuartiger Ansatz zur Side-by-Side-Präferenzbeschriftung,用于大语言模型数据说明的神学模糊数据集集:边对边贴贴标签的新颖办法,http://arxiv.org/abs/2505.24199v1
1504,"Speculative decoding (SD) is a promising method for accelerating the decoding process of Large Language Models (LLMs). The efficiency of SD primarily hinges on the consistency between the draft model and the verify model. However, existing drafting approaches typically require additional modules to be trained, which can be challenging to implement and ensure compatibility across various LLMs. In this paper, we propose CLaSp, an in-context layer-skipping strategy for self-speculative decoding. Unlike prior methods, CLaSp does not require additional drafting modules or extra training. Instead, it employs a plug-and-play mechanism by skipping intermediate layers of the verify model to construct a compressed draft model. Specifically, we develop a dynamic programming algorithm that optimizes the layer-skipping process by leveraging the complete hidden states from the last verification stage as an objective. This enables CLaSp to dynamically adjust its layer-skipping strategy after each verification stage, without relying on pre-optimized sets of skipped layers. Experimental results across diverse downstream tasks demonstrate that CLaSp achieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the original distribution of the generated text.",,"Longze Chen, Renke Shan, Huiming Wang, Lu Wang, Ziqiang Liu, Run Luo, Jiawei Wang, Hamid Alinejad-Rokny, Min Yang",2025-05-30T04:15:06Z,CLaSp: In-Context Layer Skip for Self-Speculative Decoding,CLaSp: In-Context Layer Skip for Self-Speculative Decodierung,CLASp: 用于自投机代号的在文本层跳过,http://arxiv.org/abs/2505.24196v1
1505,"Stance detection (SD) identifies the text position towards a target, typically labeled as favor, against, or none. We introduce Open-Target Stance Detection (OTSD), the most realistic task where targets are neither seen during training nor provided as input. We evaluate Large Language Models (LLMs) from GPT, Gemini, Llama, and Mistral families, comparing their performance to the only existing work, Target-Stance Extraction (TSE), which benefits from predefined targets. Unlike TSE, OTSD removes the dependency of a predefined list, making target generation and evaluation more challenging. We also provide a metric for evaluating target quality that correlates well with human judgment. Our experiments reveal that LLMs outperform TSE in target generation, both when the real target is explicitly and not explicitly mentioned in the text. Similarly, LLMs overall surpass TSE in stance detection for both explicit and non-explicit cases. However, LLMs struggle in both target generation and stance detection when the target is not explicit.",,"Abu Ubaida Akash, Ahmed Fahmy, Amine Trabelsi",2025-05-30T04:14:47Z,Can Large Language Models Address Open-Target Stance Detection?,Können große Sprachmodelle Open-Target Stance Detection ansprechen?,大语言模式 地址开放目标方位能否探测 ?,http://arxiv.org/abs/2409.00222v7
1506,"Retrieval-Augmented Generation (RAG) has been empirically shown to enhance the performance of large language models (LLMs) in knowledge-intensive domains such as healthcare, finance, and legal contexts. Given a query, RAG retrieves relevant documents from a corpus and integrates them into the LLMs' generation process. In this study, we investigate the adversarial robustness of RAG, focusing specifically on examining the retrieval system. First, across 225 different setup combinations of corpus, retriever, query, and targeted information, we show that retrieval systems are vulnerable to universal poisoning attacks in medical Q\&A. In such attacks, adversaries generate poisoned documents containing a broad spectrum of targeted information, such as personally identifiable information. When these poisoned documents are inserted into a corpus, they can be accurately retrieved by any users, as long as attacker-specified queries are used. To understand this vulnerability, we discovered that the deviation from the query's embedding to that of the poisoned document tends to follow a pattern in which the high similarity between the poisoned document and the query is retained, thereby enabling precise retrieval. Based on these findings, we develop a new detection-based defense to ensure the safe use of RAG. Through extensive experiments spanning various Q\&A domains, we observed that our proposed method consistently achieves excellent detection rates in nearly all cases.",,"Xun Xian, Ganghua Wang, Xuan Bi, Jayanth Srinivasa, Ashish Kundu, Charles Fleming, Mingyi Hong, Jie Ding",2025-05-30T04:01:35Z,On the Vulnerability of Applying Retrieval-Augmented Generation within   Knowledge-Intensive Application Domains,Zur Schwachstelle der Anwendung von Retrieval-Augmented Generation innerhalb wissensintensiver Anwendungsdomänen,关于在知识密集应用域内应用回收利用养代的脆弱性,http://arxiv.org/abs/2409.17275v2
1507,"Large Language Models (LLMs) such as GPT-4o can handle a wide range of complex tasks with the right prompt. As per token costs are reduced, the advantages of fine-tuning Small Language Models (SLMs) for real-world applications -- faster inference, lower costs -- may no longer be clear. In this work, we present evidence that, for domain-specific tasks that require structured outputs, SLMs still have a quality advantage. We compare fine-tuning an SLM against prompting LLMs on the task of generating low-code workflows in JSON form. We observe that while a good prompt can yield reasonable results, fine-tuning improves quality by 10% on average. We also perform systematic error analysis to reveal model limitations.",,"Orlando Marquez Ayala, Patrice Bechard, Emily Chen, Maggie Baird, Jingfei Chen",2025-05-30T03:59:35Z,Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code   Workflows,Fine-Tune ein SLM oder Prompt ein LLM? Der Fall der Erzeugung von Low-Code Workflows,微调可持续土地管理还是迅速提炼一个LLM? 产生低碳工作流程的案例,http://arxiv.org/abs/2505.24189v1
1508,"Accuracy remains a standard metric for evaluating AI systems, but it offers limited insight into how models arrive at their solutions. In this work, we introduce a benchmark based on brainteasers written in long narrative form to probe more deeply into the types of reasoning strategies that models use. Brainteasers are well-suited for this goal because they can be solved with multiple approaches, such as a few-step solution that uses a creative insight or a longer solution that uses more brute force. We investigate large language models (LLMs) across multiple layers of reasoning, focusing not only on correctness but also on the quality and creativity of their solutions. We investigate many aspects of the reasoning process: (1) semantic parsing of the brainteasers into precise mathematical competition style formats; (2) generating solutions from these mathematical forms; (3) self-correcting solutions based on gold solutions; (4) producing step-by-step sketches of solutions; and (5) making use of hints. We find that LLMs are in many cases able to find creative, insightful solutions to brainteasers, suggesting that they capture some of the capacities needed to solve novel problems in creative ways. Nonetheless, there also remain situations where they rely on brute force despite the availability of more efficient, creative solutions, highlighting a potential direction for improvement in the reasoning abilities of LLMs.",,"Simeng Han, Stephen Xia, Grant Zhang, Howard Dai, Chen Liu, Lichang Chen, Hoang Huy Nguyen, Hongyuan Mei, Jiayuan Mao, R. Thomas McCoy",2025-05-30T03:59:03Z,Creativity or Brute Force? Using Brainteasers as a Window into the   Problem-Solving Abilities of Large Language Models,Kreativität oder Brute Force? Einsatz von Brainteasern als Fenster in die problemlösenden Fähigkeiten großer Sprachmodelle,创意还是布鲁特力? 利用脑食器作为大语言模型解决问题能力的一个窗口,http://arxiv.org/abs/2505.10844v2
1509,"The prevailing assumption of an exponential decay in large language model (LLM) reliability with sequence length, predicated on independent per-token error probabilities, posits an inherent limitation for long autoregressive outputs. Our research fundamentally challenges this view by synthesizing emerging evidence that LLM errors are not uniformly distributed but are concentrated at sparse ""key tokens"" ($5-10\%$ of total tokens) representing critical decision junctions. By distinguishing these high-impact tokens from the increasingly predictable majority, we introduce a new reliability formula explaining the sustained coherence of modern LLMs over thousands of tokens. Converging research streams reveal that long-context performance primarily depends on accurately navigating a few crucial semantic decision points rather than on uniform token-level accuracy, enabling targeted strategies that significantly outperform brute-force approaches. We thus propose a framework for next-generation systems centered on selective preservation of semantically vital tokens, dynamic computational allocation at uncertain decision boundaries, multi-path exploration at ambiguities, and architectures aligned with natural semantic domains. This marks a fundamental shift from raw scaling to strategic reasoning, promising breakthrough performance without proportionate computational scaling and offering a more nuanced understanding that supersedes the exponential decay hypothesis, thereby opening pathways toward substantially more powerful and efficient language systems.",,"Mikhail L. Arbuzov, Alexey A. Shvets, Sisong Beir",2025-05-30T03:57:31Z,Beyond Exponential Decay: Rethinking Error Accumulation in Large   Language Models,Beyond Exponential Decay: Fehleransammlung in großen Sprachmodellen neu denken,超过指数衰减:在大语言模型中重新思考错误累积,http://arxiv.org/abs/2505.24187v1
1510,"Modern phonetic research regularly makes use of automatic tools for the annotation of speech data, however few tools exist for the annotation of many variable phonetic phenomena. At the same time, pre-trained self-supervised models, such as wav2vec2.0, have been shown to perform well at speech classification tasks and latently encode fine-grained phonetic information. We demonstrate that wav2vec2.0 models can be trained to automatically classify stop burst presence with high accuracy in both English and Japanese, robust across both finely-curated and unprepared speech corpora. Patterns of variability in stop realisation are replicated with the automatic annotations, and closely follow those of manual annotations. These results demonstrate the potential of pre-trained speech models as tools for the automatic annotation and processing of speech corpus data, enabling researchers to 'scale-up' the scope of phonetic research with relative ease.",,"James Tanner, Morgan Sonderegger, Jane Stuart-Smith, Jeff Mielke, Tyler Kendall",2025-05-30T03:54:35Z,Automatic classification of stop realisation with wav2vec2.0,Automatische Klassifizierung der Stop-Umsetzung mit wav2vec2.0,以 wav2vec2. 0 自动分类停止实现时间,http://arxiv.org/abs/2505.23688v2
1511,"The Javanese language features a complex system of honorifics that vary according to the social status of the speaker, listener, and referent. Despite its cultural and linguistic significance, there has been limited progress in developing a comprehensive corpus to capture these variations for natural language processing (NLP) tasks. In this paper, we present Unggah-Ungguh, a carefully curated dataset designed to encapsulate the nuances of Unggah-Ungguh Basa, the Javanese speech etiquette framework that dictates the choice of words and phrases based on social hierarchy and context. Using Unggah-Ungguh, we assess the ability of language models (LMs) to process various levels of Javanese honorifics through classification and machine translation tasks. To further evaluate cross-lingual LMs, we conduct machine translation experiments between Javanese (at specific honorific levels) and Indonesian. Additionally, we explore whether LMs can generate contextually appropriate Javanese honorifics in conversation tasks, where the honorific usage should align with the social role and contextual cues. Our findings indicate that current LMs struggle with most honorific levels, exhibitinga bias toward certain honorific tiers.",,"Mohammad Rifqi Farhansyah, Iwan Darmawan, Adryan Kusumawardhana, Genta Indra Winata, Alham Fikri Aji, Derry Tanti Wijaya",2025-05-30T03:50:03Z,Do Language Models Understand Honorific Systems in Javanese?,Verstehen Sprachmodelle Honorific Systems auf Javanese?,Javanese语言模型是否理解荣誉体系?,http://arxiv.org/abs/2502.20864v2
1512,"Modern scientific discovery increasingly relies on high-performance computing for complex modeling and simulation. A key challenge in improving parallel program performance is efficiently mapping tasks to processors and data to memory, a process dictated by intricate, low-level system code known as mappers. Developing high-performance mappers demands days of manual tuning, posing a significant barrier for domain scientists without systems expertise. We introduce a framework that automates mapper development with generative optimization, leveraging richer feedback beyond scalar performance metrics. Our approach features the Agent-System Interface, which includes a Domain-Specific Language (DSL) to abstract away the low-level complexity of system code and define a structured search space, as well as AutoGuide, a mechanism that interprets raw execution output into actionable feedback. Unlike traditional reinforcement learning methods such as OpenTuner, which rely solely on scalar feedback, our method finds superior mappers in far fewer iterations. With just 10 iterations, it outperforms OpenTuner even after 1000 iterations, achieving 3.8X faster performance. Our approach finds mappers that surpass expert-written mappers by up to 1.34X speedup across nine benchmarks while reducing tuning time from days to minutes.",,"Anjiang Wei, Allen Nie, Thiago S. F. X. Teixeira, Rohan Yadav, Wonchan Lee, Ke Wang, Alex Aiken",2025-05-30T03:34:55Z,Improving Parallel Program Performance with LLM Optimizers via   Agent-System Interfaces,Verbesserung der parallelen Programmleistung mit LLM-Optimierern über Agent-System-Schnittstellen,通过代理-系统接口改进与LLM优化器的平行方案绩效,http://arxiv.org/abs/2410.15625v4
1513,"This study proposes a simple yet effective LoRA merge method to achieve LLM adaptation for low-resource language generation tasks. The LoRA merge technique, which integrates multiple LoRA modules trained on different tasks, has gained attention as an effective and efficient approach for adapting LLMs to target tasks. However, previous methods are limited in adaptability as they keep the LoRA parameters frozen. Additionally, the low-resource problem has been out of their scope. We propose a LoRA merge method that updates and prunes LoRA parameters through fine-tuning with minimal target task data, which allows finer-grained adjustments of LoRA parameters and enhancement of task adaptability. Extensive experiments have been conducted taking summarization as a benchmark task. Our datasets cover various domains and multiple languages of English and Japanese. The results confirm that the proposed method achieves significant and consistent improvements in task adaptability over the previous methods.",,"Ryota Miyano, Yuki Arase",2025-05-30T03:34:25Z,Adaptive LoRA Merge with Parameter Pruning for Low-Resource Generation,Adaptive LoRA-Zusammenführung mit Parameter-Bearbeitung für Low-Resource-Erzeugung,适应性 LoRA 与低资源发电参数结合,http://arxiv.org/abs/2505.24174v1
1514,"Ensuring safety alignment is a critical requirement for large language models (LLMs), particularly given increasing deployment in real-world applications. Despite considerable advancements, LLMs remain susceptible to jailbreak attacks, which exploit system vulnerabilities to circumvent safety measures and elicit harmful or inappropriate outputs. Furthermore, while adversarial training-based defense methods have shown promise, a prevalent issue is the unintended over-defense behavior, wherein models excessively reject benign queries, significantly undermining their practical utility. To address these limitations, we introduce LATPC, a Latent-space Adversarial Training with Post-aware Calibration framework. LATPC dynamically identifies safety-critical latent dimensions by contrasting harmful and benign inputs, enabling the adaptive construction of targeted refusal feature removal attacks. This mechanism allows adversarial training to concentrate on real-world jailbreak tactics that disguise harmful queries as benign ones. During inference, LATPC employs an efficient embedding-level calibration mechanism to minimize over-defense behaviors with negligible computational overhead. Experimental results across five types of disguise-based jailbreak attacks demonstrate that LATPC achieves a superior balance between safety and utility compared to existing defense frameworks. Further analysis demonstrates the effectiveness of leveraging safety-critical dimensions in developing robust defense methods against jailbreak attacks.",,"Xin Yi, Yue Li, Dongsheng Shi, Linlin Wang, Xiaoling Wang, Liang He",2025-05-30T03:31:24Z,Latent-space adversarial training with post-aware calibration for   defending large language models against jailbreak attacks,Latent-Space-Adversarial-Training mit post-aware Kalibrierung zur Verteidigung großer Sprachmodelle gegen Jailbreak-Angriffe,为防御大型语言模式以防范越狱袭击而进行后天校准的后备空间对抗性培训,http://arxiv.org/abs/2501.10639v3
1515,"Evol-Instruct has made significant improvements as a data synthesis method in several areas. Existing methods typically rely on a fixed set of strategies to evolve, which require manual design and are monolithic in form. In addition, iterative evolution also makes the acquisition of hard samples expensive. In view of this, we propose the Tag-Evol framework, a more diverse and efficient instruction evolving method. Specifically, Tag-Evol uses diverse and specific knowledge tags as strategies to achieve controlled evolution by injecting different combinations of tags into the original instructions. Experiments with multiple backbones in diverse domain benchmarks show that the proposed method generates significantly better evolved data than other methods. Furthermore, we conduct a thorough analysis of the evolved data, demonstrating that Tag-Evol is not only efficient but also generates more diverse and challenging data.",,"Yixuan Wang, Shiqi Zhou, Chuanzhe Guo, Qingfu Zhu",2025-05-30T03:14:17Z,Tag-Evol: Achieving Efficient Instruction Evolving via Tag Injection,Tag-Evol: Effiziente Instruction Evolving via Tag Injection erreichen,Tag-Evol:通过标签注射实现高效教学,http://arxiv.org/abs/2505.24165v1
1516,"Large language models (LLMs) have shown impressive performance on downstream tasks through in-context learning (ICL), which heavily relies on the demonstrations selected from annotated datasets. However, these datasets often exhibit long-tailed class distributions in real-world scenarios, leading to biased demonstration selection. In this work, we show that such class imbalances significantly degrade the ICL performance across various tasks, regardless of selection methods. Moreover, classical rebalancing methods, which focus solely on class weights, yield poor performance due to neglecting condition bias--skewed feature distributions within classes. To address this, we propose Reweighting with Conditional Bias (dubbed RCB), a simple and complementary approach to enhance ICL performance under class imbalance. In particular, RCB estimates conditional bias using a balanced subset and re-weights demonstration scores based on both class weight and conditional bias. In effect, RCB prevents over-selection from dominant classes while preserving the efficacy of current selection methods. Extensive experiments on common benchmarks demonstrate the effectiveness of our method, improving the average accuracy of current selection methods by up to 5.42%.",,"Hongfu Gao, Feipeng Zhang, Hao Zeng, Deyu Meng, Bingyi Jing, Hongxin Wei",2025-05-30T03:11:54Z,Exploring Imbalanced Annotations for Effective In-Context Learning,Unausgeglichene Anmerkungen für effektives In-Context-Lernen,探讨有效内文学习的不平衡说明,http://arxiv.org/abs/2502.04037v2
1517,"Recent works on large language models (LLMs) have successfully demonstrated the emergence of reasoning capabilities via reinforcement learning (RL). Although recent efforts leverage group relative policy optimization (GRPO) for MLLMs post-training, they constantly explore one specific aspect, such as grounding tasks, math problems, or chart analysis. There are no works that can leverage multi-source MLLM tasks for stable reinforcement learning. In this work, we present a unified perspective to solve this problem. We present Mixed-R1, a unified yet straightforward framework that contains a mixed reward function design (Mixed-Reward) and a mixed post-training dataset (Mixed-45K). We first design a data engine to select high-quality examples to build the Mixed-45K post-training dataset. Then, we present a Mixed-Reward design, which contains various reward functions for various MLLM tasks. In particular, it has four different reward functions: matching reward for binary answer or multiple-choice problems, chart reward for chart-aware datasets, IoU reward for grounding problems, and open-ended reward for long-form text responses such as caption datasets. To handle the various long-form text content, we propose a new open-ended reward named Bidirectional Max-Average Similarity (BMAS) by leveraging tokenizer embedding matching between the generated response and the ground truth. Extensive experiments show the effectiveness of our proposed method on various MLLMs, including Qwen2.5-VL and Intern-VL on various sizes. Our dataset and model are available at https://github.com/xushilin1/mixed-r1.",,"Shilin Xu, Yanwei Li, Rui Yang, Tao Zhang, Yueyi Sun, Wei Chow, Linfeng Li, Hang Song, Qi Xu, Yunhai Tong, Xiangtai Li, Hao Fei",2025-05-30T03:11:46Z,Mixed-R1: Unified Reward Perspective For Reasoning Capability in   Multimodal Large Language Models,Mixed-R1: Unified Reward Perspective für die Vernunft in multimodalen großen Sprachmodellen,混合R1:多模式大语言模式中合理能力的统一奖励视角,http://arxiv.org/abs/2505.24164v1
1518,"Knowledge Graphs (KGs) structure real-world entities and their relationships into triples, enhancing machine reasoning for various tasks. While domain-specific KGs offer substantial benefits, their manual construction is often inefficient and requires specialized knowledge. Recent approaches for knowledge graph construction (KGC) based on large language models (LLMs), such as schema-guided KGC and reference knowledge integration, have proven efficient. However, these methods are constrained by their reliance on manually defined schema, single-document processing, and public-domain references, making them less effective for domain-specific corpora that exhibit complex knowledge dependencies and specificity, as well as limited reference knowledge. To address these challenges, we propose LKD-KGC, a novel framework for unsupervised domain-specific KG construction. LKD-KGC autonomously analyzes document repositories to infer knowledge dependencies, determines optimal processing sequences via LLM driven prioritization, and autoregressively generates entity schema by integrating hierarchical inter-document contexts. This schema guides the unsupervised extraction of entities and relationships, eliminating reliance on predefined structures or external knowledge. Extensive experiments show that compared with state-of-the-art baselines, LKD-KGC generally achieves improvements of 10% to 20% in both precision and recall rate, demonstrating its potential in constructing high-quality domain-specific KGs.",,"Jiaqi Sun, Shiyou Qian, Zhangchi Han, Wei Li, Zelin Qian, Dingyu Yang, Jian Cao, Guangtao Xue",2025-05-30T03:10:23Z,LKD-KGC: Domain-Specific KG Construction via LLM-driven Knowledge   Dependency Parsing,LKD-KGC: Domain-Specific KG Aufbau über LLM-gesteuerte Wissensabhängigkeit Parsing,LKD-KGC:通过由LLM驱动的知识依赖分析进行域特定KG建设,http://arxiv.org/abs/2505.24163v1
1519,"Determining and ranking the most salient entities in a text is critical for user-facing systems, especially as users increasingly rely on models to interpret long documents they only partially read. Graded entity salience addresses this need by assigning entities scores that reflect their relative importance in a text. Existing approaches fall into two main categories: subjective judgments of salience, which allow for gradient scoring but lack consistency, and summarization-based methods, which define salience as mention-worthiness in a summary, promoting explainability but limiting outputs to binary labels (entities are either summary-worthy or not). In this paper, we introduce a novel approach for graded entity salience that combines the strengths of both approaches. Using an English dataset spanning 12 spoken and written genres, we collect 5 summaries per document and calculate each entity's salience score based on its presence across these summaries. Our approach shows stronger correlation with scores based on human summaries and alignments, and outperforms existing techniques, including LLMs. We release our data and code at https://github.com/jl908069/gum_sum_salience to support further research on graded salient entity extraction.",,"Jessica Lin, Amir Zeldes",2025-05-30T03:05:57Z,GUM-SAGE: A Novel Dataset and Approach for Graded Entity Salience   Prediction,GUM-SAGE: Ein neuartiger Datensatz und Ansatz für abgestufte Entity Salience-Vorhersage,GUM-SAGE:一个新数据集和分级实体价值预测方法,http://arxiv.org/abs/2504.10792v2
1520,"Large language model (LLM) based agents have shown great potential in following human instructions and automatically completing various tasks. To complete a task, the agent needs to decompose it into easily executed steps by planning. Existing studies mainly conduct the planning by inferring what steps should be executed next starting from the agent's initial state. However, this forward reasoning paradigm doesn't work well for complex tasks. We propose to study this issue in Minecraft, a virtual environment that simulates complex tasks based on real-world scenarios. We believe that the failure of forward reasoning is caused by the big perception gap between the agent's initial state and task goal. To this end, we leverage backward reasoning and make the planning starting from the terminal state, which can directly achieve the task goal in one step. Specifically, we design a BAckward Reasoning based agent (BAR). It is equipped with a recursive goal decomposition module, a state consistency maintaining module and a stage memory module to make robust, consistent, and efficient planning starting from the terminal state. Experimental results demonstrate the superiority of BAR over existing methods and the effectiveness of proposed modules.",,"Weihong Du, Wenrui Liao, Binyu Yan, Hongru Liang, Anthony G. Cohn, Wenqiang Lei",2025-05-30T03:04:33Z,BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks,BAR: Ein nach hinten gerichteter Agent für komplexe Minecraft-Aufgaben,BAR: 复杂地雷任务小组的后向理由解释代理人,http://arxiv.org/abs/2505.14079v3
1521,"This paper presents a novel approach for unified retrieval-augmented generation (RAG) systems using the recent emerging large language model (LLM) agent concept. Specifically, Agent LLM, which utilizes LLM as fundamental controllers, has become a promising approach to enable the interpretability of RAG tasks, especially for complex reasoning question-answering systems (e.g., multi-hop queries). Nonetheless, previous works mainly focus on solving RAG systems with either single-hop or multi-hop approaches separately, which limits the application of those approaches to real-world applications. In this study, we propose a trainable agent framework called Agent-UniRAG for unified retrieval-augmented LLM systems, which enhances the effectiveness and interpretability of RAG systems. The main idea is to design an LLM agent framework to solve RAG tasks step-by-step based on the complexity of the inputs, simultaneously including single-hop and multi-hop queries in an end-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset to enable the proposed agent framework for small open-source LLMs (e.g., Llama-3-8B). The results show comparable performances with closed-source and larger open-source LLMs across various RAG benchmarks. Our source code and dataset are publicly available for further exploitation.",,"Hoang Pham, Thuy-Duong Nguyen, Khac-Hoai Nam Bui",2025-05-30T02:44:41Z,Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified   Retrieval-Augmented Generation Systems,Agent-UniRAG: Ein trainingables Open-Source LLM Agent Framework für unified Retrieval-Augmented Generation Systems,Agent-UniRAG: 一个可培训的开放源码的LLM Agent Form for United Retreval-Augsing System(统一回收-提款发电系统框架),http://arxiv.org/abs/2505.22571v3
1522,"Training language models with rationales augmentation has been shown to be beneficial in many existing works. In this paper, we identify that such a prevailing view does not hold consistently. We conduct comprehensive investigations to thoroughly inspect the impact of rationales on model performance as well as a novel perspective of model reliability. The results lead to several key findings that add new insights upon existing understandings: 1) Rationales can, at times, deteriorate model performance; 2) Rationales can, at times, improve model reliability, even outperforming their untrained counterparts; 3) A linear correspondence exists in between the performance and reliability improvements, while both are driven by the intrinsic difficulty of the task. These findings provide informative regulations on the broad utilization of rationales and raise critical implications on the procedure of explicitly aligning language models with implicit human thoughts. Codes can be found at https://github.com/Ignoramus0817/rationales.",,"Chiwei Zhu, Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Zhendong Mao",2025-05-30T02:39:37Z,Rationales Are Not Silver Bullets: Measuring the Impact of Rationales on   Model Performance and Reliability,Rationales sind keine Silberkugeln: Messung der Auswirkungen von Rationales auf die Modellleistung und Zuverlässigkeit,理由说明不是银子弹:衡量理由说明对模型性能和可靠性的影响,http://arxiv.org/abs/2505.24147v1
1523,"Real-world decision-making often requires integrating and reasoning over information from multiple modalities. While recent multimodal large language models (MLLMs) have shown promise in such tasks, their ability to perform multi-hop reasoning across diverse sources remains insufficiently evaluated. Existing benchmarks, such as MMQA, face challenges due to (1) data contamination and (2) a lack of complex queries that necessitate operations across more than two modalities, hindering accurate performance assessment. To address this, we present Financial Cross-Modal Multi-Hop Reasoning (FCMR), a benchmark created to analyze the reasoning capabilities of MLLMs by urging them to combine information from textual reports, tables, and charts within the financial domain. FCMR is categorized into three difficulty levels-Easy, Medium, and Hard-facilitating a step-by-step evaluation. In particular, problems at the Hard level require precise cross-modal three-hop reasoning and are designed to prevent the disregard of any modality. Experiments on this new benchmark reveal that even state-of-the-art MLLMs struggle, with the best-performing model (Claude 3.5 Sonnet) achieving only 30.4% accuracy on the most challenging tier. We also conduct analysis to provide insights into the inner workings of the models, including the discovery of a critical bottleneck in the information retrieval phase.",,"Seunghee Kim, Changhyeon Kim, Taeuk Kim",2025-05-30T02:28:44Z,FCMR: Robust Evaluation of Financial Cross-Modal Multi-Hop Reasoning,FCMR: Robuste Bewertung der finanziellen Cross-Modal Multi-Hop Reasoning,FCMR: 对跨模式、多渠道金融理由的有力评价,http://arxiv.org/abs/2412.12567v4
1524,"In-Context Learning (ICL) enhances the performance of large language models (LLMs) with demonstrations. However, obtaining these demonstrations primarily relies on manual effort. In most real-world scenarios, users are often unwilling or unable to provide such demonstrations. Inspired by the human analogy, we explore a new ICL paradigm CrossICL to study how to utilize existing source task demonstrations in the ICL for target tasks, thereby obtaining reliable guidance without any additional manual effort. To explore this, we first design a two-stage alignment strategy to mitigate the interference caused by gaps across tasks, as the foundation for our experimental exploration. Based on it, we conduct comprehensive exploration of CrossICL, with 875 NLP tasks from the Super-NI benchmark and six types of LLMs, including GPT-4o. Experimental results demonstrate the effectiveness of CrossICL and provide valuable insights on questions like the criteria for selecting cross-task demonstrations, as well as the types of task-gap-induced interference in CrossICL.",,"Jinglong Gao, Xiao Ding, Lingxiao Zou, Bing Qin, Ting Liu",2025-05-30T02:26:05Z,CrossICL: Cross-Task In-Context Learning via Unsupervised Demonstration   Transfer,CrossICL: Cross-Task In-Context Learning durch unüberwachten Demonstrationstransfer,CrossICL: 通过不受监督的示范转让进行跨任务信息学习,http://arxiv.org/abs/2505.24143v1
1525,"The widespread success of large language models (LLMs) on NLP benchmarks has been accompanied by concerns that LLMs function primarily as stochastic parrots that reproduce texts similar to what they saw during pre-training, often erroneously. But what is the nature of their errors, and do these errors exhibit any regularities? In this work, we examine irrelevant context hallucinations, in which models integrate misleading contextual cues into their predictions. Through behavioral analysis, we show that these errors result from a structured yet flawed mechanism that we term class-based (mis)generalization, in which models combine abstract class cues with features extracted from the query or context to derive answers. Furthermore, mechanistic interpretability experiments on Llama-3, Mistral, and Pythia across 39 factual recall relation types reveal that this behavior is reflected in the model's internal computations: (i) abstract class representations are constructed in lower layers before being refined into specific answers in higher layers, (ii) feature selection is governed by two competing circuits -- one prioritizing direct query-based reasoning, the other incorporating contextual cues -- whose relative influences determine the final output. Our findings provide a more nuanced perspective on the stochastic parrot argument: through form-based training, LLMs can exhibit generalization leveraging abstractions, albeit in unreliable ways based on contextual cues -- what we term stochastic chameleons.",,"Ziling Cheng, Meng Cao, Marc-Antoine Rondeau, Jackie Chi Kit Cheung",2025-05-30T02:10:54Z,Stochastic Chameleons: Irrelevant Context Hallucinations Reveal   Class-Based (Mis)Generalization in LLMs,Stochastische Chamäleons: irrelevanter Kontext Halluzinationen Offenbarung Klassenbasierte (Mis)Verallgemeinerung in LLMs,电磁变色龙:无关联的地貌幻觉流星级(Mis),http://arxiv.org/abs/2505.22630v2
1526,"While multimodal data sources are increasingly available from real-world forecasting, most existing research remains on unimodal time series. In this work, we present MoTime, a suite of multimodal time series forecasting datasets that pair temporal signals with external modalities such as text, metadata, and images. Covering diverse domains, MoTime supports structured evaluation of modality utility under two scenarios: 1) the common forecasting task, where varying-length history is available, and 2) cold-start forecasting, where no historical data is available. Experiments show that external modalities can improve forecasting performance in both scenarios, with particularly strong benefits for short series in some datasets, though the impact varies depending on data characteristics. By making datasets and findings publicly available, we aim to support more comprehensive and realistic benchmarks in future multimodal time series forecasting research.",,"Xin Zhou, Weiqing Wang, Francisco J. Baldán, Wray Buntine, Christoph Bergmeir",2025-05-30T01:59:05Z,MoTime: A Dataset Suite for Multimodal Time Series Forecasting,MoTime: Eine Dataset-Suite für multimodale Zeitreihenprognosen,MoTime:多时序列预测数据集套件,http://arxiv.org/abs/2505.15072v2
1527,"Syllogistic reasoning is crucial for sound legal decision-making, allowing legal professionals to draw logical conclusions by applying general principles to specific case facts. While large language models (LLMs) can answer legal questions, they often struggle with explicit syllogistic reasoning. Their outputs tend to be implicit, unstructured, and consequently, less explainable and trustworthy. To overcome these limitations, we introduce SyLeR, a novel framework designed to enable LLMs to perform explicit syllogistic legal reasoning. SyLeR employs a tree-structured hierarchical retrieval mechanism to synthesize relevant legal statutes and precedents, thereby constructing comprehensive major premises. This is followed by a two-stage fine-tuning process: an initial supervised fine-tuning warm-up establishes a foundational understanding of syllogistic reasoning, while reinforcement learning, guided by a structure-aware reward mechanism, refines the model's capacity to generate diverse, logically sound, and well-structured reasoning paths. We conducted extensive experiments to evaluate SyLeR's performance. Our evaluations spanned diverse dimensions, including both in-domain and cross-domain user groups (legal laypersons and practitioners), multiple languages (Chinese and French), and various LLM backbones (legal-specific and open-domain LLMs). The results consistently demonstrate that SyLeR significantly enhances response accuracy and reliably produces explicit, explainable, and trustworthy legal reasoning.",,"Kepu Zhang, Weijie Yu, Zhongxiang Sun, Jun Xu",2025-05-30T01:49:16Z,An Explicit Syllogistic Legal Reasoning Framework for Large Language   Models,Ein expliziter syllogistischer Rechtsrahmen für große Sprachmodelle,用于大语言模式的清晰的协同法律理由框架,http://arxiv.org/abs/2504.04042v2
1528,"With the growing importance of AI governance, numerous high-level frameworks and principles have been articulated by policymakers, institutions, and expert communities to guide the development and application of AI. While such frameworks offer valuable normative orientation, they may not fully capture the practical concerns of those who interact with AI systems in organizational and operational contexts. To address this gap, this study adopts a bottom-up approach to explore how governance-relevant themes are expressed in user discourse. Drawing on over 100,000 user reviews of AI products from G2.com, we apply BERTopic to extract latent themes and identify those most semantically related to AI governance. The analysis reveals a diverse set of governance-relevant topics spanning both technical and non-technical domains. These include concerns across organizational processes-such as planning, coordination, and communication-as well as stages of the AI value chain, including deployment infrastructure, data handling, and analytics. The findings show considerable overlap with institutional AI governance and ethics frameworks on issues like privacy and transparency, but also surface overlooked areas such as project management, strategy development, and customer interaction. This highlights the need for more empirically grounded, user-centered approaches to AI governance-approaches that complement normative models by capturing how governance unfolds in applied settings. By foregrounding how governance is enacted in practice, this study contributes to more inclusive and operationally grounded approaches to AI governance and digital policy.",,Stefan Pasch,2025-05-30T01:33:21Z,Bottom-Up Perspectives on AI Governance: Insights from User Reviews of   AI Products,Bottom-Up-Perspektiven zur KI-Governance: Einblicke aus Nutzerbewertungen von KI-Produkten,关于AI 治理的自下而上的观点:对AI 产品的用户审查的展望,http://arxiv.org/abs/2506.00080v1
1529,"This paper presents a comprehensive analysis of the linguistic diversity of LLM safety research, highlighting the English-centric nature of the field. Through a systematic review of nearly 300 publications from 2020--2024 across major NLP conferences and workshops at *ACL, we identify a significant and growing language gap in LLM safety research, with even high-resource non-English languages receiving minimal attention. We further observe that non-English languages are rarely studied as a standalone language and that English safety research exhibits poor language documentation practice. To motivate future research into multilingual safety, we make several recommendations based on our survey, and we then pose three concrete future directions on safety evaluation, training data generation, and crosslingual safety generalization. Based on our survey and proposed directions, the field can develop more robust, inclusive AI safety practices for diverse global populations.",,"Zheng-Xin Yong, Beyza Ermis, Marzieh Fadaee, Stephen H. Bach, Julia Kreutzer",2025-05-30T01:32:44Z,The State of Multilingual LLM Safety Research: From Measuring the   Language Gap to Mitigating It,Der Stand der Mehrsprachigen LLM-Sicherheitsforschung: Von der Messung der Sprachlücke bis zur Abmilderung,多语言LLM安全研究现状:从衡量语言差距到缩小语言差距,http://arxiv.org/abs/2505.24119v1
1530,"Mixture-of-Experts (MoE) models mostly use a router to assign tokens to specific expert modules, activating only partial parameters and often outperforming dense models. We argue that the separation between the router's decision-making and the experts' execution is a critical yet overlooked issue, leading to suboptimal expert selection and ineffective learning. To address this, we propose Autonomy-of-Experts (AoE), a novel MoE paradigm in which experts autonomously select themselves to process inputs. AoE is based on the insight that an expert is aware of its own capacity to effectively process a token, an awareness reflected in the scale of its internal activations. In AoE, routers are removed; instead, experts pre-compute internal activations for inputs and are ranked based on their activation norms. Only the top-ranking experts proceed with the forward pass, while the others abort. The overhead of pre-computing activations is reduced through a low-rank weight factorization. This self-evaluating-then-partner-comparing approach ensures improved expert selection and effective learning. We pre-train language models having 700M up to 4B parameters, demonstrating that AoE outperforms traditional MoE models with comparable efficiency.",,"Ang Lv, Ruobing Xie, Yining Qian, Songhao Wu, Xingwu Sun, Zhanhui Kang, Di Wang, Rui Yan",2025-05-30T01:32:21Z,Autonomy-of-Experts Models,Modellautonomie der Experten,专家自主模型,http://arxiv.org/abs/2501.13074v2
1531,"We present EHRMIND, a practical recipe for adapting large language models (LLMs) to complex clinical reasoning tasks using reinforcement learning with verifiable rewards (RLVR). While RLVR has succeeded in mathematics and coding, its application to healthcare contexts presents unique challenges due to the specialized knowledge and reasoning required for electronic health record (EHR) interpretation. Our pilot study on the MEDCALC benchmark reveals two key failure modes: (1) misapplied knowledge, where models possess relevant medical knowledge but apply it incorrectly, and (2) missing knowledge, where models lack essential domain knowledge. To address these cases, EHRMIND applies a two-stage solution: a lightweight supervised fine-tuning (SFT) warm-up that injects missing domain knowledge, stabilizes subsequent training, and encourages structured, interpretable outputs; followed by RLVR, which reinforces outcome correctness and refines the model's decision-making. We demonstrate the effectiveness of our method across diverse clinical applications, including medical calculations (MEDCALC), patient-trial matching (TREC CLINICAL TRIALS), and disease diagnosis (EHRSHOT). EHRMIND delivers consistent gains in accuracy, interpretability, and cross-task generalization. These findings offer practical guidance for applying RLVR to enhance LLM capabilities in healthcare settings.",,"Jiacheng Lin, Zhenbang Wu, Jimeng Sun",2025-05-30T01:13:22Z,Training LLMs for EHR-Based Reasoning Tasks via Reinforcement Learning,Schulung von LLMs für EHR-basierte mit Gründen versehene Aufgaben durch Verstärkung des Lernens,通过强化学习为基于EHR的理据任务提供培训LLMS,http://arxiv.org/abs/2505.24105v1
1532,"Verifiers play a crucial role in large language model (LLM) reasoning, needed by post-training techniques such as reinforcement learning. However, reliable verifiers are hard to get for difficult coding problems, because a well-disguised wrong solution may only be detected by carefully human-written edge cases that are difficult to synthesize. To address this issue, we propose HARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this pipeline, we curate a comprehensive competitive programming dataset HARDTESTS with 47k problems and synthetic high-quality tests. Compared with existing tests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points higher and recall that is 17.5 percentage points higher when evaluating LLM-generated code. For harder problems, the improvement in precision can be as large as 40 points. HARDTESTS also proves to be more effective for model training, measured by downstream code generation performance. We will open-source our dataset and synthesis pipeline at https://leililab.github.io/HardTests/.",,"Zhongmou He, Yee Man Choi, Kexun Zhang, Jiabao Ji, Junting Zhou, Dejia Xu, Ivan Bercovich, Aidan Zhang, Lei Li",2025-05-30T01:00:34Z,HardTests: Synthesizing High-Quality Test Cases for LLM Coding,HardTests: Synthese hochwertiger Testfälle für die LLM-Codierung,"硬测试:综合高品质测试案例,用于LLM编码",http://arxiv.org/abs/2505.24098v1
1533,"Text emotion detection constitutes a crucial foundation for advancing artificial intelligence from basic comprehension to the exploration of emotional reasoning. Most existing emotion detection datasets rely on manual annotations, which are associated with high costs, substantial subjectivity, and severe label imbalances. This is particularly evident in the inadequate annotation of micro-emotions and the absence of emotional intensity representation, which fail to capture the rich emotions embedded in sentences and adversely affect the quality of downstream task completion. By proposing an all-labels and training-set label regression method, we map label values to energy intensity levels, thereby fully leveraging the learning capabilities of machine models and the interdependencies among labels to uncover multiple emotions within samples. This led to the establishment of the Emotion Quantization Network (EQN) framework for micro-emotion detection and annotation. Using five commonly employed sentiment datasets, we conducted comparative experiments with various models, validating the broad applicability of our framework within NLP machine learning models. Based on the EQN framework, emotion detection and annotation are conducted on the GoEmotions dataset. A comprehensive comparison with the results from Google literature demonstrates that the EQN framework possesses a high capability for automatic detection and annotation of micro-emotions. The EQN framework is the first to achieve automatic micro-emotion annotation with energy-level scores, providing strong support for further emotion detection analysis and the quantitative research of emotion computing.",,"Jingyi Zhou, Senlin Luo, Haofan Chen",2025-05-30T00:44:12Z,Expansion Quantization Network: An Efficient Micro-emotion Annotation   and Detection Framework,Expansion Quantization Network: Ein effizientes Mikroemotion-Annotations- und Detektions-Framework,扩大量化网络:高效微情绪批注和检测框架,http://arxiv.org/abs/2411.06160v3
1534,"Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 97-100% of accuracy.",,"Shantanu Acharya, Fei Jia, Boris Ginsburg",2025-05-30T00:36:37Z,Star Attention: Efficient LLM Inference over Long Sequences,Star Achtung: Effiziente LLM-Inferenz über lange Sequenzen,恒星注意: 有效LLM 长序列的推理,http://arxiv.org/abs/2411.17116v3
1535,"Automated definition generation systems have been proposed to support vocabulary expansion for language learners. The main barrier to the success of these systems is that learners often struggle to understand definitions due to the presence of potentially unfamiliar words and grammar, particularly when non-standard language is involved. To address these challenges, we propose CLIX, the task of Cross-Lingual explanations of Idiomatic eXpressions. We explore the capabilities of current NLP models for this task, and observe that while it remains challenging, large language models show promise. Finally, we perform a detailed error analysis to highlight the key challenges that need to be addressed before we can reliably incorporate these systems into educational tools.",,"Aaron Gluck, Katharina von der Wense, Maria Leonor Pacheco",2025-05-30T00:16:02Z,CLIX: Cross-Lingual Explanations of Idiomatic Expressions,CLIX: Cross-Lingual Erklärungen idiomatischer Ausdrücke,CLIX: 跨语言表达式的跨语言解释,http://arxiv.org/abs/2501.03191v3
1536,"Traditional recommender systems usually take the user-platform paradigm, where users are directly exposed under the control of the platform's recommendation algorithms. However, the defect of recommendation algorithms may put users in very vulnerable positions under this paradigm. First, many sophisticated models are often designed with commercial objectives in mind, focusing on the platform's benefits, which may hinder their ability to protect and capture users' true interests. Second, these models are typically optimized using data from all users, which may overlook individual user's preferences. Due to these shortcomings, users may experience several disadvantages under the traditional user-platform direct exposure paradigm, such as lack of control over the recommender system, potential manipulation by the platform, echo chamber effects, or lack of personalization for less active users due to the dominance of active users during collaborative learning. Therefore, there is an urgent need to develop a new paradigm to protect user interests and alleviate these issues. Recently, some researchers have introduced LLM agents to simulate user behaviors, these approaches primarily aim to optimize platform-side performance, leaving core issues in recommender systems unresolved. To address these limitations, we propose a new user-agent-platform paradigm, where agent serves as the protective shield between user and recommender system that enables indirect exposure.",,"Wujiang Xu, Yunxiao Shi, Zujie Liang, Xuying Ning, Kai Mei, Kun Wang, Xi Zhu, Min Xu, Yongfeng Zhang",2025-05-29T23:51:24Z,iAgent: LLM Agent as a Shield between User and Recommender Systems,iAgent: LLM Agent als Shield zwischen Anwender- und Recommender-Systemen,iAgendy:LLM代理作为用户与建议系统之间的盾牌,http://arxiv.org/abs/2502.14662v4
1537,"As Contrastive Language-Image Pre-training (CLIP) models are increasingly adopted for diverse downstream tasks and integrated into large vision-language models (VLMs), their susceptibility to adversarial perturbations has emerged as a critical concern. In this work, we introduce \textbf{X-Transfer}, a novel attack method that exposes a universal adversarial vulnerability in CLIP. X-Transfer generates a Universal Adversarial Perturbation (UAP) capable of deceiving various CLIP encoders and downstream VLMs across different samples, tasks, and domains. We refer to this property as \textbf{super transferability}--a single perturbation achieving cross-data, cross-domain, cross-model, and cross-task adversarial transferability simultaneously. This is achieved through \textbf{surrogate scaling}, a key innovation of our approach. Unlike existing methods that rely on fixed surrogate models, which are computationally intensive to scale, X-Transfer employs an efficient surrogate scaling strategy that dynamically selects a small subset of suitable surrogates from a large search space. Extensive evaluations demonstrate that X-Transfer significantly outperforms previous state-of-the-art UAP methods, establishing a new benchmark for adversarial transferability across CLIP models. The code is publicly available in our \href{https://github.com/HanxunH/XTransferBench}{GitHub repository}.",,"Hanxun Huang, Sarah Erfani, Yige Li, Xingjun Ma, James Bailey",2025-05-29T23:50:01Z,X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on   CLIP,X-Transfer-Angriffe: Auf dem Weg zu superübertragbaren adversarialen Angriffen auf CLIP,X Transfer攻击:对CLIP的超可转移反向攻击,http://arxiv.org/abs/2505.05528v3
1538,"Understanding how effectively large vision language models (VLMs) compare visual inputs is crucial across numerous applications, yet this fundamental capability remains insufficiently assessed. While VLMs are increasingly deployed for tasks requiring comparative judgment, including automated evaluation, re-ranking, and retrieval-augmented generation, no systematic framework exists to measure their performance in these scenarios. We present PairBench, a simple framework that evaluates VLMs as customizable similarity tools using widely available image datasets. Our approach introduces four key metrics for reliable comparison: alignment with human annotations, consistency across pair ordering, distribution smoothness, and controllability through prompting. Our analysis reveals that no model consistently excels across all metrics, with each demonstrating distinct strengths and weaknesses. Most concerning is the widespread inability of VLMs to maintain symmetric similarity scores. Interestingly, we demonstrate that performance on our benchmark strongly correlates with popular benchmarks used for more complex tasks, while providing additional metrics into controllability, smoothness and ordering. This makes PairBench a unique and comprehensive framework to evaluate the performance of VLMs for automatic evaluation depending on the task.",,"Aarash Feizi, Sai Rajeswar, Adriana Romero-Soriano, Reihaneh Rabbany, Valentina Zantedeschi, Spandana Gella, João Monteiro",2025-05-29T23:35:10Z,PairBench: Are Vision-Language Models Reliable at Comparing What They   See?,"PairBench: Sind Vision-Sprachen-Modelle beim Vergleich dessen, was sie sehen, zuverlässig?",PairBench:远景 -- -- 语言模型在比较它们所看到的情况时是否可靠?,http://arxiv.org/abs/2502.15210v3
1539,"Large Vision-Language Models (LVLMs) have made remarkable strides in multimodal tasks such as visual question answering, visual grounding, and complex reasoning. However, they remain limited by static training data, susceptibility to hallucinations, and inability to verify claims against up-to-date, external evidence, compromising their performance in dynamic real-world applications. Retrieval-Augmented Generation (RAG) offers a practical solution to mitigate these challenges by allowing the LVLMs to access large-scale knowledge databases via retrieval mechanisms, thereby grounding model outputs in factual, contextually relevant information. Here in this paper, we conduct the first systematic dissection of the multimodal RAG pipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the modality configurations and retrieval strategies, (2) the re-ranking stage: on strategies to mitigate positional biases and improve the relevance of retrieved evidence, and (3) the generation phase: we further investigate how to best integrate retrieved candidates into the final generation process. Finally, we extend to explore a unified agentic framework that integrates re-ranking and generation through self-reflection, enabling LVLMs to select relevant evidence and suppress irrelevant context dynamically. Our full-stack exploration of RAG for LVLMs yields substantial insights, resulting in an average performance boost of 5% without any fine-tuning.",,"Chan-Wei Hu, Yueqi Wang, Shuo Xing, Chia-Ju Chen, Zhengzhong Tu",2025-05-29T23:32:03Z,mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented   Generation,mRAG: Erklärung des Planungsraums multimodaler retrieval-Augmented Generation,mRAG: 描述多式回收-加速一代人的设计空间,http://arxiv.org/abs/2505.24073v1
1540,"Traditional Chinese Medicine (TCM), as an effective alternative medicine, has been receiving increasing attention. In recent years, the rapid development of large language models (LLMs) tailored for TCM has underscored the need for an objective and comprehensive evaluation framework to assess their performance on real-world tasks. However, existing evaluation datasets are limited in scope and primarily text-based, lacking a unified and standardized multimodal question-answering (QA) benchmark. To address this issue, we introduce TCM-Ladder, the first multimodal QA dataset specifically designed for evaluating large TCM language models. The dataset spans multiple core disciplines of TCM, including fundamental theory, diagnostics, herbal formulas, internal medicine, surgery, pharmacognosy, and pediatrics. In addition to textual content, TCM-Ladder incorporates various modalities such as images and videos. The datasets were constructed using a combination of automated and manual filtering processes and comprise 52,000+ questions in total. These questions include single-choice, multiple-choice, fill-in-the-blank, diagnostic dialogue, and visual comprehension tasks. We trained a reasoning model on TCM-Ladder and conducted comparative experiments against 9 state-of-the-art general domain and 5 leading TCM-specific LLMs to evaluate their performance on the datasets. Moreover, we propose Ladder-Score, an evaluation method specifically designed for TCM question answering that effectively assesses answer quality regarding terminology usage and semantic expression. To our knowledge, this is the first work to evaluate mainstream general domain and TCM-specific LLMs on a unified multimodal benchmark. The datasets and leaderboard are publicly available at https://tcmladder.com or https://54.211.107.106 and will be continuously updated.",,"Jiacheng Xie, Yang Yu, Ziyang Zhang, Shuai Zeng, Jiaxuan He, Ayush Vasireddy, Xiaoting Tang, Congyu Guo, Lening Zhao, Congcong Jing, Guanghui An, Dong Xu",2025-05-29T23:13:57Z,TCM-Ladder: A Benchmark for Multimodal Question Answering on Traditional   Chinese Medicine,TCM-Leiter: Benchmark für multimodale Fragen zur traditionellen chinesischen Medizin,TCM-Ladter:中国传统医学多式联运问题回答基准,http://arxiv.org/abs/2505.24063v1
1541,"Television networks face high financial risk when making programming decisions, often relying on limited historical data to forecast episodic viewership. This study introduces a machine learning framework that integrates natural language processing (NLP) features from over 25000 television episodes with traditional viewership data to enhance predictive accuracy. By extracting emotional tone, cognitive complexity, and narrative structure from episode dialogue, we evaluate forecasting performance using SARIMAX, rolling XGBoost, and feature selection models. While prior viewership remains a strong baseline predictor, NLP features contribute meaningful improvements for some series. We also introduce a similarity scoring method based on Euclidean distance between aggregate dialogue vectors to compare shows by content. Tested across diverse genres, including Better Call Saul and Abbott Elementary, our framework reveals genre-specific performance and offers interpretable metrics for writers, executives, and marketers seeking data-driven insight into audience behavior.",,"Andrew Cornfeld, Ashley Miller, Mercedes Mora-Figueroa, Kurt Samuels, Anthony Palomba",2025-05-29T23:01:54Z,"Optimizing Storytelling, Improving Audience Retention, and Reducing   Waste in the Entertainment Industry","Optimierung von Storytelling, Verbesserung der Audienzhaltung und Reduzierung von Abfall in der Unterhaltungsindustrie",优化童说、改进留学和减少娱乐业的废物,http://arxiv.org/abs/2506.00076v1
1542,"Large Language Models (LLMs) have demonstrated remarkable performance on various medical question-answering (QA) benchmarks, including standardized medical exams. However, correct answers alone do not ensure correct logic, and models may reach accurate conclusions through flawed processes. In this study, we introduce the MedPAIR (Medical Dataset Comparing Physicians and AI Relevance Estimation and Question Answering) dataset to evaluate how physician trainees and LLMs prioritize relevant information when answering QA questions. We obtain annotations on 1,300 QA pairs from 36 physician trainees, labeling each sentence within the question components for relevance. We compare these relevance estimates to those for LLMs, and further evaluate the impact of these ""relevant"" subsets on downstream task performance for both physician trainees and LLMs. We find that LLMs are frequently not aligned with the content relevance estimates of physician trainees. After filtering out physician trainee-labeled irrelevant sentences, accuracy improves for both the trainees and the LLMs. All LLM and physician trainee-labeled data are available at: http://medpair.csail.mit.edu/.",,"Yuexing Hao, Kumail Alhamoud, Hyewon Jeong, Haoran Zhang, Isha Puri, Philip Torr, Mike Schaekermann, Ariel D. Stern, Marzyeh Ghassemi",2025-05-29T22:23:48Z,MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical   Question Answering,MedPAIR: Mediziner und KI-Relevanz messen Ausrichtung in der medizinischen Fragestellung,MedPAIR: 计量医生和AI在医疗问题回答中的相关性协调,http://arxiv.org/abs/2505.24040v1
1543,"Multilingual Large Language Models (LLMs) develop cross-lingual abilities despite being trained on limited parallel data. However, they often struggle to generate responses in the intended language, favoring high-resource languages such as English. In this work, we introduce CoCo-CoLa (Correct Concept - Correct Language), a novel metric to evaluate language adherence in multilingual LLMs. Using fine-tuning experiments on a closed-book QA task across seven languages, we analyze how training in one language affects others' performance. Our findings reveal that multilingual models share task knowledge across languages but exhibit biases in the selection of output language. We identify language-specific layers, showing that final layers play a crucial role in determining output language. Accordingly, we propose a partial training strategy that selectively fine-tunes key layers, improving language adherence while significantly reducing computational cost. Our method achieves comparable or superior performance to full fine-tuning, particularly for low-resource languages, offering a more efficient multilingual adaptation.",,"Elnaz Rahmati, Alireza S. Ziabari, Morteza Dehghani",2025-05-29T22:15:32Z,CoCo-CoLa: Evaluating and Improving Language Adherence in Multilingual   LLMs,CoCo-CoLa: Bewertung und Verbesserung der Sprachbefolgung in mehrsprachigen LLMs,Co-CoLA:多语种LLM评价和改进语言遵守情况,http://arxiv.org/abs/2502.12476v2
1544,"We investigate whether hidden states from Structured State Space Models (SSMs) can be merged post-hoc to support downstream reasoning. Inspired by model souping, we propose a strategy where documents are encoded independently and their representations are pooled -- via simple operations like averaging -- into a single context state. This approach, which we call document souping, enables modular encoding and reuse without reprocessing the full input for each query. We finetune Mamba2 models to produce soupable representations and find that they support multi-hop QA, sparse retrieval, and long-document reasoning with strong accuracy. On HotpotQA, souping ten independently encoded documents nearly matches the performance of a cross-encoder trained on the same inputs.",,"Yasaman Jafari, Zixian Wang, Leon Bergen, Taylor Berg-Kirkpatrick",2025-05-29T22:13:21Z,The Surprising Soupability of Documents in State Space Models,Die überraschende Soupability von Dokumenten in State Space Models,国家空间模型中文件的惊人可复制性,http://arxiv.org/abs/2505.24033v1
1545,"This paper presents one of the top-performing solutions to the UNLP 2025 Shared Task on Detecting Manipulation in Social Media. The task focuses on detecting and classifying rhetorical and stylistic manipulation techniques used to influence Ukrainian Telegram users. For the classification subtask, we fine-tuned the Gemma 2 language model with LoRA adapters and applied a second-level classifier leveraging meta-features and threshold optimization. For span detection, we employed an XLM-RoBERTa model trained for multi-target, including token binary classification. Our approach achieved 2nd place in classification and 3rd place in span detection.",,"Kateryna Akhynko, Oleksandr Kosovan, Mykola Trokhymovych",2025-05-29T22:01:42Z,Hidden Persuasion: Detecting Manipulative Narratives on Social Media   During the 2022 Russian Invasion of Ukraine,Hidden Persuasion: Manipulative Narrative in sozialen Medien entdecken Während der russischen Invasion der Ukraine 2022,"2022年俄罗斯入侵乌克兰期间,",http://arxiv.org/abs/2505.24028v1
1546,"Cardiovascular diseases are a leading cause of death and disability worldwide. Electrocardiogram (ECG) is critical for diagnosing and monitoring cardiac health, but obtaining large-scale annotated ECG datasets is labor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL) methods mitigate this by learning features without extensive labels but fail to capture fine-grained clinical semantics and require extensive task-specific fine-tuning. To address these challenges, we propose $\textbf{SuPreME}$, a $\textbf{Su}$pervised $\textbf{Pre}$-training framework for $\textbf{M}$ultimodal $\textbf{E}$CG representation learning. SuPreME is pre-trained using structured diagnostic labels derived from ECG report entities through a one-time offline extraction with Large Language Models (LLMs), which help denoise, standardize cardiac concepts, and improve clinical representation learning. By fusing ECG signals with textual cardiac queries instead of fixed labels, SuPreME enables zero-shot classification of unseen conditions without further fine-tuning. We evaluate SuPreME on six downstream datasets covering 106 cardiac conditions, achieving superior zero-shot AUC performance of $77.20\%$, surpassing state-of-the-art eSSLs by $4.98\%$. Results demonstrate SuPreME's effectiveness in leveraging structured, clinically relevant knowledge for high-quality ECG representations.",,"Mingsheng Cai, Jiuming Jiang, Wenhao Huang, Che Liu, Rossella Arcucci",2025-05-29T21:49:46Z,SuPreME: A Supervised Pre-training Framework for Multimodal ECG   Representation Learning,SuPreME: Ein überwachter Vorausbildungsrahmen für multimodales EKG-Repräsentanzlernen,SuPREME:多式ECG代表性学习培训前框架监督,http://arxiv.org/abs/2502.19668v2
1547,"This paper discusses the construction, fine-tuning, and deployment of BeaverTalk, a cascaded system for speech-to-text translation as part of the IWSLT 2025 simultaneous translation task. The system architecture employs a VAD segmenter for breaking a speech stream into segments, Whisper Large V2 for automatic speech recognition (ASR), and Gemma 3 12B for simultaneous translation. Regarding the simultaneous translation LLM, it is fine-tuned via low-rank adaptors (LoRAs) for a conversational prompting strategy that leverages a single prior-sentence memory bank from the source language as context. The cascaded system participated in the English$\rightarrow$German and English$\rightarrow$Chinese language directions for both the low and high latency regimes. In particular, on the English$\rightarrow$German task, the system achieves a BLEU of 24.64 and 27.83 at a StreamLAAL of 1837.86 and 3343.73, respectively. Then, on the English$\rightarrow$Chinese task, the system achieves a BLEU of 34.07 and 37.23 at a StreamLAAL of 2216.99 and 3521.35, respectively.",,"Matthew Raffel, Victor Agostinelli, Lizhong Chen",2025-05-29T21:34:49Z,BeaverTalk: Oregon State University's IWSLT 2025 Simultaneous Speech   Translation System,BeaverTalk: IWSLT 2025 der Oregon State University Simultanübersetzungssystem,俄勒冈州立大学IWSLT 2025同声语音翻译系统,http://arxiv.org/abs/2505.24016v1
1548,"As the era of autonomous agents making decisions on behalf of users unfolds, ensuring contextual integrity (CI) -- what is the appropriate information to share while carrying out a certain task -- becomes a central question to the field. We posit that CI demands a form of reasoning where the agent needs to reason about the context in which it is operating. To test this, we first prompt LLMs to reason explicitly about CI when deciding what information to disclose. We then extend this approach by developing a reinforcement learning (RL) framework that further instills in models the reasoning necessary to achieve CI. Using a synthetic, automatically created, dataset of only $\sim700$ examples but with diverse contexts and information disclosure norms, we show that our method substantially reduces inappropriate information disclosure while maintaining task performance across multiple model sizes and families. Importantly, improvements transfer from this synthetic dataset to established CI benchmarks such as PrivacyLens that has human annotations and evaluates privacy leakage of AI assistants in actions and tool calls.",,"Guangchen Lan, Huseyin A. Inan, Sahar Abdelnabi, Janardhan Kulkarni, Lukas Wutschitz, Reza Shokri, Christopher G. Brinton, Robert Sim",2025-05-29T21:26:21Z,Contextual Integrity in LLMs via Reasoning and Reinforcement Learning,Kontextuelle Integrität in LLMs durch Vernunft- und Stärkungslernen,通过教学理由和强化学习在LLMs中实现背景廉正,http://arxiv.org/abs/2506.04245v1
1549,"Large Language Models (LLMs) excel at generating fluent text but struggle to enforce external constraints because they generate tokens sequentially without explicit control mechanisms. GenCP addresses this limitation by combining LLM predictions with Constraint Programming (CP) reasoning, formulating text generation as a Constraint Satisfaction Problem (CSP). In this paper, we improve GenCP by integrating Masked Language Models (MLMs) for domain generation, which allows bidirectional constraint propagation that leverages both past and future tokens. This integration bridges the gap between token-level prediction and structured constraint enforcement, leading to more reliable and constraint-aware text generation. Our evaluation on COLLIE benchmarks demonstrates that incorporating domain preview via MLM calls significantly improves GenCP's performance. Although this approach incurs additional MLM calls and, in some cases, increased backtracking, the overall effect is a more efficient use of LLM inferences and an enhanced ability to generate feasible and meaningful solutions, particularly in tasks with strict content constraints.",,"Alexandre Bonlarron, Florian Régin, Elisabetta De Maria, Jean-Charles Régin",2025-05-29T21:18:12Z,Large Language Model Meets Constraint Propagation,Großes Sprachmodell trifft Beschränkungspropagation,大型语言模式满足约束性推进,http://arxiv.org/abs/2505.24012v1
1550,"Transformers deliver outstanding performance across a wide range of tasks and are now a dominant backbone architecture for large language models (LLMs). Their task-solving performance is improved by increasing parameter size, as shown in the recent studies on parameter scaling laws. Although recent mechanistic-interpretability studies have deepened our understanding of the internal behavior of Transformers by analyzing their residual stream, the relationship between these internal mechanisms and the parameter scaling laws remains unclear. To bridge this gap, we focus on layers and their size, which mainly decide the parameter size of Transformers. For this purpose, we first theoretically investigate the layers within the residual stream through a bias-diversity decomposition. The decomposition separates (i) bias, the error of each layer's output from the ground truth, and (ii) diversity, which indicates how much the outputs of each layer differ from each other. Analyzing Transformers under this theory reveals that performance improves when individual layers make predictions close to the correct answer and remain mutually diverse. We show that diversity becomes especially critical when individual layers' outputs are far from the ground truth. Finally, we introduce an information-theoretic diversity and show our main findings that adding layers enhances performance only when those layers behave differently, i.e., are diverse. We also reveal the performance gains from increasing the number of layers exhibit submodularity: marginal improvements diminish as additional layers increase, mirroring the logarithmic convergence predicted by the parameter scaling laws. Experiments on multiple semantic-understanding tasks with various LLMs empirically confirm the theoretical properties derived in this study.",,"Hidetaka Kamigaito, Ying Zhang, Jingun Kwon, Katsuhiko Hayashi, Manabu Okumura, Taro Watanabe",2025-05-29T21:13:31Z,Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws,Diversity of Transformer Layers: Ein Aspekt der Parameterskalierungsgesetze,变形层多样性:参数面积法的一个方面,http://arxiv.org/abs/2505.24009v1
1551,"This study presents BanStereoSet, a dataset designed to evaluate stereotypical social biases in multilingual LLMs for the Bangla language. In an effort to extend the focus of bias research beyond English-centric datasets, we have localized the content from the StereoSet, IndiBias, and Kamruzzaman et. al.'s datasets, producing a resource tailored to capture biases prevalent within the Bangla-speaking community. Our BanStereoSet dataset consists of 1,194 sentences spanning 9 categories of bias: race, profession, gender, ageism, beauty, beauty in profession, region, caste, and religion. This dataset not only serves as a crucial tool for measuring bias in multilingual LLMs but also facilitates the exploration of stereotypical bias across different social categories, potentially guiding the development of more equitable language technologies in Bangladeshi contexts. Our analysis of several language models using this dataset indicates significant biases, reinforcing the necessity for culturally and linguistically adapted datasets to develop more equitable language technologies.",,"Mahammed Kamruzzaman, Abdullah Al Monsur, Shrabon Das, Enamul Hassan, Gene Louis Kim",2025-05-29T21:02:54Z,BanStereoSet: A Dataset to Measure Stereotypical Social Biases in LLMs   for Bangla,BanStereoSet: Ein Datensatz zur Messung stereotypischer sozialer Biasen in LLMs für Bangla,"BanstereoSet:一套数据集,用于测量孟加拉LMLM(孟加拉国)中立定型社会两立立立体现象",http://arxiv.org/abs/2409.11638v2
1552,"Crowd work platforms like Amazon Mechanical Turk and Prolific are vital for research, yet workers' growing use of generative AI tools poses challenges. Researchers face compromised data validity as AI responses replace authentic human behavior, while workers risk diminished roles as AI automates tasks. To address this, we propose a hybrid framework using digital twins, personalized AI models that emulate workers' behaviors and preferences while keeping humans in the loop. We evaluate our system with an experiment (n=88 crowd workers) and in-depth interviews with crowd workers (n=5) and social science researchers (n=4). Our results suggest that digital twins may enhance productivity and reduce decision fatigue while maintaining response quality. Both researchers and workers emphasized the importance of transparency, ethical data use, and worker agency. By automating repetitive tasks and preserving human engagement for nuanced ones, digital twins may help balance scalability with authenticity.",,"Amanda Chan, Catherine Di, Joseph Rupertus, Gary Smith, Varun Nagaraj Rao, Manoel Horta Ribeiro, Andrés Monroy-Hernández",2025-05-29T20:55:27Z,Redefining Research Crowdsourcing: Incorporating Human Feedback with   LLM-Powered Digital Twins,Neudefinition von Research Crowdsourcing: Aufnahme von menschlichem Feedback mit LLM-Powered Digital Twins,重新定义研究众包:将人类反馈与LLM有权力的数字双双结合,http://arxiv.org/abs/2505.24004v1
1553,"Fine-tuning large language models (LLMs) with low-rank adaptations (LoRAs) has become common practice, often yielding numerous copies of the same LLM differing only in their LoRA updates. This paradigm presents challenges for systems that serve real-time responses to queries that each involve a different LoRA. Prior works optimize the design of such systems but still require continuous loading and offloading of LoRAs, as it is infeasible to store thousands of LoRAs in GPU memory. To mitigate this issue, we investigate the efficacy of compression when serving LoRAs. We propose a method for the joint compression of LoRAs into a shared basis paired with LoRA-specific scaling matrices. We extend our algorithm to learn clusters of LoRAs that are amenable to joint compression, allowing it to scale gracefully to large LoRA collections. Our experiments with up to 1000 LoRAs demonstrate that compressed LoRAs preserve performance while offering major throughput gains in realistic serving scenarios with over a thousand LoRAs, maintaining 80% of the throughput of serving a single LoRA.",,"Rickard Brüel-Gabrielsson, Jiacheng Zhu, Onkar Bhardwaj, Leshem Choshen, Kristjan Greenewald, Mikhail Yurochkin, Justin Solomon",2025-05-29T20:47:12Z,Compress then Serve: Serving Thousands of LoRA Adapters with Little   Overhead,Komprimieren Sie dann Servieren: Tausende von LoRA-Adaptern mit wenig Overhead,"压缩后服务:为成千上万的LORA适应者服务,",http://arxiv.org/abs/2407.00066v4
1554,"The recent rapid adoption of large language models (LLMs) highlights the critical need for benchmarking their fairness. Conventional fairness metrics, which focus on discrete accuracy-based evaluations (i.e., prediction correctness), fail to capture the implicit impact of model uncertainty (e.g., higher model confidence about one group over another despite similar accuracy). To address this limitation, we propose an uncertainty-aware fairness metric, UCerF, to enable a fine-grained evaluation of model fairness that is more reflective of the internal bias in model decisions compared to conventional fairness measures. Furthermore, observing data size, diversity, and clarity issues in current datasets, we introduce a new gender-occupation fairness evaluation dataset with 31,756 samples for co-reference resolution, offering a more diverse and suitable dataset for evaluating modern LLMs. We establish a benchmark, using our metric and dataset, and apply it to evaluate the behavior of ten open-source LLMs. For example, Mistral-7B exhibits suboptimal fairness due to high confidence in incorrect predictions, a detail overlooked by Equalized Odds but captured by UCerF. Overall, our proposed LLM benchmark, which evaluates fairness with uncertainty awareness, paves the way for developing more transparent and accountable AI systems.",,"Yinong Oliver Wang, Nivedha Sivakumar, Falaah Arif Khan, Rin Metcalf Susa, Adam Golinski, Natalie Mackraz, Barry-John Theobald, Luca Zappella, Nicholas Apostoloff",2025-05-29T20:45:18Z,Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for   LLMs,Ist Ihr Modell ziemlich sicher? Ungewissheitsbewusste Fairness-Bewertung für LLMs,您的模型是否公正可靠?,http://arxiv.org/abs/2505.23996v1
1555,"This study formalizes a computational model to simulate classical Persian poets' dynamics of influence through constructing a multi-dimensional similarity network. Using a rigorously curated dataset based on Ganjoor's corpus, we draw upon semantic, lexical, stylistic, thematic, and metrical features to demarcate each poet's corpus. Each is contained within weighted similarity matrices, which are then appended to generate an aggregate graph showing poet-to-poet influence. Further network investigation is carried out to identify key poets, style hubs, and bridging poets by calculating degree, closeness, betweenness, eigenvector, and Katz centrality measures. Further, for typological insight, we use the Louvain community detection algorithm to demarcate clusters of poets sharing both style and theme coherence, which correspond closely to acknowledged schools of literature like Sabk-e Hindi, Sabk-e Khorasani, and the Bazgasht-e Adabi phenomenon. Our findings provide a new data-driven view of Persian literature distinguished between canonical significance and interextual influence, thus highlighting relatively lesser-known figures who hold great structural significance. Combining computational linguistics with literary study, this paper produces an interpretable and scalable model for poetic tradition, enabling retrospective reflection as well as forward-looking research within digital humanities.",,"Kourosh Shahnazari, Seyed Moein Ayyoubzadeh, Mohammadamin Fazli, Mohammadali Keshtparvar",2025-05-29T20:44:10Z,NAZM: Network Analysis of Zonal Metrics in Persian Poetic Tradition,NAZM: Netzwerkanalyse von Zonal Metrics in der persischen Poetischen Tradition,NAZM:波斯波斯波斯诗变传统中Zonal 计量器网络分析,http://arxiv.org/abs/2505.08052v2
1556,"In real-world drug design, molecule optimization requires selectively improving multiple molecular properties up to pharmaceutically relevant levels, while maintaining others that already meet such criteria. However, existing computational approaches and instruction-tuned LLMs fail to capture such nuanced property-specific objectives, limiting their practical applicability. To address this, we introduce C-MuMOInstruct, the first instruction-tuning dataset focused on multi-property optimization with explicit, property-specific objectives. Leveraging C-MuMOInstruct, we develop GeLLMO-Cs, a series of instruction-tuned LLMs that can perform targeted property-specific optimization. Our experiments across 5 in-distribution and 5 out-of-distribution tasks show that GeLLMO-Cs consistently outperform strong baselines, achieving up to 126% higher success rate. Notably, GeLLMO-Cs exhibit impressive 0-shot generalization to novel optimization tasks and unseen instructions. This offers a step toward a foundational LLM to support realistic, diverse optimizations with property-specific objectives. C-MuMOInstruct and code are accessible through https://github.com/ninglab/GeLLMO-C.",,"Vishal Dey, Xiao Hu, Xia Ning",2025-05-29T20:29:14Z,Large Language Models for Controllable Multi-property Multi-objective   Molecule Optimization,Große Sprachmodelle für steuerbare Multi-Eigentums-Multiobjektive Moleküloptimierung,可控多财产多目标分子优化大语言模型,http://arxiv.org/abs/2505.23987v1
1557,"Large language models (LLMs) demonstrate remarkable capabilities but face challenges from hallucinations, which typically arise from insufficient knowledge or context. While instructing LLMs to acknowledge knowledge limitations by responding with ""I don't know"" appears promising, we find that models consistently struggle with admitting knowledge gaps. This challenge may originate from current instruction datasets that emphasise answer generation over knowledge boundary awareness. To address this limitation, we introduce Uncertainty-and-Sensitivity-Aware Tuning (US-Tuning), a novel two-stage approach for contextual question answering (QA). The first stage enhances LLMs' ability to recognise their knowledge boundaries, while the second stage reinforces instruction adherence through carefully designed causal prompts. Our experimental results demonstrate that US-Tuning not only significantly reduces incorrect answers in contextual QA but also improves models' faithfulness to their parametric knowledge, mitigating hallucinations in general QA tasks. Our fine-tuned Llama2-7B model achieves up to a 34.7% improvement in handling out-of-knowledge questions and outperforms GPT-4 by 4.2% in overall performance.",,"Jiaqi Li, Yixuan Tang, Yi Yang",2025-05-29T20:18:17Z,Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction   Tuning,Kennen Sie das Unbekannte: Eine unsicher-sensitive Methode für LLM-Instruktionstuning,已知未知: LLM 教学图示的不确定性敏感度方法,http://arxiv.org/abs/2406.10099v3
1558,"Large Language Models (LLMs) have enabled remarkable progress in natural language processing, yet their high computational and memory demands pose challenges for deployment in resource-constrained environments. Although recent low-rank decomposition methods offer a promising path for structural compression, they often suffer from accuracy degradation, expensive calibration procedures, and result in inefficient model architectures that hinder real-world inference speedups. In this paper, we propose FLAT-LLM, a fast and accurate, training-free structural compression method based on fine-grained low-rank transformations in the activation space. Specifically, we reduce the hidden dimension by transforming the weights using truncated eigenvectors computed via head-wise Principal Component Analysis (PCA), and employ an importance-based metric to adaptively allocate ranks across decoders. FLAT-LLM achieves efficient and effective weight compression without recovery fine-tuning, which could complete the calibration within a few minutes. Evaluated across 4 models and 11 datasets, FLAT-LLM outperforms structural pruning baselines in generalization and downstream performance, while delivering inference speedups over decomposition-based methods.",,"Jiayi Tian, Ryan Solgi, Jinming Lu, Yifan Yang, Hai Li, Zheng Zhang",2025-05-29T19:42:35Z,FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for   Large Language Model Compression,FLAT-LLM: Feinkörnige Low-rank Aktivierung Raumtransformation für großsprachliche Modellkompression,FLAT-LLM: 用于大语言模型压缩的精制低级激活空间转换,http://arxiv.org/abs/2505.23966v1
1559,"Despite the remarkable success of large large-scale neural networks, we still lack unified notation for thinking about and describing their representational spaces. We lack methods to reliably describe how their representations are structured, how that structure emerges over training, and what kinds of structures are desirable. This thesis introduces quantitative methods for identifying systematic structure in a mapping between spaces, and leverages them to understand how deep-learning models learn to represent information, what representational structures drive generalisation, and how design decisions condition the structures that emerge. To do this I identify structural primitives present in a mapping, along with information theoretic quantifications of each. These allow us to analyse learning, structure, and generalisation across multi-agent reinforcement learning models, sequence-to-sequence models trained on a single task, and Large Language Models. I also introduce a novel, performant, approach to estimating the entropy of vector space, that allows this analysis to be applied to models ranging in size from 1 million to 12 billion parameters.   The experiments here work to shed light on how large-scale distributed models of cognition learn, while allowing us to draw parallels between those systems and their human analogs. They show how the structures of language and the constraints that give rise to them in many ways parallel the kinds of structures that drive performance of contemporary neural networks.",,Henry Conklin,2025-05-29T19:27:50Z,"Information Structure in Mappings: An Approach to Learning,   Representation, and Generalisation","Informationsstruktur in Mappings: Ein Ansatz zu Lernen, Repräsentation und Verallgemeinerung",制图信息结构:学习、代表性和一般化的方法,http://arxiv.org/abs/2505.23960v1
1560,"The rapid advancement of large language models (LLMs) has enabled the development of multi-agent systems where multiple LLM-based agents collaborate on complex tasks. However, existing systems often rely on centralized coordination, leading to scalability bottlenecks, reduced adaptability, and single points of failure. Privacy and proprietary knowledge concerns further hinder cross-organizational collaboration, resulting in siloed expertise. We propose AgentNet, a decentralized, Retrieval-Augmented Generation (RAG)-based framework that enables LLM-based agents to specialize, evolve, and collaborate autonomously in a dynamically structured Directed Acyclic Graph (DAG). Unlike prior approaches with static roles or centralized control, AgentNet allows agents to adjust connectivity and route tasks based on local expertise and context. AgentNet introduces three key innovations: (1) a fully decentralized coordination mechanism that eliminates the need for a central orchestrator, enhancing robustness and emergent intelligence; (2) dynamic agent graph topology that adapts in real time to task demands, ensuring scalability and resilience; and (3) a retrieval-based memory system for agents that supports continual skill refinement and specialization. By minimizing centralized control and data exchange, AgentNet enables fault-tolerant, privacy-preserving collaboration across organizations. Experiments show that AgentNet achieves higher task accuracy than both single-agent and centralized multi-agent baselines.",,"Yingxuan Yang, Huacan Chai, Shuai Shao, Yuanyi Song, Siyuan Qi, Renting Rui, Weinan Zhang",2025-05-29T18:55:08Z,AgentNet: Decentralized Evolutionary Coordination for LLM-based   Multi-Agent Systems,AgentNet: Dezentralisierte Evolutionskoordination für LLM-basierte Multi-Agent-Systeme,AgentNet: 以LLM为基础的多机构系统分散化演进协调,http://arxiv.org/abs/2504.00587v2
1561,"Chain-of-thought (CoT) reasoning enhances performance of large language models, but questions remain about whether these reasoning traces faithfully reflect the internal processes of the model. We present the first comprehensive study of CoT faithfulness in large vision-language models (LVLMs), investigating how both text-based and previously unexplored image-based biases affect reasoning and bias articulation. Our work introduces a novel, fine-grained evaluation pipeline for categorizing bias articulation patterns, enabling significantly more precise analysis of CoT reasoning than previous methods. This framework reveals critical distinctions in how models process and respond to different types of biases, providing new insights into LVLM CoT faithfulness. Our findings reveal that subtle image-based biases are rarely articulated compared to explicit text-based ones, even in models specialized for reasoning. Additionally, many models exhibit a previously unidentified phenomenon we term ``inconsistent'' reasoning - correctly reasoning before abruptly changing answers, serving as a potential canary for detecting biased reasoning from unfaithful CoTs. We then apply the same evaluation pipeline to revisit CoT faithfulness in LLMs across various levels of implicit cues. Our findings reveal that current language-only reasoning models continue to struggle with articulating cues that are not overtly stated.",,"Sriram Balasubramanian, Samyadeep Basu, Soheil Feizi",2025-05-29T18:55:05Z,A Closer Look at Bias and Chain-of-Thought Faithfulness of Large   (Vision) Language Models,Ein genauerer Blick auf Bias und Ketten-of-Thought Treue von großen (Vision) Sprachmodellen,更仔细地审视大(愿景)语言模式的偏见和寻求的连锁信任,http://arxiv.org/abs/2505.23945v1
1562,"Causality detection and mining are important tasks in information retrieval due to their enormous use in information extraction, and knowledge graph construction. To solve these tasks, in existing literature there exist several solutions -- both unsupervised and supervised. However, the unsupervised methods suffer from poor performance and they often require significant human intervention for causal rule selection, leading to poor generalization across different domains. On the other hand, supervised methods suffer from the lack of large training datasets. Recently, large language models (LLMs) with effective prompt engineering are found to be effective to overcome the issue of unavailability of large training dataset. Yet, in existing literature, there does not exist comprehensive works on causality detection and mining using LLM prompting. In this paper, we present several retrieval-augmented generation (RAG) based dynamic prompting schemes to enhance LLM performance in causality detection and extraction tasks. Extensive experiments over three datasets and five LLMs validate the superiority of our proposed RAG-based dynamic prompting over other static prompting schemes.",,"Thushara Manjari Naduvilakandy, Hyeju Jang, Mohammad Al Hasan",2025-05-29T18:51:00Z,Retrieval Augmented Generation based Large Language Models for Causality   Mining,Retrieval Augmented Generation basierte große Sprachmodelle für den Kausalitätsabbau,以世代为基础的因果采矿业大语言模型,http://arxiv.org/abs/2505.23944v1
1563,"Developing robust automatic speech recognition (ASR) systems for Arabic requires effective strategies to manage its diversity. Existing ASR systems mainly cover the modern standard Arabic (MSA) variety and few high-resource dialects, but fall short in coverage and generalization across the multitude of spoken variants. Code-switching with English and French is also common in different regions of the Arab world, which challenges the performance of monolingual Arabic models. In this work, we introduce a suite of ASR models optimized to effectively recognize multiple variants of spoken Arabic, including MSA, various dialects, and code-switching. We provide open-source pre-trained models that cover data from 17 Arabic-speaking countries, and fine-tuned MSA and dialectal ASR models that include at least 11 variants, as well as multi-lingual ASR models covering embedded languages in code-switched utterances. We evaluate ASR performance across these spoken varieties and demonstrate both coverage and performance gains compared to prior models.",,"Amirbek Djanibekov, Hawau Olamide Toyin, Raghad Alshalan, Abdullah Alitr, Hanan Aldarmaki",2025-05-29T18:48:44Z,Dialectal Coverage And Generalization in Arabic Speech Recognition,Dialektische Abdeckung und Verallgemeinerung in arabischer Spracherkennung,阿拉伯语语音识别,http://arxiv.org/abs/2411.05872v3
1564,"Robust alignment is vital for safely deploying large language models (LLMs). Existing techniques are either reward-based -- training a reward model on preference pairs and optimizing with reinforcement learning (RL) -- or reward-free -- directly fine-tuning on ranked outputs. Recent research shows that well-tuned reward-based pipelines remain the most robust, and single-response demonstrations can outperform pairwise preference data. However, two key challenges remain: (i) imbalanced safety datasets that over-represent common hazards while neglecting long-tail threats; and (ii) static reward models that ignore task difficulty, limiting optimization efficiency and attainable gains. To address these limitations, we propose \textbf{DR-IRL}, which dynamically adjusts rewards through inverse reinforcement learning. We first construct a balanced safety dataset of seven harmful categories using Chain-of-Draft (CoD) template prompts, which reduce token usage and generation time compared to Chain-of-Thought (CoT). We then train category-specific reward models on this dataset via IRL. Finally, to align the LLM, we introduce \textbf{GRPO-S} (Group Relative Policy Optimization--Scaling), a variant of GRPO that scales the reward during optimization to task difficulty -- data-level hardness measured by CLIP similarity and model-level responsiveness measured by reward gaps. Extensive experiments on multiple benchmarks and LLMs demonstrate that DR-IRL outperforms all baselines in safety alignment while maintaining usefulness.",,"Ruoxi Cheng, Haoxuan Ma, Weixin Wang, Zhiqiang Wang, Xiaoshuang Jia, Simeng Qin, Xiaochun Cao, Yang Liu, Xiaojun Jia",2025-05-29T18:47:59Z,Inverse Reinforcement Learning with Dynamic Reward Scaling for LLM   Alignment,Inverses Verstärkungslernen mit dynamischer Belohnungsskalierung für LLM-Ausrichtung,为LLLM一致化进行反向强化学习和动态回报增分缩放,http://arxiv.org/abs/2503.18991v3
1565,"Tool use has turned large language models (LLMs) into powerful agents that can perform complex multi-step tasks by dynamically utilising external software components. However, these tools must be implemented in advance by human developers, hindering the applicability of LLM agents in domains demanding large numbers of highly specialised tools, like in life sciences and medicine. Motivated by the growing trend of scientific studies accompanied by public code repositories, we propose ToolMaker, an agentic framework that autonomously transforms papers with code into LLM-compatible tools. Given a GitHub URL and short task description, ToolMaker autonomously installs dependencies and generates code to perform the task, using a closed-loop self-correction mechanism for debugging. To evaluate our approach, we introduce a benchmark comprising 15 complex computational tasks spanning various domains with over 100 unit tests to assess correctness and robustness. Our method correctly implements 80% of the tasks, substantially outperforming current state-of-the-art software engineering agents. ToolMaker therefore is a step towards fully autonomous agent-based scientific workflows. Our code and benchmark are publicly available at https://github.com/KatherLab/ToolMaker.",,"Georg Wölflein, Dyke Ferber, Daniel Truhn, Ognjen Arandjelović, Jakob Nikolas Kather",2025-05-29T18:47:41Z,LLM Agents Making Agent Tools,"LLM-Agenten, die Agenten-Werkzeuge herstellen",LLM LLM 代理代理代理代理代理商工具,http://arxiv.org/abs/2502.11705v2
1566,"Diabetes is a civilization chronic disease characterized by a constant elevated concentration of glucose in the blood. Many processes are involved in the glucose regulation, and their interactions are very complex. To better understand those processes we set ourselves a goal to create a Petri net model of the glucose regulation in the whole body. So far we have managed to create a model of glycolysis and synthesis of glucose in the liver, and the general overview models of the glucose regulation in a healthy and diabetic person. In this paper we introduce Petri nets models of insulin secretion in beta cell of the pancreas, and glucagon in the pancreas alpha cells. Those two hormones have mutually opposite effects: insulin preventing hyperglycemia, and glucagon preventing hypoglycemia. Understanding the mechanisms of insulin and glucagon secretion constitutes the basis for understanding diabetes. We also present a model in which both processes occur together, depending on the blood glucose level. The dynamics of each model is analysed. Additionally, we transform the overall insulin and glucagon secretion system to a Boolean network, following standard transformation rules.",,"Kamila Barylska, Franck Delaplace, Anna Gogolińska, Ewa Pańkowska",2025-05-29T18:35:16Z,Glucagon and insulin production in pancreatic cells modeled using Petri   nets and Boolean networks,"Glucagon- und Insulinproduktion in Pankreaszellen, modelliert mit Petrinetzen und Booleschen Netzwerken",利用Petri Net和Boolean网络模型制作的胰腺细胞格外贡和胰岛素生产,http://arxiv.org/abs/2504.21578v2
1567,"The think-aloud method, where participants voice their thoughts as they solve a task, is a valuable source of rich data about human reasoning processes. Yet, it has declined in popularity in contemporary cognitive science, largely because labor-intensive transcription and annotation preclude large sample sizes. Here, we develop methods to automate the transcription and annotation of verbal reports of reasoning using natural language processing tools, allowing for large-scale analysis of think-aloud data. In our study, 640 participants thought aloud while playing the Game of 24, a mathematical reasoning task. We automatically transcribed the recordings and coded the transcripts as search graphs, finding moderate inter-rater reliability with humans. We analyze these graphs and characterize consistency and variation in human reasoning traces. Our work demonstrates the value of think-aloud data at scale and serves as a proof of concept for the automated analysis of verbal reports.",,"Daniel Wurgaft, Ben Prystawski, Kanishk Gandhi, Cedegao E. Zhang, Joshua B. Tenenbaum, Noah D. Goodman",2025-05-29T18:26:23Z,Scaling up the think-aloud method,Skalierung der think-aloud-Methode,缩放智多图方法,http://arxiv.org/abs/2505.23931v1
1568,"Role-Playing Language Agents (RPLAs) aim to simulate characters for realistic and engaging human-computer interactions. However, traditional reward models often struggle with scalability and adapting to subjective conversational preferences. We propose ChARM, a Character-based Act-adaptive Reward Model, addressing these challenges through two innovations: (1) an act-adaptive margin that significantly enhances learning efficiency and generalizability, and (2) a self-evolution mechanism leveraging large-scale unlabeled data to improve training coverage. Additionally, we introduce RoleplayPref, the first large-scale preference dataset specifically for RPLAs, featuring 1,108 characters, 13 subcategories, and 16,888 bilingual dialogues, alongside RoleplayEval, a dedicated evaluation benchmark. Experimental results show a 13% improvement over the conventional Bradley-Terry model in preference rankings. Furthermore, applying ChARM-generated rewards to preference learning techniques (e.g., direct preference optimization) achieves state-of-the-art results on CharacterEval and RoleplayEval. Code and dataset are available at https://github.com/calubkk/ChARM.",,"Feiteng Fang, Ting-En Lin, Yuchuan Wu, Xiong Liu, Xiang Huang, Dingwei Chen, Jing Ye, Haonan Zhang, Liang Zhu, Hamid Alinejad-Rokny, Min Yang, Fei Huang, Yongbin Li",2025-05-29T18:15:18Z,ChARM: Character-based Act-adaptive Reward Modeling for Advanced   Role-Playing Language Agents,ChARM: Charakterbasiertes Act-adaptives Reward-Modelling für fortgeschrittene Rollenspiel-Sprachen-Agenten,CARM: 高级角色扮演语言媒介的基于性的行为适应性奖赏模型,http://arxiv.org/abs/2505.23923v1
1569,"Although long-video understanding demands that models capture hierarchical temporal information -- from clip (seconds) and shot (tens of seconds) to event (minutes) and story (hours) -- existing benchmarks either neglect this multi-scale design or scatter scale-specific questions across different videos, preventing direct comparison of model performance across timescales on the same content. To address this, we introduce ScaleLong, the first benchmark to disentangle these factors by embedding questions targeting four hierarchical timescales -- clip (seconds), shot (tens of seconds), event (minutes), and story (hours) -- all within the same video content. This within-content multi-timescale questioning design enables direct comparison of model performance across timescales on identical videos. ScaleLong features 269 long videos (avg.\ 86\,min) from 5 main categories and 36 sub-categories, with 4--8 carefully designed questions, including at least one question for each timescale. Evaluating 23 MLLMs reveals a U-shaped performance curve, with higher accuracy at the shortest and longest timescales and a dip at intermediate levels. Furthermore, ablation studies show that increased visual token capacity consistently enhances reasoning across all timescales. ScaleLong offers a fine-grained, multi-timescale benchmark for advancing MLLM capabilities in long-video understanding. The code and dataset are available https://github.com/multimodal-art-projection/ScaleLong.",,"David Ma, Huaqing Yuan, Xingjian Wang, Qianbo Zang, Tianci Liu, Xinyang He, Yanbin Wei, Jiawei Guo, Ni Jiahui, Zhenzhu Yang, Meng Cao, Shanghaoran Quan, Yizhi Li, Wangchunshu Zhou, Jiaheng Liu, Wenhao Huang, Ge Zhang, Shiwen Ni, Xiaojie Jin",2025-05-29T18:15:07Z,ScaleLong: A Multi-Timescale Benchmark for Long Video Understanding,ScaleLong: Ein Multi-Timescale-Benchmark für langes Videoverständnis,缩放表:长视频理解的多时间尺度基准,http://arxiv.org/abs/2505.23922v1
1570,"The ever-increasing size of open-source Large Language Models (LLMs) renders local deployment impractical for individual users. Decentralized computing has emerged as a cost-effective solution, allowing individuals and small companies to perform LLM inference for users using surplus computational power. However, a computing provider may stealthily substitute the requested LLM with a smaller, less capable model without consent from users, thereby benefiting from cost savings. We introduce SVIP, a secret-based verifiable LLM inference protocol. Unlike existing solutions based on cryptographic or game-theoretic techniques, our method is computationally effective and does not rest on strong assumptions. Our protocol requires the computing provider to return both the generated text and processed hidden representations from LLMs. We then train a proxy task on these representations, effectively transforming them into a unique model identifier. With our protocol, users can reliably verify whether the computing provider is acting honestly. A carefully integrated secret mechanism further strengthens its security. We thoroughly analyze our protocol under multiple strong and adaptive adversarial scenarios. Our extensive experiments demonstrate that SVIP is accurate, generalizable, computationally efficient, and resistant to various attacks. Notably, SVIP achieves false negative rates below 5% and false positive rates below 3%, while requiring less than 0.01 seconds per prompt query for verification.",,"Yifan Sun, Yuhang Li, Yue Zhang, Yuchen Jin, Huan Zhang",2025-05-29T18:09:31Z,SVIP: Towards Verifiable Inference of Open-source Large Language Models,SVIP: Auf dem Weg zu verifizierbarer Schlussfolgerung von Open-Source-Großsprachenmodellen,SVIP: 走向开放源码大语言模式的可核实推论,http://arxiv.org/abs/2410.22307v2
1571,"Large Language Models are widely used for content moderation but often misclassify benign comments as toxic, leading to over-sensitivity. While previous research attributes this issue primarily to the presence of offensive terms, we reveal a potential cause beyond token level: LLMs exhibit systematic topic biases in their implicit associations. Inspired by cognitive psychology's implicit association tests, we introduce Topic Association Analysis, a semantic-level approach to quantify how LLMs associate certain topics with toxicity. By prompting LLMs to generate free-form scenario imagination for misclassified benign comments and analyzing their topic amplification levels, we find that more advanced models (e.g., GPT-4 Turbo) demonstrate stronger topic stereotype despite lower overall false positive rates. These biases suggest that LLMs do not merely react to explicit, offensive language but rely on learned topic associations, shaping their moderation decisions. Our findings highlight the need for refinement beyond keyword-based filtering, providing insights into the underlying mechanisms driving LLM over-sensitivity.",,"Yuxin Wang, Botao Yu, Ivory Yang, Saeed Hassanpour, Soroush Vosoughi",2025-05-29T18:07:48Z,Probing Association Biases in LLM Moderation Over-Sensitivity,Probing Association Biases in LLM Moderation Überempfindlichkeit,检验协会LLM 中度过敏的分量协会,http://arxiv.org/abs/2505.23914v1
1572,"Hallucination remains a major challenge for the safe and trustworthy deployment of large language models (LLMs) in factual content generation. Prior work has explored confidence estimation as an effective approach to hallucination detection, but often relies on post-hoc self-consistency methods that require computationally expensive sampling. Verbalized confidence offers a more efficient alternative, but existing approaches are largely limited to short-form question answering (QA) tasks and do not generalize well to open-ended generation. In this paper, we propose LoVeC (Long-form Verbalized Confidence), an on-the-fly verbalized confidence estimation method for long-form generation. Specifically, we use reinforcement learning (RL) to train LLMs to append numerical confidence scores to each generated statement, serving as a direct and interpretable signal of the factuality of generation. Our experiments consider both on-policy and off-policy RL methods, including DPO, ORPO, and GRPO, to enhance the model calibration. We introduce two novel evaluation settings, free-form tagging and iterative tagging, to assess different verbalized confidence estimation methods. Experiments on three long-form QA datasets show that our RL-trained models achieve better calibration and generalize robustly across domains. Also, our method is highly efficient, as it only requires adding a few tokens to the output being decoded.",,"Caiqi Zhang, Xiaochen Zhu, Chengzu Li, Nigel Collier, Andreas Vlachos",2025-05-29T18:05:20Z,Reinforcement Learning for Better Verbalized Confidence in Long-Form   Generation,Verstärktes Lernen für ein besseres verbalisiertes Vertrauen in die lange Generation,加强学习促进长代人的更合理信任,http://arxiv.org/abs/2505.23912v1
1573,"In-context learning (ICL) enables Large Language Models (LLMs) to adapt to new tasks using few examples, with task vectors - specific hidden state activations - hypothesized to encode task information. Existing studies are limited by small-scale benchmarks, restricting comprehensive analysis. We introduce QuiteAFew, a novel dataset of 3,096 diverse few-shot tasks, each with 30 input-output pairs derived from the Alpaca dataset. Experiments with Llama-3-8B on QuiteAFew reveal: (1) task vector performance peaks at an intermediate layer (e.g., 15th), (2) effectiveness varies significantly by task type, and (3) complex tasks rely on multiple, subtask-specific vectors rather than a single vector, suggesting distributed task knowledge representation.",,"Pavel Tikhonov, Ivan Oseledets, Elena Tutubalina",2025-05-29T18:05:12Z,One Task Vector is not Enough: A Large-Scale Study for In-Context   Learning,Ein Aufgaben-Vektor ist nicht genug: Eine groß angelegte Studie zum In-Context-Lernen,一个任务矢量是不够的:一个内容内学习的大型研究,http://arxiv.org/abs/2505.23911v1
1574,"Conversational agents powered by large language models (LLMs) are rapidly becoming integral to our daily interactions, generating unprecedented amounts of conversational data. Such datasets offer a powerful lens into societal interests, trending topics, and collective concerns. Yet, existing approaches typically treat these interactions as independent and miss critical insights that could emerge from aggregating and reasoning across large-scale conversation logs. In this paper, we introduce Aggregative Question Answering, a novel task requiring models to reason explicitly over thousands of user-chatbot interactions to answer aggregative queries, such as identifying emerging concerns among specific demographics. To enable research in this direction, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative questions derived from 182,330 real-world chatbot conversations. Experiments show that existing methods either struggle to reason effectively or incur prohibitive computational costs, underscoring the need for new approaches capable of extracting collective insights from large-scale conversational data.",,"Wentao Zhang, Woojeong Kim, Yuntian Deng",2025-05-29T17:59:55Z,From Chat Logs to Collective Insights: Aggregative Question Answering,Von Chat Logs zu Collective Insights: Aggregative Question Answering,从聊天日志到集体透视:聚合问题解答,http://arxiv.org/abs/2505.23765v1
1575,"Spatial intelligence is essential for multimodal large language models (MLLMs) operating in the complex physical world. Existing benchmarks, however, probe only single-image relations and thus fail to assess the multi-image spatial reasoning that real-world deployments demand. We introduce MMSI-Bench, a VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision researchers spent more than 300 hours meticulously crafting 1,000 challenging, unambiguous multiple-choice questions from over 120,000 images, each paired with carefully designed distractors and a step-by-step reasoning process. We conduct extensive experiments and thoroughly evaluate 34 open-source and proprietary MLLMs, observing a wide gap: the strongest open-source model attains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while humans score 97%. These results underscore the challenging nature of MMSI-Bench and the substantial headroom for future research. Leveraging the annotated reasoning processes, we also provide an automated error analysis pipeline that diagnoses four dominant failure modes, including (1) grounding errors, (2) overlap-matching and scene-reconstruction errors, (3) situation-transformation reasoning errors, and (4) spatial-logic errors, offering valuable insights for advancing multi-image spatial intelligence. Project page: https://runsenxu.com/projects/MMSI_Bench .",,"Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, Dahua Lin, Tai Wang, Jiangmiao Pang",2025-05-29T17:59:52Z,MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence,MMSI-Bench: Ein Benchmark für multi-Image-Spatial Intelligence,MMSI-Bunch:多图像空间情报基准,http://arxiv.org/abs/2505.23764v1
1576,"The rapid advancement of large Vision-Language Models (VLMs) has propelled the development of pure-vision-based GUI Agents, capable of perceiving and operating Graphical User Interfaces (GUI) to autonomously fulfill user instructions. However, existing approaches usually adopt an offline learning framework, which faces two core limitations: (1) heavy reliance on high-quality manual annotations for element grounding and action supervision, and (2) limited adaptability to dynamic and interactive environments. To address these limitations, we propose ZeroGUI, a scalable, online learning framework for automating GUI Agent training at Zero human cost. Specifically, ZeroGUI integrates (i) VLM-based automatic task generation to produce diverse training goals from the current environment state, (ii) VLM-based automatic reward estimation to assess task success without hand-crafted evaluation functions, and (iii) two-stage online reinforcement learning to continuously interact with and learn from GUI environments. Experiments on two advanced GUI Agents (UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance across OSWorld and AndroidLab environments. The code is available at https://github.com/OpenGVLab/ZeroGUI.",,"Chenyu Yang, Shiqian Su, Shi Liu, Xuan Dong, Yue Yu, Weijie Su, Xuehui Wang, Zhaoyang Liu, Jinguo Zhu, Hao Li, Wenhai Wang, Yu Qiao, Xizhou Zhu, Jifeng Dai",2025-05-29T17:59:51Z,ZeroGUI: Automating Online GUI Learning at Zero Human Cost,ZeroGUI: Automatisieren des Online-GUI-Lernens zu null menschlichen Kosten,零GUI: 实现零人成本在线用户界面学习自动化,http://arxiv.org/abs/2505.23762v1
1577,"Direct Preference Optimization (DPO) has become a standard technique for aligning language models with human preferences in a supervised manner. Despite its empirical success, the theoretical justification behind its log-ratio reward parameterization remains incomplete. In this work, we address this gap by utilizing the Differential Information Distribution (DID): a distribution over token sequences that captures the information gained during policy updates. First, we show that when preference labels encode the differential information required to transform a reference policy into a target policy, the log-ratio reward in DPO emerges as the uniquely optimal form for learning the target policy via preference optimization. This result naturally yields a closed-form expression for the optimal sampling distribution over rejected responses. Second, we find that the condition for preferences to encode differential information is fundamentally linked to an implicit assumption regarding log-margin ordered policies-an inductive bias widely used in preference optimization yet previously unrecognized. Finally, by analyzing the entropy of the DID, we characterize how learning low-entropy differential information reinforces the policy distribution, while high-entropy differential information induces a smoothing effect, which explains the log-likelihood displacement phenomenon. We validate our theoretical findings in synthetic experiments and extend them to real-world instruction-following datasets. Our results suggest that learning high-entropy differential information is crucial for general instruction-following, while learning low-entropy differential information benefits knowledge-intensive question answering. Overall, our work presents a unifying perspective on the DPO objective, the structure of preference data, and resulting policy behaviors through the lens of differential information.",,"Yunjae Won, Hyunji Lee, Hyeonbin Hwang, Minjoon Seo",2025-05-29T17:59:50Z,Differential Information: An Information-Theoretic Perspective on   Preference Optimization,Differentialinformation: Eine informationstheoretische Perspektive zur Preference-Optimierung,差别信息:关于首选优化的信息理论观点,http://arxiv.org/abs/2505.23761v1
1578,"Rebus puzzles, visual riddles that encode language through imagery, spatial arrangement, and symbolic substitution, pose a unique challenge to current vision-language models (VLMs). Unlike traditional image captioning or question answering tasks, rebus solving requires multi-modal abstraction, symbolic reasoning, and a grasp of cultural, phonetic and linguistic puns. In this paper, we investigate the capacity of contemporary VLMs to interpret and solve rebus puzzles by constructing a hand-generated and annotated benchmark of diverse English-language rebus puzzles, ranging from simple pictographic substitutions to spatially-dependent cues (""head"" over ""heels""). We analyze how different VLMs perform, and our findings reveal that while VLMs exhibit some surprising capabilities in decoding simple visual clues, they struggle significantly with tasks requiring abstract reasoning, lateral thinking, and understanding visual metaphors.",,"Heekyung Lee, Jiaxin Ge, Tsung-Han Wu, Minwoo Kang, Trevor Darrell, David M. Chan",2025-05-29T17:59:47Z,Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint,Puzzlet von Puzzles: Wenn Vision-Language-Modelle keinen Hinweis aufnehmen können,由谜题拼取的谜题: 当视觉语言模型无法使用提示时,http://arxiv.org/abs/2505.23759v1
1579,"Transformers have been established as the most popular backbones in sequence modeling, mainly due to their effectiveness in in-context retrieval tasks and the ability to learn at scale. Their quadratic memory and time complexity, however, bound their applicability in longer sequences and so has motivated researchers to explore effective alternative architectures such as modern recurrent neural networks (a.k.a long-term recurrent memory module). Despite their recent success in diverse downstream tasks, they struggle in tasks that requires long context understanding and extrapolation to longer sequences. We observe that these shortcomings come from three disjoint aspects in their design: (1) limited memory capacity that is bounded by the architecture of memory and feature mapping of the input; (2) online nature of update, i.e., optimizing the memory only with respect to the last input; and (3) less expressive management of their fixed-size memory. To enhance all these three aspects, we present ATLAS, a long-term memory module with high capacity that learns to memorize the context by optimizing the memory based on the current and past tokens, overcoming the online nature of long-term memory models. Building on this insight, we present a new family of Transformer-like architectures, called DeepTransformers, that are strict generalizations of the original Transformer architecture. Our experimental results on language modeling, common-sense reasoning, recall-intensive, and long-context understanding tasks show that ATLAS surpasses the performance of Transformers and recent linear recurrent models. ATLAS further improves the long context performance of Titans, achieving +80\% accuracy in 10M context length of BABILong benchmark.",,"Ali Behrouz, Zeman Li, Praneeth Kacham, Majid Daliri, Yuan Deng, Peilin Zhong, Meisam Razaviyayn, Vahab Mirrokni",2025-05-29T17:57:16Z,ATLAS: Learning to Optimally Memorize the Context at Test Time,ATLAS: Optimales Erlernen des Kontextes zur Testzeit,ATLAS: 学习在测试时最充分记住上下文,http://arxiv.org/abs/2505.23735v1
1580,"The emergence of large language model (LLM)-based agents has significantly advanced the development of autonomous machine learning (ML) engineering. However, most existing approaches rely heavily on manual prompt engineering, failing to adapt and optimize based on diverse experimental experiences. Focusing on this, for the first time, we explore the paradigm of learning-based agentic ML, where an LLM agent learns through interactive experimentation on ML tasks using online reinforcement learning (RL). To realize this, we propose a novel agentic ML training framework with three key components: (1) exploration-enriched fine-tuning, which enables LLM agents to generate diverse actions for enhanced RL exploration; (2) step-wise RL, which enables training on a single action step, accelerating experience collection and improving training efficiency; (3) an agentic ML-specific reward module, which unifies varied ML feedback signals into consistent rewards for RL optimization. Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our 7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it achieves continuous performance improvements and demonstrates exceptional cross-task generalization capabilities.",,"Zexi Liu, Jingyi Chai, Xinyu Zhu, Shuo Tang, Rui Ye, Bo Zhang, Lei Bai, Siheng Chen",2025-05-29T17:54:44Z,ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning   Engineering,ML-Agent: Verstärkung von LLM-Agenten für autonome Maschinenbautechnik,ML-代理:加强自动机械学习工程的LLM代理,http://arxiv.org/abs/2505.23723v1
1581,"In-context learning (ICL) enables large language models (LLMs) to perform new tasks using only a few demonstrations. In Named Entity Recognition (NER), demonstrations are typically selected based on semantic similarity to the test instance, ignoring training labels and resulting in suboptimal performance. We introduce DEER, a new method that leverages training labels through token-level statistics to improve ICL performance. DEER first enhances example selection with a label-guided, token-based retriever that prioritizes tokens most informative for entity recognition. It then prompts the LLM to revisit error-prone tokens, which are also identified using label statistics, and make targeted corrections. Evaluated on five NER datasets using four different LLMs, DEER consistently outperforms existing ICL methods and approaches the performance of supervised fine-tuning. Further analysis shows its effectiveness on both seen and unseen entities and its robustness in low-resource settings.",,"Fan Bai, Hamid Hassanzadeh, Ardavan Saeedi, Mark Dredze",2025-05-29T17:54:32Z,Label-Guided In-Context Learning for Named Entity Recognition,Labelgeführtes In-Context-Lernen für die benannte Entitätserkennung,为识别命名实体进行Label-Guided InFincle 学习,http://arxiv.org/abs/2505.23722v1
1582,"Direct Preference Optimization (DPO) is a widely adopted offline algorithm for preference-based reinforcement learning from human feedback (RLHF), designed to improve training simplicity and stability by redefining reward functions. However, DPO is hindered by several limitations, including length bias, memory inefficiency, and probability degradation. To address these challenges, we propose Length-Controlled Margin-Based Preference Optimization (LMPO), a more efficient and robust alternative. LMPO introduces a uniform reference model as an upper bound for the DPO loss, enabling a more accurate approximation of the original optimization objective. Additionally, an average log-probability optimization strategy is employed to minimize discrepancies between training and inference phases. A key innovation of LMPO lies in its Length-Controlled Margin-Based loss function, integrated within the Bradley-Terry framework. This loss function regulates response length while simultaneously widening the margin between preferred and rejected outputs. By doing so, it mitigates probability degradation for both accepted and discarded responses, addressing a significant limitation of existing methods. We evaluate LMPO against state-of-the-art preference optimization techniques on two open-ended large language models, Mistral and LLaMA3, across six conditional benchmarks. Our experimental results demonstrate that LMPO effectively controls response length, reduces probability degradation, and outperforms existing approaches. The code is available at https://github.com/gengxuli/LMPO.",,"Gengxu Li, Tingyu Xia, Yi Chang, Yuan Wu",2025-05-29T17:52:30Z,Length-Controlled Margin-Based Preference Optimization without Reference   Model,Längengesteuerte Margenbasierte Preference-Optimierung ohne Referenzmodell,无参考模型的优化,http://arxiv.org/abs/2502.14643v2
1583,"Large Language Model (LLM)-based multi-agent systems show promise for automating real-world tasks but struggle to transfer across domains due to their domain-specific nature. Current approaches face two critical shortcomings: they require complete architectural redesign and full retraining of all components when applied to new domains. We introduce Workforce, a hierarchical multi-agent framework that decouples strategic planning from specialized execution through a modular architecture comprising: (i) a domain-agnostic Planner for task decomposition, (ii) a Coordinator for subtask management, and (iii) specialized Workers with domain-specific tool-calling capabilities. This decoupling enables cross-domain transferability during both inference and training phases: During inference, Workforce seamlessly adapts to new domains by adding or modifying worker agents; For training, we introduce Optimized Workforce Learning (OWL), which improves generalization across domains by optimizing a domain-agnostic planner with reinforcement learning from real-world feedback. To validate our approach, we evaluate Workforce on the GAIA benchmark, covering various realistic, multi-domain agentic tasks. Experimental results demonstrate Workforce achieves open-source state-of-the-art performance (69.70%), outperforming commercial systems like OpenAI's Deep Research by 2.34%. More notably, our OWL-trained 32B model achieves 52.73% accuracy (+16.37%) and demonstrates performance comparable to GPT-4o on challenging tasks. To summarize, by enabling scalable generalization and modular domain transfer, our work establishes a foundation for the next generation of general-purpose AI assistants.",,"Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru Li, Qiguang Chen, Zeyu Zhang, Yifeng Wang, Qianshuo Ye, Bernard Ghanem, Ping Luo, Guohao Li",2025-05-29T17:51:58Z,OWL: Optimized Workforce Learning for General Multi-Agent Assistance in   Real-World Task Automation,OWL: Optimiertes Workforce-Learning für die allgemeine Multi-Agent-Hilfe in der Real-World Task Automation,"OWL: 优化劳动力学习,为现实世界任务自动化提供一般多机构援助",http://arxiv.org/abs/2505.23885v1
1584,"Test-Time Training (TTT) models context dependencies by adapting part of the model's weights (referred to as fast weights) during inference. This fast weight, akin to recurrent states in RNNs, stores temporary memories of past tokens in the current sequence. Existing TTT methods struggled to show effectiveness in handling long-context data, due to their inefficiency on modern GPUs. The TTT layers in many of these approaches operate with extremely low FLOPs utilization (often <5%) because they deliberately apply small online minibatch sizes (e.g., updating fast weights every 16 or 64 tokens). Moreover, a small minibatch implies fine-grained block-wise causal dependencies in the data, unsuitable for data beyond 1D ordered sequences, like sets or N-dimensional grids such as images or videos. In contrast, we pursue the opposite direction by using an extremely large chunk update, ranging from 2K to 1M tokens across tasks of varying modalities, which we refer to as Large Chunk Test-Time Training (LaCT). It improves hardware utilization by orders of magnitude, and more importantly, facilitates scaling of nonlinear state size (up to 40% of model parameters), hence substantially improving state capacity, all without requiring cumbersome and error-prone kernel implementations. It also allows easy integration of sophisticated optimizers, e.g. Muon for online updates. We validate our approach across diverse modalities and tasks, including novel view synthesis with image set, language models, and auto-regressive video diffusion. Our approach can scale up to 14B-parameter AR video diffusion model on sequences up to 56K tokens. In our longest sequence experiment, we perform novel view synthesis with 1 million context length. We hope this work will inspire and accelerate new research in the field of long-context modeling and test-time training. Website: https://tianyuanzhang.com/projects/ttt-done-right",,"Tianyuan Zhang, Sai Bi, Yicong Hong, Kai Zhang, Fujun Luan, Songlin Yang, Kalyan Sunkavalli, William T. Freeman, Hao Tan",2025-05-29T17:50:34Z,Test-Time Training Done Right,Test-Zeit-Training richtig durchgeführt,完成的试验时间训练,http://arxiv.org/abs/2505.23884v1
1585,"Large language models (LLMs) have witnessed rapid advancements, demonstrating remarkable capabilities. However, a notable vulnerability persists: LLMs often uncritically accept flawed or contradictory premises, leading to inefficient reasoning and unreliable outputs. This emphasizes the significance of possessing the \textbf{Premise Critique Ability} for LLMs, defined as the capacity to proactively identify and articulate errors in input premises. Most existing studies assess LLMs' reasoning ability in ideal settings, largely ignoring their vulnerabilities when faced with flawed premises. Thus, we introduce the \textbf{Premise Critique Bench (PCBench)}, designed by incorporating four error types across three difficulty levels, paired with multi-faceted evaluation metrics. We conducted systematic evaluations of 15 representative LLMs. Our findings reveal: (1) Most models rely heavily on explicit prompts to detect errors, with limited autonomous critique; (2) Premise critique ability depends on question difficulty and error type, with direct contradictions being easier to detect than complex or procedural errors; (3) Reasoning ability does not consistently correlate with the premise critique ability; (4) Flawed premises trigger overthinking in reasoning models, markedly lengthening responses due to repeated attempts at resolving conflicts. These insights underscore the urgent need to enhance LLMs' proactive evaluation of input validity, positioning premise critique as a foundational capability for developing reliable, human-centric systems. The code is available at https://github.com/MLGroupJLU/Premise_Critique.",,"Jinzhe Li, Gengxu Li, Yi Chang, Yuan Wu",2025-05-29T17:49:44Z,Don't Take the Premise for Granted: Evaluating the Premise Critique   Ability of Large Language Models,Nehmen Sie nicht die Prämisse für gewährt: Bewertung der Premise Critique Fähigkeit von großen Sprachmodellen,评估大语言模型的精密克里米亚能力,http://arxiv.org/abs/2505.23715v1
1586,"Foundation models trained at scale exhibit remarkable emergent behaviors, learning new capabilities beyond their initial training objectives. We find such emergent behaviors in biological vision models via large-scale contrastive vision-language training. To achieve this, we first curate TreeOfLife-200M, comprising 214 million images of living organisms, the largest and most diverse biological organism image dataset to date. We then train BioCLIP 2 on TreeOfLife-200M to distinguish different species. Despite the narrow training objective, BioCLIP 2 yields extraordinary accuracy when applied to various biological visual tasks such as habitat classification and trait prediction. We identify emergent properties in the learned embedding space of BioCLIP 2. At the inter-species level, the embedding distribution of different species aligns closely with functional and ecological meanings (e.g., beak sizes and habitats). At the intra-species level, instead of being diminished, the intra-species variations (e.g., life stages and sexes) are preserved and better separated in subspaces orthogonal to inter-species distinctions. We provide formal proof and analyses to explain why hierarchical supervision and contrastive objectives encourage these emergent properties. Crucially, our results reveal that these properties become increasingly significant with larger-scale training data, leading to a biologically meaningful embedding space.",,"Jianyang Gu, Samuel Stevens, Elizabeth G Campolongo, Matthew J Thompson, Net Zhang, Jiaman Wu, Andrei Kopanev, Zheda Mai, Alexander E. White, James Balhoff, Wasila Dahdul, Daniel Rubenstein, Hilmar Lapp, Tanya Berger-Wolf, Wei-Lun Chao, Yu Su",2025-05-29T17:48:20Z,BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive   Learning,BioCLIP 2: Emergente Eigenschaften von Skalierung Hierarchische Kontrastives Lernen,BioCLIP 2: 扩大等级差异学习的新兴属性,http://arxiv.org/abs/2505.23883v1
1587,"This paper addresses the critical need for high-quality evaluation datasets in low-resource languages to advance cross-lingual transfer. While cross-lingual transfer offers a key strategy for leveraging multilingual pretraining to expand language technologies to understudied and typologically diverse languages, its effectiveness is dependent on quality and suitable benchmarks. We release new sense-annotated datasets of sentences containing polysemous words, spanning nine low-resource languages across diverse language families and scripts. To facilitate dataset creation, the paper presents a demonstrably beneficial semi-automatic annotation method. The utility of the datasets is demonstrated through Word-in-Context (WiC) formatted experiments that evaluate transfer on these low-resource languages. Results highlight the importance of targeted dataset creation and evaluation for effective polysemy disambiguation in low-resource settings and transfer studies. The released datasets and code aim to support further research into fair, robust, and truly multilingual NLP.",,"Roksana Goworek, Harpal Karlcut, Muhammad Shezad, Nijaguna Darshana, Abhishek Mane, Syam Bondada, Raghav Sikka, Ulvi Mammadov, Rauf Allahverdiyev, Sriram Purighella, Paridhi Gupta, Muhinyia Ndegwa, Haim Dubossarsky",2025-05-29T17:48:08Z,SenWiCh: Sense-Annotation of Low-Resource Languages for WiC using Hybrid   Methods,SenWiCh: Sense-Annotation von Low-Resource-Sprachen für WiC mit Hybrid-Methoden,SenWiCH: 使用混合方法为无线电通信中心提供低资源语言的高级说明,http://arxiv.org/abs/2505.23714v1
1588,"Large language models (LLMs) are increasingly applied to socially grounded tasks, such as online community moderation, media content analysis, and social reasoning games. Success in these contexts depends on a model's social reasoning ability - the capacity to interpret social contexts, infer others' mental states, and assess the truthfulness of presented information. However, there is currently no systematic evaluation framework that comprehensively assesses the social reasoning capabilities of LLMs. Existing efforts often oversimplify real-world scenarios and consist of tasks that are too basic to challenge advanced models. To address this gap, we introduce SocialMaze, a new benchmark specifically designed to evaluate social reasoning. SocialMaze systematically incorporates three core challenges: deep reasoning, dynamic interaction, and information uncertainty. It provides six diverse tasks across three key settings: social reasoning games, daily-life interactions, and digital community platforms. Both automated and human validation are used to ensure data quality. Our evaluation reveals several key insights: models vary substantially in their ability to handle dynamic interactions and integrate temporally evolving information; models with strong chain-of-thought reasoning perform better on tasks requiring deeper inference beyond surface-level cues; and model reasoning degrades significantly under uncertainty. Furthermore, we show that targeted fine-tuning on curated reasoning examples can greatly improve model performance in complex social scenarios. The dataset is publicly available at: https://huggingface.co/datasets/MBZUAI/SocialMaze",,"Zixiang Xu, Yanbo Wang, Yue Huang, Jiayi Ye, Haomin Zhuang, Zirui Song, Lang Gao, Chenxi Wang, Zhaorun Chen, Yujun Zhou, Sixian Li, Wang Pan, Yue Zhao, Jieyu Zhao, Xiangliang Zhang, Xiuying Chen",2025-05-29T17:47:36Z,SocialMaze: A Benchmark for Evaluating Social Reasoning in Large   Language Models,SocialMaze: Ein Benchmark für die Bewertung sozialer Vernunft in großen Sprachmodellen,社会领域:用大语言模式评价社会原因的基准,http://arxiv.org/abs/2505.23713v1
1589,"Large Language Models (LLMs) with reasoning are trained to iteratively generate and refine their answers before finalizing them, which can help with applications to mathematics and code generation. We apply code generation with reasoning LLMs to a specific task in the mathematical field of combinatorial design. This field studies diverse types of combinatorial designs, many of which have lists of open instances for which existence has not yet been determined. The Constructive Protocol CPro1 uses LLMs to generate search heuristics that have the potential to construct solutions to small open instances. Starting with a textual definition and a validity verifier for a particular type of design, CPro1 guides LLMs to select and implement strategies, while providing automated hyperparameter tuning and execution feedback. CPro1 with reasoning LLMs successfully solves long-standing open instances for 7 of 16 combinatorial design problems selected from the 2006 Handbook of Combinatorial Designs, including new solved instances for 3 of these (Bhaskar Rao Designs, Symmetric Weighing Matrices, Balanced Ternary Designs) that were unsolved by CPro1 with non-reasoning LLMs. It also solves open instances for several problems from recent (2025) literature, generating new Covering Sequences, Johnson Clique Covers, Deletion Codes, and a Uniform Nested Steiner Quadruple System.",,Christopher D. Rosin,2025-05-29T17:45:50Z,Using Reasoning Models to Generate Search Heuristics that Solve Open   Instances of Combinatorial Design Problems,"Verwenden von vernünftigen Modellen zur Generierung von Such-Heuristiken, die offene Instanzen kombinatorischer Design-Probleme lösen",利用理性模型来产生解决组合设计问题公开事例的搜索力理论,http://arxiv.org/abs/2505.23881v1
1590,"Spatial reasoning based on natural language expressions is essential for everyday human tasks. This reasoning ability is also crucial for machines to interact with their environment in a human-like manner. However, recent research shows that even state-of-the-art language models struggle with spatial reasoning over text, especially when facing nesting spatial expressions. This is attributed to not achieving the right level of abstraction required for generalizability. To alleviate this issue, we propose training language models with neuro-symbolic techniques that exploit the spatial logical rules as constraints, providing additional supervision to improve spatial reasoning and question answering. Training language models to adhere to spatial reasoning rules guides them in making more effective and general abstractions for transferring spatial knowledge to various domains. We evaluate our approach on existing spatial question-answering benchmarks. Our results indicate the effectiveness of our proposed technique in improving language models in complex multi-hop spatial reasoning over text.",,"Tanawan Premsri, Parisa Kordjamshidi",2025-05-29T17:44:12Z,Neuro-symbolic Training for Reasoning over Spatial Language,Neuro-symbolisches Training zur Vernunft über räumliche Sprache,以空间语言为借口的神经主义培训,http://arxiv.org/abs/2406.13828v3
1591,"AI agents are increasingly used in consumer-facing applications to assist with tasks such as product search, negotiation, and transaction execution. In this paper, we explore a future scenario where both consumers and merchants authorize AI agents to fully automate negotiations and transactions. We aim to answer two key questions: (1) Do different LLM agents vary in their ability to secure favorable deals for users? (2) What risks arise from fully automating deal-making with AI agents in consumer markets? To address these questions, we develop an experimental framework that evaluates the performance of various LLM agents in real-world negotiation and transaction settings. Our findings reveal that AI-mediated deal-making is an inherently imbalanced game -- different agents achieve significantly different outcomes for their users. Moreover, behavioral anomalies in LLMs can result in financial losses for both consumers and merchants, such as overspending or accepting unreasonable deals. These results underscore that while automation can improve efficiency, it also introduces substantial risks. Users should exercise caution when delegating business decisions to AI agents.",,"Shenzhe Zhu, Jiao Sun, Yi Nian, Tobin South, Alex Pentland, Jiaxin Pei",2025-05-29T17:41:39Z,The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and   Transactions in Consumer Markets,"Das automatisierte, aber riskante Spiel: Modellierung von Agent-zu-Agent-Verhandlungen und Transaktionen in Verbrauchermärkten",自动但有风险游戏:消费者市场代理对代理谈判和交易的模拟,http://arxiv.org/abs/2506.00073v1
1592,"Final-answer-based metrics are commonly used for evaluating large language models (LLMs) on math word problems, often taken as proxies for reasoning ability. However, such metrics conflate two distinct sub-skills: abstract formulation (capturing mathematical relationships using expressions) and arithmetic computation (executing the calculations). Through a disentangled evaluation on GSM8K and SVAMP, we find that the final-answer accuracy of Llama-3 and Qwen2.5 (1B-32B) without CoT is overwhelmingly bottlenecked by the arithmetic computation step and not by the abstract formulation step. Contrary to the common belief, we show that CoT primarily aids in computation, with limited impact on abstract formulation. Mechanistically, we show that these two skills are composed conjunctively even in a single forward pass without any reasoning steps via an abstract-then-compute mechanism: models first capture problem abstractions, then handle computation. Causal patching confirms these abstractions are present, transferable, composable, and precede computation. These behavioural and mechanistic findings highlight the need for disentangled evaluation to accurately assess LLM reasoning and to guide future improvements.",,"Ziling Cheng, Meng Cao, Leila Pishdad, Yanshuai Cao, Jackie Chi Kit Cheung",2025-05-29T17:37:57Z,Can LLMs Reason Abstractly Over Math Word Problems Without CoT?   Disentangling Abstract Formulation From Arithmetic Computation,Kann LLMs abstrakt über Math Word Probleme ohne CoT? Entwirren Abstrakte Formulierung von Arithmetik Computation,"没有 CoT,LLMs 理学原理可以抽象地克服数学词问题吗?",http://arxiv.org/abs/2505.23701v1
1593,"MLLMs have been widely studied for video question answering recently. However, most existing assessments focus on natural videos, overlooking synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in video generation rely on MLLMs to evaluate the quality of generated videos, but the capabilities of MLLMs on interpreting AIGC videos remain largely underexplored. To address this, we propose a new benchmark, VF-Eval, which introduces four tasks-coherence validation, error awareness, error type detection, and reasoning evaluation-to comprehensively evaluate the abilities of MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that even the best-performing model, GPT-4.1, struggles to achieve consistently good performance across all tasks. This highlights the challenging nature of our benchmark. Additionally, to investigate the practical applications of VF-Eval in improving video generation, we conduct an experiment, RePrompt, demonstrating that aligning MLLMs more closely with human feedback can benefit video generation.",,"Tingyu Song, Tongyan Hu, Guo Gan, Yilun Zhao",2025-05-29T17:31:13Z,VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC   Videos,VF-Eval: Bewertung multimodaler LLMs zur Erzeugung von Feedback auf AIGC-Videos,"VF-Eval:评价多式LLMs,以生成对AIGC视频的反馈",http://arxiv.org/abs/2505.23693v1
1594,"Seminal work by Huebner et al. (2021) showed that language models (LMs) trained on English Child-Directed Language (CDL) can reach similar syntactic abilities as LMs trained on much larger amounts of adult-directed written text, suggesting that CDL could provide more effective LM training material than the commonly used internet-crawled data. However, the generalizability of these results across languages, model types, and evaluation settings remains unclear. We test this by comparing models trained on CDL vs. Wikipedia across two LM objectives (masked and causal), three languages (English, French, German), and three syntactic minimal-pair benchmarks. Our results on these benchmarks show inconsistent benefits of CDL, which in most cases is outperformed by Wikipedia models. We then identify various shortcomings in previous benchmarks, and introduce a novel testing methodology, FIT-CLAMS, which uses a frequency-controlled design to enable balanced comparisons across training corpora. Through minimal pair evaluations and regression analysis we show that training on CDL does not yield stronger generalizations for acquiring syntax and highlight the importance of controlling for frequency effects when evaluating syntactic ability.",,"Francesca Padovani, Jaap Jumelet, Yevgen Matusevych, Arianna Bisazza",2025-05-29T17:25:36Z,Child-Directed Language Does Not Consistently Boost Syntax Learning in   Language Models,Kinderorientierte Sprache fördert nicht konsequent das Syntax-Lernen in Sprachmodellen,在语言模式中促进语法学习,http://arxiv.org/abs/2505.23689v1
1595,"This paper investigates how prompt engineering techniques impact both accuracy and confidence elicitation in Large Language Models (LLMs) applied to medical contexts. Using a stratified dataset of Persian board exam questions across multiple specialties, we evaluated five LLMs - GPT-4o, o3-mini, Llama-3.3-70b, Llama-3.1-8b, and DeepSeek-v3 - across 156 configurations. These configurations varied in temperature settings (0.3, 0.7, 1.0), prompt styles (Chain-of-Thought, Few-Shot, Emotional, Expert Mimicry), and confidence scales (1-10, 1-100). We used AUC-ROC, Brier Score, and Expected Calibration Error (ECE) to evaluate alignment between confidence and actual performance. Chain-of-Thought prompts improved accuracy but also led to overconfidence, highlighting the need for calibration. Emotional prompting further inflated confidence, risking poor decisions. Smaller models like Llama-3.1-8b underperformed across all metrics, while proprietary models showed higher accuracy but still lacked calibrated confidence. These results suggest prompt engineering must address both accuracy and uncertainty to be effective in high-stakes medical tasks.",,"Nariman Naderi, Zahra Atf, Peter R Lewis, Aref Mahjoub far, Seyed Amir Ahmad Safavi-Naini, Ali Soroush",2025-05-29T17:13:26Z,Evaluating Prompt Engineering Techniques for Accuracy and Confidence   Elicitation in Medical LLMs,Evaluieren von Prompt Engineering-Techniken für Genauigkeit und vertrauensvolle Elizitation in medizinischen LLMs,评估医疗LLM中准确度和信任度的迅速工程技术,http://arxiv.org/abs/2506.00072v1
1596,"Transformer-based large language models suffer from quadratic complexity at inference on long sequences. Linear attention methods are efficient alternatives, however, they fail to provide an accurate approximation of softmax attention. By additionally incorporating sliding window attention into each linear attention head, this gap can be closed for short context-length tasks. Unfortunately, these approaches cannot recall important information from long contexts due to ""memory collisions"". In this paper , we propose LoLA: Low-rank Linear Attention with sparse caching. LoLA separately stores additional key-value pairs that would otherwise interfere with past associative memories. Moreover, LoLA further closes the gap between linear attention models and transformers by distributing past key-value pairs into three forms of memory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize pairs in a sparse, global cache; and (iii) generic pairs in the recurrent hidden state of linear attention. As an inference-only strategy, LoLA enables pass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks from RULER. It boosts the accuracy of the base subquadratic model from 0.6% to 97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1 8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning tasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an extremely lightweight approach: Nearly all of our results can be reproduced on a single consumer GPU.",,"Luke McDermott, Robert W. Heath Jr., Rahul Parhi",2025-05-29T17:12:42Z,LoLA: Low-Rank Linear Attention With Sparse Caching,LoLA: Low-Rank Lineare Aufmerksamkeit mit Sparse Caching,"LoLA: 低兰克线性注意, 以粗糙的缓存",http://arxiv.org/abs/2505.23666v1
1597,"In this work, we provide DZEN, a dataset of parallel Dzongkha and English test questions for Bhutanese middle and high school students. The over 5K questions in our collection span a variety of scientific topics and include factual, application, and reasoning-based questions. We use our parallel dataset to test a number of Large Language Models (LLMs) and find a significant performance difference between the models in English and Dzongkha. We also look at different prompting strategies and discover that Chain-of-Thought (CoT) prompting works well for reasoning questions but less well for factual ones. We also find that adding English translations enhances the precision of Dzongkha question responses. Our results point to exciting avenues for further study to improve LLM performance in Dzongkha and, more generally, in low-resource languages. We release the dataset at: https://github.com/kraritt/llm_dzongkha_evaluation.",,"Md. Tanzib Hosain, Rajan Das Gupta, Md. Kishor Morol",2025-05-29T17:11:54Z,Multilingual Question Answering in Low-Resource Settings: A   Dzongkha-English Benchmark for Foundation Models,Mehrsprachige Frage-Antworten in Low-Resource-Einstellungen: Ein Dzongkha-Englischer Benchmark für Stiftungsmodelle,低资源环境下的多语言问题解答:基础模型的Dzongkha-英语基准,http://arxiv.org/abs/2505.18638v2
1598,"Large language models (LLMs) have demonstrated strong capabilities in using external tools to address user inquiries. However, most existing evaluations assume tool use in short contexts, offering limited insight into model behavior during realistic long-term interactions. To fill this gap, we introduce ToolHaystack, a benchmark for testing the tool use capabilities in long-term interactions. Each test instance in ToolHaystack includes multiple tasks execution contexts and realistic noise within a continuous conversation, enabling assessment of how well models maintain context and handle various disruptions. By applying this benchmark to 14 state-of-the-art LLMs, we find that while current models perform well in standard multi-turn settings, they often significantly struggle in ToolHaystack, highlighting critical gaps in their long-term robustness not revealed by previous tool benchmarks.",,"Beong-woo Kwak, Minju Kim, Dongha Lim, Hyungjoo Chae, Dongjin Kang, Sunghwan Kim, Dongil Yang, Jinyoung Yeo",2025-05-29T17:10:12Z,ToolHaystack: Stress-Testing Tool-Augmented Language Models in Realistic   Long-Term Interactions,ToolHaystack: Stress-Testing Tool-Augmented Language Models in realistischen Langzeit-Interaktionen,工具 Haystack:现实长期互动中的压力测试工具增强语言模式,http://arxiv.org/abs/2505.23662v1
1599,"Integrating structured information has long improved the quality of abstractive summarization, particularly in retaining salient content. In this work, we focus on a specific form of structure: argument roles, which are crucial for summarizing documents in high-stakes domains such as law. We investigate whether instruction-tuned large language models (LLMs) adequately preserve this information. To this end, we introduce Argument Representation Coverage (ARC), a framework for measuring how well LLM-generated summaries capture salient arguments. Using ARC, we analyze summaries produced by three open-weight LLMs in two domains where argument roles are central: long legal opinions and scientific articles. Our results show that while LLMs cover salient argument roles to some extent, critical information is often omitted in generated summaries, particularly when arguments are sparsely distributed throughout the input. Further, we use ARC to uncover behavioral patterns -- specifically, how the positional bias of LLM context windows and role-specific preferences impact the coverage of key arguments in generated summaries, emphasizing the need for more argument-aware summarization strategies.",,"Mohamed Elaraby, Diane Litman",2025-05-29T17:04:02Z,ARC: Argument Representation and Coverage Analysis for Zero-Shot Long   Document Summarization with Instruction Following LLMs,ARC: Argumentationsdarstellungs- und Coverage-Analyse für eine Null-Shot-Lang-Dokument-Zusammenfassung mit Instruktion nach LLMs,"ARC: "" 零张长文件摘要 "" 的参数代表性和覆盖面分析,在 "" LLM "" 之后指示",http://arxiv.org/abs/2505.23654v1
1600,"Small Language Models (SLMs) have gained substantial attention due to their ability to execute diverse language tasks successfully while using fewer computer resources. These models are particularly ideal for deployment in limited environments, such as mobile devices, on-device processing, and edge systems. In this study, we present a complete assessment of SLMs, focussing on their design frameworks, training approaches, and techniques for lowering model size and complexity. We offer a novel classification system to organize the optimization approaches applied for SLMs, encompassing strategies like pruning, quantization, and model compression. Furthermore, we assemble SLM's studies of evaluation suite with some existing datasets, establishing a rigorous platform for measuring SLM capabilities. Alongside this, we discuss the important difficulties that remain unresolved in this sector, including trade-offs between efficiency and performance, and we suggest directions for future study. We anticipate this study to serve as a beneficial guide for researchers and practitioners who aim to construct compact, efficient, and high-performing language models.",,"Tanjil Hasan Sakib, Md. Tanzib Hosain, Md. Kishor Morol",2025-05-29T16:57:36Z,"Small Language Models: Architectures, Techniques, Evaluation, Problems   and Future Adaptation","Kleine Sprachmodelle: Architekturen, Techniken, Evaluation, Probleme und zukünftige Anpassung",小型语言模式:建筑、技术、评价、问题和未来适应,http://arxiv.org/abs/2505.19529v2
1601,"Recently evolved large reasoning models (LRMs) show powerful performance in solving complex tasks with long chain-of-thought (CoT) reasoning capability. As these LRMs are mostly developed by post-training on formal reasoning tasks, whether they generalize the reasoning capability to help reduce hallucination in fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1 reports increased performance on SimpleQA, a fact-seeking benchmark, while OpenAI-o3 observes even severer hallucination. This discrepancy naturally raises the following research question: Are reasoning models more prone to hallucination? This paper addresses the question from three perspectives. (1) We first conduct a holistic evaluation for the hallucination in LRMs. Our analysis reveals that LRMs undergo a full post-training pipeline with cold start supervised fine-tuning (SFT) and verifiable reward RL generally alleviate their hallucination. In contrast, both distillation alone and RL training without cold start fine-tuning introduce more nuanced hallucinations. (2) To explore why different post-training pipelines alters the impact on hallucination in LRMs, we conduct behavior analysis. We characterize two critical cognitive behaviors that directly affect the factuality of a LRM: Flaw Repetition, where the surface-level reasoning attempts repeatedly follow the same underlying flawed logic, and Think-Answer Mismatch, where the final answer fails to faithfully match the previous CoT process. (3) Further, we investigate the mechanism behind the hallucination of LRMs from the perspective of model uncertainty. We find that increased hallucination of LRMs is usually associated with the misalignment between model uncertainty and factual accuracy. Our work provides an initial understanding of the hallucination in LRMs.",,"Zijun Yao, Yantao Liu, Yanxu Chen, Jianhui Chen, Junfeng Fang, Lei Hou, Juanzi Li, Tat-Seng Chua",2025-05-29T16:53:41Z,Are Reasoning Models More Prone to Hallucination?,Sind vernünftigere Modelle eher halluzinierend?,理性模型更能让人产生幻觉吗?,http://arxiv.org/abs/2505.23646v1
1602,"Decomposing hard problems into subproblems often makes them easier and more efficient to solve. With large language models (LLMs) crossing critical reliability thresholds for a growing slate of capabilities, there is an increasing effort to decompose systems into sets of LLM-based agents, each of whom can be delegated sub-tasks. However, this decomposition (even when automated) is often intuitive, e.g., based on how a human might assign roles to members of a human team. How close are these role decompositions to optimal? This position paper argues that asymptotic analysis with LLM primitives is needed to reason about the efficiency of such decomposed systems, and that insights from such analysis will unlock opportunities for scaling them. By treating the LLM forward pass as the atomic unit of computational cost, one can separate out the (often opaque) inner workings of a particular LLM from the inherent efficiency of how a set of LLMs are orchestrated to solve hard problems. In other words, if we want to scale the deployment of LLMs to the limit, instead of anthropomorphizing LLMs, asymptotic analysis with LLM primitives should be used to reason about and develop more powerful decompositions of large problems into LLM agents.",,"Elliot Meyerson, Xin Qiu",2025-05-29T16:46:00Z,Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM   Primitives,Position: Skalierung von LLM-Agenten erfordert asymptotische Analyse mit LLM-Primitiven,位置: 缩放 LLM 代理需要用 LLM 原始功能进行抗药性分析,http://arxiv.org/abs/2502.04358v2
1603,"Large Language Models (LLMs) drive scientific question-answering on modern search engines, yet their evaluation robustness remains underexplored. We introduce YESciEval, an open-source framework that combines fine-grained rubric-based assessment with reinforcement learning to mitigate optimism bias in LLM evaluators. We release multidisciplinary scienceQ&A datasets, including adversarial variants, with evaluation scores from multiple LLMs. Independent of proprietary models and human feedback, our approach enables scalable, cost-free evaluation. By advancing reliable LLM-as-a-judge models, this work supports AI alignment and fosters robust, transparent evaluation essential for scientific inquiry.",,"Jennifer D'Souza, Hamed Babaei Giglou, Quentin Münch",2025-05-29T16:45:00Z,YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering,YESciEval: Robuster LLM-as-a-Richter für die Beantwortung wissenschaftlicher Fragen,YESciEval: 科学问题回答优异的LLM-as-a法官,http://arxiv.org/abs/2505.14279v2
1604,"Assessing student depression in sensitive environments like special education is challenging. Standardized questionnaires may not fully reflect students' true situations. Furthermore, automated methods often falter with rich student narratives, lacking the crucial, individualized insights stemming from teachers' empathetic connections with students. Existing methods often fail to address this ambiguity or effectively integrate educator understanding. To address these limitations by fostering a synergistic human-AI collaboration, this paper introduces Human Empathy as Encoder (HEAE), a novel, human-centered AI framework for transparent and socially responsible depression severity assessment. Our approach uniquely integrates student narrative text with a teacher-derived, 9-dimensional ""Empathy Vector"" (EV), its dimensions guided by the PHQ-9 framework,to explicitly translate tacit empathetic insight into a structured AI input enhancing rather than replacing human judgment. Rigorous experiments optimized the multimodal fusion, text representation, and classification architecture, achieving 82.74% accuracy for 7-level severity classification. This work demonstrates a path toward more responsible and ethical affective computing by structurally embedding human empathy",,Boning Zhao,2025-05-29T16:37:15Z,Human Empathy as Encoder: AI-Assisted Depression Assessment in Special   Education,Menschliche Empathie als Encoder: KI-Assisted Depression Assessment in Special Education,人类的同情作为编码器:大赦国际协助的特殊教育中抑郁症评估,http://arxiv.org/abs/2505.23631v1
1605,"A significant portion of the textual data used in the field of Natural Language Processing (NLP) exhibits gender biases, particularly due to the use of masculine generics (masculine words that are supposed to refer to mixed groups of men and women), which can perpetuate and amplify stereotypes. Gender rewriting, an NLP task that involves automatically detecting and replacing gendered forms with neutral or opposite forms (e.g., from masculine to feminine), can be employed to mitigate these biases. While such systems have been developed in a number of languages (English, Arabic, Portuguese, German, French), automatic use of gender neutralization techniques (as opposed to inclusive or gender-switching techniques) has only been studied for English. This paper presents GeNRe, the very first French gender-neutral rewriting system using collective nouns, which are gender-fixed in French. We introduce a rule-based system (RBS) tailored for the French language alongside two fine-tuned language models trained on data generated by our RBS. We also explore the use of instruct-based models to enhance the performance of our other systems and find that Claude 3 Opus combined with our dictionary achieves results close to our RBS. Through this contribution, we hope to promote the advancement of gender bias mitigation techniques in NLP for French.",,"Enzo Doyen, Amalia Todirascu",2025-05-29T16:36:31Z,GeNRe: A French Gender-Neutral Rewriting System Using Collective Nouns,GeNRe: Ein französisches Gender-Neutral-Rewriting-System mit kollektiven Substantiven,GENRe:法国使用集体名词的性别-新书改写系统,http://arxiv.org/abs/2505.23630v1
1606,"We present AutoSchemaKG, a framework for fully autonomous knowledge graph construction that eliminates the need for predefined schemas. Our system leverages large language models to simultaneously extract knowledge triples and induce comprehensive schemas directly from text, modeling both entities and events while employing conceptualization to organize instances into semantic categories. Processing over 50 million documents, we construct ATLAS (Automated Triple Linking And Schema induction), a family of knowledge graphs with 900+ million nodes and 5.9 billion edges. This approach outperforms state-of-the-art baselines on multi-hop QA tasks and enhances LLM factuality. Notably, our schema induction achieves 95\% semantic alignment with human-crafted schemas with zero manual intervention, demonstrating that billion-scale knowledge graphs with dynamically induced schemas can effectively complement parametric knowledge in large language models.",,"Jiaxin Bai, Wei Fan, Qi Hu, Qing Zong, Chunyang Li, Hong Ting Tsang, Hongyu Luo, Yauwai Yim, Haoyu Huang, Xiao Zhou, Feng Qin, Tianshi Zheng, Xi Peng, Xin Yao, Huiwen Yang, Leijie Wu, Yi Ji, Gong Zhang, Renhai Chen, Yangqiu Song",2025-05-29T16:34:58Z,AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic   Schema Induction from Web-Scale Corpora,AutoSchemaKG: Autonome Wissensgraphenkonstruktion durch dynamische Schemainduktion aus Web-Scale Corpora,"AutoSchemaKG:通过网络规模公司动态气相引入,建立自主知识图",http://arxiv.org/abs/2505.23628v1
1607,"Transformer-based language models (LMs) have achieved widespread empirical success, but their theoretical expressive power remains only partially understood. Prior work often relies on idealized models with assumptions -- such as arbitrary numerical precision and hard attention -- that diverge from real-world transformers. In this work, we provide an exact characterization of fixed-precision transformers with strict future masking and soft attention, an idealization that more closely mirrors practical implementations. We show that these models are precisely as expressive as a specific fragment of linear temporal logic that includes only a single temporal operator: the past operator. We further relate this logic to established classes in formal language theory, automata theory, and algebra, yielding a rich and unified theoretical framework for understanding transformer expressivity. Finally, we present empirical results that align closely with our theory: transformers trained on languages within their theoretical capacity generalize perfectly over lengths, while they consistently fail to generalize on languages beyond it.",,"Jiaoda Li, Ryan Cotterell",2025-05-29T16:30:30Z,Characterizing the Expressivity of Transformer Language Models,Charakterisierung der Expressivität von Transformer-Sprachmodellen,描述变换语言模式的表达性,http://arxiv.org/abs/2505.23623v1
1608,"In this work, we present the first study to explore inference-time scaling on table reasoning tasks. We develop and evaluate two post-training strategies to enable inference-time scaling: distillation from frontier model reasoning traces and reinforcement learning with verifiable rewards (RLVR). For distillation, we introduce a large-scale dataset of reasoning traces generated by DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For RLVR, we propose task-specific verifiable reward functions and apply the GRPO algorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series models across diverse table reasoning tasks, including short-form QA, fact verification, and free-form QA. Notably, the Table-R1-Zero model matches or exceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a 7B-parameter LLM. It also demonstrates strong generalization to out-of-domain datasets. Extensive ablation and qualitative analyses reveal the benefits of instruction tuning, model architecture choices, and cross-task generalization, as well as emergence of essential table reasoning skills during RL training.",,"Zheyuan Yang, Lyuhao Chen, Arman Cohan, Yilun Zhao",2025-05-29T16:28:50Z,Table-R1: Inference-Time Scaling for Table Reasoning,Tabelle-R1: Inferenz-Zeit-Skalierung für Tabellenveranlagung,表-R1:表格理由推理的推断时间尺度,http://arxiv.org/abs/2505.23621v1
1609,"We introduce EXIT, an extractive context compression framework that enhances both the effectiveness and efficiency of retrieval-augmented generation (RAG) in question answering (QA). Current RAG systems often struggle when retrieval models fail to rank the most relevant documents, leading to the inclusion of more context at the expense of latency and accuracy. While abstractive compression methods can drastically reduce token counts, their token-by-token generation process significantly increases end-to-end latency. Conversely, existing extractive methods reduce latency but rely on independent, non-adaptive sentence selection, failing to fully utilize contextual information. EXIT addresses these limitations by classifying sentences from retrieved documents - while preserving their contextual dependencies - enabling parallelizable, context-aware extraction that adapts to query complexity and retrieval quality. Our evaluations on both single-hop and multi-hop QA tasks show that EXIT consistently surpasses existing compression methods and even uncompressed baselines in QA accuracy, while also delivering substantial reductions in inference time and token count. By improving both effectiveness and efficiency, EXIT provides a promising direction for developing scalable, high-quality QA solutions in RAG pipelines. Our code is available at https://github.com/ThisIsHwang/EXIT",,"Taeho Hwang, Sukmin Cho, Soyeong Jeong, Hoyun Song, SeungYoon Han, Jong C. Park",2025-05-29T16:18:33Z,EXIT: Context-Aware Extractive Compression for Enhancing   Retrieval-Augmented Generation,EXIT: Context-Aware Extractive Compression zur Verbesserung der Retrieval-Augmented Generation,EXIT: 为加强回流-提款一代而实行的背景软件抽取压缩,http://arxiv.org/abs/2412.12559v3
1610,"Language models (LMs) perform well on standardized coding benchmarks but struggle with real-world software engineering tasks such as resolving GitHub issues in SWE-Bench, especially when model parameters are less than 100B. While smaller models are preferable in practice due to their lower computational cost, improving their performance remains challenging. Existing approaches primarily rely on supervised fine-tuning (SFT) with high-quality data, which is expensive to curate at scale. An alternative is test-time scaling: generating multiple outputs, scoring them using a verifier, and selecting the best one. Although effective, this strategy often requires excessive sampling and costly scoring, limiting its practical application. We propose Evolutionary Test-Time Scaling (EvoScale), a sample-efficient method that treats generation as an evolutionary process. By iteratively refining outputs via selection and mutation, EvoScale shifts the output distribution toward higher-scoring regions, reducing the number of samples needed to find correct solutions. To reduce the overhead from repeatedly sampling and selection, we train the model to self-evolve using reinforcement learning (RL). Rather than relying on external verifiers at inference time, the model learns to self-improve the scores of its own generations across iterations. Evaluated on SWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or exceed the performance of models with over 100B parameters while using a few samples. Code, data, and models will be fully open-sourced.",,"Guangtao Zeng, Maohao Shen, Delin Chen, Zhenting Qi, Subhro Das, Dan Gutfreund, David Cox, Gregory Wornell, Wei Lu, Zhang-Wei Hong, Chuang Gan",2025-05-29T16:15:36Z,Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software   Engineering,Satori-SWE: Evolutionäre Test-Zeit-Skalierung für probeneffiziente Software-Engineering,Satori-SWE:样本高效软件工程的进化测试-时间尺度,http://arxiv.org/abs/2505.23604v1
1611,"Large language model (LLM)-based agents have shown promise in tackling complex tasks by interacting dynamically with the environment. Existing work primarily focuses on behavior cloning from expert demonstrations or preference learning through exploratory trajectory sampling. However, these methods often struggle to address long-horizon tasks, where suboptimal actions accumulate step by step, causing agents to deviate from correct task trajectories. To address this, we highlight the importance of timely calibration and the need to automatically construct calibration trajectories for training agents. We propose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM agent learning. Specifically, STeCa identifies suboptimal actions through a step-level reward comparison during exploration. It constructs calibrated trajectories using LLM-driven reflection, enabling agents to learn from improved decision-making processes. We finally leverage these calibrated trajectories with successful trajectories for reinforced training. Extensive experiments demonstrate that STeCa significantly outperforms existing methods. Further analysis highlights that timely calibration enables agents to complete tasks with greater robustness. Our code and data are available at https://github.com/WangHanLinHenry/STeCa.",,"Hanlin Wang, Jian Wang, Chak Tou Leong, Wenjie Li",2025-05-29T16:13:21Z,STeCa: Step-level Trajectory Calibration for LLM Agent Learning,STeCa: Schritt-Level-Trajektorienkalibrierung für LLM Agent Learning,STeCa:LLM代理学习的职级轨迹校准,http://arxiv.org/abs/2502.14276v2
1612,"As large language models (LLMs) are increasingly deployed in multi-turn dialogue and other sustained interactive scenarios, it is essential to understand how extended context affects their performance. Popular benchmarks, focusing primarily on single-turn question answering (QA) tasks, fail to capture the effects of multi-turn exchanges. To address this gap, we introduce a novel set of benchmarks that systematically vary the volume and nature of prior context. We evaluate multiple conventional LLMs, including GPT, Claude, and Gemini, across these benchmarks to measure their sensitivity to contextual variations. Our findings reveal that LLM performance on multiple-choice questions can degrade dramatically in multi-turn interactions, with performance drops as large as 73% for certain models. Even highly capable models such as GPT-4o exhibit up to a 32% decrease in accuracy. Notably, the relative performance of larger versus smaller models is not always predictable. Moreover, the strategic placement of the task description within the context can substantially mitigate performance drops, improving the accuracy by as much as a factor of 3.5. These findings underscore the need for robust strategies to design, evaluate, and mitigate context-related sensitivity in LLMs.",,"Robert Hankache, Kingsley Nketia Acheampong, Liang Song, Marek Brynda, Raad Khraishi, Greig A. Cowan",2025-05-29T16:09:32Z,Evaluating the Sensitivity of LLMs to Prior Context,Bewertung der Sensitivität von LLMs im vorherigen Kontext,评估以往情况下LLMs的敏感性,http://arxiv.org/abs/2506.00069v1
1613,"The Turing test examines whether AIs exhibit human-like behaviour in natural language conversations. The traditional setting limits each participant to one message at a time and requires constant human participation. This fails to reflect a natural conversational style and hinders the evaluation of dialogue agents based on Large Language Models (LLMs) in complex and prolonged interactions. This paper proposes \textbf{\textsc{X-Turing}}, which enhances the original test with a \textit{burst dialogue} pattern, allowing more dynamic exchanges using consecutive messages. It further reduces human workload by iteratively generating dialogues that simulate the long-term interaction between the agent and a human to compose the majority of the test process. With the \textit{pseudo-dialogue} history, the agent then engages in a shorter dialogue with a real human, which is paired with a human-human conversation on the same topic to be judged using questionnaires. We introduce the \textit{X-Turn Pass-Rate} metric to assess the human likeness of LLMs across varying durations. While LLMs like GPT-4 initially perform well, achieving pass rates of 51.9\% and 38.9\% during 3 turns and 10 turns of dialogues respectively, their performance drops as the dialogue progresses, which underscores the difficulty in maintaining consistency in the long term.",,"Weiqi Wu, Hongqiu Wu, Hai Zhao",2025-05-29T16:08:23Z,X-TURING: Towards an Enhanced and Efficient Turing Test for Long-Term   Dialogue Agents,X-TURING: Auf dem Weg zu einem verbesserten und effizienten Turing-Test für Langzeit-Dialogagenten,XTurning:争取对长期对话代理机构进行强化和高效率的图示测试,http://arxiv.org/abs/2408.09853v2
1614,"Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and reward models, are central to aligning and evaluating large language models (LLMs). Yet, the underlying concepts that drive these preferences remain poorly understood. In this work, we propose a fully automated method for generating local and global concept-based explanations of preferences across multiple domains. Our method utilizes an LLM to identify concepts that distinguish between chosen and rejected responses, and to represent them with concept-based vectors. To model the relationships between concepts and preferences, we propose a white-box Hierarchical Multi-Domain Regression model that captures both domain-general and domain-specific effects. To evaluate our method, we curate a dataset spanning eight challenging and diverse domains and explain twelve mechanisms. Our method achieves strong preference prediction performance, outperforming baselines while also being explainable. Additionally, we assess explanations in two application-driven settings. First, guiding LLM outputs with concepts from LaaJ explanations yields responses that those judges consistently prefer. Second, prompting LaaJs with concepts explaining humans improves their preference predictions. Together, our work establishes a new paradigm for explainability in the era of LLMs.",,"Nitay Calderon, Liat Ein-Dor, Roi Reichart",2025-05-29T15:47:53Z,Multi-Domain Explainability of Preferences,Multi-Domain-Erklärbarkeit von Präferenzen,优惠的多功能可解释性,http://arxiv.org/abs/2505.20088v2
1615,"As a leading online platform with a vast global audience, YouTube's extensive reach also makes it susceptible to hosting harmful content, including disinformation and conspiracy theories. This study explores the use of open-weight Large Language Models (LLMs), both text-only and multimodal, for identifying conspiracy theory videos shared on YouTube. Leveraging a labeled dataset of thousands of videos, we evaluate a variety of LLMs in a zero-shot setting and compare their performance to a fine-tuned RoBERTa baseline. Results show that text-based LLMs achieve high recall but lower precision, leading to increased false positives. Multimodal models lag behind their text-only counterparts, indicating limited benefits from visual data integration. To assess real-world applicability, we evaluate the most accurate models on an unlabeled dataset, finding that RoBERTa achieves performance close to LLMs with a larger number of parameters. Our work highlights the strengths and limitations of current LLM-based approaches for online harmful content detection, emphasizing the need for more precise and robust systems.",,"Leonardo La Rocca, Francesco Corso, Francesco Pierri",2025-05-29T15:44:36Z,Evaluating AI capabilities in detecting conspiracy theories on YouTube,Bewertung von KI-Fähigkeiten bei der Entdeckung von Verschwörungstheorien auf YouTube,评价大赦国际在YouTube上发现阴谋论的能力,http://arxiv.org/abs/2505.23570v1
1616,"Enhancing the reasoning capabilities of large language models effectively using reinforcement learning (RL) remains a crucial challenge. Existing approaches primarily adopt two contrasting advantage estimation granularities: Token-level methods (e.g., PPO) aim to provide the fine-grained advantage signals but suffer from inaccurate estimation due to difficulties in training an accurate critic model. On the other extreme, trajectory-level methods (e.g., GRPO) solely rely on a coarse-grained advantage signal from the final reward, leading to imprecise credit assignment. To address these limitations, we propose Segment Policy Optimization (SPO), a novel RL framework that leverages segment-level advantage estimation at an intermediate granularity, achieving a better balance by offering more precise credit assignment than trajectory-level methods and requiring fewer estimation points than token-level methods, enabling accurate advantage estimation based on Monte Carlo (MC) without a critic model. SPO features three components with novel strategies: (1) flexible segment partition; (2) accurate segment advantage estimation; and (3) policy optimization using segment advantages, including a novel probability-mask strategy. We further instantiate SPO for two specific scenarios: (1) SPO-chain for short chain-of-thought (CoT), featuring novel cutpoint-based partition and chain-based advantage estimation, achieving $6$-$12$ percentage point improvements in accuracy over PPO and GRPO on GSM8K. (2) SPO-tree for long CoT, featuring novel tree-based advantage estimation, which significantly reduces the cost of MC estimation, achieving $7$-$11$ percentage point improvements over GRPO on MATH500 under 2K and 4K context evaluation. We make our code publicly available at https://github.com/AIFrameResearch/SPO.",,"Yiran Guo, Lijie Xu, Jie Liu, Dan Ye, Shuang Qiu",2025-05-29T15:38:19Z,Segment Policy Optimization: Effective Segment-Level Credit Assignment   in RL for Large Language Models,Segment Policy Optimization: Effektive Segment-Level-Kreditvergabe in RL für große Sprachmodelle,政策优化优化:大语言模式RL中有效的分部一级信用分配,http://arxiv.org/abs/2505.23564v1
1617,"Long-form legal reasoning remains a key challenge for large language models (LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a novel benchmark derived from 340 law exams spanning 116 law school courses across a range of subjects and degree levels. The dataset comprises 4,886 law exam questions in English and German, including 2,841 long-form, open-ended questions and 2,045 multiple-choice questions. Besides reference answers, the open questions are also accompanied by explicit guidance outlining the expected legal reasoning approach such as issue spotting, rule recall, or rule application. Our evaluation on both open-ended and multiple-choice questions present significant challenges for current LLMs; in particular, they notably struggle with open questions that require structured, multi-step legal reasoning. Moreover, our results underscore the effectiveness of the dataset in differentiating between models with varying capabilities. Adopting an LLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate how model-generated reasoning steps can be evaluated consistently and accurately. Our evaluation setup provides a scalable method to assess legal reasoning quality beyond simple accuracy metrics. Project page: https://lexam-benchmark.github.io/",,"Yu Fan, Jingwei Ni, Jakob Merane, Etienne Salimbeni, Yang Tian, Yoan Hermstrüwer, Yinya Huang, Mubashara Akhtar, Florian Geering, Oliver Dreyer, Daniel Brunner, Markus Leippold, Mrinmaya Sachan, Alexander Stremitzer, Christoph Engel, Elliott Ash, Joel Niklaus",2025-05-29T15:37:57Z,LEXam: Benchmarking Legal Reasoning on 340 Law Exams,LEXam: Benchmarking der rechtlichen Begründung von 340 Rechtsprüfungen,LEXam:340项法律考试的法律依据基准,http://arxiv.org/abs/2505.12864v2
1618,"Refusal is a key safety behavior in aligned language models, yet the internal mechanisms driving refusals remain opaque. In this work, we conduct a mechanistic study of refusal in instruction-tuned LLMs using sparse autoencoders to identify latent features that causally mediate refusal behaviors. We apply our method to two open-source chat models and intervene on refusal-related features to assess their influence on generation, validating their behavioral impact across multiple harmful datasets. This enables a fine-grained inspection of how refusal manifests at the activation level and addresses key research questions such as investigating upstream-downstream latent relationship and understanding the mechanisms of adversarial jailbreaking techniques. We also establish the usefulness of refusal features in enhancing generalization for linear probes to out-of-distribution adversarial samples in classification tasks. We open source our code in https://github.com/wj210/refusal_sae.",,"Wei Jie Yeo, Nirmalendu Prakash, Clement Neo, Roy Ka-Wei Lee, Erik Cambria, Ranjan Satapathy",2025-05-29T15:33:39Z,Understanding Refusal in Language Models with Sparse Autoencoders,Ablehnung in Sprachmodellen mit Sparse Autoencodern verstehen,使用 sparse 自动解析器理解语言模式中的拒绝拒绝模式,http://arxiv.org/abs/2505.23556v1
1619,"Automated interpretability pipelines generate natural language descriptions for the concepts represented by features in large language models (LLMs), such as plants or the first word in a sentence. These descriptions are derived using inputs that activate the feature, which may be a dimension or a direction in the model's representation space. However, identifying activating inputs is costly, and the mechanistic role of a feature in model behavior is determined both by how inputs cause a feature to activate and by how feature activation affects outputs. Using steering evaluations, we reveal that current pipelines provide descriptions that fail to capture the causal effect of the feature on outputs. To fix this, we propose efficient, output-centric methods for automatically generating feature descriptions. These methods use the tokens weighted higher after feature stimulation or the highest weight tokens after applying the vocabulary ""unembedding"" head directly to the feature. Our output-centric descriptions better capture the causal effect of a feature on model outputs than input-centric descriptions, but combining the two leads to the best performance on both input and output evaluations. Lastly, we show that output-centric descriptions can be used to find inputs that activate features previously thought to be ""dead"".",,"Yoav Gur-Arieh, Roy Mayan, Chen Agassy, Atticus Geiger, Mor Geva",2025-05-29T15:26:06Z,Enhancing Automated Interpretability with Output-Centric Feature   Descriptions,Verbesserte Automatisierte Dolmetschbarkeit mit Output-Centric-Feature-Beschreibungen,加强自动解释与产出中心特点描述的可解释性,http://arxiv.org/abs/2501.08319v2
1620,"Large Language Models (LLMs) excel in translation among other things, demonstrating competitive performance for many language pairs in zero- and few-shot settings. But unlike dedicated neural machine translation models, LLMs are not trained on any translation-related objective. What explains their remarkable translation abilities? Are these abilities grounded in ""incidental bilingualism"" (Briakou et al. 2023) in training data? Does instruction tuning contribute to it? Are LLMs capable of aligning and leveraging semantically identical or similar monolingual contents from different corners of the internet that are unlikely to fit in a single context window? I offer some reflections on this topic, informed by recent studies and growing user experience. My working hypothesis is that LLMs' translation abilities originate in two different types of pre-training data that may be internalized by the models in different ways. I discuss the prospects for testing the ""duality"" hypothesis empirically and its implications for reconceptualizing translation, human and machine, in the age of deep learning.",,Yuri Balashov,2025-05-29T15:26:04Z,Translation in the Wild,Übersetzung in der Wildnis,《野生》翻译,http://arxiv.org/abs/2505.23548v1
1621,"Recent advances in preference optimization have demonstrated significant potential for improving mathematical reasoning capabilities in large language models (LLMs). While current approaches leverage high-quality pairwise preference data through outcome-based criteria like answer correctness or consistency, they fundamentally neglect the internal logical coherence of responses. To overcome this, we propose Probability-Consistent Preference Optimization (PCPO), a novel framework that establishes dual quantitative metrics for preference selection: (1) surface-level answer correctness and (2) intrinsic token-level probability consistency across responses. Extensive experiments show that our PCPO consistently outperforms existing outcome-only criterion approaches across a diverse range of LLMs and benchmarks. Our code is publicly available at https://github.com/YunqiaoYang/PCPO.",,"Yunqiao Yang, Houxing Ren, Zimu Lu, Ke Wang, Weikang Shi, Aojun Zhou, Junting Pan, Mingjie Zhan, Hongsheng Li",2025-05-29T15:20:44Z,Probability-Consistent Preference Optimization for Enhanced LLM   Reasoning,Wahrscheinlichkeitskonsistente Preference-Optimierung für verbesserte LLM-Reasoning,增强 LLM 理由说明的优化,http://arxiv.org/abs/2505.23540v1
1622,"Large Language Model (LLM) collaborative decoding techniques improve output quality by combining the outputs of multiple models at each generation step, but they incur high computational costs. In this paper, we introduce Collaborative decoding via Speculation (CoS), a novel framework that accelerates collaborative decoding without compromising performance. Inspired by Speculative Decoding--where a small proposal model generates tokens sequentially, and a larger target model verifies them in parallel, our approach builds on two key insights: (1) the verification distribution can be the combined distribution of both the proposal and target models, and (2) alternating each model as the proposer and verifier can further enhance efficiency. We generalize this method to collaboration among n models and theoretically prove that CoS is never slower than standard collaborative decoding, typically achieving faster speed. Extensive experiments demonstrate CoS is 1.11x-2.23x faster than standard collaborative decoding without compromising generation quality. Our code is available at https://github.com/Kamichanw/CoS/.",,"Jiale Fu, Yuchu Jiang, Junkai Chen, Jiaming Fan, Xin Geng, Xu Yang",2025-05-29T15:20:23Z,Fast Large Language Model Collaborative Decoding via Speculation,Schnelles Large Language Model Kollaboratives Decodieren über Spekulation,通过投机进行快速大语言合作示范模式,http://arxiv.org/abs/2502.01662v2
1623,"This paper presents our approach to the SemEval-2025 Task~6 (PromiseEval), which focuses on verifying promises in corporate ESG (Environmental, Social, and Governance) reports. We explore three model architectures to address the four subtasks of promise identification, supporting evidence assessment, clarity evaluation, and verification timing. Our first model utilizes ESG-BERT with task-specific classifier heads, while our second model enhances this architecture with linguistic features tailored for each subtask. Our third approach implements a combined subtask model with attention-based sequence pooling, transformer representations augmented with document metadata, and multi-objective learning. Experiments on the English portion of the ML-Promise dataset demonstrate progressive improvement across our models, with our combined subtask approach achieving a leaderboard score of 0.5268, outperforming the provided baseline of 0.5227. Our work highlights the effectiveness of linguistic feature extraction, attention pooling, and multi-objective learning in promise verification tasks, despite challenges posed by class imbalance and limited training data.",,"Nawar Turk, Eeham Khan, Leila Kosseim",2025-05-29T15:19:00Z,CLaC at SemEval-2025 Task 6: A Multi-Architecture Approach for Corporate   Environmental Promise Verification,CLaC bei SemEval-2025 Task 6: Ein Multi-Architektur-Ansatz für die Verifikation von Unternehmensumweltversprechen,SemEval-2025任务6:公司环境承诺核查的多建筑方法,http://arxiv.org/abs/2505.23538v1
1624,"Tensor networks (TNs) provide efficient representations of high-dimensional data, yet identification of the optimal TN structures, the so called tensor network structure search (TN-SS) problem, remains a challenge. Current state-of-the-art (SOTA) algorithms are computationally expensive as they require extensive function evaluations, which is prohibitive for real-world applications. In addition, existing methods ignore valuable domain information inherent in real-world tensor data and lack transparency in their identified TN structures. To this end, we propose a novel TN-SS framework, termed the tnLLM, which incorporates domain information about the data and harnesses the reasoning capabilities of large language models (LLMs) to directly predict suitable TN structures. The proposed framework involves a domain-aware prompting pipeline which instructs the LLM to infer suitable TN structures based on the real-world relationships between tensor modes. In this way, our approach is capable of not only iteratively optimizing the objective function, but also generating domain-aware explanations for the identified structures. Experimental results demonstrate that tnLLM achieves comparable TN-SS objective function values with much fewer function evaluations compared to SOTA algorithms. Furthermore, we demonstrate that the LLM-enabled domain information can be used to find good initializations in the search space for sampling-based SOTA methods to accelerate their convergence while preserving theoretical performance guarantees.",,"Giorgos Iacovides, Wuyang Zhou, Chao Li, Qibin Zhao, Danilo Mandic",2025-05-29T15:18:33Z,Domain-Aware Tensor Network Structure Search,Domain-Aware Tensor Netzwerkstruktur Suche,域- 软件显示器网络网络结构搜索,http://arxiv.org/abs/2505.23537v1
1625,"Large Language Models (LLMs) are increasingly shaping public discourse, yet their politico-economic biases remain underexamined in non-Western and low-resource multilingual contexts. This paper presents a systematic analysis of political bias in 13 state-of-the-art LLMs across five low-resource languages spoken in Pakistan: Urdu, Punjabi, Sindhi, Balochi, and Pashto. We propose a novel framework that integrates an adapted Political Compass Test (PCT) with a multi-level framing analysis. Our method combines quantitative assessment of political orientation across economic (left-right) and social (libertarian-authoritarian) axes with qualitative analysis of framing through content, style, and emphasis. We further contextualize this analysis by aligning prompts with 11 key socio-political themes relevant to Pakistani society. Our results reveal that LLMs predominantly align with liberal-left values, echoing Western training data influences, but exhibit notable shifts toward authoritarian framing in regional languages, suggesting strong cultural modulation effects. We also identify consistent model-specific bias signatures and language-conditioned variations in ideological expression. These findings show the urgent need for culturally grounded, multilingual bias auditing frameworks.",,"Afrozah Nadeem, Mark Dras, Usman Naseem",2025-05-29T15:15:42Z,Probing Politico-Economic Bias in Multilingual Large Language Models: A   Cultural Analysis of Low-Resource Pakistani Languages,Probing Politico-Economic Bias in mehrsprachigen großen Sprachmodellen: Eine kulturelle Analyse der ressourcenarmen pakistanischen Sprachen,在多语言大语言模式中探究政治-经济偏见:巴基斯坦低资源语言的文化分析,http://arxiv.org/abs/2506.00068v1
1626,"Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, are commonly used to adapt LLMs. However, the effectiveness of standard PEFT methods is limited in low-resource scenarios with only a few hundred examples. Recent advances in interpretability research have inspired the emergence of activation editing (or steering) techniques, which modify the activations of specific model components. Due to their extremely small parameter counts, these methods show promise for small datasets. However, their performance is highly dependent on identifying the correct modules to edit and often lacks stability across different datasets. In this paper, we propose Joint Localization and Activation Editing (JoLA), a method that jointly learns (1) which heads in the Transformer to edit (2) whether the intervention should be additive, multiplicative, or both and (3) the intervention parameters themselves - the vectors applied as additive offsets or multiplicative scalings to the head output. Through evaluations on three benchmarks spanning commonsense reasoning, natural language understanding, and natural language generation, we demonstrate that JoLA consistently outperforms existing methods. The code for the method is released at https://github.com/wenlai-lavine/jola.",,"Wen Lai, Alexander Fraser, Ivan Titov",2025-05-29T14:57:31Z,Joint Localization and Activation Editing for Low-Resource Fine-Tuning,Gemeinsame Lokalisierungs- und Aktivierungsbearbeitung für Low-Resource Fine-Tuning,低资源微调联合定位和启动编辑,http://arxiv.org/abs/2502.01179v4
1627,"Large language models (LLMs) are increasingly explored as general-purpose reasoners, particularly in agentic contexts. However, their outputs remain prone to mathematical and logical errors. This is especially challenging in open-ended tasks, where unstructured outputs lack explicit ground truth and may contain subtle inconsistencies. To address this issue, we propose Logic-Enhanced Language Model Agents (LELMA), a framework that integrates LLMs with formal logic to enable validation and refinement of natural language reasoning. LELMA comprises three components: an LLM-Reasoner, an LLM-Translator, and a Solver, and employs autoformalization to translate reasoning into logic representations, which are then used to assess logical validity. Using game-theoretic scenarios such as the Prisoner's Dilemma as testbeds, we highlight the limitations of both less capable (Gemini 1.0 Pro) and advanced (GPT-4o) models in generating logically sound reasoning. LELMA achieves high accuracy in error detection and improves reasoning correctness via self-refinement, particularly in GPT-4o. The study also highlights challenges in autoformalization accuracy and in evaluation of inherently ambiguous open-ended reasoning tasks.",,"Agnieszka Mensfelt, Kostas Stathis, Vince Trencsenyi",2025-05-29T14:53:45Z,Towards Logically Sound Natural Language Reasoning with Logic-Enhanced   Language Model Agents,Auf dem Weg zu logisch klingender natürlicher Sprache mit logisch-erweiterten Sprachmodell-Agenten,"与逻辑增强语言示范代理商一道,争取实现逻辑合理自然语言合理",http://arxiv.org/abs/2408.16081v2
1628,"In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs for specific downstream tasks by utilizing labeled examples as demonstrations (demos) in the preconditioned prompts. Despite its promising performance, crafted adversarial attacks pose a notable threat to the robustness of LLMs. Existing attacks are either easy to detect, require a trigger in user input, or lack specificity towards ICL. To address these issues, this work introduces a novel transferable prompt injection attack against ICL, aiming to hijack LLMs to generate the target output or elicit harmful responses. In our threat model, the hacker acts as a model publisher who leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demos via prompt injection. We also propose effective defense strategies using a few shots of clean demos, enhancing the robustness of LLMs during ICL. Extensive experimental results across various classification and jailbreak tasks demonstrate the effectiveness of the proposed attack and defense strategies. This work highlights the significant security vulnerabilities of LLMs during ICL and underscores the need for further in-depth studies.",,"Xiangyu Zhou, Yao Qiang, Saleh Zare Zade, Prashant Khanduri, Dongxiao Zhu",2025-05-29T14:49:44Z,Hijacking Large Language Models via Adversarial In-Context Learning,Entführen von großen Sprachmodellen über das adversarische In-Context-Lernen,通过对抗性内书学习劫持大语言模式,http://arxiv.org/abs/2311.09948v3
1629,"Software is an essential component of research. However, little attention has been paid to it compared with that paid to research data. Recently, there has been an increase in efforts to acknowledge and highlight the importance of software in research activities.   Structured metadata from platforms like bio.tools, Bioconductor, and Galaxy ToolShed offers valuable insights into research software in the Life Sciences. Although originally intended to support discovery and integration, this metadata can be repurposed for large-scale analysis of software practices. However, its quality and completeness vary across platforms, reflecting diverse documentation practices.   To gain a comprehensive view of software development and sustainability, consolidating this metadata is necessary, but requires robust mechanisms to address its heterogeneity and scale.   This article presents an evaluation of instruction-tuned large language models for the task of software metadata identity resolution, a critical step in assembling a cohesive collection of research software. Such a collection is the reference component for the Software Observatory at OpenEBench, a platform that aggregates metadata to monitor the FAIRness of research software in the Life Sciences.   We benchmarked multiple models against a human-annotated gold standard, examined their behavior on ambiguous cases, and introduced an agreement-based proxy for high-confidence automated decisions. The proxy achieved high precision and statistical robustness, while also highlighting the limitations of current models and the broader challenges of automating semantic judgment in FAIR-aligned software metadata across registries and repositories.",,"Eva Martín del Pico, Josep Lluís Gelpí, Salvador Capella-Gutiérrez",2025-05-29T14:47:31Z,Identity resolution of software metadata using Large Language Models,Identitätsauflösung von Software-Metadaten mit großen Sprachmodellen,使用大语言模式的软件元数据的识别分辨率,http://arxiv.org/abs/2505.23500v1
1630,"Knowledge Graph Question Answering (KGQA) systems rely on high-quality benchmarks to evaluate complex multi-hop reasoning. However, despite their widespread use, popular datasets such as WebQSP and CWQ suffer from critical quality issues, including inaccurate or incomplete ground-truth annotations, poorly constructed questions that are ambiguous, trivial, or unanswerable, and outdated or inconsistent knowledge. Through a manual audit of 16 popular KGQA datasets, including WebQSP and CWQ, we find that the average factual correctness rate is only 57 %. To address these issues, we introduce KGQAGen, an LLM-in-the-loop framework that systematically resolves these pitfalls. KGQAGen combines structured knowledge grounding, LLM-guided generation, and symbolic verification to produce challenging and verifiable QA instances. Using KGQAGen, we construct KGQAGen-10k, a ten-thousand scale benchmark grounded in Wikidata, and evaluate a diverse set of KG-RAG models. Experimental results demonstrate that even state-of-the-art systems struggle on this benchmark, highlighting its ability to expose limitations of existing models. Our findings advocate for more rigorous benchmark construction and position KGQAGen as a scalable framework for advancing KGQA evaluation.",,"Liangliang Zhang, Zhuorui Jiang, Hongliang Chi, Haoyang Chen, Mohammed Elkoumy, Fali Wang, Qiong Wu, Zhengyi Zhou, Shirui Pan, Suhang Wang, Yao Ma",2025-05-29T14:44:52Z,Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More   Reliable Benchmarking,Diagnose und Bewältigung von Pitfalls in KG-RAG-Datensätzen: Zu zuverlässigerem Benchmarking,分析和处理KG-RAG数据集的缺陷:争取更可靠的基准,http://arxiv.org/abs/2505.23495v1
1631,"Spoken language models (SLMs) operate on acoustic units obtained by discretizing self-supervised speech representations. Although the characteristics of these units directly affect performance, the interaction between codebook size and unit coarseness (i.e., duration) remains unexplored. We investigate SLM performance as we vary codebook size and unit coarseness using the simple duration-penalized dynamic programming (DPDP) method. New analyses are performed across different linguistic levels. At the phone and word levels, coarseness provides little benefit, as long as the codebook size is chosen appropriately. However, when producing whole sentences in a resynthesis task, SLMs perform better with coarser units. In lexical and syntactic language modeling tasks, coarser units also give higher accuracies at lower bitrates. We therefore show that coarser units aren't always better, but that DPDP is a simple and efficient way to obtain coarser units for the tasks where they are beneficial.",,"Nicol Visser, Herman Kamper",2025-05-29T14:43:48Z,Spoken Language Modeling with Duration-Penalized Self-Supervised Units,Gesprochene Sprachmodellierung mit Dauer-Penalisierten Selbstüberwachten Einheiten,长期惩罚性自督单位的口语模拟模式,http://arxiv.org/abs/2505.23494v1
1632,"Reasoning is a fundamental capability often required in real-world text-to-image (T2I) generation, e.g., generating ``a bitten apple that has been left in the air for more than a week`` necessitates understanding temporal decay and commonsense concepts. While recent T2I models have made impressive progress in producing photorealistic images, their reasoning capability remains underdeveloped and insufficiently evaluated. To bridge this gap, we introduce R2I-Bench, a comprehensive benchmark specifically designed to rigorously assess reasoning-driven T2I generation. R2I-Bench comprises meticulously curated data instances, spanning core reasoning categories, including commonsense, mathematical, logical, compositional, numerical, causal, and concept mixing. To facilitate fine-grained evaluation, we design R2IScore, a QA-style metric based on instance-specific, reasoning-oriented evaluation questions that assess three critical dimensions: text-image alignment, reasoning accuracy, and image quality. Extensive experiments with 16 representative T2I models, including a strong pipeline-based framework that decouples reasoning and generation using the state-of-the-art language and image generation models, demonstrate consistently limited reasoning performance, highlighting the need for more robust, reasoning-aware architectures in the next generation of T2I systems. Project Page: https://r2i-bench.github.io",,"Kaijie Chen, Zihao Lin, Zhiyang Xu, Ying Shen, Yuguang Yao, Joy Rimchala, Jiaxin Zhang, Lifu Huang",2025-05-29T14:43:46Z,R2I-Bench: Benchmarking Reasoning-Driven Text-to-Image Generation,R2I-Bench: Benchmarking Reasoning-Driven Text-to-Image Generation,R2I-Bench: 基准推理-驱动生成文本到图像,http://arxiv.org/abs/2505.23493v1
1633,"The advent of Large Language Models (LLMs) has marked significant achievements in language processing and reasoning capabilities. Despite their advancements, LLMs face vulnerabilities to data poisoning attacks, where the adversary inserts backdoor triggers into training data to manipulate outputs. This work further identifies additional security risks in LLMs by designing a new data poisoning attack tailored to exploit the supervised fine-tuning (SFT) process. We propose a novel gradient-guided backdoor trigger learning (GBTL) algorithm to identify adversarial triggers efficiently, ensuring an evasion of detection by conventional defenses while maintaining content integrity. Through experimental validation across various language model tasks, including sentiment analysis, domain generation, and question answering, our poisoning strategy demonstrates a high success rate in compromising various LLMs' outputs. We further propose two defense strategies against data poisoning attacks, including in-context learning (ICL) and continuous learning (CL), which effectively rectify the behavior of LLMs and significantly reduce the decline in performance. Our work highlights the significant security risks present during SFT of LLMs and the necessity of safeguarding LLMs against data poisoning attacks.",,"Xiangyu Zhou, Yao Qiang, Saleh Zare Zade, Mohammad Amin Roshani, Prashant Khanduri, Douglas Zytko, Dongxiao Zhu",2025-05-29T14:42:38Z,Learning to Poison Large Language Models for Downstream Manipulation,Große Sprachmodelle für Downstream-Manipulation zu vergiften,学习下游操作毒物大语言模式,http://arxiv.org/abs/2402.13459v3
1634,"In-context machine translation (MT) with large language models (LLMs) is a promising approach for low-resource MT, as it can readily take advantage of linguistic resources such as grammar books and dictionaries. Such resources are usually selectively integrated into the prompt so that LLMs can directly perform translation without any specific training, via their in-context learning capability (ICL). However, the relative importance of each type of resource, e.g., dictionary, grammar book, and retrieved parallel examples, is not entirely clear. To address this gap, this study systematically investigates how each resource and its quality affect the translation performance, with the Manchu language as our case study. To remove any prior knowledge of Manchu encoded in the LLM parameters and single out the effect of ICL, we also experiment with an enciphered version of Manchu texts. Our results indicate that high-quality dictionaries and good parallel examples are very helpful, while grammars hardly help. In a follow-up study, we showcase a promising application of in-context MT: parallel data augmentation as a way to bootstrap a conventional MT model. When monolingual data abound, generating synthetic parallel data through in-context MT offers a pathway to mitigate data scarcity and build effective and efficient low-resource neural MT systems.",,"Renhao Pei, Yihong Liu, Peiqin Lin, François Yvon, Hinrich Schütze",2025-05-29T14:42:17Z,Understanding In-Context Machine Translation for Low-Resource Languages:   A Case Study on Manchu,In-Context Machine Translation für Low-Resource-Sprachen verstehen: Eine Fallstudie zu Mandschu,理解低资源语言的文内机翻译:关于满字的个案研究,http://arxiv.org/abs/2502.11862v2
1635,"Reasoning Large Language Models (RLLMs) have demonstrated impressive performance on complex tasks, largely due to the adoption of Long Chain-of-Thought (Long CoT) reasoning. However, they often exhibit overthinking -- performing unnecessary reasoning steps even after arriving at the correct answer. Prior work has largely focused on qualitative analyses of overthinking through sample-based observations of long CoTs. In contrast, we present a quantitative analysis of overthinking from the perspective of self-doubt, characterized by excessive token usage devoted to re-verifying already-correct answer. We find that self-doubt significantly contributes to overthinking. In response, we introduce a simple and effective prompting method to reduce the model's over-reliance on input questions, thereby avoiding self-doubt. Specifically, we first prompt the model to question the validity of the input question, and then respond concisely based on the outcome of that evaluation. Experiments on three mathematical reasoning tasks and four datasets with missing premises demonstrate that our method substantially reduces answer length and yields significant improvements across nearly all datasets upon 4 widely-used RLLMs. Further analysis demonstrates that our method effectively minimizes the number of reasoning steps and reduces self-doubt.",,"Keqin Peng, Liang Ding, Yuanxin Ouyang, Meng Fang, Dacheng Tao",2025-05-29T14:30:02Z,Revisiting Overthinking in Long Chain-of-Thought from the Perspective of   Self-Doubt,Überdenken in der langen Kette des Denkens aus der Perspektive des Selbstzweifels,从自杜卜特的视角重新思考长期思维链中的过度思考问题,http://arxiv.org/abs/2505.23480v1
1636,"The Congress of Neurological Surgeons Self-Assessment for Neurological Surgeons (CNS-SANS) questions are widely used by neurosurgical residents to prepare for written board examinations. Recently, these questions have also served as benchmarks for evaluating large language models' (LLMs) neurosurgical knowledge. This study aims to assess the performance of state-of-the-art LLMs on neurosurgery board-like questions and to evaluate their robustness to the inclusion of distractor statements. A comprehensive evaluation was conducted using 28 large language models. These models were tested on 2,904 neurosurgery board examination questions derived from the CNS-SANS. Additionally, the study introduced a distraction framework to assess the fragility of these models. The framework incorporated simple, irrelevant distractor statements containing polysemous words with clinical meanings used in non-clinical contexts to determine the extent to which such distractions degrade model performance on standard medical benchmarks. 6 of the 28 tested LLMs achieved board-passing outcomes, with the top-performing models scoring over 15.7% above the passing threshold. When exposed to distractions, accuracy across various model architectures was significantly reduced-by as much as 20.4%-with one model failing that had previously passed. Both general-purpose and medical open-source models experienced greater performance declines compared to proprietary variants when subjected to the added distractors. While current LLMs demonstrate an impressive ability to answer neurosurgery board-like exam questions, their performance is markedly vulnerable to extraneous, distracting information. These findings underscore the critical need for developing novel mitigation strategies aimed at bolstering LLM resilience against in-text distractions, particularly for safe and effective clinical deployment.",,"Krithik Vishwanath, Anton Alyakin, Mrigayu Ghosh, Jin Vivian Lee, Daniel Alexander Alber, Karl L. Sangwon, Douglas Kondziolka, Eric Karl Oermann",2025-05-29T14:27:14Z,Evaluating the performance and fragility of large language models on the   self-assessment for neurological surgeons,Bewertung der Leistungsfähigkeit und Fragilität großer Sprachmodelle auf der Selbsteinschätzung für neurologische Chirurgen,评价神经外科医生自我评估大语言模型的性能和脆弱性,http://arxiv.org/abs/2505.23477v1
1637,"Process Reward Models (PRMs) are crucial in complex reasoning and problem-solving tasks (e.g., LLM agents with long-horizon decision-making) by verifying the correctness of each intermediate reasoning step. In real-world scenarios, LLMs may apply various reasoning patterns (e.g., decomposition) to solve a problem, potentially suffering from errors under various reasoning patterns. Therefore, PRMs are required to identify errors under various reasoning patterns during the reasoning process. However, existing benchmarks mainly focus on evaluating PRMs with stepwise correctness, ignoring a systematic evaluation of PRMs under various reasoning patterns. To mitigate this gap, we introduce Socratic-PRMBench, a new benchmark to evaluate PRMs systematically under six reasoning patterns, including Transformation, Decomposition, Regather, Deduction, Verification, and Integration. Socratic-PRMBench}comprises 2995 reasoning paths with flaws within the aforementioned six reasoning patterns. Through our experiments on both PRMs and LLMs prompted as critic models, we identify notable deficiencies in existing PRMs. These observations underscore the significant weakness of current PRMs in conducting evaluations on reasoning steps under various reasoning patterns. We hope Socratic-PRMBench can serve as a comprehensive testbed for systematic evaluation of PRMs under diverse reasoning patterns and pave the way for future development of PRMs.",,"Xiang Li, Haiyang Yu, Xinghua Zhang, Ziyang Huang, Shizhu He, Kang Liu, Jun Zhao, Fei Huang, Yongbin Li",2025-05-29T14:26:53Z,Socratic-PRMBench: Benchmarking Process Reward Models with Systematic   Reasoning Patterns,Scratic-PRMBench: Benchmarking-Prozess-Reward-Modelle mit systematischen Begründungsmustern,Scorti-PRMBench:有系统说明理由模式的基准进程奖励模式,http://arxiv.org/abs/2505.23474v1
1638,"Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a ""flood irrigation"" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent's self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available at https://github.com/zjunlp/KnowSelf.",,"Shuofei Qiao, Zhisong Qiu, Baochang Ren, Xiaobin Wang, Xiangyuan Ru, Ningyu Zhang, Xiang Chen, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen",2025-05-29T14:15:53Z,Agentic Knowledgeable Self-awareness,Agentisch sachkundiges Selbstbewußtsein,A. 动态知识自觉意识,http://arxiv.org/abs/2504.03553v2
1639,"Handling unanswerable questions (UAQ) is crucial for LLMs, as it helps prevent misleading responses in complex situations. While previous studies have built several datasets to assess LLMs' performance on UAQ, these datasets lack factual knowledge support, which limits the evaluation of LLMs' ability to utilize their factual knowledge when handling UAQ. To address the limitation, we introduce a new unanswerable question dataset UAQFact, a bilingual dataset with auxiliary factual knowledge created from a Knowledge Graph. Based on UAQFact, we further define two new tasks to measure LLMs' ability to utilize internal and external factual knowledge, respectively. Our experimental results across multiple LLM series show that UAQFact presents significant challenges, as LLMs do not consistently perform well even when they have factual knowledge stored. Additionally, we find that incorporating external knowledge may enhance performance, but LLMs still cannot make full use of the knowledge which may result in incorrect responses.",,"Chuanyuan Tan, Wenbiao Shao, Hao Xiong, Tong Zhu, Zhenhua Liu, Kai Shi, Wenliang Chen",2025-05-29T14:10:24Z,UAQFact: Evaluating Factual Knowledge Utilization of LLMs on   Unanswerable Questions,UAQFact: Bewertung der tatsächlichen Wissensnutzung von LLMs auf unbeantwortbaren Fragen,UAQFact:评估关于无法回答问题LLMs的实情知识利用情况,http://arxiv.org/abs/2505.23461v1
1640,"Large language models (LLMs) are widely applied in various fields of society due to their powerful reasoning, understanding, and generation capabilities. However, the security issues associated with these models are becoming increasingly severe. Jailbreaking attacks, as an important method for detecting vulnerabilities in LLMs, have been explored by researchers who attempt to induce these models to generate harmful content through various attack methods. Nevertheless, existing jailbreaking methods face numerous limitations, such as excessive query counts, limited coverage of jailbreak modalities, low attack success rates, and simplistic evaluation methods. To overcome these constraints, this paper proposes a multimodal jailbreaking method: JMLLM. This method integrates multiple strategies to perform comprehensive jailbreak attacks across text, visual, and auditory modalities. Additionally, we contribute a new and comprehensive dataset for multimodal jailbreaking research: TriJail, which includes jailbreak prompts for all three modalities. Experiments on the TriJail dataset and the benchmark dataset AdvBench, conducted on 13 popular LLMs, demonstrate advanced attack success rates and significant reduction in time overhead.",,"Yanxu Mao, Peipei Liu, Tiehan Cui, Zhaoteng Yan, Congying Liu, Datao You",2025-05-29T14:05:50Z,Divide and Conquer: A Hybrid Strategy Defeats Multimodal Large Language   Models,Teilen und Erobern: Eine hybride Strategie besiegt multimodale große Sprachmodelle,差异和征服:混合战略失败 多种多模式大语言模式,http://arxiv.org/abs/2412.16555v3
1641,"Multimodal language models (MLMs) increasingly communicate in human-like ways, yet their ability to use reference words remains largely overlooked despite their ubiquity in everyday communication. Our study addresses this gap by comparing human and MLM use of three word classes with increasing cognitive demands: vocabulary words, possessive pronouns (`mine' vs `yours'), and demonstrative pronouns (`this one' vs `that one'). Evaluating seven state-of-the-art MLMs against human participants, we observe a clear difficulty hierarchy: while MLMs approach human-level performance on the vocabulary task, they show substantial deficits with possessives and demonstratives. Our analysis reveals these difficulties stem from limitations in perspective-taking and spatial reasoning. Although prompt engineering improved model performance on possessive use, demonstrative use remained well below human-level competence. These findings provide theoretical and empirical evidence that producing grammatical forms requiring pragmatics and social cognition remains a clear challenge in current NLP systems.",,"Dota Tianai Dong, Yifan Luo, Po-Ya Angela Wang, Asli Ozyurek, Paula Rubio-Fernandez",2025-05-29T13:54:44Z,"You Prefer This One, I Prefer Yours: Using Reference Words is Harder   Than Vocabulary Words for Humans and Multimodal Language Models","Sie bevorzugen diese, ich bevorzuge Ihre: Die Verwendung von Referenzwörtern ist härter als Vokabelworte für Menschen und multimodale Sprachmodelle","您更喜欢这个, 我更喜欢您: 使用参考词对人类和多模式语言模式来说比词汇更难",http://arxiv.org/abs/2506.00065v1
1642,"Large language models (LLMs) have demonstrated significant advancements in error handling. Current error-handling works are performed in a passive manner, with explicit error-handling instructions. However, in real-world scenarios, explicit error-handling instructions are usually unavailable. In this paper, our work identifies this challenge as how to conduct proactive error handling without explicit error handling instructions. To promote further research, this work introduces a new benchmark, termed Mis-prompt, consisting of four evaluation tasks, an error category taxonomy, and a new evaluation dataset. Furthermore, this work analyzes current LLMs' performance on the benchmark, and the experimental results reveal that current LLMs show poor performance on proactive error handling, and SFT on error handling instances improves LLMs' proactive error handling capabilities. The dataset will be publicly available.",,"Jiayi Zeng, Yizhe Feng, Mengliang He, Wenhui Lei, Wei Zhang, Zeming Liu, Xiaoming Shi, Aimin Zhou",2025-05-29T13:52:58Z,Mis-prompt: Benchmarking Large Language Models for Proactive Error   Handling,Mis-prompt: Benchmarking großer Sprachmodelle für proaktive Fehlerbehandlung,错误-即时:为积极主动的错误处理制定大语言模式基准,http://arxiv.org/abs/2506.00064v1
1643,"Large Language Models (LLMs) fine-tuning technologies have achieved remarkable results. However, traditional LLM fine-tuning approaches face significant challenges: they require large Floating Point (FP) computation, raising privacy concerns when handling sensitive data, and are impractical for resource-constrained edge devices. While Parameter-Efficient Fine-Tuning (PEFT) techniques reduce trainable parameters, their reliance on floating-point arithmetic creates fundamental incompatibilities with edge hardware. In this work, we introduce a novel framework for on-device LLM fine-tuning that eliminates the need for floating-point operations in both inference and training, named GSQ-Tuning. At its core is the Group-Shared Exponents Integer format, which efficiently represents model parameters in integer format using shared exponents among parameter groups. When combined with LoRA-like adapters, this enables fully integer-based fine-tuning that is both memory and compute efficient. We demonstrate that our approach achieves accuracy comparable to BF16-based fine-tuning while significantly reducing 1.85x memory usage. Moreover, compared to FP8, our method can reduce 5x power consumption and 11x chip area with same performance, making large-scale model adaptation feasible on edge devices.",,"Sifan Zhou, Shuo Wang, Zhihang Yuan, Mingjia Shi, Yuzhang Shang, Dawei Yang",2025-05-29T13:50:58Z,GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training   for LLMs On-Device Fine-tuning,GSQ-Tuning: Group-Shared Exponents integer in einer voll quantifizierten Schulung für LLMs On-Device-Fine-Tuning,GSQ-Turning:为在线设计精微调LLM女士提供全面量化培训的集团共享指数整数,http://arxiv.org/abs/2502.12913v3
1644,"Large language models (LLMs) have made significant progress in natural language understanding and generation, driven by scalable pretraining and advanced finetuning. However, enhancing reasoning abilities in LLMs, particularly via reinforcement learning from human feedback (RLHF), remains challenging due to the scarcity of high-quality preference data, which is labor-intensive to annotate and crucial for reward model (RM) finetuning. To alleviate this issue, we introduce CodePMP, a scalable preference model pretraining (PMP) pipeline that utilizes a large corpus of synthesized code-preference pairs from publicly available high-quality source code. CodePMP improves RM finetuning efficiency by pretraining preference models on large-scale synthesized code-preference pairs. We evaluate CodePMP on mathematical reasoning tasks (GSM8K, MATH) and logical reasoning tasks (ReClor, LogiQA2.0), consistently showing significant improvements in reasoning performance of LLMs and highlighting the importance of scalable preference model pretraining for efficient reward modeling.",,"Huimu Yu, Xing Wu, Haotian Xu, Debing Zhang, Songlin Hu",2025-05-29T13:40:26Z,CodePMP: Scalable Preference Model Pretraining for Large Language Model   Reasoning,CodePMP: Skalierbares Präferenzmodell Vorschulung für großsprachliche Modellaufklärung,守则PMP:可缩放的特惠模式大语言示范理由预培训模式,http://arxiv.org/abs/2410.02229v2
1645,"Knowledge graph completion (KGC) has attracted considerable attention in recent years because it is critical to improving the quality of knowledge graphs. Researchers have continuously explored various models. However, most previous efforts have neglected to take advantage of regularization from a deeper perspective and therefore have not been used to their full potential. This paper rethinks the application of regularization methods in KGC. Through extensive empirical studies on various KGC models, we find that carefully designed regularization not only alleviates overfitting and reduces variance but also enables these models to break through the upper bounds of their original performance. Furthermore, we introduce a novel sparse-regularization method that embeds the concept of rank-based selective sparsity into the KGC regularizer. The core idea is to selectively penalize those components with significant features in the embedding vector, thus effectively ignoring many components that contribute little and may only represent noise. Various comparative experiments on multiple datasets and multiple models show that the SPR regularization method is better than other regularization methods and can enable the KGC model to further break through the performance margin.",,"Linyu Li, Zhi Jin, Yuanpeng He, Dongming Jin, Haoran Duan, Zhengwei Tao, Xuan Zhang, Jiandong Li",2025-05-29T13:39:18Z,Rethinking Regularization Methods for Knowledge Graph Completion,Überdenken von Regularisierungsmethoden für Wissensgraphenvervollständigung,重新思考知识图完成正规化方法,http://arxiv.org/abs/2505.23442v1
1646,"Fine-tuning large language models (LLMs) for telecom tasks and datasets is a common practice to adapt general-purpose models to the telecom domain. However, little attention has been paid to how this process may compromise model safety. Recent research has shown that even benign fine-tuning can degrade the safety alignment of LLMs, causing them to respond to harmful or unethical user queries. In this paper, we investigate this issue for telecom-tuned LLMs using three representative datasets featured by the GenAINet initiative. We show that safety degradation persists even for structured and seemingly harmless datasets such as 3GPP standards and tabular records, indicating that telecom-specific data is not immune to safety erosion during fine-tuning. We further extend our analysis to publicly available Telecom LLMs trained via continual pre-training, revealing that safety alignment is often severely lacking, primarily due to the omission of safety-focused instruction tuning. To address these issues in both fine-tuned and pre-trained models, we conduct extensive experiments and evaluate three safety realignment defenses (SafeInstruct, SafeLoRA, and SafeMERGE) using established red-teaming benchmarks. The results show that, across all settings, the proposed defenses can effectively restore safety after harmful degradation without compromising downstream task performance, leading to Safe teleCOMMunication (SafeCOMM) models. In a nutshell, our work serves as a diagnostic study and practical guide for safety realignment in telecom-tuned LLMs, and emphasizes the importance of safety-aware instruction and fine-tuning for real-world deployments of Telecom LLMs.",,"Aladin Djuhera, Swanand Ravindra Kadhe, Farhan Ahmed, Syed Zawad, Holger Boche, Walid Saad",2025-05-29T13:31:51Z,SafeCOMM: What about Safety Alignment in Fine-Tuned Telecom Large   Language Models?,SafeCOMM: Wie steht es mit der Sicherheitsausrichtung in großformatigen Telecom-Modellen?,SafeCOMM: 精密远程大语言模型中的安全协调呢?,http://arxiv.org/abs/2506.00062v1
1647,"Large Language Models (LLMs) are primarily designed for batch processing. Existing methods for adapting LLMs to streaming rely either on expensive re-encoding or specialized architectures with limited scalability. This work identifies three key mismatches in adapting batch-oriented LLMs to streaming: (1) input-attention, (2) output-attention, and (3) position-ID mismatches. While it is commonly assumed that the latter two mismatches require frequent re-encoding, our analysis reveals that only the input-attention mismatch significantly impacts performance, indicating re-encoding outputs is largely unnecessary. To better understand this discrepancy with the common assumption, we provide the first comprehensive analysis of the impact of position encoding on LLMs in streaming, showing that preserving relative positions within source and target contexts is more critical than maintaining absolute order. Motivated by the above analysis, we introduce a group position encoding paradigm built on batch architectures to enhance consistency between streaming and batch modes. Extensive experiments on cross-lingual and cross-modal tasks demonstrate that our method outperforms existing approaches. Our method requires no architectural modifications, exhibits strong generalization in both streaming and batch modes. The code is available at repository https://github.com/EIT-NLP/StreamingLLM.",,"Junlong Tong, Jinlan Fu, Zixuan Lin, Yingqi Fan, Anhao Zhao, Hui Su, Xiaoyu Shen",2025-05-29T13:22:18Z,LLM as Effective Streaming Processor: Bridging Streaming-Batch   Mismatches with Group Position Encoding,LLM als Effektiver Streaming-Prozessor: Überbrückung von Streaming-Batch-Mismatches mit Gruppenpositionskodierung,LLM 有效流化处理程序: 将流流-批量错误与群居位置编码连接起来,http://arxiv.org/abs/2505.16983v2
1648,"Aligning Large Language Models to integrate and reflect human values, especially for tasks that demand intricate human oversight, is arduous since it is resource-intensive and time-consuming to depend on human expertise for context-specific guidance. Prior work has utilized predefined sets of rules or principles to steer the behavior of models (Bai et al., 2022; Sun et al., 2023). However, these principles tend to be generic, making it challenging to adapt them to each individual input query or context. In this work, we present Situated-PRInciples (SPRI), a framework requiring minimal or no human effort that is designed to automatically generate guiding principles in real-time for each input query and utilize them to align each response. We evaluate SPRI on three tasks, and show that 1) SPRI can derive principles in a complex domain-specific task that leads to on-par performance as expert-crafted ones; 2) SPRI-generated principles lead to instance-specific rubrics that outperform prior LLM-as-a-judge frameworks; 3) using SPRI to generate synthetic SFT data leads to substantial improvement on truthfulness. We release our code and model generations at https://github.com/honglizhan/SPRI-public.",,"Hongli Zhan, Muneeza Azmat, Raya Horesh, Junyi Jessy Li, Mikhail Yurochkin",2025-05-29T13:20:34Z,SPRI: Aligning Large Language Models with Context-Situated Principles,SPRI: Ausrichtung großer Sprachmodelle mit kontext-situierten Prinzipien,SPRI:使大语言模式与上下文原则相一致,http://arxiv.org/abs/2502.03397v2
1649,"The rapid advancement of large language models (LLMs) has significantly improved their performance in code generation tasks. However, existing code benchmarks remain static, consisting of fixed datasets with predefined problems. This makes them vulnerable to memorization during training, where LLMs recall specific test cases instead of generalizing to new problems, leading to data contamination and unreliable evaluation results. To address these issues, we introduce DynaCode, a dynamic, complexity-aware benchmark that overcomes the limitations of static datasets. DynaCode evaluates LLMs systematically using a complexity-aware metric, incorporating both code complexity and call-graph structures. DynaCode achieves large-scale diversity, generating up to 189 million unique nested code problems across four distinct levels of code complexity, referred to as units, and 16 types of call graphs. Results on 12 latest LLMs show an average performance drop of 16.8% to 45.7% compared to MBPP+, a static code generation benchmark, with performance progressively decreasing as complexity increases. This demonstrates DynaCode's ability to effectively differentiate LLMs. Additionally, by leveraging call graphs, we gain insights into LLM behavior, particularly their preference for handling subfunction interactions within nested code. Our benchmark and evaluation code are available at https://github.com/HWH-2000/DynaCode.",,"Wenhao Hu, Jinhao Duan, Chunchen Wei, Li Zhang, Yue Zhang, Kaidi Xu",2025-05-29T13:17:33Z,DynaCode: A Dynamic Complexity-Aware Code Benchmark for Evaluating Large   Language Models in Code Generation,DynaCode: Dynamischer Code Benchmark für die Bewertung großer Sprachmodelle in der Codegenerierung,DynCode:在代码生成过程中评价大语言模型的动态复杂度-软件编码基准,http://arxiv.org/abs/2503.10452v2
1650,"Reliable evaluation of large language models (LLMs) is impeded by two key challenges: objective metrics often fail to reflect human perception of natural language, and exhaustive human labeling is prohibitively expensive. Here, we propose a sample-efficient human evaluation method for LLMs based on the principle of MAximum Discrepancy (MAD) Competition. Our method automatically and adaptively selects a compact set of input instructions that maximize semantic discrepancy between pairs of LLM responses. Human evaluators then perform three-alternative forced choices on these paired responses, which are aggregated into a global ranking using Elo rating. We apply our approach to compare eight widely used LLMs across four tasks: scientific knowledge understanding, mathematical reasoning, creative and functional writing, and code generation and explanation. Experimental results show that our sample-efficient evaluation method recovers ""gold-standard"" model rankings with a handful of MAD-selected instructions, reveals respective strengths and weaknesses of each LLM, and offers nuanced insights to guide future LLM development. Code is available at https://github.com/weiji-Feng/MAD-Eval .",,"Kehua Feng, Keyan Ding, Hongzhi Tan, Kede Ma, Zhihua Wang, Shuangquan Guo, Yuzhou Cheng, Ge Sun, Guozhou Zheng, Qiang Zhang, Huajun Chen",2025-05-29T13:16:05Z,Sample-Efficient Human Evaluation of Large Language Models via Maximum   Discrepancy Competition,Probeneffiziente menschliche Bewertung großer Sprachmodelle durch maximalen Diskrepanzwettbewerb,通过最大差异竞争对大语言模式进行抽样有效人力评价,http://arxiv.org/abs/2404.08008v2
1651,"Training large-scale models presents challenges not only in terms of resource requirements but also in terms of their convergence. For this reason, the learning rate (LR) is often decreased when the size of a model is increased. Such a simple solution is not enough in the case of speech-to-text (S2T) trainings, where evolved and more complex variants of the Transformer architecture -- e.g., Conformer or Branchformer -- are used in light of their better performance. As a workaround, OWSM designed a double linear warmup of the LR, increasing it to a very small value in the first phase before updating it to a higher value in the second phase. While this solution worked well in practice, it was not compared with alternative solutions, nor was the impact on the final performance of different LR warmup schedules studied. This paper fills this gap, revealing that i) large-scale S2T trainings demand a sub-exponential LR warmup, and ii) a higher LR in the warmup phase accelerates initial convergence, but it does not boost final performance.",,"Marco Gaido, Sara Papi, Luisa Bentivogli, Alessio Brutti, Mauro Cettolo, Roberto Gretter, Marco Matassoni, Mohamed Nabih, Matteo Negri",2025-05-29T13:10:57Z,The Warmup Dilemma: How Learning Rate Strategies Impact Speech-to-Text   Model Convergence,Das Warmup-Dilemma: Wie sich Lernratenstrategien auf die Konvergenz von Sprach-Text-Modellen auswirken,暖化困境:学习速率战略如何影响演讲到文字模式模式汇合,http://arxiv.org/abs/2505.23420v1
1652,"On-device LLMs have gained increasing attention for their ability to enhance privacy and provide a personalized user experience. To facilitate private learning with scarce data, Federated Learning has become a standard approach. However, it faces challenges such as computational resource heterogeneity and data heterogeneity among end users. We propose CoMiGS ($\textbf{Co}$llaborative learning with a $\textbf{Mi}$xture of $\textbf{G}$eneralists and $\textbf{S}$pecialists), the first approach to address both challenges. A key innovation of our method is the bi-level optimization formulation of the Mixture-of-Experts learning objective, where the router is optimized using a separate validation set to ensure alignment with the target distribution. We solve our objective with alternating minimization, for which we provide a theoretical analysis. Our method shares generalist experts across users while localizing a varying number of specialist experts, thereby adapting to users' computational resources and preserving privacy. Through extensive experiments, we show CoMiGS effectively balances general and personalized knowledge for each token generation. We demonstrate that CoMiGS remains robust against overfitting-due to the generalists' regularizing effect-while adapting to local data through specialist expertise. We open source our codebase for collaborative LLMs.",,"Dongyang Fan, Bettina Messmer, Nikita Doikov, Martin Jaggi",2025-05-29T13:07:31Z,On-Device Collaborative Language Modeling via a Mixture of Generalists   and Specialists,On-Device Collaborative Language Modeling über eine Mischung aus Generalisten und Spezialisten,通过通识主义者和专家混合组合的在线合作语言建模,http://arxiv.org/abs/2409.13931v4
1653,"When the complete source sentence is provided, Large Language Models (LLMs) perform excellently in offline machine translation even with a simple prompt ""Translate the following sentence from [src lang] into [tgt lang]:"". However, in many real scenarios, the source tokens arrive in a streaming manner and simultaneous machine translation (SiMT) is required, then the efficiency and performance of decoder-only LLMs are significantly limited by their auto-regressive nature. To enable LLMs to achieve high-quality SiMT as efficiently as offline translation, we propose a novel paradigm that includes constructing supervised fine-tuning (SFT) data for SiMT, along with new training and inference strategies. To replicate the token input/output stream in SiMT, the source and target tokens are rearranged into an interleaved sequence, separated by special tokens according to varying latency requirements. This enables powerful LLMs to learn read and write operations adaptively, based on varying latency prompts, while still maintaining efficient auto-regressive decoding. Experimental results show that, even with limited SFT data, our approach achieves state-of-the-art performance across various SiMT benchmarks, and preserves the original abilities of offline translation. Moreover, our approach generalizes well to document-level SiMT setting without requiring specific fine-tuning, even beyond the offline translation model.",,"Biao Fu, Minpeng Liao, Kai Fan, Chengxi Li, Liang Zhang, Yidong Chen, Xiaodong Shi",2025-05-29T13:06:43Z,LLMs Can Achieve High-quality Simultaneous Machine Translation as   Efficiently as Offline,LLMs können qualitativ hochwertige Simultane Machine Translation so effizient wie Offline erreichen,LLM Can 能够像离线那样高效率地实现高质量同声机翻译,http://arxiv.org/abs/2504.09570v2
1654,"Factual knowledge extraction aims to explicitly extract knowledge parameterized in pre-trained language models for application in downstream tasks. While prior work has been investigating the impact of supervised fine-tuning data on the factuality of large language models (LLMs), its mechanism remains poorly understood. We revisit this impact through systematic experiments, with a particular focus on the factuality gap that arises when fine-tuning on known versus unknown knowledge. Our findings show that this gap can be mitigated at the inference stage, either under out-of-distribution (OOD) settings or by using appropriate in-context learning (ICL) prompts (i.e., few-shot learning and Chain of Thought (CoT)). We prove this phenomenon theoretically from the perspective of knowledge graphs, showing that the test-time prompt may diminish or even overshadow the impact of fine-tuning data and play a dominant role in knowledge extraction. Ultimately, our results shed light on the interaction between finetuning data and test-time prompt, demonstrating that ICL can effectively compensate for shortcomings in fine-tuning data, and highlighting the need to reconsider the use of ICL prompting as a means to evaluate the effectiveness of fine-tuning data selection methods.",,"Xuan Gong, Hanbo Huang, Shiyu Liang",2025-05-29T12:59:30Z,From Parameters to Prompts: Understanding and Mitigating the Factuality   Gap between Fine-Tuned LLMs,Von Parametern zu Prompts: Den Factuality Gap zwischen fein getunen LLMs verstehen und abschwächen,从参数到提示:了解并缩小微量贷款商之间的实际质量差距,http://arxiv.org/abs/2505.23410v1
1655,"Large language models (LLMs) are often used for infilling tasks, which involve predicting or generating missing information in a given text. These tasks typically require multiple interactions with similar context. To reduce the computation of repeated historical tokens, cross-request key-value (KV) cache reuse, a technique that stores and reuses intermediate computations, has become a crucial method in multi-round interactive services. However, in infilling tasks, the KV cache reuse is often hindered by the structure of the prompt format, which typically consists of a prefix and suffix relative to the insertion point. Specifically, the KV cache of the prefix or suffix part is frequently invalidated as the other part (suffix or prefix) is incrementally generated. To address the issue, we propose EFIM, a transformed prompt format of FIM to unleash the performance potential of KV cache reuse. Although the transformed prompt can solve the inefficiency, it exposes subtoken generation problems in current LLMs, where they have difficulty generating partial words accurately. Therefore, we introduce a fragment tokenization training method which splits text into multiple fragments before tokenization during data processing. Experiments on two representative LLMs show that LLM serving with EFIM can lower the latency by 52% and improve the throughput by 98% while maintaining the original infilling capability. EFIM's source code is publicly available at https://github.com/gty111/EFIM.",,"Tianyu Guo, Hande Dong, Yichong Leng, Feng Liu, Cheater Lin, Nong Xiao, Xianwei Zhang",2025-05-29T12:59:26Z,EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV   Cache Reuse,EFIM: Effizientes Servieren von LLMs zur Erfüllung von Aufgaben mit verbesserter KV Cache Reuse,EFIM:以改进的KV缓存再利用高效率地为完成任务的LLMs服务,http://arxiv.org/abs/2505.21889v2
1656,"Automatic speech recognition (ASR) has made remarkable progress but heavily relies on large-scale labeled data, which is scarce for low-resource languages like Vietnamese. While existing systems such as Whisper, USM, and MMS achieve promising performance, their efficacy remains inadequate in terms of training costs, latency, and accessibility. To address these issues, we propose VietASR, a novel ASR training pipeline that leverages vast amounts of unlabeled data and a small set of labeled data. Through multi-iteration ASR-biased self-supervised learning on a large-scale unlabeled dataset, VietASR offers a cost-effective and practical solution for enhancing ASR performance. Experiments demonstrate that pre-training on 70,000-hour unlabeled data and fine-tuning on merely 50-hour labeled data yield a lightweight but powerful ASR model. It outperforms Whisper Large-v3 and commercial ASR systems on real-world data. Our code and models will be open-sourced to facilitate research in low-resource ASR.",,"Jianheng Zhuo, Yifan Yang, Yiwen Shao, Yong Xu, Dong Yu, Kai Yu, Xie Chen",2025-05-29T12:55:12Z,VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled   data and Large-Scale Speech Pretraining,VietASR: Erzielen von vietnamesischen ASR auf Branchenebene mit 50-Stunden-Daten und großformatigen Sprachvorschulungen,越南:在越南工业一级实现有50小时标签数据和大型演讲预科培训的有50小时标签的数据的越南ASR,http://arxiv.org/abs/2505.21527v2
1657,"Social media platforms utilize Machine Learning (ML) and Artificial Intelligence (AI) powered recommendation algorithms to maximize user engagement, which can result in inadvertent exposure to harmful content. Current moderation efforts, reliant on classifiers trained with extensive human-annotated data, struggle with scalability and adapting to new forms of harm. To address these challenges, we propose a novel re-ranking approach using Large Language Models (LLMs) in zero-shot and few-shot settings. Our method dynamically assesses and re-ranks content sequences, effectively mitigating harmful content exposure without requiring extensive labeled data. Alongside traditional ranking metrics, we also introduce two new metrics to evaluate the effectiveness of re-ranking in reducing exposure to harmful content. Through experiments on three datasets, three models and across three configurations, we demonstrate that our LLM-based approach significantly outperforms existing proprietary moderation approaches, offering a scalable and adaptable solution for harm mitigation.",,"Rajvardhan Oak, Muhammad Haroon, Claire Jo, Magdalena Wojcieszak, Anshuman Chhabra",2025-05-29T12:42:55Z,Re-ranking Using Large Language Models for Mitigating Exposure to   Harmful Content on Social Media Platforms,Re-Ranking mit großen Sprachmodellen zur Minderung der Exposition gegenüber schädlichen Inhalten auf Social Media-Plattformen,"利用大型语言模式,在社交媒体平台上减少接触有害内容",http://arxiv.org/abs/2501.13977v3
1658,"Speculative decoding (SD) has emerged as a powerful method for accelerating autoregressive generation in large language models (LLMs), yet its integration into vision-language models (VLMs) remains underexplored. We introduce DREAM, a novel speculative decoding framework tailored for VLMs that combines three key innovations: (1) a cross-attention-based mechanism to inject intermediate features from the target model into the draft model for improved alignment, (2) adaptive intermediate feature selection based on attention entropy to guide efficient draft model training, and (3) visual token compression to reduce draft model latency. DREAM enables efficient, accurate, and parallel multimodal decoding with significant throughput improvement. Experiments across a diverse set of recent popular VLMs, including LLaVA, Pixtral, SmolVLM and Gemma3, demonstrate up to 3.6x speedup over conventional decoding and significantly outperform prior SD baselines in both inference throughput and speculative draft acceptance length across a broad range of multimodal benchmarks. The code is publicly available at: https://github.com/SAI-Lab-NYU/DREAM.git",,"Yunhai Hu, Tianhua Xia, Zining Liu, Rahul Raman, Xingyu Liu, Bo Bao, Eric Sather, Vithursan Thangarasa, Sai Qian Zhang",2025-05-29T12:40:23Z,DREAM: Drafting with Refined Target Features and Entropy-Adaptive   Cross-Attention Fusion for Multimodal Speculative Decoding,DREAM: Entwurf mit raffinierten Target-Features und Entropie-Adaptive Cross-Attention Fusion für multimodale spekulative Dekodierung,DREAM: 与改良目标特征和多模式投机下限的 Entropy-Adpy-Adpic 交叉注意聚变一起起草,http://arxiv.org/abs/2505.19201v2
1659,"Code generation plays a crucial role in various tasks, such as code auto-completion and mathematical reasoning. Previous work has proposed numerous methods to enhance code generation performance, including integrating feedback from the compiler. Inspired by this, we present ReflectionCoder, a novel approach that effectively leverages reflection sequences constructed by integrating compiler feedback to improve one-off code generation performance. Furthermore, we propose reflection self-distillation and dynamically masked distillation to effectively utilize these reflection sequences. Extensive experiments on three benchmarks, i.e., HumanEval (+), MBPP (+), and MultiPL-E, demonstrate that models fine-tuned with our method achieve state-of-the-art performance. Beyond the code domain, we believe this approach can benefit other domains that focus on final results and require long reasoning paths. Code and data are available at https://github.com/SenseLLM/ReflectionCoder.",,"Houxing Ren, Mingjie Zhan, Zhongyuan Wu, Aojun Zhou, Junting Pan, Hongsheng Li",2025-05-29T12:34:04Z,ReflectionCoder: Learning from Reflection Sequence for Enhanced One-off   Code Generation,ReflectionCoder: Aus Reflexionssequenz lernen für verbesserte Einmal-Code-Generierung,思考编码:从强化一次性代码生成的反思序列中学习,http://arxiv.org/abs/2405.17057v2
1660,"People worldwide use language in subtle and complex ways to express emotions. Although emotion recognition--an umbrella term for several NLP tasks--impacts various applications within NLP and beyond, most work in this area has focused on high-resource languages. This has led to significant disparities in research efforts and proposed solutions, particularly for under-resourced languages, which often lack high-quality annotated datasets. In this paper, we present BRIGHTER--a collection of multi-labeled, emotion-annotated datasets in 28 different languages and across several domains. BRIGHTER primarily covers low-resource languages from Africa, Asia, Eastern Europe, and Latin America, with instances labeled by fluent speakers. We highlight the challenges related to the data collection and annotation processes, and then report experimental results for monolingual and crosslingual multi-label emotion identification, as well as emotion intensity recognition. We analyse the variability in performance across languages and text domains, both with and without the use of LLMs, and show that the BRIGHTER datasets represent a meaningful step towards addressing the gap in text-based emotion recognition.",,"Shamsuddeen Hassan Muhammad, Nedjma Ousidhoum, Idris Abdulmumin, Jan Philip Wahle, Terry Ruas, Meriem Beloucif, Christine de Kock, Nirmal Surange, Daniela Teodorescu, Ibrahim Said Ahmad, David Ifeoluwa Adelani, Alham Fikri Aji, Felermino D. M. A. Ali, Ilseyar Alimova, Vladimir Araujo, Nikolay Babakov, Naomi Baes, Ana-Maria Bucur, Andiswa Bukula, Guanqun Cao, Rodrigo Tufino Cardenas, Rendi Chevi, Chiamaka Ijeoma Chukwuneke, Alexandra Ciobotaru, Daryna Dementieva, Murja Sani Gadanya, Robert Geislinger, Bela Gipp, Oumaima Hourrane, Oana Ignat, Falalu Ibrahim Lawan, Rooweither Mabuya, Rahmad Mahendra, Vukosi Marivate, Alexander Panchenko, Andrew Piper, Charles Henrique Porto Ferreira, Vitaly Protasov, Samuel Rutunda, Manish Shrivastava, Aura Cristina Udrea, Lilian Diana Awuor Wanzare, Sophie Wu, Florian Valentin Wunderlich, Hanif Muhammad Zhafran, Tianhui Zhang, Yi Zhou, Saif M. Mohammad",2025-05-29T12:33:29Z,BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion   Recognition Datasets for 28 Languages,BRIGHER: Die Lücke in Text-Emotions-Erkennungs-Datensätzen für 28 Sprachen bohren,消除28种语言在载人附加说明的文本情感识别识别数据集方面的差距,http://arxiv.org/abs/2502.11926v4
1661,"Large language models (LLMs) show impressive performance in solving complex language tasks. However, its large number of parameters presents significant challenges for the deployment. So, compressing LLMs to low bits can enable to deploy on resource-constrained devices. To address this problem, we propose gradient-aware weight quantization (GWQ), the first quantization approach for low-bit weight quantization that leverages gradients to localize outliers, requiring only a minimal amount of calibration data for outlier detection. GWQ retains the top 1\% outliers preferentially at FP16 precision, while the remaining non-outlier weights are stored in a low-bit. We widely evaluate GWQ on different task include language modeling, grounding detection, massive multitask language understanding and vision-language question and answering. Results show that models quantified by GWQ performs better than other quantization method. During quantization process, GWQ only need one calibration set to realize effective quant. Also, GWQ achieves 1.2x inference speedup in comparison to the original model and effectively reduces the inference memory.",,"Yihua Shao, Yan Gu, Siyu Chen, Haiyang Liu, Zixian Zhu, Zijian Ling, Minxi Yan, Ziyang Yan, Chenyu Zhang, Michele Magno, Haotong Qin, Yan Wang, Jingcai Guo, Ling Shao, Hao Tang",2025-05-29T11:56:28Z,GWQ: Gradient-Aware Weight Quantization for Large Language Models,GWQ: Gradient-Aware Weight Quantization für große Sprachmodelle,GWQ: 大语言模型的渐变软件重量,http://arxiv.org/abs/2411.00850v4
1662,"Retrieval-augmented generation (RAG) has revitalized Large Language Models (LLMs) by injecting non-parametric factual knowledge. Compared with long-context LLMs, RAG is considered an effective summarization tool in a more concise and lightweight manner, which can interact with LLMs multiple times using diverse queries to get comprehensive responses. However, the LLM-generated historical responses, which contain potentially insightful information, are largely neglected and discarded by existing approaches, leading to suboptimal results. In this paper, we propose $\textit{graph of records}$ ($\textbf{GoR}$), which leverages historical responses generated by LLMs to enhance RAG for long-context global summarization. Inspired by the $\textit{retrieve-then-generate}$ paradigm of RAG, we construct a graph by establishing an edge between the retrieved text chunks and the corresponding LLM-generated response. To further uncover the intricate correlations between them, GoR features a $\textit{graph neural network}$ and an elaborately designed $\textit{BERTScore}$-based objective for self-supervised model training, enabling seamless supervision signal backpropagation between reference summaries and node embeddings. We comprehensively compare GoR with 12 baselines across four long-context summarization datasets, and the results indicate that our proposed method reaches the best performance ($\textit{e.g.}$, 15%, 8%, and 19% improvement over retrievers w.r.t. Rouge-L, Rouge-1, and Rouge-2 on the WCEP dataset). Extensive experiments further demonstrate the effectiveness of GoR.",,"Haozhen Zhang, Tao Feng, Jiaxuan You",2025-05-29T11:42:45Z,Graph of Records: Boosting Retrieval Augmented Generation for   Long-context Summarization with Graphs,Graph of Records: Steigerung der retrieval Augmented Generation für Langkontext-Zusammenfassung mit Graphen,记录图图:用图表进行长文本摘要的推进检索增量生成器,http://arxiv.org/abs/2410.11001v2
1663,"Process reward models (PRMs) provide more nuanced supervision compared to outcome reward models (ORMs) for optimizing policy models, positioning them as a promising approach to enhancing the capabilities of LLMs in complex reasoning tasks. Recent efforts have advanced PRMs from step-level to token-level granularity by integrating reward modeling into the training of generative models, with reward scores derived from token generation probabilities. However, the conflict between generative language modeling and reward modeling may introduce instability and lead to inaccurate credit assignments. To address this challenge, we revisit token-level reward assignment by decoupling reward modeling from language generation and derive a token-level reward model through the optimization of a discriminative policy, termed the Q-function Reward Model (Q-RM). We theoretically demonstrate that Q-RM explicitly learns token-level Q-functions from preference data without relying on fine-grained annotations. In our experiments, Q-RM consistently outperforms all baseline methods across various benchmarks. For example, when integrated into PPO/REINFORCE algorithms, Q-RM enhances the average Pass@1 score by 5.85/4.70 points on mathematical reasoning tasks compared to the ORM baseline, and by 4.56/5.73 points compared to the token-level PRM counterpart. Moreover, reinforcement learning with Q-RM significantly enhances training efficiency, achieving convergence 12 times faster than ORM on GSM8K and 11 times faster than step-level PRM on MATH. Code and data are available at https://github.com/homzer/Q-RM.",,"Hongzhan Chen, Tao Yang, Shiping Gao, Ruijun Chen, Xiaojun Quan, Hongtao Tian, Ting Yao",2025-05-29T11:40:34Z,Discriminative Policy Optimization for Token-Level Reward Models,Diskriminative Politikoptimierung für Token-Level-Reward-Modelle,东京级奖励模式的区别对待政策优化,http://arxiv.org/abs/2505.23363v1
1664,"In this work we present the Social Influence Technique Taxonomy (SITT), a comprehensive framework of 58 empirically grounded techniques organized into nine categories, designed to detect subtle forms of social influence in textual content. We also investigate the LLMs ability to identify various forms of social influence. Building on interdisciplinary foundations, we construct the SITT dataset -- a 746-dialogue corpus annotated by 11 experts in Polish and translated into English -- to evaluate the ability of LLMs to identify these techniques. Using a hierarchical multi-label classification setup, we benchmark five LLMs, including GPT-4o, Claude 3.5, Llama-3.1, Mixtral, and PLLuM. Our results show that while some models, notably Claude 3.5, achieved moderate success (F1 score = 0.45 for categories), overall performance of models remains limited, particularly for context-sensitive techniques. The findings demonstrate key limitations in current LLMs' sensitivity to nuanced linguistic cues and underscore the importance of domain-specific fine-tuning. This work contributes a novel resource and evaluation example for understanding how LLMs detect, classify, and potentially replicate strategies of social influence in natural dialogues.",,"Wiktoria Mieleszczenko-Kowszewicz, Beata Bajcar, Aleksander Szczęsny, Maciej Markiewicz, Jolanta Babiak, Berenika Dyczek, Przemysław Kazienko",2025-05-29T11:36:25Z,Unraveling SITT: Social Influence Technique Taxonomy and Detection with   LLMs,Enthüllung von SITT: Social Influence Technique Taxonomy and Detection with LLMs,统一SITT:社会影响技术的分类学和与LLMs的探测,http://arxiv.org/abs/2506.00061v1
1665,"Quality Estimation (QE) is estimating quality of the model output during inference when the ground truth is not available. Deriving output quality from the models' output probability is the most trivial and low-effort way. However, we show that the output probability of text-generation models can appear underconfident. At each output step, there can be multiple correct options, making the probability distribution spread out more. Thus, lower probability does not necessarily mean lower output quality. Due to this observation, we propose a QE approach called BoostedProb, which boosts the model's confidence in cases where there are multiple viable output options. With no increase in complexity, BoostedProb is notably better than raw model probability in different settings, achieving on average +0.194 improvement in Pearson correlation to ground-truth quality. It also comes close to or outperforms more costly approaches like supervised or ensemble-based QE in certain settings.",,"Tu Anh Dinh, Jan Niehues",2025-05-29T11:33:24Z,Are Generative Models Underconfident? Better Quality Estimation with   Boosted Model Probability,Sind Generative Modelle unterbewusst? Bessere Qualitätsschätzung mit erhöhter Modellwahrscheinlichkeit,产生型号是否缺乏自信?更好的质量估算与促进型号的模型概率,http://arxiv.org/abs/2502.11115v2
1666,"Purpose: We investigated the utilization of privacy-preserving, locally-deployed, open-source Large Language Models (LLMs) to extract diagnostic information from free-text cardiovascular magnetic resonance (CMR) reports. Materials and Methods: We evaluated nine open-source LLMs on their ability to identify diagnoses and classify patients into various cardiac diagnostic categories based on descriptive findings in 109 clinical CMR reports. Performance was quantified using standard classification metrics including accuracy, precision, recall, and F1 score. We also employed confusion matrices to examine patterns of misclassification across models. Results: Most open-source LLMs demonstrated exceptional performance in classifying reports into different diagnostic categories. Google's Gemma2 model achieved the highest average F1 score of 0.98, followed by Qwen2.5:32B and DeepseekR1-32B with F1 scores of 0.96 and 0.95, respectively. All other evaluated models attained average scores above 0.93, with Mistral and DeepseekR1-7B being the only exceptions. The top four LLMs outperformed our board-certified cardiologist (F1 score of 0.94) across all evaluation metrics in analyzing CMR reports. Conclusion: Our findings demonstrate the feasibility of implementing open-source, privacy-preserving LLMs in clinical settings for automated analysis of imaging reports, enabling accurate, fast and resource-efficient diagnostic categorization.",,"Sina Amirrajab, Volker Vehof, Michael Bietenbeck, Ali Yilmaz",2025-05-29T11:25:10Z,Comparative analysis of privacy-preserving open-source LLMs regarding   extraction of diagnostic information from clinical CMR imaging reports,Vergleichende Analyse von datenschutzerhaltenden Open-Source-LLMs zur Extraktion von diagnostischen Informationen aus klinischen CMR-Imaging-Berichten,关于从临床遗留集束弹药成像报告中提取诊断资料的隐私保护开放源的有限来源LMs比较分析,http://arxiv.org/abs/2506.00060v1
1667,"Multimodal Large Language Models (mLLMs) are trained on a large amount of text-image data. While most mLLMs are trained on caption-like data only, Alayrac et al. (2022) showed that additionally training them on interleaved sequences of text and images can lead to the emergence of in-context learning capabilities. However, the dataset they used, M3W, is not public and is only in English. There have been attempts to reproduce their results but the released datasets are English-only. In contrast, current multilingual and multimodal datasets are either composed of caption-like only or medium-scale or fully private data. This limits mLLM research for the 7,000 other languages spoken in the world. We therefore introduce mOSCAR, to the best of our knowledge the first large-scale multilingual and multimodal document corpus crawled from the web. It covers 163 languages, 303M documents, 200B tokens and 1.15B images. We carefully conduct a set of filtering and evaluation steps to make sure mOSCAR is sufficiently safe, diverse and of good quality. We additionally train two types of multilingual model to prove the benefits of mOSCAR: (1) a model trained on a subset of mOSCAR and captioning data and (2) a model trained on captioning data only. The model additionally trained on mOSCAR shows a strong boost in few-shot learning performance across various multilingual image-text tasks and benchmarks, confirming previous findings for English-only mLLMs. The dataset is released under the Creative Commons CC BY 4.0 license and can be accessed here: https://huggingface.co/datasets/oscar-corpus/mOSCAR",,"Matthieu Futeral, Armel Zebaze, Pedro Ortiz Suarez, Julien Abadji, Rémi Lacroix, Cordelia Schmid, Rachel Bawden, Benoît Sagot",2025-05-29T11:12:18Z,mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus,"mOSCAR: Ein multimodaler, mehrsprachiger und multimodaler Korpus auf Dokumentebene",MOSCAR: 大型多语种和多模式文件级公司,http://arxiv.org/abs/2406.08707v2
1668,"We introduce Nosey (Nasalance Open Source Estimation sYstem), a low-cost, customizable, 3D-printed system for recording acoustic nasalance data that we have made available as open-source hardware (http://github.com/phoneticslab/nosey). We first outline the motivations and design principles behind our hardware nasalance system, and then present a comparison between Nosey and a commercial nasalance device. Nosey shows consistently higher nasalance scores than the commercial device, but the magnitude of contrast between phonological environments is comparable between systems. We also review ways of customizing the hardware to facilitate testing, such as comparison of microphones and different construction materials. We conclude that Nosey is a flexible and cost-effective alternative to commercial nasometry devices and propose some methodological considerations for its use in data collection.",,"Maya Dewhurst, Jack Collins, Justin J. H. Lo, Roy Alderton, Sam Kirkham",2025-05-29T11:02:41Z,Nosey: Open-source hardware for acoustic nasalance,Nosey: Open-Source-Hardware für akustische Nasalance,鼻鼻:用于音响鼻鼻腔的开源硬件,http://arxiv.org/abs/2505.23339v1
1669,"Multimodal large language models (MLLMs) have demonstrated promising prospects in healthcare, particularly for addressing complex medical tasks, supporting multidisciplinary treatment (MDT), and enabling personalized precision medicine. However, their practical deployment faces critical challenges in resource efficiency, diagnostic accuracy, clinical considerations, and ethical privacy. To address these limitations, we propose Infi-Med, a comprehensive framework for medical MLLMs that introduces three key innovations: (1) a resource-efficient approach through curating and constructing high-quality supervised fine-tuning (SFT) datasets with minimal sample requirements, with a forward-looking design that extends to both pretraining and posttraining phases; (2) enhanced multimodal reasoning capabilities for cross-modal integration and clinical task understanding; and (3) a systematic evaluation system that assesses model performance across medical modalities and task types. Our experiments demonstrate that Infi-Med achieves state-of-the-art (SOTA) performance in general medical reasoning while maintaining rapid adaptability to clinical scenarios. The framework establishes a solid foundation for deploying MLLMs in real-world healthcare settings by balancing model effectiveness with operational constraints.",,"Zeyu Liu, Zhitian Hou, Yining Di, Kejing Yang, Zhijie Sang, Congkai Xie, Jingwen Yang, Siyuan Liu, Jialu Wang, Chunming Li, Ming Li, Hongxia Yang",2025-05-29T10:31:57Z,Infi-Med: Low-Resource Medical MLLMs with Robust Reasoning Evaluation,Infi-Med: Low-Resource medizinische MLLMs mit robuster Reasoning-Bewertung,Infi-Med: 配有强有力的合理理由评估的低资源医疗MLLMs,http://arxiv.org/abs/2505.23867v1
1670,"In this position paper we raise critical awareness of a realistic view of LLM capabilities that eschews extreme alternative views that LLMs are either ""stochastic parrots"" or in possession of ""emergent"" advanced reasoning capabilities, which, due to their unpredictable emergence, constitute an existential threat. Our middle-ground view is that LLMs extrapolate from priors from their training data, and that a mechanism akin to in-context learning enables the targeting of the appropriate information from which to extrapolate. We call this ""context-directed extrapolation."" Under this view, substantiated though existing literature, while reasoning capabilities go well beyond stochastic parroting, such capabilities are predictable, controllable, not indicative of advanced reasoning akin to high-level cognitive capabilities in humans, and not infinitely scalable with additional training. As a result, fears of uncontrollable emergence of agency are allayed, while research advances are appropriately refocused on the processes of context-directed extrapolation and how this interacts with training data to produce valuable capabilities in LLMs. Future work can therefore explore alternative augmenting techniques that do not rely on inherent advanced reasoning in LLMs.",,"Harish Tayyar Madabushi, Melissa Torgbi, Claire Bonial",2025-05-29T10:31:42Z,Neither Stochastic Parroting nor AGI: LLMs Solve Tasks through   Context-Directed Extrapolation from Training Data Priors,Weder Stochastic Parroting noch AGI: LLMs lösen Aufgaben durch kontextorientierte Extrapolation von Trainingsdaten Priors,"既不是蒸蒸碎剖析,也不是AGI:通过根据培训数据前期进行的背景差异外推法解解解任务LLMs Solve任务",http://arxiv.org/abs/2505.23323v1
1671,"Speculative decoding (SD) accelerates Large Language Model (LLM) generation by using an efficient draft model to propose the next few tokens, which are verified by the LLM in a single forward call, reducing latency while preserving its outputs. We focus on retrieval-based SD where the draft model retrieves the next tokens from a non-parametric datastore. Sparse retrieval (REST), which operates on the surface form of strings, is currently the dominant paradigm due to its simplicity and scalability. However, its effectiveness is limited due to the usage of short contexts and exact string matching. Instead, we introduce Dense Retrieval for Speculative Decoding (DReSD), a novel framework that uses approximate nearest neighbour search with contextualised token embeddings to retrieve the most semantically relevant token sequences for SD. Extensive experiments show that DReSD achieves (on average) 87% higher acceptance rates, 65% longer accepted tokens and 19% faster generation speeds compared to sparse retrieval (REST).",,"Milan Gritta, Huiyin Xue, Gerasimos Lampouras",2025-05-29T10:31:14Z,DReSD: Dense Retrieval for Speculative Decoding,DResD: Dense Retrieval für spekulative Dekodierung,DRESD: 用于投机性代号的高级检索值,http://arxiv.org/abs/2502.15572v2
1672,"Direct alignment methods typically optimize large language models (LLMs) by contrasting the likelihoods of preferred versus dispreferred responses. While effective in steering LLMs to match relative preference, these methods are frequently noted for decreasing the absolute likelihoods of example responses. As a result, aligned models tend to generate outputs that deviate from the expected patterns, exhibiting reward-hacking effect even without a reward model. This undesired consequence exposes a fundamental limitation in contrastive alignment, which we characterize as likelihood underdetermination. In this work, we revisit direct preference optimization (DPO) -- the seminal direct alignment method -- and demonstrate that its loss theoretically admits a decomposed reformulation. The reformulated loss not only broadens applicability to a wider range of feedback types, but also provides novel insights into the underlying cause of likelihood underdetermination. Specifically, the standard DPO implementation implicitly oversimplifies a regularizer in the reformulated loss, and reinstating its complete version effectively resolves the underdetermination issue. Leveraging these findings, we introduce PRoximalized PReference Optimization (PRO), a unified method to align with diverse feeback types, eliminating likelihood underdetermination through an efficient approximation of the complete regularizer. Comprehensive experiments show the superiority of PRO over existing methods in scenarios involving pairwise, binary and scalar feedback.",,"Kaiyang Guo, Yinchuan Li, Zhitang Chen",2025-05-29T10:23:22Z,Proximalized Preference Optimization for Diverse Feedback Types: A   Decomposed Perspective on DPO,Proximalisierte Preference-Optimierung für unterschiedliche Feedback-Typen: Eine zersetzte Perspektive auf DPO,多种反馈类型最佳优化:对残疾人组织拆解的视角,http://arxiv.org/abs/2505.23316v1
1673,"A key ethical challenge in Automated Essay Scoring (AES) is ensuring that scores are only released when they meet high reliability standards. Confidence modelling addresses this by assigning a reliability estimate measure, in the form of a confidence score, to each automated score. In this study, we frame confidence estimation as a classification task: predicting whether an AES-generated score correctly places a candidate in the appropriate CEFR level. While this is a binary decision, we leverage the inherent granularity of the scoring domain in two ways. First, we reformulate the task as an n-ary classification problem using score binning. Second, we introduce a set of novel Kernel Weighted Ordinal Categorical Cross Entropy (KWOCCE) loss functions that incorporate the ordinal structure of CEFR labels. Our best-performing model achieves an F1 score of 0.97, and enables the system to release 47% of scores with 100% CEFR agreement and 99% with at least 95% CEFR agreement -compared to approximately 92% (approx.) CEFR agreement from the standalone AES model where we release all AM predicted scores.",,"Abhirup Chakravarty, Mark Brenchley, Trevor Breakspear, Ian Lewin, Yan Huang",2025-05-29T10:23:20Z,Enhancing Marker Scoring Accuracy through Ordinal Confidence Modelling   in Educational Assessments,Verbesserung der Genauigkeit der Markerbewertung durch ordinelles Vertrauensmodellierung in Bildungsbewertungen,"通过在教育评估中建立常规信任模型,加强标标码的准确度",http://arxiv.org/abs/2505.23315v1
1674,"Interpreting data is central to modern research. Large language models (LLMs) show promise in providing such natural language interpretations of data, yet simple feature extraction methods such as prompting often fail to produce accurate and versatile descriptions for diverse datasets and lack control over granularity and scale. To address these limitations, we propose a domain-agnostic method for dataset featurization that provides precise control over the number of features extracted while maintaining compact and descriptive representations comparable to human labeling. Our method optimizes the selection of informative binary features by evaluating the ability of an LLM to reconstruct the original data using those features. We demonstrate its effectiveness in dataset modeling tasks and through two case studies: (1) Constructing a feature representation of jailbreak tactics that compactly captures both the effectiveness and diversity of a larger set of human-crafted attacks; and (2) automating the discovery of features that align with human preferences, achieving accuracy and robustness comparable to human-crafted features. Moreover, we show that the pipeline scales effectively, improving as additional features are sampled, making it suitable for large and diverse datasets.",,"Michal Bravansky, Vaclav Kubon, Suhas Hariharan, Robert Kirk",2025-05-29T10:04:29Z,Dataset Featurization: Uncovering Natural Language Features through   Unsupervised Data Reconstruction,Datensatz-Featurierung: Enthüllen natürlicher Sprach-Features durch unüberwachte Daten-Rekonstruktion,Dataset Featuriz化:通过未受监督的数据重建发现自然语言特征,http://arxiv.org/abs/2502.17541v2
1675,"Generalized Category Discovery (GCD) aims to classify both known and novel categories using partially labeled data that contains only known classes. Despite achieving strong performance on existing benchmarks, current textual GCD methods lack sufficient validation in realistic settings. We introduce Event-Centric GCD (EC-GCD), characterized by long, complex narratives and highly imbalanced class distributions, posing two main challenges: (1) divergent clustering versus classification groupings caused by subjective criteria, and (2) Unfair alignment for minority classes. To tackle these, we propose PaMA, a framework leveraging LLMs to extract and refine event patterns for improved cluster-class alignment. Additionally, a ranking-filtering-mining pipeline ensures balanced representation of prototypes across imbalanced categories. Evaluations on two EC-GCD benchmarks, including a newly constructed Scam Report dataset, demonstrate that PaMA outperforms prior methods with up to 12.58% H-score gains, while maintaining strong generalization on base GCD datasets.",,"Yi Luo, Qiwen Wang, Junqi Yang, Luyao Tang, Zhenghao Lin, Zhenzhe Ying, Weiqiang Wang, Chen Lin",2025-05-29T10:02:04Z,Generalized Category Discovery in Event-Centric Contexts: Latent Pattern   Mining with LLMs,Generalisierte Category Discovery in Event-Centric Kontexten: Latent Pattern Mining mit LLMs,事件发生时发现的情况:利用LLMM公司进行原型采矿,http://arxiv.org/abs/2505.23304v1
1676,"Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems are increasingly deployed in industry applications, yet their reliability remains hampered by challenges in detecting hallucinations. While supervised state-of-the-art (SOTA) methods that leverage LLM hidden states -- such as activation tracing and representation analysis -- show promise, their dependence on extensively annotated datasets limits scalability in real-world applications. This paper addresses the critical bottleneck of data annotation by investigating the feasibility of reducing training data requirements for two SOTA hallucination detection frameworks: Lookback Lens, which analyzes attention head dynamics, and probing-based approaches, which decode internal model representations. We propose a methodology combining efficient classification algorithms with dimensionality reduction techniques to minimize sample size demands while maintaining competitive performance. Evaluations on standardized question-answering RAG benchmarks show that our approach achieves performance comparable to strong proprietary LLM-based baselines with only 250 training samples. These results highlight the potential of lightweight, data-efficient paradigms for industrial deployment, particularly in annotation-constrained scenarios.",,"Julia Belikova, Konstantin Polev, Rauf Parchiev, Dmitry Simakov",2025-05-29T09:50:56Z,Data-efficient Meta-models for Evaluation of Context-based Questions and   Answers in LLMs,Dateneffiziente Meta-Modelle zur Auswertung kontextbasierter Fragen und Antworten in LLMs,评价LLMM基于背景的问答的元模型,http://arxiv.org/abs/2505.23299v1
1677,"While Ukrainian NLP has seen progress in many texts processing tasks, emotion classification remains an underexplored area with no publicly available benchmark to date. In this work, we introduce EmoBench-UA, the first annotated dataset for emotion detection in Ukrainian texts. Our annotation schema is adapted from the previous English-centric works on emotion detection (Mohammad et al., 2018; Mohammad, 2022) guidelines. The dataset was created through crowdsourcing using the Toloka.ai platform ensuring high-quality of the annotation process. Then, we evaluate a range of approaches on the collected dataset, starting from linguistic-based baselines, synthetic data translated from English, to large language models (LLMs). Our findings highlight the challenges of emotion classification in non-mainstream languages like Ukrainian and emphasize the need for further development of Ukrainian-specific models and training resources.",,"Daryna Dementieva, Nikolay Babakov, Alexander Fraser",2025-05-29T09:49:57Z,EmoBench-UA: A Benchmark Dataset for Emotion Detection in Ukrainian,EmoBench-UA: Ein Benchmark-Datensatz für Emotionserkennung in der Ukraine,EmoBenich-UA:乌克兰情感检测基准数据集,http://arxiv.org/abs/2505.23297v1
1678,"Large language models (LLMs) are widely used for long-form text generation. However, factual errors in the responses would undermine their reliability. Despite growing attention to LLM factuality, the effect of response length on factuality remains underexplored. In this work, we systematically investigate this relationship by first introducing an automatic and bi-level long-form factuality evaluation framework, which achieves high agreement with human annotations while being cost-effective. Using this framework, we conduct controlled experiments and find that longer responses exhibit lower factual precision, confirming the presence of length bias. To explain this phenomenon, we empirically examine three hypotheses: error propagation, long context, and facts exhaustion. Our results reveal that facts exhaustion, where the model gradually exhausts more reliable knowledge, is the primary cause of factual degradation, rather than the other two hypotheses.",,"James Xu Zhao, Jimmy Z. J. Liu, Bryan Hooi, See-Kiong Ng",2025-05-29T09:47:56Z,How Does Response Length Affect Long-Form Factuality,Wie wirkt sich die Response-Länge auf die Langform-Faktizität aus?,反应时间长度如何影响长期事实质量,http://arxiv.org/abs/2505.23295v1
1679,"Automated frame analysis of political communication is a popular task in computational social science that is used to study how authors select aspects of a topic to frame its reception. So far, such studies have been narrow, in that they use a fixed set of pre-defined frames and focus only on the text, ignoring the visual contexts in which those texts appear. Especially for framing in the news, this leaves out valuable information about editorial choices, which include not just the written article but also accompanying photographs. To overcome such limitations, we present a method for conducting multi-modal, multi-label framing analysis at scale using large (vision-) language models. Grounding our work in framing theory, we extract latent meaning embedded in images used to convey a certain point and contrast that to the text by comparing the respective frames used. We also identify highly partisan framing of topics with issue-specific frame analysis found in prior qualitative work. We demonstrate a method for doing scalable integrative framing analysis of both text and image in news, providing a more complete picture for understanding media bias.",,"Arnav Arora, Srishti Yadav, Maria Antoniak, Serge Belongie, Isabelle Augenstein",2025-05-29T09:45:28Z,Multi-Modal Framing Analysis of News,Multi-Modal Framing Analyse der Nachrichten,新闻多模式结构分析,http://arxiv.org/abs/2503.20960v3
1680,"Uncertainty quantification (UQ) methods for Large Language Models (LLMs) encompass a variety of approaches, with two major types being particularly prominent: information-based, which focus on model confidence expressed as token probabilities, and consistency-based, which assess the semantic relationship between multiple outputs generated using repeated sampling. Several recent methods have combined these two approaches to boost UQ performance. However, they sometimes fail to outperform much simpler baseline methods. Our work discusses the fundamental approach to constructing uncertainty measures that directly links uncertainty with the minimum Bayes risks achieved by LLM decoding. Building on these findings, we propose a novel approach to integrating model confidence with output consistency, resulting in a family of efficient and robust UQ methods. Our investigation reveals distinctive characteristics of LLMs as probabilistic models, which help to explain why these UQ methods underperform in certain tasks. Based on these findings, we propose a new way of synthesizing model confidence and output consistency, leading to a family of efficient and robust UQ methods. We evaluate our approach across various tasks such as question answering, abstractive summarization, and machine translation, demonstrating sizable improvements over state-of-the-art UQ approaches.",,"Roman Vashurin, Maiya Goloburda, Albina Ilina, Aleksandr Rubashevskii, Preslav Nakov, Artem Shelmanov, Maxim Panov",2025-05-29T09:39:51Z,Uncertainty Quantification for LLMs through Minimum Bayes Risk: Bridging   Confidence and Consistency,Unsicherheit Quantifizierung für LLMs durch Minimum Bayes Risiko: Vertrauensüberbrückung und Konsistenz,通过最低贝谷风险对LLMs的不确定性量化: 建立互信和一致性,http://arxiv.org/abs/2502.04964v4
1681,"The rapid advancement of reasoning capabilities in large language models (LLMs) has led to notable improvements on mathematical benchmarks. However, many of the most commonly used evaluation datasets (e.g., AIME 2024) are widely available online, making it difficult to disentangle genuine reasoning from potential memorization. Furthermore, these benchmarks do not evaluate proof-writing capabilities, which are crucial for many mathematical tasks. To address this, we introduce MathArena, a new benchmark based on the following key insight: recurring math competitions provide a stream of high-quality, challenging problems that can be used for real-time evaluation of LLMs. By evaluating models as soon as new problems are released, we effectively eliminate the risk of contamination. Using this framework, we find strong signs of contamination in AIME 2024. Nonetheless, evaluations on harder competitions, such as SMT 2025 -- published well after model release dates -- demonstrate impressive reasoning capabilities in top-performing models. MathArena is also the first benchmark for proof-writing capabilities. On USAMO 2025, even top models score below 25%, far behind their performance on final-answer tasks. So far, we have evaluated 30 models across five competitions, totaling 149 problems. As an evolving benchmark, MathArena will continue to track the progress of LLMs on newly released competitions, ensuring rigorous and up-to-date evaluation of mathematical reasoning.",,"Mislav Balunović, Jasper Dekoninck, Ivo Petrov, Nikola Jovanović, Martin Vechev",2025-05-29T09:28:06Z,MathArena: Evaluating LLMs on Uncontaminated Math Competitions,MathArena: Bewertung von LLMs auf nicht kontaminierten Math-Wettbewerben,Matharena:评估未受污染数学竞赛的LLMs,http://arxiv.org/abs/2505.23281v1
1682,"Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external context, but retrieved passages are often lengthy, noisy, or exceed input limits. Existing compression methods typically require supervised training of dedicated compression models, increasing cost and reducing portability. We propose Sentinel, a lightweight sentence-level compression framework that reframes context filtering as an attention-based understanding task. Rather than training a compression model, Sentinel probes decoder attention from an off-the-shelf 0.5B proxy LLM using a lightweight classifier to identify sentence relevance. Empirically, we find that query-context relevance estimation is consistent across model scales, with 0.5B proxies closely matching the behaviors of larger models. On the LongBench benchmark, Sentinel achieves up to 5$\times$ compression while matching the QA performance of 7B-scale compression systems. Our results suggest that probing native attention signals enables fast, effective, and question-aware context compression. Code available at: https://github.com/yzhangchuck/Sentinel.",,"Yong Zhang, Yanwen Huang, Ning Cheng, Yang Guo, Yun Zhu, Yanmeng Wang, Shaojun Wang, Jing Xiao",2025-05-29T09:24:12Z,Sentinel: Attention Probing of Proxy Models for LLM Context Compression   with an Understanding Perspective,Sentinel: Aufmerksamkeitsprobierung von Proxy-Modellen für LLM-Kontextkompression mit verstehender Perspektive,哨兵:注意从理解角度观察LLM背景压缩的代理模型,http://arxiv.org/abs/2505.23277v1
1683,"This paper introduces BioVL-QR, a biochemical vision-and-language dataset comprising 23 egocentric experiment videos, corresponding protocols, and vision-and-language alignments. A major challenge in understanding biochemical videos is detecting equipment, reagents, and containers because of the cluttered environment and indistinguishable objects. Previous studies assumed manual object annotation, which is costly and time-consuming. To address the issue, we focus on Micro QR Codes. However, detecting objects using only Micro QR Codes is still difficult due to blur and occlusion caused by object manipulation. To overcome this, we propose an object labeling method combining a Micro QR Code detector with an off-the-shelf hand object detector. As an application of the method and BioVL-QR, we tackled the task of localizing the procedural steps in an instructional video. The experimental results show that using Micro QR Codes and our method improves biochemical video understanding. Data and code are available through https://nishi10mo.github.io/BioVL-QR/",,"Tomohiro Nishimoto, Taichi Nishimura, Koki Yamamoto, Keisuke Shirai, Hirotaka Kameko, Yuto Haneji, Tomoya Yoshida, Keiya Kajimura, Taiyu Cui, Chihiro Nishiwaki, Eriko Daikoku, Natsuko Okuda, Fumihito Ono, Shinsuke Mori",2025-05-29T09:22:51Z,BioVL-QR: Egocentric Biochemical Vision-and-Language Dataset Using Micro   QR Codes,BioVL-QR: Egozentrischer biochemischer Vision- und Sprachdatensatz mit Micro-QR-Codes,BioVL-QR:使用微质变码的Egocent 生物化学视觉和语言数据集,http://arxiv.org/abs/2404.03161v3
1684,"In recent years, Large Language Models (LLMs) have achieved remarkable advancements, drawing significant attention from the research community. Their capabilities are largely attributed to large-scale architectures, which require extensive training on massive datasets. However, such datasets often contain sensitive or copyrighted content sourced from the public internet, raising concerns about data privacy and ownership. Regulatory frameworks, such as the General Data Protection Regulation (GDPR), grant individuals the right to request the removal of such sensitive information. This has motivated the development of machine unlearning algorithms that aim to remove specific knowledge from models without the need for costly retraining. Despite these advancements, evaluating the efficacy of unlearning algorithms remains a challenge due to the inherent complexity and generative nature of LLMs. In this work, we introduce a comprehensive auditing framework for unlearning evaluation, comprising three benchmark datasets, six unlearning algorithms, and five prompt-based auditing methods. By using various auditing algorithms, we evaluate the effectiveness and robustness of different unlearning strategies. To explore alternatives beyond prompt-based auditing, we propose a novel technique that leverages intermediate activation perturbations, addressing the limitations of auditing methods that rely solely on model inputs and outputs.",,"Haokun Chen, Yueqi Zhang, Yuan Bi, Yao Zhang, Tong Liu, Jinhe Bi, Jian Lan, Jindong Gu, Claudia Grosser, Denis Krompass, Nassir Navab, Volker Tresp",2025-05-29T09:19:07Z,Does Machine Unlearning Truly Remove Model Knowledge? A Framework for   Auditing Unlearning in LLMs,Entfernt Machine Unlearning wirklich Modellwissen? Ein Rahmen für die Prüfung von Unlearning in LLMs,机器取消学习是否真正删除了示范知识? 审计框架是否在LLMM中取消学习?,http://arxiv.org/abs/2505.23270v1
1685,"Multimodal large language models (MLLMs) have shown remarkable performance for cross-modal understanding and generation, yet still suffer from severe inference costs. Recently, abundant works have been proposed to solve this problem with token pruning, which identifies the redundant tokens in MLLMs and then prunes them to reduce the computation and KV storage costs, leading to significant acceleration without training. While these methods claim efficiency gains, critical questions about their fundamental design and evaluation remain unanswered: Why do many existing approaches underperform even compared to naive random token selection? Are attention-based scoring sufficient for reliably identifying redundant tokens? Is language information really helpful during token pruning? What makes a good trade-off between token importance and duplication? Are current evaluation protocols comprehensive and unbiased? The ignorance of previous research on these problems hinders the long-term development of token pruning. In this paper, we answer these questions one by one, providing insights into the design of future token pruning methods.",,"Zichen Wen, Yifeng Gao, Weijia Li, Conghui He, Linfeng Zhang",2025-05-29T09:18:35Z,Token Pruning in Multimodal Large Language Models: Are We Solving the   Right Problem?,Token Pruning in multimodalen großen Sprachmodellen: Lösen wir das richtige Problem?,在多式大语言模式中的 Token Prurning:我们是否解决了正确的问题?,http://arxiv.org/abs/2502.11501v2
1686,"Retrieval-augmented generation (RAG) helps address the limitations of parametric knowledge embedded within a language model (LM). In real world settings, retrieved information can vary in complexity, yet most investigations of LM utilisation of context has been limited to synthetic text. We introduce DRUID (Dataset of Retrieved Unreliable, Insufficient and Difficult-to-understand contexts) with real-world queries and contexts manually annotated for stance. The dataset is based on the prototypical task of automated claim verification, for which automated retrieval of real-world evidence is crucial. We compare DRUID to synthetic datasets (CounterFact, ConflictQA) and find that artificial datasets often fail to represent the complexity and diversity of realistically retrieved context. We show that synthetic datasets exaggerate context characteristics rare in real retrieved data, which leads to inflated context utilisation results, as measured by our novel ACU score. Moreover, while previous work has mainly focused on singleton context characteristics to explain context utilisation, correlations between singleton context properties and ACU on DRUID are surprisingly small compared to other properties related to context source. Overall, our work underscores the need for real-world aligned context utilisation studies to represent and improve performance in real-world RAG settings.",,"Lovisa Hagström, Sara Vera Marjanović, Haeun Yu, Arnav Arora, Christina Lioma, Maria Maistro, Pepa Atanasova, Isabelle Augenstein",2025-05-29T09:10:25Z,A Reality Check on Context Utilisation for Retrieval-Augmented   Generation,Ein Realitätscheck auf Kontext-Auslastung für retrieval-Augmented Generation,关于回收-提款人一代的上下文利用情况的现实检查,http://arxiv.org/abs/2412.17031v2
1687,"Proteins, as essential biomolecules, play a central role in biological processes, including metabolic reactions and DNA replication. Accurate prediction of their properties and functions is crucial in biological applications. Recent development of protein language models (pLMs) with supervised fine tuning provides a promising solution to this problem. However, the fine-tuned model is tailored for particular downstream prediction task, and achieving general-purpose protein understanding remains a challenge. In this paper, we introduce Structure-Enhanced Protein Instruction Tuning (SEPIT) framework to bridge this gap. Our approach incorporates a novel structure-aware module into pLMs to enrich their structural knowledge, and subsequently integrates these enhanced pLMs with large language models (LLMs) to advance protein understanding. In this framework, we propose a novel instruction tuning pipeline. First, we warm up the enhanced pLMs using contrastive learning and structure denoising. Then, caption-based instructions are used to establish a basic understanding of proteins. Finally, we refine this understanding by employing a mixture of experts (MoEs) to capture more complex properties and functional information with the same number of activated parameters. Moreover, we construct the largest and most comprehensive protein instruction dataset to date, which allows us to train and evaluate the general-purpose protein understanding model. Extensive experiments on both open-ended generation and closed-set answer tasks demonstrate the superior performance of SEPIT over both closed-source general LLMs and open-source LLMs trained with protein knowledge.",,"Wei Wu, Chao Wang, Liyi Chen, Mingze Yin, Yiheng Zhu, Kun Fu, Jieping Ye, Hui Xiong, Zheng Wang",2025-05-29T09:07:57Z,Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose   Protein Understanding with LLMs,Strukturverstärkte Protein-Instruktions-Tuning: Auf dem Weg zu einem allgemeinen Protein-Verständnis mit LLMs,结构强化的蛋白质指导指示图示:争取与LLMs达成一般用途的蛋白性了解,http://arxiv.org/abs/2410.03553v3
1688,"The success of DeepSeek-R1 underscores the significant role of reinforcement learning (RL) in enhancing the reasoning capabilities of large language models (LLMs). In this work, we present Skywork-OR1, an effective and scalable RL implementation for long Chain-of-Thought (CoT) models. Building on the DeepSeek-R1-Distill model series, our RL approach achieves notable performance gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%) for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models demonstrate competitive reasoning capabilities among models of similar size. We perform comprehensive ablation studies on the core components of our training pipeline to validate their effectiveness. Additionally, we thoroughly investigate the phenomenon of entropy collapse, identify key factors affecting entropy dynamics, and demonstrate that mitigating premature entropy collapse is critical for improved test performance. To support community research, we fully open-source our model weights, training code, and training datasets.",,"Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, Yahui Zhou",2025-05-29T09:07:33Z,Skywork Open Reasoner 1 Technical Report,Skywork Open Reasoner 1 Technischer Bericht,""" 天窗开放理由1 "" 技术报告",http://arxiv.org/abs/2505.22312v2
1689,"Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, substantially shrinking the KV cache size at inference time. By factorizing these representations into contextual low-rank components and seamlessly integrating with Rotary Position Embedding (RoPE), TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor Product Attention Transformer,(T6), a new model architecture for sequence modeling. Through extensive empirical evaluation on language modeling tasks, we demonstrate that T6 surpasses or matches the performance of standard Transformer baselines, including Multi-Head Attention (MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and Multi-Head Latent Attention (MLA) across various metrics, including perplexity and a range of established evaluation benchmarks. Notably, TPA's memory efficiency and computational efficiency at the decoding stage enable processing longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. The code is available at https://github.com/tensorgi/T6.",,"Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Zhen Qin, Yang Yuan, Quanquan Gu, Andrew C Yao",2025-05-29T09:01:23Z,Tensor Product Attention Is All You Need,"Tensor Produkt-Achtung ist alles, was Sie brauchen",色素产品 关注是所有你需要的,http://arxiv.org/abs/2501.06425v4
1690,"Approaches form the foundation for conducting scientific research. Querying approaches from a vast body of scientific papers is extremely time-consuming, and without a well-organized management framework, researchers may face significant challenges in querying and utilizing relevant approaches. Constructing multiple dimensions on approaches and managing them from these dimensions can provide an efficient solution. Firstly, this paper identifies approach patterns using a top-down way, refining the patterns through four distinct linguistic levels: semantic level, discourse level, syntactic level, and lexical level. Approaches in scientific papers are extracted based on approach patterns. Additionally, five dimensions for categorizing approaches are identified using these patterns. This paper proposes using tree structure to represent step and measuring the similarity between different steps with a tree-structure-based similarity measure that focuses on syntactic-level similarities. A collection similarity measure is proposed to compute the similarity between approaches. A bottom-up clustering algorithm is proposed to construct class trees for approach components within each dimension by merging each approach component or class with its most similar approach component or class in each iteration. The class labels generated during the clustering process indicate the common semantics of the step components within the approach components in each class and are used to manage the approaches within the class. The class trees of the five dimensions collectively form a multi-dimensional approach space. The application of approach queries on the multi-dimensional approach space demonstrates that querying within this space ensures strong relevance between user queries and results and rapidly reduces search space through a class-based query mechanism.",,"Bing Ma, Hai Zhuge",2025-05-29T08:57:11Z,Automatic Construction of Multiple Classification Dimensions for   Managing Approaches in Scientific Papers,Automatische Konstruktion mehrerer Klassifizierungsdimensionen für die Verwaltung von Ansätzen in wissenschaftlichen Papieren,科学文件中管理方法的多重分类方面自动构建,http://arxiv.org/abs/2505.23252v1
1691,"Despite the abundance of prior social strategies possessed by humans, there remains a paucity of research dedicated to their transfer and integration into social agents. Our proposed SOTOPIA-$\Omega$ framework aims to address and bridge this gap, with a particular focus on enhancing the social capabilities of language agents. This framework dynamically injects multi-step reasoning strategies inspired by negotiation theory and two simple direct strategies into expert agents, thereby automating the construction of a high-quality social dialogue training corpus. Additionally, we introduce the concept of Social Instruction Following (S-IF) and propose two new S-IF evaluation metrics that complement social capability. We demonstrate that several 7B models trained on high-quality corpus not only significantly surpass the expert agent (GPT-4) in achieving social goals but also enhance S-IF performance. Analysis and variant experiments validate the advantages of dynamic construction, which can especially break the agent's prolonged deadlock.",,"Wenyuan Zhang, Tianyun Liu, Mengxiao Song, Xiaodong Li, Tingwen Liu",2025-05-29T08:54:31Z,SOTOPIA-$Ω$: Dynamic Strategy Injection Learning and Social   Instruction Following Evaluation for Social Agents,SOTOPIA-$Ω$: Dynamic Strategy Injection Learning and Social Instruction Following Evaluation for Social Agents,SOTOPIA-美元/美元/美元:在评估社会代理人后进行动态战略注射学习和社会指导,http://arxiv.org/abs/2502.15538v3
1692,"We present Autonomous Data Selection (AutoDS), a method that leverages base language models themselves as zero-shot ""generative classifiers"" to automatically curate high-quality mathematical texts. Unlike prior approaches that require human annotations or training a dedicated data filter, AutoDS relies solely on a model's logits to determine whether a given passage is mathematically informative and educational. By integrating AutoDS into a continual pretraining pipeline, we substantially boost downstream performance on challenging math benchmarks (MATH, GSM8K, and BBH) while using far fewer tokens than previous methods. Empirically, our approach achieves roughly a twofold improvement in pretraining token efficiency over strong baselines, underscoring the potential of self-directed data selection in enhancing mathematical reasoning. We release our curated AutoMathText dataset to facilitate future research in automated domain-specific data curation. The AutoMathText dataset is available at https://huggingface.co/datasets/math-ai/AutoMathText. The code is available at https://github.com/yifanzhang-pro/AutoMathText.",,"Yifan Zhang, Yifan Luo, Yang Yuan, Andrew C Yao",2025-05-29T08:51:54Z,Autonomous Data Selection with Zero-shot Generative Classifiers for   Mathematical Texts,Autonome Datenauswahl mit Zero-shot Generative Klassifikatoren für mathematische Texte,具有数学文本零光生成分类器的自动数据选择,http://arxiv.org/abs/2402.07625v6
1693,"Chart question answering (CQA) has become a critical multimodal task for evaluating the reasoning capabilities of vision-language models. While early approaches have shown promising performance by focusing on visual features or leveraging large-scale pre-training, most existing evaluations rely on rigid output formats and objective metrics, thus ignoring the complex, real-world demands of practical chart analysis. In this paper, we introduce ChartMind, a new benchmark designed for complex CQA tasks in real-world settings. ChartMind covers seven task categories, incorporates multilingual contexts, supports open-domain textual outputs, and accommodates diverse chart formats, bridging the gap between real-world applications and traditional academic benchmarks. Furthermore, we propose a context-aware yet model-agnostic framework, ChartLLM, that focuses on extracting key contextual elements, reducing noise, and enhancing the reasoning accuracy of multimodal large language models. Extensive evaluations on ChartMind and three representative public benchmarks with 14 mainstream multimodal models show our framework significantly outperforms the previous three common CQA paradigms: instruction-following, OCR-enhanced, and chain-of-thought, highlighting the importance of flexible chart understanding for real-world CQA. These findings suggest new directions for developing more robust chart reasoning in future research.",,"Jingxuan Wei, Nan Xu, Junnan Zhu, Yanni Hao, Gaowei Wu, Bihui Yu, Lei Wang",2025-05-29T08:46:03Z,ChartMind: A Comprehensive Benchmark for Complex Real-world Multimodal   Chart Question Answering,ChartMind: Ein umfassender Benchmark für komplexe multimodale Chart-Fragebeantwortung,图表Mind:复杂现实世界多式联运图表问题回答综合基准,http://arxiv.org/abs/2505.23242v1
1694,"In this paper, we introduce PolyMath, a multilingual mathematical reasoning benchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our benchmark ensures difficulty comprehensiveness, language diversity, and high-quality translation, making it a highly discriminative multilingual mathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive evaluation for advanced LLMs and find that even Qwen-3-235B-A22B-Thinking and Gemini-2.5-pro, achieve only 54.6 and 52.2 benchmark scores, with about 40% accuracy under the highest level From a language perspective, our benchmark reveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning performance varies widely across languages for current LLMs; (2) Input-output language consistency is low in reasoning LLMs and may be correlated with performance; (3) The thinking length differs significantly by language for current LLMs. Additionally, we demonstrate that controlling the output language in the instructions has the potential to affect reasoning performance, especially for some low-resource languages, suggesting a promising direction for improving multilingual capabilities in LLMs.",,"Yiming Wang, Pei Zhang, Jialong Tang, Haoran Wei, Baosong Yang, Rui Wang, Chenshu Sun, Feitong Sun, Jiran Zhang, Junxuan Wu, Qiqian Cang, Yichang Zhang, Fei Huang, Junyang Lin, Fei Huang, Jingren Zhou",2025-05-29T08:42:37Z,PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts,PolyMath: Mathematische Vernunft in multilingualen Kontexten bewerten,多语制:多语种背景下的数学理由评估,http://arxiv.org/abs/2504.18428v3
1695,"The integration of Monte Carlo Tree Search (MCTS) with Large Language Models (LLMs) has demonstrated significant success in structured, problem-oriented tasks. However, applying these methods to open-ended dialogues, such as those in psychological counseling, presents unique challenges. Unlike tasks with objective correctness, success in therapeutic conversations depends on subjective factors like empathetic engagement, ethical adherence, and alignment with human preferences, for which strict ""correctness"" criteria are ill-defined. Existing result-oriented MCTS approaches can therefore produce misaligned responses. To address this, we introduce MCTSr-Zero, an MCTS framework designed for open-ended, human-centric dialogues. Its core innovation is ""domain alignment"", which shifts the MCTS search objective from predefined end-states towards conversational trajectories that conform to target domain principles (e.g., empathy in counseling). Furthermore, MCTSr-Zero incorporates ""Regeneration"" and ""Meta-Prompt Adaptation"" mechanisms to substantially broaden exploration by allowing the MCTS to consider fundamentally different initial dialogue strategies. We evaluate MCTSr-Zero in psychological counseling by generating multi-turn dialogue data, which is used to fine-tune an LLM, PsyLLM. We also introduce PsyEval, a benchmark for assessing multi-turn psychological counseling dialogues. Experiments demonstrate that PsyLLM achieves state-of-the-art performance on PsyEval and other relevant metrics, validating MCTSr-Zero's effectiveness in generating high-quality, principle-aligned conversational data for human-centric domains and addressing the LLM challenge of consistently adhering to complex psychological standards.",,"Hao Lu, Yanchi Gu, Haoyuan Huang, Yulin Zhou, Ningxin Zhu, Chen Li",2025-05-29T08:30:15Z,MCTSr-Zero: Self-Reflective Psychological Counseling Dialogues   Generation via Principles and Adaptive Exploration,MCTSr-Zero: Selbstreflektierende Psychologische Beratung Dialoge Generation über Prinzipien und Adaptive Exploration,MMCTSr-Zero:通过原则和适应性探索进行自我反应心理辅导对话,http://arxiv.org/abs/2505.23229v1
1696,"Instruction tuning is widely used to improve a pre-trained Multimodal Large Language Model (MLLM) by training it on curated task-specific datasets, enabling better comprehension of human instructions. However, it is infeasible to collect all possible instruction datasets simultaneously in real-world scenarios. Thus, enabling MLLM with continual instruction tuning is essential for maintaining their adaptability. However, existing methods often trade off memory efficiency for performance gains, significantly compromising overall efficiency. In this paper, we propose a task-specific expansion and task-general fusion framework based on the variations in Centered Kernel Alignment (CKA) similarity across different model layers when trained on diverse datasets. Furthermore, we analyze the information leakage present in the existing benchmark and propose a new and more challenging benchmark to rationally evaluate the performance of different methods. Comprehensive experiments showcase a significant performance improvement of our method compared to existing state-of-the-art methods. Code and dataset are released at https://github.com/Ghy0501/HiDe-LLaVA.",,"Haiyang Guo, Fanhu Zeng, Ziwei Xiang, Fei Zhu, Da-Han Wang, Xu-Yao Zhang, Cheng-Lin Liu",2025-05-29T08:30:07Z,HiDe-LLaVA: Hierarchical Decoupling for Continual Instruction Tuning of   Multimodal Large Language Model,HiDe-LlaVA: Hierarchische Entkopplung zur kontinuierlichen Instruktionstuning von multimodalen Großsprachenmodellen,HIDE-LLALAVA:多式大语言模式连续教学制导的等级脱钩,http://arxiv.org/abs/2503.12941v2
1697,"The rapid development of large language models (LLMs) has provided significant support and opportunities for the advancement of domain-specific LLMs. However, fine-tuning these large models using Intangible Cultural Heritage (ICH) data inevitably faces challenges such as bias, incorrect knowledge inheritance, and catastrophic forgetting. To address these issues, we propose a novel training method that integrates a bidirectional chains of thought and a reward mechanism. This method is built upon ICH-Qwen, a large language model specifically designed for the field of intangible cultural heritage. The proposed method enables the model to not only perform forward reasoning but also enhances the accuracy of the generated answers by utilizing reverse questioning and reverse reasoning to activate the model's latent knowledge. Additionally, a reward mechanism is introduced during training to optimize the decision-making process. This mechanism improves the quality of the model's outputs through structural and content evaluations with different weighting schemes. We conduct comparative experiments on ICH-Qwen, with results demonstrating that our method outperforms 0-shot, step-by-step reasoning, knowledge distillation, and question augmentation methods in terms of accuracy, Bleu-4, and Rouge-L scores on the question-answering task. Furthermore, the paper highlights the effectiveness of combining the bidirectional chains of thought and reward mechanism through ablation experiments. In addition, a series of generalizability experiments are conducted, with results showing that the proposed method yields improvements on various domain-specific datasets and advanced models in areas such as Finance, Wikidata, and StrategyQA. This demonstrates that the method is adaptable to multiple domains and provides a valuable approach for model training in future applications across diverse fields.",,"Ruilin Liu, Zhixiao Zhao, Jieqiong Li, Chang Liu, Dongbo Wang",2025-05-29T08:27:23Z,Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method   for Enhancing Question-Answering Capabilities of Large Language Models for   Chinese Intangible Cultural Heritage,Bidirektionale Ketten von Gedanken- und Belohnungsmechanismen zusammenführen Eine Methode zur Verbesserung von Frage-Antwort-Fähigkeiten von großen Sprachmodellen für chinesisches immaterielles Kulturerbe,利用思想和奖赏机制的双向双向两向链 提高中国非物质文化遗产大语言模式的回答问题能力的方法,http://arxiv.org/abs/2505.08167v3
1698,"Large Reasoning Models (LRMs) have demonstrated impressive performances across diverse domains. However, how safety of Large Language Models (LLMs) benefits from enhanced reasoning capabilities against jailbreak queries remains unexplored. To bridge this gap, in this paper, we propose Reasoning-to-Defend (R2D), a novel training paradigm that integrates a safety-aware reasoning mechanism into LLMs' generation. This enables self-evaluation at each step of the reasoning process, forming safety pivot tokens as indicators of the safety status of responses. Furthermore, in order to improve the accuracy of predicting pivot tokens, we propose Contrastive Pivot Optimization (CPO), which enhances the model's perception of the safety status of given dialogues. LLMs dynamically adjust their response strategies during reasoning, significantly enhancing their safety capabilities defending jailbreak attacks. Extensive experiments demonstrate that R2D effectively mitigates various attacks and improves overall safety, while maintaining the original performances. This highlights the substantial potential of safety-aware reasoning in improving robustness of LRMs and LLMs against various jailbreaks.",,"Junda Zhu, Lingyong Yan, Shuaiqiang Wang, Dawei Yin, Lei Sha",2025-05-29T08:25:47Z,Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language   Models from Jailbreaking,Reasoning-to-Defend: Sicherheitsbewusste Reasoning kann große Sprachmodelle von Jailbreaking verteidigen,理由到理由:安全意识理由能够捍卫从破室中使用大语言的模型,http://arxiv.org/abs/2502.12970v2
1699,"The emergence of groundbreaking large language models capable of performing complex reasoning tasks holds significant promise for addressing various scientific challenges, including those arising in complex clinical scenarios. To enable their safe and effective deployment in real-world healthcare settings, it is urgently necessary to benchmark the diagnostic capabilities of current models systematically. Given the limitations of existing medical benchmarks in evaluating advanced diagnostic reasoning, we present DiagnosisArena, a comprehensive and challenging benchmark designed to rigorously assess professional-level diagnostic competence. DiagnosisArena consists of 1,113 pairs of segmented patient cases and corresponding diagnoses, spanning 28 medical specialties, deriving from clinical case reports published in 10 top-tier medical journals. The benchmark is developed through a meticulous construction pipeline, involving multiple rounds of screening and review by both AI systems and human experts, with thorough checks conducted to prevent data leakage. Our study reveals that even the most advanced reasoning models, o3, o1, and DeepSeek-R1, achieve only 51.12%, 31.09%, and 17.79% accuracy, respectively. This finding highlights a significant generalization bottleneck in current large language models when faced with clinical diagnostic reasoning challenges. Through DiagnosisArena, we aim to drive further advancements in AI's diagnostic reasoning capabilities, enabling more effective solutions for real-world clinical diagnostic challenges. We provide the benchmark and evaluation tools for further research and development https://github.com/SPIRAL-MED/DiagnosisArena.",,"Yakun Zhu, Zhongzhen Huang, Linjie Mu, Yutong Huang, Wei Nie, Jiaji Liu, Shaoting Zhang, Pengfei Liu, Xiaofan Zhang",2025-05-29T08:24:00Z,DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language   Models,DiagnoseArena: Benchmarking Diagnostic Reasoning für große Sprachmodelle,诊断阿勒纳:大语言模型诊断依据基准,http://arxiv.org/abs/2505.14107v4
1700,"Large Language Models (LLMs) have expanded their capabilities beyond language generation to interact with external tools, enabling automation and real-world applications. However, tool hallucinations, where models either select inappropriate tools or misuse them, pose significant challenges, leading to erroneous task execution, increased computational costs, and reduced system reliability. To systematically address this issue, we define and categorize tool hallucinations into two main types, tool selection hallucination and tool usage hallucination. To evaluate and mitigate these issues, we introduce RelyToolBench, which integrates specialized test cases and novel metrics to assess hallucination-aware task success and efficiency. Finally, we propose Relign, a reliability alignment framework that expands the tool-use action space to include indecisive actions, allowing LLMs to defer tool use, seek clarification, or adjust tool selection dynamically. Through extensive experiments, we demonstrate that Relign significantly reduces tool hallucinations, improves task reliability, and enhances the efficiency of LLM tool interactions.",,"Hongshen Xu, Zichen Zhu, Lei Pan, Zihan Wang, Su Zhu, Da Ma, Ruisheng Cao, Lu Chen, Kai Yu",2025-05-29T08:04:32Z,Reducing Tool Hallucination via Reliability Alignment,Reduzieren der Werkzeughalluzination durch Zuverlässigkeitsanpassung,通过可靠性调整减少工具幻觉,http://arxiv.org/abs/2412.04141v3
1701,"Multimodal fake news detection has garnered significant attention due to its profound implications for social security. While existing approaches have contributed to understanding cross-modal consistency, they often fail to leverage modal-specific representations and explicit discrepant features. To address these limitations, we propose a Multimodal Inverse Attention Network (MIAN), a novel framework that explores intrinsic discriminative features based on news content to advance fake news detection. Specifically, MIAN introduces a hierarchical learning module that captures diverse intra-modal relationships through local-to-global and local-to-local interactions, thereby generating enhanced unimodal representations to improve the identification of fake news at the intra-modal level. Additionally, a cross-modal interaction module employs a co-attention mechanism to establish and model dependencies between the refined unimodal representations, facilitating seamless semantic integration across modalities. To explicitly extract inconsistency features, we propose an inverse attention mechanism that effectively highlights the conflicting patterns and semantic deviations introduced by fake news in both intra- and inter-modality. Extensive experiments on benchmark datasets demonstrate that MIAN significantly outperforms state-of-the-art methods, underscoring its pivotal contribution to advancing social security through enhanced multimodal fake news detection.",,"Tianlin Zhang, En Yu, Yi Shao, Jiande Sun",2025-05-29T07:33:39Z,Multimodal Inverse Attention Network with Intrinsic Discriminant Feature   Exploitation for Fake News Detection,Multimodale Inverse Aufmerksamkeit Netzwerk mit Intrinsic Discriminant Feature Exploitation für gefälschte Nachrichten Erkennung,"多式反向关注网络,利用内在差异性地貌特征利用假新闻探测",http://arxiv.org/abs/2502.01699v2
1702,"Biological protocols are fundamental to reproducibility and safety in life science research. While large language models (LLMs) perform well on general tasks, their systematic evaluation on these highly specialized, accuracy-critical, and inherently procedural texts remains limited. In this work, we present BioProBench, the first large-scale, multi-task benchmark for biological protocol understanding and reasoning. While there are several benchmark tasks involving protocol question answering, BioProBench provides a comprehensive suite of five core tasks: Protocol Question Answering, Step Ordering, Error Correction, Protocol Generation, and Protocol Reasoning, enabling a holistic evaluation of LLMs on procedural biological texts. Built upon 27K original protocols, it yields nearly 556K high-quality structured instances. We evaluate 12 mainstream open/closed-source LLMs. Experimental results reveal that some models perform well on basic understanding tasks (e.g., \sim70% PQA-Acc., >64% ERR F1), but struggle significantly with deep reasoning and structured generation tasks like ordering and generation. Furthermore, model comparisons show diverse performance: certain open-source models approach closed-source levels on some tasks, yet bio-specific small models lag behind general LLMs, indicating limitations on complex procedural content. Overall, BioProBench, through its task design and experimental findings, systematically reveals the fundamental challenges for current LLMs in procedural knowledge understanding, deep adaptability to specific domains, reliability of structured reasoning, and handling of sophisticated precision and safety constraints, providing key directions for future AI in the field of scientific experiment automation. The code and data are available at: https://github.com/YuyangSunshine/bioprotocolbench and https://huggingface.co/datasets/BioProBench/BioProBench.",,"Yuyang Liu, Liuzhenghao Lv, Xiancheng Zhang, Li Yuan, Yonghong Tian",2025-05-29T07:31:28Z,BioProBench: Comprehensive Dataset and Benchmark in Biological Protocol   Understanding and Reasoning,BioProBench: Umfassender Datensatz und Benchmark im Biologischen Protokoll Verständnis und Vernunft,BioProBench:生物议定书理解和理由的综合数据集和基准,http://arxiv.org/abs/2505.07889v2
1703,"Using LLMs for Multi-Document Topic Extraction has recently gained popularity due to their apparent high-quality outputs, expressiveness, and ease of use. However, most existing evaluation practices are not designed for LLM-generated topics and result in low inter-annotator agreement scores, hindering the reliable use of LLMs for the task. To address this, we introduce $T^5Score$, an evaluation methodology that decomposes the quality of a topic set into quantifiable aspects, measurable through easy-to-perform annotation tasks. This framing enables a convenient, manual or automatic, evaluation procedure resulting in a strong inter-annotator agreement score. To substantiate our methodology and claims, we perform extensive experimentation on multiple datasets and report the results.",,"Itamar Trainin, Omri Abend",2025-05-29T07:31:13Z,$T^5Score$: A Methodology for Automatically Assessing the Quality of LLM   Generated Multi-Document Topic Sets,$T^5Score$: Eine Methode zur automatischen Bewertung der Qualität von LLM Generated Multi-Document Topic Sets,$T$5STR$:自动评估LLM生成的多文件专题集质量的方法,http://arxiv.org/abs/2407.17390v3
1704,"Recent studies provide large language models (LLMs) with textual task-solving experiences via prompts to improve their performance. However, previous methods rely on substantial human labor or time to gather such experiences for each task, which is impractical given the growing variety of task types in user queries to LLMs. To address this issue, we design an autonomous experience transfer framework to explore whether LLMs can mimic human cognitive intelligence to autonomously transfer experience from existing source tasks to newly encountered target tasks. This not only allows the acquisition of experience without extensive costs of previous methods, but also offers a novel path for the generalization of LLMs. Experimental results on 13 datasets demonstrate that our framework effectively improves the performance of LLMs. Furthermore, we provide a detailed analysis of each module in the framework.",,"Jinglong Gao, Xiao Ding, Lingxiao Zou, Bibo Cai, Bing Qin, Ting Liu",2025-05-29T07:30:58Z,ExpeTrans: LLMs Are Experiential Transfer Learners,ExpeTrans: LLMs sind erfahrene Transfer-Lerner,Expetrary: LLMs 是经验性转移学习者,http://arxiv.org/abs/2505.23191v1
1705,"Large Language Model-based multi-agent systems (MAS) have shown remarkable progress in solving complex tasks through collaborative reasoning and inter-agent critique. However, existing approaches typically treat each task in isolation, resulting in redundant computations and limited generalization across structurally similar tasks. To address this, we introduce multi-agent cross-task experiential learning (MAEL), a novel framework that endows LLM-driven agents with explicit cross-task learning and experience accumulation. We model the task-solving workflow on a graph-structured multi-agent collaboration network, where agents propagate information and coordinate via explicit connectivity. During the experiential learning phase, we quantify the quality for each step in the task-solving workflow and store the resulting rewards along with the corresponding inputs and outputs into each agent's individual experience pool. During inference, agents retrieve high-reward, task-relevant experiences as few-shot examples to enhance the effectiveness of each reasoning step, thereby enabling more accurate and efficient multi-agent collaboration. Experimental results on diverse datasets demonstrate that MAEL empowers agents to learn from prior task experiences effectively-achieving faster convergence and producing higher-quality solutions on current tasks.",,"Yilong Li, Chen Qian, Yu Xia, Ruijie Shi, Yufan Dang, Zihao Xie, Ziming You, Weize Chen, Cheng Yang, Weichuan Liu, Ye Tian, Xuantang Xiong, Lei Han, Zhiyuan Liu, Maosong Sun",2025-05-29T07:24:37Z,Cross-Task Experiential Learning on LLM-based Multi-Agent Collaboration,Erfahrungsübergreifendes Lernen auf LLM-basierter Multi-Agent-Kollaboration,关于基于LLM的多机构合作的跨任务跨任务经验学习,http://arxiv.org/abs/2505.23187v1
1706,"Word-level quality estimation (WQE) aims to automatically identify fine-grained error spans in machine-translated outputs and has found many uses, including assisting translators during post-editing. Modern WQE techniques are often expensive, involving prompting of large language models or ad-hoc training on large amounts of human-labeled data. In this work, we investigate efficient alternatives exploiting recent advances in language model interpretability and uncertainty quantification to identify translation errors from the inner workings of translation models. In our evaluation spanning 14 metrics across 12 translation directions, we quantify the impact of human label variation on metric performance by using multiple sets of human labels. Our results highlight the untapped potential of unsupervised metrics, the shortcomings of supervised methods when faced with label uncertainty, and the brittleness of single-annotator evaluation practices.",,"Gabriele Sarti, Vilém Zouhar, Malvina Nissim, Arianna Bisazza",2025-05-29T07:20:36Z,Unsupervised Word-level Quality Estimation for Machine Translation   Through the Lens of Annotators (Dis)agreement,Unüberwachte Bewertung auf Word-Level-Qualität für maschinelle Übersetzung durch die Linse der Annotatoren (Dis)Vereinbarung,未经监督的通过标注员的镜头进行机器翻译的字级质量估计,http://arxiv.org/abs/2505.23183v1
1707,"Continual pre-training has demonstrated significant potential in enhancing model performance, particularly in domain-specific scenarios. The most common approach for packing data before continual pre-training involves concatenating input texts and splitting them into fixed-length sequences. While straightforward and efficient, this method often leads to excessive truncation and context discontinuity, which can hinder model performance. To address these issues, we explore the potential of data engineering to enhance continual pre-training, particularly its impact on model performance and efficiency. We propose Seamless Packing (SP), a novel data packing strategy aimed at preserving contextual information more effectively and enhancing model performance. Our approach employs a sliding window technique in the first stage that synchronizes overlapping tokens across consecutive sequences, ensuring better continuity and contextual coherence. In the second stage, we adopt a First-Fit-Decreasing algorithm to pack shorter texts into bins slightly larger than the target sequence length, thereby minimizing padding and truncation. Empirical evaluations across various model architectures and corpus domains demonstrate the effectiveness of our method, outperforming baseline method in 99% of all settings. Code is available at https://github.com/Infernus-WIND/Seamless-Packing.",,"Ruicheng Yin, Xuan Gao, Changze Lv, Xiaohua Wang, Xiaoqing Zheng, Xuanjing Huang",2025-05-29T07:20:02Z,Improving Continual Pre-training Through Seamless Data Packing,Verbesserung der kontinuierlichen Vorschulung durch nahtloses Datenpaket,通过无缝无缝数据包装改进持续培训前培训,http://arxiv.org/abs/2505.22018v2
1708,"Traditional code instruction data synthesis methods suffer from limited diversity and poor logic. We introduce Infinite-Instruct, an automated framework for synthesizing high-quality question-answer pairs, designed to enhance the code generation capabilities of large language models (LLMs). The framework focuses on improving the internal logic of synthesized problems and the quality of synthesized code. First, ""Reverse Construction"" transforms code snippets into diverse programming problems. Then, through ""Backfeeding Construction,"" keywords in programming problems are structured into a knowledge graph to reconstruct them into programming problems with stronger internal logic. Finally, a cross-lingual static code analysis pipeline filters invalid samples to ensure data quality. Experiments show that on mainstream code generation benchmarks, our fine-tuned models achieve an average performance improvement of 21.70% on 7B-parameter models and 36.95% on 32B-parameter models. Using less than one-tenth of the instruction fine-tuning data, we achieved performance comparable to the Qwen-2.5-Coder-Instruct. Infinite-Instruct provides a scalable solution for LLM training in programming. We open-source the datasets used in the experiments, including both unfiltered versions and filtered versions via static analysis. The data are available at https://github.com/xingwenjing417/Infinite-Instruct-dataset",,"Wenjing Xing, Wenke Lu, Yeheng Duan, Bing Zhao, Zhenghui kang, Yaolong Wang, Kai Gao, Lei Qiao",2025-05-29T07:14:43Z,Infinite-Instruct: Synthesizing Scaling Code instruction Data with   Bidirectional Synthesis and Static Verification,Infinite-Instruct: Synthesizing Scaling Code instruction Daten mit bidirektionaler Synthese und statischer Verifikation,无限指令:以双向合成和静态核查将缩放码指示数据与双向合成和静态核查结合起来,http://arxiv.org/abs/2505.23177v1
1709,"Transforming dense, detailed, unstructured text into an interpretable and summarised table, also colloquially known as Text-to-Table generation, is an essential task for information retrieval. Current methods, however, miss out on how and what complex information to extract; they also lack the ability to infer data from the text. In this paper, we introduce a versatile approach, Map&Make, which ""dissects"" text into propositional atomic statements. This facilitates granular decomposition to extract the latent schema. The schema is then used to populate the tables that capture the qualitative nuances and the quantitative facts in the original text. Our approach is tested against two challenging datasets, Rotowire, renowned for its complex and multi-table schema, and Livesum, which demands numerical aggregation. By carefully identifying and correcting hallucination errors in Rotowire, we aim to achieve a cleaner and more reliable benchmark. We evaluate our method rigorously on a comprehensive suite of comparative and referenceless metrics. Our findings demonstrate significant improvement results across both datasets with better interpretability in Text-to-Table generation. Moreover, through detailed ablation studies and analyses, we investigate the factors contributing to superior performance and validate the practicality of our framework in structured summarization tasks.",,"Naman Ahuja, Fenil Bardoliya, Chitta Baral, Vivek Gupta",2025-05-29T07:12:46Z,Map&Make: Schema Guided Text to Table Generation,Map&Make: Schema-Leittext zur Tabellenerstellung,Mag&Make: 生成表格的图表向导文本,http://arxiv.org/abs/2505.23174v1
1710,"We present ZIPA, a family of efficient speech models that advances the state-of-the-art performance of crosslinguistic phone recognition. We first curated IPAPack++, a large-scale multilingual speech corpus with 17,132 hours of normalized phone transcriptions and a novel evaluation set capturing unseen languages and sociophonetic variation. With the large-scale training data, ZIPA, including transducer (ZIPA-T) and CTC-based (ZIPA-CR) variants, leverage the efficient Zipformer backbones and outperform existing phone recognition systems with much fewer parameters. Further scaling via noisy student training on 11,000 hours of pseudo-labeled multilingual data yields further improvement. While ZIPA achieves strong performance on benchmarks, error analysis reveals persistent limitations in modeling sociophonetic diversity, underscoring challenges for future research.",,"Jian Zhu, Farhan Samir, Eleanor Chodroff, David R. Mortensen",2025-05-29T07:08:23Z,ZIPA: A family of efficient models for multilingual phone recognition,ZIPA: Eine Familie von effizienten Modellen für mehrsprachige Telefonerkennung,ZIPA:一套有效的多语言电话识别模式,http://arxiv.org/abs/2505.23170v1
1711,"Conventional bag-of-words approaches for topic modeling, like latent Dirichlet allocation (LDA), struggle with literary text. Literature challenges lexical methods because narrative language focuses on immersive sensory details instead of abstractive description or exposition: writers are advised to ""show, don't tell."" We propose Retell, a simple, accessible topic modeling approach for literature. Here, we prompt resource-efficient, generative language models (LMs) to tell what passages show, thereby translating narratives' surface forms into higher-level concepts and themes. By running LDA on LMs' retellings of passages, we can obtain more precise and informative topics than by running LDA alone or by directly asking LMs to list topics. To investigate the potential of our method for cultural analytics, we compare our method's outputs to expert-guided annotations in a case study on racial/cultural identity in high school English language arts books.",,"Li Lucy, Camilla Griffiths, Sarah Levine, Jennifer L. Eberhardt, Dorottya Demszky, David Bamman",2025-05-29T06:59:21Z,"Tell, Don't Show: Leveraging Language Models' Abstractive Retellings to   Model Literary Themes","Tell, Don't Show: Die abstrakten Retellings von Sprachmodellen nutzen, um literarische Themen zu modellieren","Tell, don't show: 利用语言模型对示范文学主题的抽象引用",http://arxiv.org/abs/2505.23166v1
1712,"Temporal information extraction from unstructured text is essential for contextualizing events and deriving actionable insights, particularly in the medical domain. We address the task of extracting clinical events and their temporal relations using the well-studied I2B2 2012 Temporal Relations Challenge corpus. This task is inherently challenging due to complex clinical language, long documents, and sparse annotations. We introduce GRAPHTREX, a novel method integrating span-based entity-relation extraction, clinical large pre-trained language models (LPLMs), and Heterogeneous Graph Transformers (HGT) to capture local and global dependencies. Our HGT component facilitates information propagation across the document through innovative global landmarks that bridge distant entities. Our method improves the state-of-the-art with 5.5% improvement in the tempeval $F_1$ score over the previous best and up to 8.9% improvement on long-range relations, which presents a formidable challenge. We further demonstrate generalizability by establishing a strong baseline on the E3C corpus. This work not only advances temporal information extraction but also lays the groundwork for improved diagnostic and prognostic models through enhanced temporal reasoning.",,"Rochana Chaturvedi, Peyman Baghershahi, Sourav Medya, Barbara Di Eugenio",2025-05-29T06:56:54Z,Temporal Relation Extraction in Clinical Texts: A Span-based Graph   Transformer Approach,Temporale Beziehungsextraktion in klinischen Texten: Ein Span-basierter Graph Transformer-Ansatz,临床文本中的时间关系抽取时间关系:基于泛泛面的图形变形器方法,http://arxiv.org/abs/2503.18085v2
1713,"As large language models (LLMs) often generate plausible but incorrect content, error detection has become increasingly critical to ensure truthfulness. However, existing detection methods often overlook a critical problem we term as self-consistent error, where LLMs repeatly generate the same incorrect response across multiple stochastic samples. This work formally defines self-consistent errors and evaluates mainstream detection methods on them. Our investigation reveals two key findings: (1) Unlike inconsistent errors, whose frequency diminishes significantly as LLM scale increases, the frequency of self-consistent errors remains stable or even increases. (2) All four types of detection methshods significantly struggle to detect self-consistent errors. These findings reveal critical limitations in current detection methods and underscore the need for improved methods. Motivated by the observation that self-consistent errors often differ across LLMs, we propose a simple but effective cross-model probe method that fuses hidden state evidence from an external verifier LLM. Our method significantly enhances performance on self-consistent errors across three LLM families.",,"Hexiang Tan, Fei Sun, Sha Liu, Du Su, Qi Cao, Xin Chen, Jingang Wang, Xunliang Cai, Yuanzhuo Wang, Huawei Shen, Xueqi Cheng",2025-05-29T06:51:44Z,Too Consistent to Detect: A Study of Self-Consistent Errors in LLMs,"Zu konsequent, um zu erkennen: Eine Studie über selbstkonsistente Fehler in LLMs","过于一致,无法检测:LLMM中自相矛盾错误的研究",http://arxiv.org/abs/2505.17656v2
1714,"Bilingual Lexicon Induction (BLI) is generally based on common domain data to obtain monolingual word embedding, and by aligning the monolingual word embeddings to obtain the cross-lingual embeddings which are used to get the word translation pairs. In this paper, we propose a new task of BLI, which is to use the monolingual corpus of the general domain and target domain to extract domain-specific bilingual dictionaries. Motivated by the ability of Pre-trained models, we propose a method to get better word embeddings that build on the recent work on BLI. This way, we introduce the Code Switch(Qin et al., 2020) firstly in the cross-domain BLI task, which can match differit is yet to be seen whether these methods are suitable for bilingual lexicon extraction in professional fields. As we can see in table 1, the classic and efficient BLI approach, Muse and Vecmap, perform much worse on the Medical dataset than on the Wiki dataset. On one hand, the specialized domain data set is relatively smaller compared to the generic domain data set generally, and specialized words have a lower frequency, which will directly affect the translation quality of bilingual dictionaries. On the other hand, static word embeddings are widely used for BLI, however, in some specific fields, the meaning of words is greatly influenced by context, in this case, using only static word embeddings may lead to greater bias. ent strategies in different contexts, making the model more suitable for this task. Experimental results show that our method can improve performances over robust BLI baselines on three specific domains by averagely improving 0.78 points.",,"Qiuyu Ding, Zhiqiang Cao, Hailong Cao, Tiejun Zhao",2025-05-29T06:37:02Z,Cross-Domain Bilingual Lexicon Induction via Pretrained Language Models,Cross-Domain Zweisprachige Lexikoninduktion über vorgebildete Sprachmodelle,通过预先培训语言模式的跨域双语双语双语,http://arxiv.org/abs/2505.23146v1
1715,"Large language models (LLMs) integrated with retrieval-augmented generation (RAG) have improved factuality by grounding outputs in external evidence. However, they remain susceptible to unfaithful generation, where outputs contradict retrieved context despite its relevance and accuracy. Existing approaches aiming to improve faithfulness primarily focus on enhancing the utilization of external context, but often overlook the persistent influence of internal parametric knowledge during generation. In this work, we investigate the internal mechanisms behind unfaithful generation and identify a subset of mid-to-deep feed-forward networks (FFNs) that are disproportionately activated in such cases. Building on this insight, we propose Parametric Knowledge Muting through FFN Suppression (ParamMute), a framework that improves contextual faithfulness by suppressing the activation of unfaithfulness-associated FFNs and calibrating the model toward retrieved knowledge. To evaluate our approach, we introduce CoFaithfulQA, a benchmark specifically designed to evaluate faithfulness in scenarios where internal knowledge conflicts with accurate external evidence. Experimental results show that ParamMute significantly enhances faithfulness across both CoFaithfulQA and the established ConFiQA benchmark, achieving substantial reductions in reliance on parametric memory. These findings underscore the importance of mitigating internal knowledge dominance and provide a new direction for improving LLM trustworthiness in RAG. All code will be released via GitHub.",,"Pengcheng Huang, Zhenghao Liu, Yukun Yan, Haiyan Zhao, Xiaoyuan Yi, Hao Chen, Zhiyuan Liu, Maosong Sun, Tong Xiao, Ge Yu, Chenyan Xiong",2025-05-29T06:35:30Z,ParamMute: Suppressing Knowledge-Critical FFNs for Faithful   Retrieval-Augmented Generation,Parammute: Unterdrückende wissenskritische FFNs für treue retrieval-erweiterte Generation,"分量:制止知识-关键FFFF,以用于忠实检索-养殖一代",http://arxiv.org/abs/2502.15543v2
1716,"Large language models have demonstrated exceptional performance across multiple crosslingual NLP tasks, including machine translation (MT). However, persistent challenges remain in addressing context-sensitive units (CSUs), such as polysemous words. These CSUs not only affect the local translation accuracy of LLMs, but also affect LLMs' understanding capability for sentences and tasks, and even lead to translation failure. To address this problem, we propose a simple but effective method to enhance LLMs' MT capabilities by acquiring CSUs and applying semantic focus. Specifically, we dynamically analyze and identify translation challenges, then incorporate them into LLMs in a structured manner to mitigate mistranslations or misunderstandings of CSUs caused by information flattening. Efficiently activate LLMs to identify and apply relevant knowledge from its vast data pool in this way, ensuring more accurate translations for translating difficult terms. On a benchmark dataset of MT, our proposed method achieved competitive performance compared to multiple existing open-sourced MT baseline models. It demonstrates effectiveness and robustness across multiple language pairs, including both similar language pairs and distant language pairs. Notably, the proposed method requires no additional model training and enhances LLMs' performance across multiple NLP tasks with minimal resource consumption.",,"Qiuyu Ding, Zhiqiang Cao, Hailong Cao, Tiejun Zhao",2025-05-29T06:29:57Z,Enhancing Large Language Models'Machine Translation via Dynamic Focus   Anchoring,Verbesserung der Übersetzung großer Sprachmodelle durch Dynamic Focus Anchoring,通过动态焦点拼接加强大语言模型的“Machine ”翻译,http://arxiv.org/abs/2505.23140v1
1717,"The paper focuses on the interpretability of Grammatical Error Correction (GEC) evaluation metrics, which received little attention in previous studies. To bridge the gap, we introduce **CLEME2.0**, a reference-based metric describing four fundamental aspects of GEC systems: hit-correction, wrong-correction, under-correction, and over-correction. They collectively contribute to exposing critical qualities and locating drawbacks of GEC systems. Evaluating systems by combining these aspects also leads to superior human consistency over other reference-based and reference-less metrics. Extensive experiments on two human judgment datasets and six reference datasets demonstrate the effectiveness and robustness of our method, achieving a new state-of-the-art result. Our codes are released at https://github.com/THUKElab/CLEME.",,"Jingheng Ye, Zishan Xu, Yinghui Li, Linlin Song, Qingyu Zhou, Hai-Tao Zheng, Ying Shen, Wenhao Jiang, Hong-Gee Kim, Ruitong Liu, Xin Su, Zifei Shan",2025-05-29T06:15:38Z,CLEME2.0: Towards Interpretable Evaluation by Disentangling Edits for   Grammatical Error Correction,CLEME2.0: Auf dem Weg zur Interpretierbaren Bewertung durch Entwirren von Edits für die Korrektur von Grammatikfehlern,CLEME2.0:通过拆分文体错误校正的编辑版实现可解释性评价,http://arxiv.org/abs/2407.00934v2
1718,"Recent advances in large reasoning models (LRMs) demonstrate that sophisticated behaviors such as multi-step reasoning and self-reflection can emerge via reinforcement learning with verifiable rewards~(\textit{RLVR}). However, existing \textit{RLVR} approaches are inherently ``on-policy'', limiting learning to a model's own outputs and failing to acquire reasoning abilities beyond its initial capabilities. To address this issue, we introduce \textbf{LUFFY} (\textbf{L}earning to reason \textbf{U}nder o\textbf{FF}-polic\textbf{Y} guidance), a framework that augments \textit{RLVR} with off-policy reasoning traces. LUFFY dynamically balances imitation and exploration by combining off-policy demonstrations with on-policy rollouts during training. Specifically, LUFFY combines the Mixed-Policy GRPO framework, which has a theoretically guaranteed convergence rate, alongside policy shaping via regularized importance sampling to avoid superficial and rigid imitation during mixed-policy training. Compared with previous RLVR methods, LUFFY achieves an over \textbf{+6.4} average gain across six math benchmarks and an advantage of over \textbf{+6.2} points in out-of-distribution tasks. Most significantly, we show that LUFFY successfully trains weak models in scenarios where on-policy RLVR completely fails. These results provide compelling evidence that LUFFY transcends the fundamental limitations of on-policy RLVR and demonstrates the great potential of utilizing off-policy guidance in RLVR.",,"Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, Yue Zhang",2025-05-29T06:14:43Z,Learning to Reason under Off-Policy Guidance,Unter außerpolitischer Anleitung zur Vernunft lernen,根据非政策指导学习理由,http://arxiv.org/abs/2504.14945v4
1719,"Large Language Models (LLMs) can be used to red team other models (e.g. jailbreaking) to elicit harmful contents. While prior works commonly employ open-weight models or private uncensored models for doing jailbreaking, as the refusal-training of strong LLMs (e.g. OpenAI o3) refuse to help jailbreaking, our work turn (almost) any black-box LLMs into attackers. The resulting $J_2$ (jailbreaking-to-jailbreak) attackers can effectively jailbreak the safeguard of target models using various strategies, both created by themselves or from expert human red teamers. In doing so, we show their strong but under-researched jailbreaking capabilities. Our experiments demonstrate that 1) prompts used to create $J_2$ attackers transfer across almost all black-box models; 2) an $J_2$ attacker can jailbreak a copy of itself, and this vulnerability develops rapidly over the past 12 months; 3) reasong models, such as Sonnet-3.7, are strong $J_2$ attackers compared to others. For example, when used against the safeguard of GPT-4o, $J_2$ (Sonnet-3.7) achieves 0.975 attack success rate (ASR), which matches expert human red teamers and surpasses the state-of-the-art algorithm-based attacks. Among $J_2$ attackers, $J_2$ (o3) achieves highest ASR (0.605) against Sonnet-3.5, one of the most robust models.",,"Jeremy Kritz, Vaughn Robinson, Robert Vacareanu, Bijan Varjavand, Michael Choi, Bobby Gogov, Scale Red Team, Summer Yue, Willow E. Primack, Zifan Wang",2025-05-29T06:12:00Z,Jailbreaking to Jailbreak,Gefängnisbruch zum Gefängnisbruch,"破门而入,破门而入,",http://arxiv.org/abs/2502.09638v2
1720,"Language models (LMs) risk inadvertently memorizing and divulging sensitive or personally identifiable information (PII) seen in training data, causing privacy concerns. Current approaches to address this issue involve costly dataset scrubbing, or model filtering through unlearning and model editing, which can be bypassed through extraction attacks. We propose REVS, a novel non-gradient-based method for unlearning sensitive information from LMs. REVS identifies and modifies a small subset of neurons relevant for constituent tokens that form sensitive information. To adequately evaluate our method on truly sensitive information, we curate three datasets: email and URL datasets naturally memorized by the models, and a synthetic social security number dataset that we tune the models to memorize. Compared to other methods, REVS demonstrates superior performance in unlearning sensitive information and robustness to extraction attacks, while retaining underlying model integrity.",,"Tomer Ashuach, Martin Tutek, Yonatan Belinkov",2025-05-29T06:06:35Z,REVS: Unlearning Sensitive Information in Language Models via Rank   Editing in the Vocabulary Space,REVS: Unlearning Sensible Information in Language Models via Rank Editing im Vokabelfeld,REVS:通过词汇空间排行编辑在语言模型中学习敏感信息,http://arxiv.org/abs/2406.09325v5
1721,"Despite the growing development of long-context large language models (LLMs), data-centric approaches relying on synthetic data have been hindered by issues related to faithfulness, which limit their effectiveness in enhancing model performance on tasks such as long-context reasoning and question answering (QA). These challenges are often exacerbated by misinformation caused by lack of verification, reasoning without attribution, and potential knowledge conflicts. We propose LongFaith, a novel pipeline for synthesizing faithful long-context reasoning instruction datasets. By integrating ground truth and citation-based reasoning prompts, we eliminate distractions and improve the accuracy of reasoning chains, thus mitigating the need for costly verification processes. We open-source two synthesized datasets, LongFaith-SFT and LongFaith-PO, which systematically address multiple dimensions of faithfulness, including verified reasoning, attribution, and contextual grounding. Extensive experiments on multi-hop reasoning datasets and LongBench demonstrate that models fine-tuned on these datasets significantly improve performance. Our ablation studies highlight the scalability and adaptability of the LongFaith pipeline, showcasing its broad applicability in developing long-context LLMs.",,"Cehao Yang, Xueyuan Lin, Chengjin Xu, Xuhui Jiang, Shengjie Ma, Aofan Liu, Hui Xiong, Jian Guo",2025-05-29T06:01:22Z,LongFaith: Enhancing Long-Context Reasoning in LLMs with Faithful   Synthetic Data,LongFaith: Verbesserung der Langkontext-Reasonierung in LLMs mit treuen synthetischen Daten,长面:利用忠实合成数据加强LLMs中的长方理由,http://arxiv.org/abs/2502.12583v2
1722,"As the AI systems become deeply embedded in social media platforms, we've uncovered a concerning security vulnerability that goes beyond traditional adversarial attacks. It becomes important to assess the risks of LLMs before the general public use them on social media platforms to avoid any adverse impacts. Unlike obvious nonsensical text strings that safety systems can easily catch, our work reveals that human-readable situation-driven adversarial full-prompts that leverage situational context are effective but much harder to detect. We found that skilled attackers can exploit the vulnerabilities in open-source and proprietary LLMs to make a malicious user query safe for LLMs, resulting in generating a harmful response. This raises an important question about the vulnerabilities of LLMs. To measure the robustness against human-readable attacks, which now present a potent threat, our research makes three major contributions. First, we developed attacks that use movie scripts as situational contextual frameworks, creating natural-looking full-prompts that trick LLMs into generating harmful content. Second, we developed a method to transform gibberish adversarial text into readable, innocuous content that still exploits vulnerabilities when used within the full-prompts. Finally, we enhanced the AdvPrompter framework with p-nucleus sampling to generate diverse human-readable adversarial texts that significantly improve attack effectiveness against models like GPT-3.5-Turbo-0125 and Gemma-7b. Our findings show that these systems can be manipulated to operate beyond their intended ethical boundaries when presented with seemingly normal prompts that contain hidden adversarial elements. By identifying these vulnerabilities, we aim to drive the development of more robust safety mechanisms that can withstand sophisticated attacks in real-world applications.",,"Nilanjana Das, Edward Raff, Aman Chadha, Manas Gaur",2025-05-29T05:54:54Z,Human-Readable Adversarial Prompts: An Investigation into LLM   Vulnerabilities Using Situational Context,Human-Readable Adversarial Prompts: Eine Untersuchung von LLM-Fehlern mit situationsbezogenem Kontext,人类可以读取的反向提示:利用情况背景调查LLM脆弱性,http://arxiv.org/abs/2412.16359v3
1723,"We introduce CASS, the first large-scale dataset and model suite for cross-architecture GPU code transpilation, targeting both source-level (CUDA <--> HIP) and assembly-level (Nvidia SASS <--> AMD RDNA3) translation. The dataset comprises 70k verified code pairs across host and device, addressing a critical gap in low-level GPU code portability. Leveraging this resource, we train the CASS family of domain-specific language models, achieving 95% source translation accuracy and 37.5% assembly translation accuracy, substantially outperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our generated code matches native performance in over 85% of test cases, preserving runtime and memory behavior. To support rigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16 GPU domains with ground-truth execution. All data, models, and evaluation tools are released as open source to foster progress in GPU compiler tooling, binary compatibility, and LLM-guided hardware translation.",,"Ahmed Heakl, Sarim Hashmi, Gustavo Bertolo Stahl, Seung Hun Eddie Han, Salman Khan, Abdulrahman Mahmoud",2025-05-29T05:44:32Z,"CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark","CASS: Nvidia zu AMD Transpilation mit Daten, Modellen und Benchmark",CASS: Nvidia 到AMD 传输数据、模型和基准,http://arxiv.org/abs/2505.16968v3
1724,"Brain-to-Image reconstruction aims to recover visual stimuli perceived by humans from brain activity. However, the reconstructed visual stimuli often missing details and semantic inconsistencies, which may be attributed to insufficient semantic information. To address this issue, we propose an approach named Fine-grained Brain-to-Image reconstruction (FgB2I), which employs fine-grained text as bridge to improve image reconstruction. FgB2I comprises three key stages: detail enhancement, decoding fine-grained text descriptions, and text-bridged brain-to-image reconstruction. In the detail-enhancement stage, we leverage large vision-language models to generate fine-grained captions for visual stimuli and experimentally validate its importance. We propose three reward metrics (object accuracy, text-image semantic similarity, and image-image semantic similarity) to guide the language model in decoding fine-grained text descriptions from fMRI signals. The fine-grained text descriptions can be integrated into existing reconstruction methods to achieve fine-grained Brain-to-Image reconstruction.",,"Runze Xia, Shuo Feng, Renzhi Wang, Congchi Yin, Xuyun Wen, Piji Li",2025-05-29T05:43:34Z,Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging,Verbesserung des Brain-to-Image-Reconstructions durch feinkörnige Text-Bridging,通过完善的文本连接改进脑到图像重建,http://arxiv.org/abs/2505.22150v2
1725,"Multi-modal large language models have demonstrated remarkable zero-shot abilities and powerful image-understanding capabilities. However, the existing open-source multi-modal models suffer from the weak capability of multi-turn interaction, especially for long contexts. To address the issue, we first introduce a context modeling module, termed ContextQFormer, which utilizes a memory block to enhance the presentation of contextual information. Furthermore, to facilitate further research, we carefully build a new multi-turn multi-modal dialogue dataset (TMDialog) for pre-training, instruction-tuning, and evaluation, which will be open-sourced lately. Compared with other multi-modal dialogue datasets, TMDialog contains longer conversations, which supports the research of multi-turn multi-modal dialogue. In addition, ContextQFormer is compared with three baselines on TMDialog and experimental results illustrate that ContextQFormer achieves an improvement of 2%-4% in available rate over baselines.",,"Yiming Lei, Zhizheng Yang, Zeming Liu, Haitao Leng, Shaoguo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang",2025-05-29T05:41:26Z,ContextQFormer: A New Context Modeling Method for Multi-Turn Multi-Modal   Conversations,ContextQFormer: Eine neue Context-Modellierungsmethode für Multi-Turn Multi-Modal-Gespräche,上下文前:多发多式多模式对话的新背景建模方法,http://arxiv.org/abs/2505.23121v1
1726,"Effective clinical decision-making depends on iterative, multimodal reasoning across diverse sources of evidence. The recent emergence of multimodal reasoning models has significantly transformed the landscape of solving complex tasks. Although such models have achieved notable success in mathematics and science, their application to medical domains remains underexplored. In this work, we propose \textit{MedE$^2$}, a two-stage post-training pipeline that elicits and then enhances multimodal reasoning for medical domains. In Stage-I, we fine-tune models using 2,000 text-only data samples containing precisely orchestrated reasoning demonstrations to elicit reasoning behaviors. In Stage-II, we further enhance the model's reasoning capabilities using 1,500 rigorously curated multimodal medical cases, aligning model reasoning outputs with our proposed multimodal medical reasoning preference. Extensive experiments demonstrate the efficacy and reliability of \textit{MedE$^2$} in improving the reasoning performance of medical multimodal models. Notably, models trained with \textit{MedE$^2$} consistently outperform baselines across multiple medical multimodal benchmarks. Additional validation on larger models and under inference-time scaling further confirms the robustness and practical utility of our approach.",,"Linjie Mu, Zhongzhen Huang, Yakun Zhu, Xiangyu Zhao, Shaoting Zhang, Xiaofan Zhang",2025-05-29T05:39:23Z,Elicit and Enhance: Advancing Multimodal Reasoning in Medical Scenarios,Elicit und Enhance: Multimodale Reasoning in medizinischen Szenarien fördern,明确和强化:推进医疗假想中的多式联运理由,http://arxiv.org/abs/2505.23118v1
1727,"Solving complex tasks in a single attempt is challenging for large language models (LLMs). Iterative interaction with the environment and feedback is often required to achieve success, making effective feedback utilization a critical topic. Existing approaches either struggle with length generalization or rely on naive retries without leveraging prior information. In this paper, we introduce FTTT, a novel paradigm that formulates feedback utilization as an optimization problem at test time. Additionally, we propose a learnable test-time optimizer, OpTune, to effectively exploit feedback. Experiments on two LLMs across four reasoning datasets demonstrate that FTTT and OpTune achieve superior scalability and performance.",,"Yanyang Li, Michael Lyu, Liwei Wang",2025-05-29T05:35:57Z,Learning to Reason from Feedback at Test-Time,Von Feedback bei Test-Time zur Vernunft lernen,从测试时的反馈中学习到理由,http://arxiv.org/abs/2502.15771v2
1728,"Recent advances in large language models (LLMs) have shown significant promise, yet their evaluation raises concerns, particularly regarding data contamination due to the lack of access to proprietary training data. To address this issue, we present C$^2$LEVA, a comprehensive bilingual benchmark featuring systematic contamination prevention. C$^2$LEVA firstly offers a holistic evaluation encompassing 22 tasks, each targeting a specific application or ability of LLMs, and secondly a trustworthy assessment due to our contamination-free tasks, ensured by a systematic contamination prevention strategy that fully automates test data renewal and enforces data protection during benchmark data release. Our large-scale evaluation of 15 open-source and proprietary models demonstrates the effectiveness of C$^2$LEVA.",,"Yanyang Li, Tin Long Wong, Cheung To Hung, Jianqiao Zhao, Duo Zheng, Ka Wai Liu, Michael R. Lyu, Liwei Wang",2025-05-29T05:29:28Z,C$^2$LEVA: Toward Comprehensive and Contamination-Free Language Model   Evaluation,C$^2$LEVA: Auf dem Weg zu einer umfassenden und kontaminationsfreien Sprachmodellbewertung,C$$2$LEVA:努力实现全面和无污染、无污染的无语言模式评价,http://arxiv.org/abs/2412.04947v3
1729,"The emerging capabilities of large language models (LLMs) have sparked concerns about their immediate potential for harmful misuse. The core approach to mitigate these concerns is the detection of harmful queries to the model. Current detection approaches are fallible, and are particularly susceptible to attacks that exploit mismatched generalization of model capabilities (e.g., prompts in low-resource languages or prompts provided in non-text modalities such as image and audio). To tackle this challenge, we propose OMNIGUARD, an approach for detecting harmful prompts across languages and modalities. Our approach (i) identifies internal representations of an LLM/MLLM that are aligned across languages or modalities and then (ii) uses them to build a language-agnostic or modality-agnostic classifier for detecting harmful prompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\% over the strongest baseline in a multilingual setting, by 20.44\% for image-based prompts, and sets a new SOTA for audio-based prompts. By repurposing embeddings computed during generation, OMNIGUARD is also very efficient ($\approx 120 \times$ faster than the next fastest baseline). Code and data are available at: https://github.com/vsahil/OmniGuard.",,"Sahil Verma, Keegan Hines, Jeff Bilmes, Charlotte Siska, Luke Zettlemoyer, Hila Gonen, Chandan Singh",2025-05-29T05:25:27Z,OMNIGUARD: An Efficient Approach for AI Safety Moderation Across   Modalities,OMNIGUARD: Ein effizienter Ansatz für KI-Sicherheitsmoderation über Modalitäten hinweg,OMNIGUARD: AI安全调控全模式的有效方法,http://arxiv.org/abs/2505.23856v1
1730,"The future work section of a scientific article outlines potential research directions by identifying gaps and limitations of a current study. This section serves as a valuable resource for early-career researchers seeking unexplored areas and experienced researchers looking for new projects or collaborations. In this study, we generate future work suggestions from key sections of a scientific article alongside related papers and analyze how the trends have evolved. We experimented with various Large Language Models (LLMs) and integrated Retrieval-Augmented Generation (RAG) to enhance the generation process. We incorporate a LLM feedback mechanism to improve the quality of the generated content and propose an LLM-as-a-judge approach for evaluation. Our results demonstrated that the RAG-based approach with LLM feedback outperforms other methods evaluated through qualitative and quantitative metrics. Moreover, we conduct a human evaluation to assess the LLM as an extractor and judge. The code and dataset for this project are here, code: HuggingFace",,"Ibrahim Al Azher, Miftahul Jannat Mokarrama, Zhishuai Guo, Sagnik Ray Choudhury, Hamed Alhoori",2025-05-29T05:23:48Z,FutureGen: LLM-RAG Approach to Generate the Future Work of Scientific   Article,FutureGen: LLM-RAG Ansatz zur Generierung der zukünftigen Arbeit des wissenschaftlichen Artikels,FutureGen:LLM-RAG 产生科学条款未来工作的方法,http://arxiv.org/abs/2503.16561v2
1731,"The remarkable reasoning and generalization capabilities of Large Language Models (LLMs) have paved the way for their expanding applications in embodied AI, robotics, and other real-world tasks. To effectively support these applications, grounding in spatial and temporal understanding in multimodal environments is essential. To this end, recent works have leveraged scene graphs, a structured representation that encodes entities, attributes, and their relationships in a scene. However, a comprehensive evaluation of LLMs' ability to utilize scene graphs remains limited. In this work, we introduce Text-Scene Graph (TSG) Bench, a benchmark designed to systematically assess LLMs' ability to (1) understand scene graphs and (2) generate them from textual narratives. With TSG Bench we evaluate 11 LLMs and reveal that, while models perform well on scene graph understanding, they struggle with scene graph generation, particularly for complex narratives. Our analysis indicates that these models fail to effectively decompose discrete scenes from a complex narrative, leading to a bottleneck when generating scene graphs. These findings underscore the need for improved methodologies in scene graph generation and provide valuable insights for future research. The demonstration of our benchmark is available at https://tsg-bench.netlify.app. Additionally, our code and evaluation data are publicly available at https://github.com/docworlds/tsg-bench.",,"Dongil Yang, Minjin Kim, Sunghwan Kim, Beong-woo Kwak, Minjun Park, Jinseok Hong, Woontack Woo, Jinyoung Yeo",2025-05-29T05:23:38Z,LLM Meets Scene Graph: Can Large Language Models Understand and Generate   Scene Graphs? A Benchmark and Empirical Study,LLM trifft Szenegraph: Können große Sprachmodelle Szenengraphen verstehen und generieren? Eine Benchmark- und Empirische Studie,LLM 满足景象图:大语言模型能够理解和产生景象图吗? 基准和经验研究,http://arxiv.org/abs/2505.19510v2
1732,"Using Large Language Models (LLMs) to generate training data can potentially be a preferable way to improve zero or few-shot NLP tasks. However, many problems remain to be investigated for this direction. For the task of Relation Extraction (RE), we find that samples generated by directly prompting LLMs may easily have high structural similarities with each other. They tend to use a limited variety of phrasing while expressing the relation between a pair of entities. Therefore, in this paper, we study how to effectively improve the diversity of the training samples generated with LLMs for RE, while also maintaining their correctness. We first try to make the LLMs produce dissimilar samples by directly giving instructions in In-Context Learning (ICL) prompts. Then, we propose an approach to fine-tune LLMs for diversity training sample generation through Direct Preference Optimization (DPO). Our experiments on commonly used RE datasets show that both attempts can improve the quality of the generated training data. We also find that comparing with directly performing RE with an LLM, training a non-LLM RE model with its generated samples may lead to better performance.",,"Zexuan Li, Hongliang Dai, Piji Li",2025-05-29T05:21:54Z,Generating Diverse Training Samples for Relation Extraction with Large   Language Models,Erzeugen von unterschiedlichen Trainingsbeispielen für die Beziehungsextraktion mit großen Sprachmodellen,"生成多种培训样本,用于与大语言模式的抽取关系",http://arxiv.org/abs/2505.23108v1
1733,"Evaluating large vision-language models (LVLMs) is very expensive, due to high computational cost and the wide variety of tasks. The good news is that if we already have some observed performance scores, we may be able to infer unknown ones. In this study, we propose a new framework for predicting unknown performance scores based on observed ones from other LVLMs or tasks. We first formulate the performance prediction as a matrix completion task. Specifically, we construct a sparse performance matrix $\boldsymbol{R}$, where each entry $R_{mn}$ represents the performance score of the $m$-th model on the $n$-th dataset. By applying probabilistic matrix factorization (PMF) with Markov chain Monte Carlo (MCMC), we can complete the performance matrix, i.e., predict unknown scores. Additionally, we estimate the uncertainty of performance prediction based on MCMC. Practitioners can evaluate their models on untested tasks with higher uncertainty first, which quickly reduces the prediction errors. We further introduce several improvements to enhance PMF for scenarios with sparse observed performance scores. Our experiments demonstrate the accuracy of PMF in predicting unknown scores, the reliability of uncertainty estimates in ordering evaluations, and the effectiveness of our enhancements for handling sparse data.",,"Qinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng, Stephen Gould",2025-05-29T05:17:47Z,Can We Predict Performance of Large Models across Vision-Language Tasks?,Können wir die Leistung großer Modelle über Vision-Language-Aufgaben hinweg voraussagen?,我们能否预测大型模型在愿景-语言任务中的绩效?,http://arxiv.org/abs/2410.10112v2
1734,"LLM providers typically offer multiple LLM tiers, varying in performance and price. As NLP tasks become more complex and modularized, selecting the suitable LLM tier for each subtask is a key challenge to balance between cost and performance. To address the problem, we introduce LLM Automatic Transmission (LLM-AT) framework that automatically selects LLM tiers without training. LLM-AT consists of Starter, Generator, and Judge. The starter selects the initial LLM tier expected to solve the given question, the generator produces a response using the LLM of the selected tier, and the judge evaluates the validity of the response. If the response is invalid, LLM-AT iteratively upgrades to a higher-tier model, generates a new response, and re-evaluates until a valid response is obtained. Additionally, we propose accuracy estimator, which enables the suitable initial LLM tier selection without training. Given an input question, accuracy estimator estimates the expected accuracy of each LLM tier by computing the valid response rate across top-k similar queries from past inference records. Experiments demonstrate that LLM-AT achieves superior performance while reducing costs, making it a practical solution for real-world applications.",,"Injae Na, Keonwoong Noh, Woohwan Jung",2025-05-29T05:05:27Z,Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in   Large Language Models,Automatische Übertragung für LLM-Tiers: Kosten- und Genauigkeitsoptimierung in großen Sprachmodellen,LLM Tiers 自动传输: 优化大语言模型的成本和准确度,http://arxiv.org/abs/2505.20921v2
1735,"Fine-tuning pre-trained language models (PLMs) has become a dominant paradigm in applying PLMs to downstream tasks. However, with limited fine-tuning, PLMs still struggle with the discrepancies between the representation obtained from the PLMs' encoder and the optimal input to the PLMs' decoder. This paper tackles this challenge by learning to calibrate the representation of PLMs in the latent space. In the proposed representation calibration method (RepCali), we integrate a specific calibration block to the latent space after the encoder and use the calibrated output as the decoder input. The merits of the proposed RepCali include its universality to all PLMs with encoder-decoder architectures, its plug-and-play nature, and ease of implementation. Extensive experiments on 25 PLM-based models across 8 tasks (including both English and Chinese datasets) demonstrate that the proposed RepCali offers desirable enhancements to PLMs (including LLMs) and significantly improves the performance of downstream tasks. Comparison experiments across 4 benchmark tasks indicate that RepCali is superior to the representative fine-tuning baselines.",,"Fujun Zhang, Xiaoying Fan, XiangDong Su, Guanglai Gao",2025-05-29T05:01:48Z,RepCali: High Efficient Fine-tuning Via Representation Calibration in   Latent Space for Pre-trained Language Models,RepCali: Hocheffizientes Feintuning über Darstellungskalibrierung im Latent Space für vortrainierte Sprachmodelle,RepCali:为预培训语言模型在冷藏空间进行高效的精微微调 Via代表比例校准,http://arxiv.org/abs/2505.08463v2
1736,"Recent advancements in large language models (LLMs) have shown impressive versatility across various tasks. To eliminate their hallucinations, retrieval-augmented generation (RAG) has emerged as a powerful approach, leveraging external knowledge sources like knowledge graphs (KGs). In this paper, we study the task of KG-driven RAG and propose a novel Similar Graph Enhanced Retrieval-Augmented Generation (SimGRAG) method. It effectively addresses the challenge of aligning query texts and KG structures through a two-stage process: (1) query-to-pattern, which uses an LLM to transform queries into a desired graph pattern, and (2) pattern-to-subgraph, which quantifies the alignment between the pattern and candidate subgraphs using a graph semantic distance (GSD) metric. We also develop an optimized retrieval algorithm that efficiently identifies the top-k subgraphs within 1-second on a 10-million-scale KG. Extensive experiments show that SimGRAG outperforms state-of-the-art KG-driven RAG methods in both question answering and fact verification. Our code is available at https://github.com/YZ-Cai/SimGRAG.",,"Yuzheng Cai, Zhenyue Guo, Yiwen Pei, Wanrui Bian, Weiguo Zheng",2025-05-29T04:58:17Z,SimGRAG: Leveraging Similar Subgraphs for Knowledge Graphs Driven   Retrieval-Augmented Generation,SimGRAG: Nutzung ähnlicher Subgraphen für Wissensgraphen Driven Retrieval-Augmented Generation,SimGRAG: 利用知识图形驱动回溯源的类似子集,http://arxiv.org/abs/2412.15272v2
1737,"The rapid development of large language models has revolutionized natural language processing, but their fine-tuning remains computationally expensive, hindering broad deployment. Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, have emerged as solutions. Recent work like DoRA attempts to further decompose weight adaptation into direction and magnitude components. However, existing formulations often define direction heuristically at the column level, lacking a principled geometric foundation. In this paper, we propose MAP, a novel framework that reformulates weight matrices as high-dimensional vectors and decouples their adaptation into direction and magnitude in a rigorous manner. MAP normalizes the pre-trained weights, learns a directional update, and introduces two scalar coefficients to independently scale the magnitude of the base and update vectors. This design enables more interpretable and flexible adaptation, and can be seamlessly integrated into existing PEFT methods. Extensive experiments show that MAP significantly improves performance when coupling with existing methods, offering a simple yet powerful enhancement to existing PEFT methods. Given the universality and simplicity of MAP, we hope it can serve as a default setting for designing future PEFT methods.",,"Chongjie Si, Zhiyi Shi, Yadao Wang, Xiaokang Yang, Susanto Rahardja, Wei Shen",2025-05-29T04:56:35Z,MAP: Revisiting Weight Decomposition for Low-Rank Adaptation,KARTE: Wiederbesuchen der Gewichtsverringerung für Low-Rank-Anpassung,MAP: 重新审视低浓度适应的重量分解,http://arxiv.org/abs/2505.23094v1
1738,"Recent advancements in large language models (LLMs) have demonstrated substantial progress in reasoning capabilities, such as DeepSeek-R1, which leverages rule-based reinforcement learning to enhance logical reasoning significantly. However, extending these achievements to multimodal large language models (MLLMs) presents critical challenges, which are frequently more pronounced for Multimodal Small Language Models (MSLMs) given their typically weaker foundational reasoning abilities: (1) the scarcity of high-quality multimodal reasoning datasets, (2) the degradation of reasoning capabilities due to the integration of visual processing, and (3) the risk that direct application of reinforcement learning may produce complex yet incorrect reasoning processes. To address these challenges, we design a novel framework Infi-MMR to systematically unlock the reasoning potential of MSLMs through a curriculum of three carefully structured phases and propose our multimodal reasoning model Infi-MMR-3B. The first phase, Foundational Reasoning Activation, leverages high-quality textual reasoning datasets to activate and strengthen the model's logical reasoning capabilities. The second phase, Cross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to facilitate the progressive transfer of reasoning skills to multimodal contexts. The third phase, Multimodal Reasoning Enhancement, employs curated, caption-free multimodal data to mitigate linguistic biases and promote robust cross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal math reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision test, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on MathVista testmini).",,"Zeyu Liu, Yuhang Liu, Guanghao Zhu, Congkai Xie, Zhen Li, Jianbo Yuan, Xinyao Wang, Qing Li, Shing-Chi Cheung, Shengyu Zhang, Fei Wu, Hongxia Yang",2025-05-29T04:51:56Z,Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased   Reinforcement Learning in Multimodal Small Language Models,Infi-MMR: Curriculumbasiertes Entsperren multimodaler Vernunft durch schrittweises Verstärktes Lernen in multimodalen Small Language-Modellen,"Infi-MMMR:通过在多模式小型语言模式中分阶段强化学习,以课程为基础解锁多模式原因",http://arxiv.org/abs/2505.23091v1
1739,"Document-level text generation tasks are known to be more difficult than sentence-level text generation tasks as they require the understanding of longer context to generate high-quality texts. In this paper, we investigate the adaption of Minimum Bayes Risk (MBR) decoding for document-level text generation tasks. MBR decoding makes use of a utility function to estimate the output with the highest expected utility from a set of candidate outputs. Although MBR decoding is shown to be effective in a wide range of sentence-level text generation tasks, its performance on document-level text generation tasks is limited as many of the utility functions are designed for evaluating the utility of sentences. To this end, we propose MBR-OT, a variant of MBR decoding using Wasserstein distance to compute the utility of a document using a sentence-level utility function. The experimental result shows that the performance of MBR-OT outperforms that of the standard MBR in document-level machine translation, text simplification, and dense image captioning tasks. Our code is available at https://github.com/jinnaiyuu/mbr-optimal-transport",,Yuu Jinnai,2025-05-29T04:34:04Z,Document-Level Text Generation with Minimum Bayes Risk Decoding using   Optimal Transport,Document-Level Text Generierung mit minimalen Bayes Risikodekodierung mit optimalem Transport,"采用最佳运输方式,以文件水平生成具有最低比值风险解码的文本",http://arxiv.org/abs/2505.23078v1
1740,"Deep biasing improves automatic speech recognition (ASR) performance by incorporating contextual phrases. However, most existing methods enhance subwords in a contextual phrase as independent units, potentially compromising contextual phrase integrity, leading to accuracy reduction. In this paper, we propose an encoder-based phrase-level contextualized ASR method that leverages dynamic vocabulary prediction and activation. We introduce architectural optimizations and integrate a bias loss to extend phrase-level predictions based on frame-level outputs. We also introduce a confidence-activated decoding method that ensures the complete output of contextual phrases while suppressing incorrect bias. Experiments on Librispeech and Wenetspeech datasets demonstrate that our approach achieves relative WER reductions of 28.31% and 23.49% compared to baseline, with the WER on contextual phrases decreasing relatively by 72.04% and 75.69%.",,"Zhennan Lin, Kaixun Huang, Wei Ren, Linju Yang, Lei Xie",2025-05-29T04:31:33Z,Contextualized Automatic Speech Recognition with Dynamic Vocabulary   Prediction and Activation,Kontextualisierte automatische Spracherkennung mit dynamischer Vokabelvorhersage und Aktivierung,具有动态词汇预测和启动功能的实用自动语音识别,http://arxiv.org/abs/2505.23077v1
1741,"Expert parallelism has emerged as a key strategy for distributing the computational workload of sparsely-gated mixture-of-experts (MoE) models across multiple devices, enabling the processing of increasingly large-scale models. However, the All-to-All communication inherent to expert parallelism poses a significant bottleneck, limiting the efficiency of MoE models. Although existing optimization methods partially mitigate this issue, they remain constrained by the sequential dependency between communication and computation operations. To address this challenge, we propose ScMoE, a novel shortcut-connected MoE architecture integrated with an overlapping parallelization strategy. ScMoE decouples communication from its conventional sequential ordering, enabling up to 100% overlap with computation. Compared to the prevalent top-2 MoE baseline, ScMoE achieves speedups of 1.49 times in training and 1.82 times in inference. Moreover, our experiments and analyses indicate that ScMoE not only achieves comparable but in some instances surpasses the model quality of existing approaches.",,"Weilin Cai, Juyong Jiang, Le Qin, Junwei Cui, Sunghun Kim, Jiayi Huang",2025-05-29T04:25:16Z,Shortcut-connected Expert Parallelism for Accelerating   Mixture-of-Experts,Shortcut-verbundene Experten-Parallelität für die Beschleunigung von Mixture-of-Experts,加速混合专家专家专家平行专家,http://arxiv.org/abs/2404.05019v3
1742,"In this paper, we propose Singular Values and Orthonormal Regularized Singular Vectors Adaptation, or SORSA, a novel parameter efficient fine-tuning (PEFT) method. Each SORSA adapter consists of two main parts: trainable principal singular weights $W_p = U_p \text{diag}(S_p) V^\top_p$, and frozen residual weights $W_r = U_r \text{diag}(S_r) V^\top_r$. These parts are initialized by performing singular value decomposition (SVD) on pre-trained weights. Moreover, we implement and analyze an orthonormal regularizer, which we prove could decrease the condition number of $W_p$ and make the optimization more efficient. SORSA adapters could be merged during inference, thus eliminating any inference latency. We also introduce a method to analyze the variation of the parameters by performing SVD and discuss and analyze SORSA's superiority in minimizing the alteration in the SVD aspect. After all, SORSA shows a faster convergence than LoRA and PiSSA in our experiments. On the GSM-8K benchmark, Llama 2 7B adapted using SORSA achieved 56.03\% accuracy, surpassing LoRA (42.30\%) and Full FT (49.05\%). We conclude that SORSA offers a new perspective on parameter-efficient fine-tuning, demonstrating remarkable performance.",,"Yang Cao, Zhao Song",2025-05-29T04:19:56Z,SORSA: Singular Values and Orthonormal Regularized Singular Vectors   Adaptation of Large Language Models,SORSA: Singuläre Werte und Orthonormale Regularisierte Singuläre Vektoren Anpassung großer Sprachmodelle,"SORSA: 单项价值和正正正的正规化的单项矢量,以适应大语言模式",http://arxiv.org/abs/2409.00055v6
1743,"With the increasing integration of visual and textual content in Social Networking Services (SNS), evaluating the multimodal capabilities of Large Language Models (LLMs) is crucial for enhancing user experience, content understanding, and platform intelligence. Existing benchmarks primarily focus on text-centric tasks, lacking coverage of the multimodal contexts prevalent in modern SNS ecosystems. In this paper, we introduce SNS-Bench-VL, a comprehensive multimodal benchmark designed to assess the performance of Vision-Language LLMs in real-world social media scenarios. SNS-Bench-VL incorporates images and text across 8 multimodal tasks, including note comprehension, user engagement analysis, information retrieval, and personalized recommendation. It comprises 4,001 carefully curated multimodal question-answer pairs, covering single-choice, multiple-choice, and open-ended tasks. We evaluate over 25 state-of-the-art multimodal LLMs, analyzing their performance across tasks. Our findings highlight persistent challenges in multimodal social context comprehension. We hope SNS-Bench-VL will inspire future research towards robust, context-aware, and human-aligned multimodal intelligence for next-generation social networking services.",,"Hongcheng Guo, Zheyong Xie, Shaosheng Cao, Boyang Wang, Weiting Liu, Anjie Le, Lei Li, Zhoujun Li",2025-05-29T04:16:24Z,SNS-Bench-VL: Benchmarking Multimodal Large Language Models in Social   Networking Services,SNS-Bench-VL: Benchmarking multimodaler Großsprachenmodelle in Social Networking Services,SNS-Bench-VL:确定社会联网服务中多式大语言模式基准,http://arxiv.org/abs/2505.23065v1
1744,"Existing approaches based on context prompting or reinforcement learning (RL) to improve the reasoning capacities of large language models (LLMs) depend on the LLMs' internal knowledge to produce reliable Chain-Of-Thought (CoT). However, no matter the size of LLMs, certain problems cannot be resolved in a single forward pass. Meanwhile, agent-based reasoning systems require access to a comprehensive nonparametric knowledge base, which is often costly or not feasible for use in scientific and niche domains. We present Graph Inspired Veracity Extrapolation (GIVE), a novel reasoning method that merges parametric and non-parametric memories to improve accurate reasoning with minimal external input. GIVE guides the LLM agent to select the most pertinent expert data (observe), engage in query-specific divergent thinking (reflect), and then synthesize this information to produce the final output (speak). Extensive experiments demonstrated the following benefits of our framework: (1) GIVE boosts the performance of LLMs across various sizes. (2) In some scenarios, GIVE allows smaller LLMs to surpass larger, more sophisticated ones in scientific tasks (GPT3.5T + GIVE > GPT4). (3) GIVE is effective on scientific and open-domain assessments. (4) GIVE is a training-free method that enables LLMs to tackle new problems that extend beyond their training data (up to 43.5% -> 88.2%} accuracy improvement). (5) GIVE allows LLM agents to reason using both restricted (very small) and noisy (very large) knowledge sources, accommodating knowledge graphs (KG) ranging from 135 to more than 840k nodes. (6) The reasoning process involved in GIVE is fully interpretable.",,"Jiashu He, Mingyu Derek Ma, Jinxuan Fan, Dan Roth, Wei Wang, Alejandro Ribeiro",2025-05-29T04:09:28Z,GIVE: Structured Reasoning of Large Language Models with Knowledge Graph   Inspired Veracity Extrapolation,GIVE: Strukturierte Begründung großer Sprachmodelle mit Wissensgrafik inspirierte Veracity-Extrapolation,"特具:大语言模式结构原因说明,以知识图激发的多才多艺外推法",http://arxiv.org/abs/2410.08475v3
1745,"Speculative decoding and quantization effectively accelerate memory-bound inference of large language models. Speculative decoding mitigates the memory bandwidth bottleneck by verifying multiple tokens within a single forward pass, which increases computational effort. Quantization achieves this optimization by compressing weights and activations into lower bit-widths and also reduces computations via low-bit matrix multiplications. To further leverage their strengths, we investigate the integration of these two techniques. Surprisingly, experiments applying the advanced speculative decoding method EAGLE-2 to various quantized models reveal that the memory benefits from 4-bit weight quantization are diminished by the computational load from speculative decoding. Specifically, verifying a tree-style draft incurs significantly more time overhead than a single-token forward pass on 4-bit weight quantized models. This finding led to our new speculative decoding design: a hierarchical framework that employs a small model as an intermediate stage to turn tree-style drafts into sequence drafts, leveraging the memory access benefits of the target quantized model. Experimental results show that our hierarchical approach achieves a 2.78$\times$ speedup across various tasks for the 4-bit weight Llama-3-70B model on an A100 GPU, outperforming EAGLE-2 by 1.31$\times$. Code available at https://github.com/AI9Stars/SpecMQuant.",,"Yudi Zhang, Weilin Zhao, Xu Han, Tiejun Zhao, Wang Xu, Hailong Cao, Conghui Zhu",2025-05-29T04:07:33Z,Speculative Decoding Meets Quantization: Compatibility Evaluation and   Hierarchical Framework Design,Spekulative Dekodierung trifft auf Quantisierung: Kompatibilitätsbewertung und Hierarchisches Framework Design,投机性下限符合量化:兼容性评价和等级框架设计,http://arxiv.org/abs/2505.22179v2
1746,"Self-correction has demonstrated potential in code generation by allowing language models to revise and improve their outputs through successive refinement. Recent studies have explored prompting-based strategies that incorporate verification or feedback loops using proprietary models, as well as training-based methods that leverage their strong reasoning capabilities. However, whether smaller models possess the capacity to effectively guide their outputs through self-reflection remains unexplored. Our findings reveal that smaller models struggle to exhibit reflective revision behavior across both self-correction paradigms. In response, we introduce CoCoS, an approach designed to enhance the ability of small language models for multi-turn code correction. Specifically, we propose an online reinforcement learning objective that trains the model to confidently maintain correct outputs while progressively correcting incorrect outputs as turns proceed. Our approach features an accumulated reward function that aggregates rewards across the entire trajectory and a fine-grained reward better suited to multi-turn correction scenarios. This facilitates the model in enhancing initial response quality while achieving substantial improvements through self-correction. With 1B-scale models, CoCoS achieves improvements of 35.8% on the MBPP and 27.7% on HumanEval compared to the baselines.",,"Jeonghun Cho, Deokhyung Kang, Hyounghun Kim, Gary Geunbae Lee",2025-05-29T04:04:44Z,Self-Correcting Code Generation Using Small Language Models,Selbstkorrekte Code-Generierung mit kleinen Sprachmodellen,使用小型语言模式自行校正代码生成,http://arxiv.org/abs/2505.23060v1
1747,"Despite their success in numerous fields, the potential of foundation models for modeling and understanding human behavior remains largely unexplored. We introduce Be.FM, one of the first open foundation models designed for human behavior modeling. Built upon open-source large language models and fine-tuned on a diverse range of behavioral data, Be.FM can be used to understand and predict human decision-making. We construct a comprehensive set of benchmark tasks for testing the capabilities of behavioral foundation models. Our results demonstrate that Be.FM can predict behaviors, infer characteristics of individuals and populations, generate insights about contexts, and apply behavioral science knowledge.",,"Yutong Xie, Zhuoheng Li, Xiyuan Wang, Yijun Pan, Qijia Liu, Xingzhi Cui, Kuang-Yu Lo, Ruoyi Gao, Xingjian Zhang, Jin Huang, Walter Yuan, Matthew O. Jackson, Qiaozhu Mei",2025-05-29T04:03:51Z,Be.FM: Open Foundation Models for Human Behavior,Be.FM: Open Foundation Modelle für menschliches Verhalten,BeFM: 人类行为开放基础模型,http://arxiv.org/abs/2505.23058v1
1748,"Given the central role of charts in scientific, business, and communication contexts, enhancing the chart understanding capabilities of vision-language models (VLMs) has become increasingly critical. A key limitation of existing VLMs lies in their inaccurate visual grounding of infographic elements, including charts and human-recognizable objects (HROs) such as icons and images. However, chart understanding often requires identifying relevant elements and reasoning over them. To address this limitation, we introduce OrionBench, a benchmark designed to support the development of accurate object detection models for charts and HROs in infographics. It contains 26,250 real and 78,750 synthetic infographics, with over 6.9 million bounding box annotations. These annotations are created by combining the model-in-the-loop and programmatic methods. We demonstrate the usefulness of OrionBench through three applications: 1) constructing a Thinking-with-Boxes scheme to boost the chart understanding performance of VLMs, 2) comparing existing object detection models, and 3) applying the developed detection model to document layout and UI element detection.",,"Jiangning Zhu, Yuxing Zhou, Zheng Wang, Juntao Yao, Yima Gu, Yuhui Yuan, Shixia Liu",2025-05-29T03:56:55Z,OrionBench: A Benchmark for Chart and Human-Recognizable Object   Detection in Infographics,OrionBench: Ein Benchmark für Diagramm- und Mensch-erkennbare Objekterkennung in Infografiken,Orion Bunch:图表和人类可识别的在信息图中探测物体的基准,http://arxiv.org/abs/2505.17473v3
1749,"Retrieval-Augmented Generation (RAG) significantly improves the performance of Large Language Models (LLMs) on knowledge-intensive tasks. However, varying response quality across LLMs under RAG necessitates intelligent routing mechanisms, which select the most suitable model for each query from multiple retrieval-augmented LLMs via a dedicated router model. We observe that external documents dynamically affect LLMs' ability to answer queries, while existing routing methods, which rely on static parametric knowledge representations, exhibit suboptimal performance in RAG scenarios. To address this, we formally define the new retrieval-augmented LLM routing problem, incorporating the influence of retrieved documents into the routing framework. We propose RAGRouter, a RAG-aware routing design, which leverages document embeddings and RAG capability embeddings with contrastive learning to capture knowledge representation shifts and enable informed routing decisions. Extensive experiments on diverse knowledge-intensive tasks and retrieval settings show that RAGRouter outperforms the best individual LLM by 3.61% on average and existing routing methods by 3.29%-9.33%. With an extended score-threshold-based mechanism, it also achieves strong performance-efficiency trade-offs under low-latency constraints.",,"Jiarui Zhang, Xiangyu Liu, Yong Hu, Chaoyue Niu, Fan Wu, Guihai Chen",2025-05-29T03:44:56Z,Query Routing for Retrieval-Augmented Language Models,Abfrage-Routing für Retrieval-Augmented Language-Modelle,查询检索推荐语言模型的查询路径,http://arxiv.org/abs/2505.23052v1
1750,"Pruning is a widely used technique to compress large language models (LLMs) by removing unimportant weights, but it often suffers from significant performance degradation - especially under semi-structured sparsity constraints. Existing pruning methods primarily focus on estimating the importance of individual weights, which limits their ability to preserve critical capabilities of the model. In this work, we propose a new perspective: rather than merely selecting which weights to prune, we first redistribute parameter importance to make the model inherently more amenable to pruning. By minimizing the information entropy of normalized importance scores, our approach concentrates importance onto a smaller subset of weights, thereby enhancing pruning robustness. We instantiate this idea through DenoiseRotator, which applies learnable orthogonal transformations to the model's weight matrices. Our method is model-agnostic and can be seamlessly integrated with existing pruning techniques such as Magnitude, SparseGPT, and Wanda. Evaluated on LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4 semi-structured sparsity, DenoiseRotator consistently improves perplexity and zero-shot accuracy. For instance, on LLaMA3-70B pruned with SparseGPT at 2:4 semi-structured sparsity, DenoiseRotator reduces the perplexity gap to the dense model by 58%, narrowing the degradation from 8.1 to 3.4 points. Codes are available at https://github.com/Axel-gu/DenoiseRotator.",,"Tianteng Gu, Bei Liu, Bo Xiao, Ke Zeng, Jiacheng Liu, Yanmin Qian",2025-05-29T03:44:09Z,DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance   Concentration,DenoiseRotator: Verbesserung der Beschneidungsfestigkeit für LLMs durch Bedeutungskonzentration,DenoisRotator:通过重视浓度提高LLMs的稳健力,http://arxiv.org/abs/2505.23049v1
1751,"In this work, we study the effect of annotation guidelines -- textual descriptions of event types and arguments, when instruction-tuning large language models for event extraction. We conducted a series of experiments with both human-provided and machine-generated guidelines in both full- and low-data settings. Our results demonstrate the promise of annotation guidelines when there is a decent amount of training data and highlight its effectiveness in improving cross-schema generalization and low-frequency event-type performance.",,"Saurabh Srivastava, Sweta Pati, Ziyu Yao",2025-05-29T03:34:03Z,Instruction-Tuning LLMs for Event Extraction with Annotation Guidelines,Instruction-Tuning LLMs für die Ereignisextraktion mit Annotationsrichtlinien,说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性说明性准则,http://arxiv.org/abs/2502.16377v2
1752,"Full-Duplex Speech Dialogue Systems (Full-Duplex SDS) have significantly enhanced the naturalness of human-machine interaction by enabling real-time bidirectional communication. However, existing approaches face challenges such as difficulties in independent module optimization and contextual noise interference due to highly coupled architectural designs and oversimplified binary state modeling. This paper proposes FlexDuo, a flexible full-duplex control module that decouples duplex control from spoken dialogue systems through a plug-and-play architectural design. Furthermore, inspired by human information-filtering mechanisms in conversations, we introduce an explicit Idle state. On one hand, the Idle state filters redundant noise and irrelevant audio to enhance dialogue quality. On the other hand, it establishes a semantic integrity-based buffering mechanism, reducing the risk of mutual interruptions while ensuring accurate response transitions. Experimental results on the Fisher corpus demonstrate that FlexDuo reduces the false interruption rate by 24.9% and improves response accuracy by 7.6% compared to integrated full-duplex dialogue system baselines. It also outperforms voice activity detection (VAD) controlled baseline systems in both Chinese and English dialogue quality. The proposed modular architecture and state-based dialogue model provide a novel technical pathway for building flexible and efficient duplex dialogue systems.",,"Borui Liao, Yulong Xu, Jiao Ou, Kaiyuan Yang, Weihua Jian, Pengfei Wan, Di Zhang",2025-05-29T03:32:21Z,FlexDuo: A Pluggable System for Enabling Full-Duplex Capabilities in   Speech Dialogue Systems,FlexDuo: Ein Pluggable-System zur Ermöglichung von Full-Duplex-Fähigkeiten in Sprachdialogsystemen,FlexDuo:一个促进语音对话系统全面灵活能力的插件系统,http://arxiv.org/abs/2502.13472v2
1753,"Processing structured tabular data, particularly large and lengthy tables, constitutes a fundamental yet challenging task for large language models (LLMs). However, existing long-context benchmarks like Needle-in-a-Haystack primarily focus on unstructured text, neglecting the challenge of diverse structured tables. Meanwhile, previous tabular benchmarks mainly consider downstream tasks that require high-level reasoning abilities, and overlook models' underlying fine-grained perception of individual table cells, which is crucial for practical and robust LLM-based table applications. To address this gap, we introduce \textsc{NeedleInATable} (NIAT), a new long-context tabular benchmark that treats each table cell as a ``needle'' and requires models to extract the target cell based on cell locations or lookup questions. Our comprehensive evaluation of various LLMs and multimodal LLMs reveals a substantial performance gap between popular downstream tabular tasks and the simpler NIAT task, suggesting that they may rely on dataset-specific correlations or shortcuts to obtain better benchmark results but lack truly robust long-context understanding towards structured tables. Furthermore, we demonstrate that using synthesized NIAT training data can effectively improve performance on both NIAT task and downstream tabular tasks, which validates the importance of NIAT capability for LLMs' genuine table understanding ability. Our data, code and models will be released to facilitate future research.",,"Lanrui Wang, Mingyu Zheng, Hongyin Tang, Zheng Lin, Yanan Cao, Jingang Wang, Xunliang Cai, Weiping Wang",2025-05-29T03:31:02Z,NeedleInATable: Exploring Long-Context Capability of Large Language   Models towards Long-Structured Tables,NeedleInATable: Erforschen von Langkontext-Kapazität von großen Sprachmodellen zu langstrukturierten Tabellen,针线表:探索长结构表格中大语言模型的长文能力,http://arxiv.org/abs/2504.06560v2
1754,"Text-to-image generation increasingly demands access to domain-specific, fine-grained, and rapidly evolving knowledge that pretrained models cannot fully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to address this by retrieving globally relevant images, but they fail when no single image contains all desired elements from a complex user query. We propose Cross-modal RAG, a novel framework that decomposes both queries and images into sub-dimensional components, enabling subquery-aware retrieval and generation. Our method introduces a hybrid retrieval strategy - combining a sub-dimensional sparse retriever with a dense retriever - to identify a Pareto-optimal set of images, each contributing complementary aspects of the query. During generation, a multimodal large language model is guided to selectively condition on relevant visual features aligned to specific subqueries, ensuring subquery-aware image synthesis. Extensive experiments on MS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal RAG significantly outperforms existing baselines in both retrieval and generation quality, while maintaining high efficiency.",,"Mengdan Zhu, Senhao Cheng, Guangji Bai, Yifei Zhang, Liang Zhao",2025-05-29T03:27:40Z,Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image   Generation,Cross-modal RAG: Sub-dimensionale Retrieval-Augmented Text-to-Image Generation,跨模式RAG:次二维检索增强的文本到图像生成,http://arxiv.org/abs/2505.21956v2
1755,"NL2SQL (natural language to SQL) translates natural language questions into SQL queries, thereby making structured data accessible to non-technical users, serving as the foundation for intelligent data applications. State-of-the-art NL2SQL techniques typically perform translation by retrieving database-specific information, such as the database schema, and invoking a pre-trained large language model (LLM) using the question and retrieved information to generate the SQL query.   However, existing NL2SQL techniques miss a key opportunity which is present in real-world settings: NL2SQL is typically applied on existing databases which have already served many SQL queries in the past. The past query workload implicitly contains information which is helpful for accurate NL2SQL translation and is not apparent from the database schema alone, such as common join paths and the semantics of obscurely-named tables and columns. We introduce TailorSQL, a NL2SQL system that takes advantage of information in the past query workload to improve both the accuracy and latency of translating natural language questions into SQL. By specializing to a given workload, TailorSQL achieves up to 2$\times$ improvement in execution accuracy on standardized benchmarks.",,"Kapil Vaidya, Jialin Ding, Sebastian Kosak, David Kernert, Chuan Lei, Xiao Qin, Abhinav Tripathy, Ramesh Balan, Balakrishnan Narayanaswamy, Tim Kraska",2025-05-29T03:27:22Z,TailorSQL: An NL2SQL System Tailored to Your Query Workload,"TailorSQL: Ein NL2SQL-System, das auf Ihre Abfrage-Workloads zugeschnitten ist",定制SQL: 适合您查询工作量的 NL2SQL 系统,http://arxiv.org/abs/2505.23039v1
1756,"In-Context Learning (ICL) technique based on Large Language Models (LLMs) has gained prominence in Named Entity Recognition (NER) tasks for its lower computing resource consumption, less manual labeling overhead, and stronger generalizability. Nevertheless, most ICL-based NER methods depend on large-parameter LLMs: the open-source models demand substantial computational resources for deployment and inference, while the closed-source ones incur high API costs, raise data-privacy concerns, and hinder community collaboration. To address this question, we propose an Ensemble Learning Method for Named Entity Recognition (EL4NER), which aims at aggregating the ICL outputs of multiple open-source, small-parameter LLMs to enhance overall performance in NER tasks at less deployment and inference cost. Specifically, our method comprises three key components. First, we design a task decomposition-based pipeline that facilitates deep, multi-stage ensemble learning. Second, we introduce a novel span-level sentence similarity algorithm to establish an ICL demonstration retrieval mechanism better suited for NER tasks. Third, we incorporate a self-validation mechanism to mitigate the noise introduced during the ensemble process. We evaluated EL4NER on multiple widely adopted NER datasets from diverse domains. Our experimental results indicate that EL4NER surpasses most closed-source, large-parameter LLM-based methods at a lower parameter cost and even attains state-of-the-art (SOTA) performance among ICL-based methods on certain datasets. These results show the parameter efficiency of EL4NER and underscore the feasibility of employing open-source, small-parameter LLMs within the ICL paradigm for NER tasks.",,"Yuzhen Xiao, Jiahe Song, Yongxin Xu, Ruizhe Zhang, Yiqi Xiao, Xin Lu, Runchuan Zhu, Bowen Jiang, Junfeng Zhao",2025-05-29T03:25:14Z,EL4NER: Ensemble Learning for Named Entity Recognition via Multiple   Small-Parameter Large Language Models,EL4NER: Ensemble Lernen für die benannte Entity-Erkennung über mehrere kleine Parameter große Sprachmodelle,EL4NER:通过多小口径大语言模型进行命名实体识别的结合学习,http://arxiv.org/abs/2505.23038v1
1757,"The inherent nature of social media posts, characterized by the freedom of language use with a disjointed array of diverse opinions and topics, poses significant challenges to downstream NLP tasks such as comment clustering, comment summarization, and social media opinion analysis. To address this, we propose a granular level of identifying and generating aspect terms from individual comments to guide model attention. Specifically, we leverage multilingual large language models with supervised fine-tuning for comment aspect term generation (CAT-G), further aligning the model's predictions with human expectations through DPO. We demonstrate the effectiveness of our method in enhancing the comprehension of social media discourse on two NLP tasks. Moreover, this paper contributes the first multilingual CAT-G test set on English, Chinese, Malay, and Bahasa Indonesian. As LLM capabilities vary among languages, this test set allows for a comparative analysis of performance across languages with varying levels of LLM proficiency.",,"Longyin Zhang, Bowei Zou, Ai Ti Aw",2025-05-29T03:24:39Z,Improving Multilingual Social Media Insights: Aspect-based Comment   Analysis,Mehrsprachige Social Media-Insights verbessern: Aspect-based Comment Analysis,改进多语种社会媒体透视:基于背景的评论分析,http://arxiv.org/abs/2505.23037v1
1758,"Parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), enable efficient adaptation of large language models (LLMs) via low-rank matrix optimization with frozen weights. However, LoRA typically exhibits ""double descent"" in training loss as rank increases, characterized by a three-phase dynamics: initial convergence, transient divergence, and eventual stabilization. This non-monotonic behavior delays convergence and impairs generalization through unstable gradients and attraction to sharp minima. To address these challenges, we propose LoRA-MGPO, a novel LoRA-based framework incorporating Momentum-Guided Perturbation Optimization (MGPO). First, MGPO eliminates Sharpness-Aware Minimization (SAM)'s dual gradient computations by reusing momentum vectors from optimizer states to guide perturbation directions. This retains SAM's training stability and flat minima preference with maintained efficiency. Second, MGPO incorporates adaptive perturbation normalization, scaling perturbation intensity via exponential moving average (EMA)-smoothed gradient magnitudes. Experiments on natural language understanding and generation benchmarks demonstrate that LoRA-MGPO outperforms LoRA and state-of-the-art PEFT methods. Further analysis confirms its ability to stabilize training and reduce sharp minima attraction, with smoother loss curves and improved convergence behavior. The code is available at https://github.com/llm172/LoRA-MGPO",,"Yupeng Chang, Chenlu Guo, Yi Chang, Yuan Wu",2025-05-29T03:24:32Z,LoRA-MGPO: Mitigating Double Descent in Low-Rank Adaptation via   Momentum-Guided Perturbation Optimization,LoRA-MGPO: Doppelabstieg in der Low-Rank-Anpassung durch Momentum-geführte Perturbierungs-Optimierung abmildern,"LoRA-MGPO:通过动力调节-受控渗透优化,减少低辐射适应中的双重来源",http://arxiv.org/abs/2502.14538v2
1759,"Machine-Facing English (MFE) is an emergent register shaped by the adaptation of everyday language to the expanding presence of AI interlocutors. Drawing on register theory (Halliday 1985, 2006), enregisterment (Agha 2003), audience design (Bell 1984), and interactional pragmatics (Giles & Ogay 2007), this study traces how sustained human-AI interaction normalizes syntactic rigidity, pragmatic simplification, and hyper-explicit phrasing - features that enhance machine parseability at the expense of natural fluency. Our analysis is grounded in qualitative observations from bilingual (Korean/English) voice- and text-based product testing sessions, with reflexive drafting conducted using Natural Language Declarative Prompting (NLD-P) under human curation. Thematic analysis identifies five recurrent traits - redundant clarity, directive syntax, controlled vocabulary, flattened prosody, and single-intent structuring - that improve execution accuracy but compress expressive range. MFE's evolution highlights a persistent tension between communicative efficiency and linguistic richness, raising design challenges for conversational interfaces and pedagogical considerations for multilingual users. We conclude by underscoring the need for comprehensive methodological exposition and future empirical validation.",,"Hyunwoo Kim, Hanau Yi",2025-05-29T03:22:39Z,Machine-Facing English: Defining a Hybrid Register Shaped by Human-AI   Discourse,"Machine-Facing English: Definition eines hybriden Registers, geformt von Human-AI Diskurs",面向机器的英语: 定义由人类-AI 论文构成的混合登记册,http://arxiv.org/abs/2505.23035v1
1760,"Transformers have become the backbone of modern Large Language Models (LLMs); however, their inference overhead grows linearly with the sequence length, posing challenges for modeling long sequences. In light of this, Mamba has attracted attention for maintaining a constant inference size, with empirical evidence demonstrating that it can match Transformer performance in sequence modeling while significantly reducing computational costs. However, an open question remains: can Mamba always bring savings while achieving performance comparable to Transformers? In this paper, we focus on analyzing the expressive ability of Mamba to perform our defined COPY operation and Chain of Thought (CoT) reasoning. First, inspired by the connection between Mamba and linear attention, we show that constant-sized Mamba may struggle to perform COPY operations while Transformers can handle them more easily. However, when the size of Mamba grows linearly with the input sequence length, it can accurately perform COPY, but in this case, Mamba no longer provides overhead savings. Based on this observation, we further analyze Mamba's ability to tackle CoT tasks, which can be described by the Dynamic Programming (DP) problems. Our findings suggest that to solve arbitrary DP problems, the total cost of Mamba is still comparable to standard Transformers. However, similar to efficient Transformers, when facing DP problems with favorable properties such as locality, Mamba can provide savings in overhead. Our experiments on the copy and CoT tasks further demonstrate Mamba's limitations compared to Transformers in learning these tasks.",,"Ruifeng Ren, Zhicong Li, Yong Liu",2025-05-29T03:19:51Z,Exploring the Limitations of Mamba in COPY and CoT Reasoning,Erforschung der Grenzen von Mamba in COPY und CoT Reasoning,探索COPY和COT理由解释中Mamba的局限性,http://arxiv.org/abs/2410.03810v3
1761,"Data contamination hinders fair LLM evaluation by introducing test data into newer models' training sets. Existing studies solve this challenge by updating benchmarks with newly collected data. However, they fail to guarantee contamination-free evaluation as the newly collected data may contain pre-existing knowledge, and their benchmark updates rely on intensive human labor. To address these issues, we in this paper propose AntiLeak-Bench, an automated anti-leakage benchmarking framework. Instead of simply using newly collected data, we construct samples with explicitly new knowledge absent from LLMs' training sets, which thus ensures strictly contamination-free evaluation. We further design a fully automated workflow to build and update our benchmark without human labor. This significantly reduces the cost of benchmark maintenance to accommodate emerging LLMs. Through extensive experiments, we highlight that data contamination likely exists before LLMs' cutoff time and demonstrate AntiLeak-Bench effectively overcomes this challenge.",,"Xiaobao Wu, Liangming Pan, Yuxi Xie, Ruiwen Zhou, Shuai Zhao, Yubo Ma, Mingzhe Du, Rui Mao, Anh Tuan Luu, William Yang Wang",2025-05-29T03:19:42Z,AntiLeakBench: Preventing Data Contamination by Automatically   Constructing Benchmarks with Updated Real-World Knowledge,AntiLeakBench: Datenkontamination durch automatisches Konstruieren von Benchmarks mit aktualisiertem Real-World-Wissen verhindern,"防止泄漏:利用最新现实世界知识自动建立基准,防止数据污染",http://arxiv.org/abs/2412.13670v2
1762,"Evidence-enhanced detectors present remarkable abilities in identifying malicious social text. However, the rise of large language models (LLMs) brings potential risks of evidence pollution to confuse detectors. This paper explores potential manipulation scenarios including basic pollution, and rephrasing or generating evidence by LLMs. To mitigate the negative impact, we propose three defense strategies from the data and model sides, including machine-generated text detection, a mixture of experts, and parameter updating. Extensive experiments on four malicious social text detection tasks with ten datasets illustrate that evidence pollution significantly compromises detectors, where the generating strategy causes up to a 14.4% performance drop. Meanwhile, the defense strategies could mitigate evidence pollution, but they faced limitations for practical employment. Further analysis illustrates that polluted evidence (i) is of high quality, evaluated by metrics and humans; (ii) would compromise the model calibration, increasing expected calibration error up to 21.6%; and (iii) could be integrated to amplify the negative impact, especially for encoder-based LMs, where the accuracy drops by 21.8%.",,"Herun Wan, Minnan Luo, Zhixiong Su, Guang Dai, Xiang Zhao",2025-05-29T03:17:33Z,On the Risk of Evidence Pollution for Malicious Social Text Detection in   the Era of LLMs,Über das Risiko der Beweisverschmutzung für bösartige Social Text Detection in der Ära der LLMs,关于在LLMM公司时代对恶性社会文本进行侦破的证据污染风险,http://arxiv.org/abs/2410.12600v2
1763,"General-purpose clinical natural language processing (NLP) tools are increasingly used for the automatic labeling of clinical reports. However, independent evaluations for specific tasks, such as pediatric chest radiograph (CXR) report labeling, are limited. This study compares four commercial clinical NLP systems - Amazon Comprehend Medical (AWS), Google Healthcare NLP (GC), Azure Clinical NLP (AZ), and SparkNLP (SP) - for entity extraction and assertion detection in pediatric CXR reports. Additionally, CheXpert and CheXbert, two dedicated chest radiograph report labelers, were evaluated on the same task using CheXpert-defined labels. We analyzed 95,008 pediatric CXR reports from a large academic pediatric hospital. Entities and assertion statuses (positive, negative, uncertain) from the findings and impression sections were extracted by the NLP systems, with impression section entities mapped to 12 disease categories and a No Findings category. CheXpert and CheXbert extracted the same 13 categories. Outputs were compared using Fleiss Kappa and accuracy against a consensus pseudo-ground truth. Significant differences were found in the number of extracted entities and assertion distributions across NLP systems. SP extracted 49,688 unique entities, GC 16,477, AZ 31,543, and AWS 27,216. Assertion accuracy across models averaged around 62%, with SP highest (76%) and AWS lowest (50%). CheXpert and CheXbert achieved 56% accuracy. Considerable variability in performance highlights the need for careful validation and review before deploying NLP tools for clinical report labeling.",,"Shruti Hegde, Mabon Manoj Ninan, Jonathan R. Dillman, Shireen Hayatghaibi, Lynn Babcock, Elanchezhian Somasundaram",2025-05-29T03:16:18Z,"Can Modern NLP Systems Reliably Annotate Chest Radiography Exams? A   Pre-Purchase Evaluation and Comparative Study of Solutions from AWS, Google,   Azure, John Snow Labs, and Open-Source Models on an Independent Pediatric   Dataset","Können moderne NLP-Systeme zuverlässig Röntgenuntersuchungen im Brustkorb annotieren? Eine Pre-Purchase-Bewertung und vergleichende Untersuchung von Lösungen von AWS, Google, Azure, John Snow Labs und Open-Source-Modellen auf einem unabhängigen Kinderdatensatz",现代NLP系统能否可靠地说明胸前射电测量? 对AWS、Google、Azure、John Snow实验室和独立儿科数据集开放来源模型的解决方案进行采购前评估和比较研究,http://arxiv.org/abs/2505.23030v1
1764,"The acquisition of agentic capabilities has transformed LLMs from ""knowledge providers"" to ""action executors"", a trend that while expanding LLMs' capability boundaries, significantly increases their susceptibility to malicious use. Previous work has shown that current LLM-based agents execute numerous malicious tasks even without being attacked, indicating a deficiency in agentic use safety alignment during the post-training phase. To address this gap, we propose AgentAlign, a novel framework that leverages abstract behavior chains as a medium for safety alignment data synthesis. By instantiating these behavior chains in simulated environments with diverse tool instances, our framework enables the generation of highly authentic and executable instructions while capturing complex multi-step dynamics. The framework further ensures model utility by proportionally synthesizing benign instructions through non-malicious interpretations of behavior chains, precisely calibrating the boundary between helpfulness and harmlessness. Evaluation results on AgentHarm demonstrate that fine-tuning three families of open-source models using our method substantially improves their safety (35.8% to 79.5% improvement) while minimally impacting or even positively enhancing their helpfulness, outperforming various prompting methods. The dataset and code have both been open-sourced.",,"Jinchuan Zhang, Lu Yin, Yan Zhou, Songlin Hu",2025-05-29T03:02:18Z,AgentAlign: Navigating Safety Alignment in the Shift from Informative to   Agentic Large Language Models,AgentAlign: Navigieren der Sicherheitsausrichtung im Wechsel von Informativ zu Agentischen Großsprachenmodellen,代理对齐: 导航从信息型转向大语言型的移动中的安全对齐,http://arxiv.org/abs/2505.23020v1
1765,"In recent years, the rapid advancement of Artificial Intelligence (AI) technologies, particularly Large Language Models (LLMs), has revolutionized the paradigm of scientific discovery, establishing AI-for-Science (AI4Science) as a dynamic and evolving field. However, there is still a lack of an effective framework for the overall assessment of AI4Science, particularly from a holistic perspective on data quality and model capability. Therefore, in this study, we propose SciHorizon, a comprehensive assessment framework designed to benchmark the readiness of AI4Science from both scientific data and LLM perspectives. First, we introduce a generalizable framework for assessing AI-ready scientific data, encompassing four key dimensions: Quality, FAIRness, Explainability, and Compliance-which are subdivided into 15 sub-dimensions. Drawing on data resource papers published between 2018 and 2023 in peer-reviewed journals, we present recommendation lists of AI-ready datasets for Earth, Life, and Materials Sciences, making a novel and original contribution to the field. Concurrently, to assess the capabilities of LLMs across multiple scientific disciplines, we establish 16 assessment dimensions based on five core indicators Knowledge, Understanding, Reasoning, Multimodality, and Values spanning Mathematics, Physics, Chemistry, Life Sciences, and Earth and Space Sciences. Using the developed benchmark datasets, we have conducted a comprehensive evaluation of over 50 representative open-source and closed source LLMs. All the results are publicly available and can be accessed online at www.scihorizon.cn/en.",,"Chuan Qin, Xin Chen, Chengrui Wang, Pengmin Wu, Xi Chen, Yihang Cheng, Jingyi Zhao, Meng Xiao, Xiangchao Dong, Qingqing Long, Boya Pan, Han Wu, Chengzan Li, Yuanchun Zhou, Hui Xiong, Hengshu Zhu",2025-05-29T02:56:23Z,SciHorizon: Benchmarking AI-for-Science Readiness from Scientific Data   to Large Language Models,SciHorizon: Benchmarking von KI-für-Science Readiness von wissenschaftlichen Daten zu großen Sprachmodellen,SciHorizon:将AI-SciHorizon科学准备程度从科学数据基准确定为大语言模式,http://arxiv.org/abs/2503.13503v3
1766,"While multilingual language models like XLM-R have advanced multilingualism in NLP, they still perform poorly in extremely low-resource languages. This situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen support far fewer languages than XLM-R, making text generation models non-existent for many languages in the world. To tackle this challenge, we propose a novel framework for adapting multilingual encoders to text generation in extremely low-resource languages. By reusing the weights between the encoder and the decoder, our framework allows the model to leverage the learned semantic space of the encoder, enabling efficient learning and effective generalization in low-resource languages. Applying this framework to four Chinese minority languages, we present XLM-SWCM, and demonstrate its superior performance on various downstream tasks even when compared with much larger models.",,"Zeli Su, Ziyin Zhang, Guixian Xu, Jianing Liu, XU Han, Ting Zhang, Yushuang Dong",2025-05-29T02:55:59Z,Multilingual Encoder Knows more than You Realize: Shared Weights   Pretraining for Extremely Low-Resource Languages,Mehrsprachiger Encoder weiß mehr als Sie realisieren: Geteilte Gewichte Vortraining für extrem ressourcenarme Sprachen,多语种编码器者比你所认识的要多得多: 极低资源语言的共有重力预培训,http://arxiv.org/abs/2502.10852v2
1767,"Fine-tuning LLMs with datasets containing stealthy backdoors from publishers poses security risks to downstream applications. Mainstream detection methods either identify poisoned samples by analyzing the prediction probability of poisoned classification models or rely on the rewriting model to eliminate the stealthy triggers. However, the former cannot be applied to generation tasks, while the latter may degrade generation performance and introduce new triggers. Therefore, efficiently eliminating stealthy poisoned samples for LLMs remains an urgent problem. We observe that after applying TF-IDF clustering to the sample response, there are notable differences in the intra-class distances between clean and poisoned samples. Poisoned samples tend to cluster closely because of their specific malicious outputs, whereas clean samples are more scattered due to their more varied responses. Thus, in this paper, we propose a stealthy backdoor sample detection method based on Reference-Filtration and Tfidf-Clustering mechanisms (RFTC). Specifically, we first compare the sample response with the reference model's outputs and consider the sample suspicious if there's a significant discrepancy. And then we perform TF-IDF clustering on these suspicious samples to identify the true poisoned samples based on the intra-class distance. Experiments on two machine translation datasets and one QA dataset demonstrate that RFTC outperforms baselines in backdoor detection and model performance. Further analysis of different reference models also confirms the effectiveness of our Reference-Filtration.",,"Jinwen Chen, Hainan Zhang, Fei Sun, Qinnan Zhang, Sijia Wen, Ziwei Wang, Zhiming Zheng",2025-05-29T02:49:29Z,Detecting Stealthy Backdoor Samples based on Intra-class Distance for   Large Language Models,Ermittlung von Stealthy Backdoor-Proben auf Basis von Intra-Klasse-Abstand für große Sprachmodelle,检测基于大语言模型班级内部距离的隐形后门样本,http://arxiv.org/abs/2505.23015v1
1768,"Large language models (LLMs) have demonstrated remarkable proficiency across various natural language processing (NLP) tasks. However, adapting LLMs to downstream applications requires computationally intensive and memory-demanding fine-tuning procedures. To alleviate these burdens, parameter-efficient fine-tuning (PEFT) techniques have emerged as a promising approach to tailor LLMs with minimal computational overhead. While PEFT methods offer substantial advantages, they do not fully address the pervasive issue of bias propagation from pre-training data. This work introduces Bias-Alleviating Low-Rank Adaptation (BA-LoRA), a novel PEFT method designed to counteract bias inheritance. BA-LoRA incorporates three distinct regularization terms: (1) a consistency regularizer, (2) a diversity regularizer, and (3) a singular value decomposition regularizer. These regularizers aim to enhance the models' consistency, diversity, and generalization capabilities during fine-tuning. We conduct extensive experiments on natural language understanding (NLU) and natural language generation (NLG) tasks using prominent LLMs such as LLaMA, Mistral, and Gemma. The results demonstrate that BA-LoRA outperforms LoRA and its state-of-the-art variants. Moreover, the extended experiments demonstrate that our method effectively mitigates the adverse effects of pre-training bias, leading to more reliable and robust model outputs. The code is available at https://github.com/cyp-jlu-ai/BA-LoRA.",,"Yupeng Chang, Yi Chang, Yuan Wu",2025-05-29T02:40:42Z,BA-LoRA: Bias-Alleviating Low-Rank Adaptation to Mitigate Catastrophic   Inheritance in Large Language Models,BA-LoRA: Bias-Alleviating Low-Rank Anpassung an Mitigate Katastrophische Vererbung in großen Sprachmodellen,"BA-LORA:在大语言模型中,对减轻灾害传承的低率适应",http://arxiv.org/abs/2408.04556v5
1769,"Modern VLMs have achieved near-saturation accuracy in English document visual question-answering (VQA). However, this task remains challenging in lower resource languages due to a dearth of suitable training and evaluation data. In this paper we present scalable methods for curating such datasets by focusing on Hungarian, approximately the 17th highest resource language on the internet. Specifically, we present HuDocVQA and HuDocVQA-manual, document VQA datasets that modern VLMs significantly underperform on compared to English DocVQA. HuDocVQA-manual is a small manually curated dataset based on Hungarian documents from Common Crawl, while HuDocVQA is a larger synthetically generated VQA data set from the same source. We apply multiple rounds of quality filtering and deduplication to HuDocVQA in order to match human-level quality in this dataset. We also present HuCCPDF, a dataset of 117k pages from Hungarian Common Crawl PDFs along with their transcriptions, which can be used for training a model for Hungarian OCR. To validate the quality of our datasets, we show how finetuning on a mixture of these datasets can improve accuracy on HuDocVQA for Llama 3.2 11B Instruct by +7.2%. Our datasets and code will be released to the public to foster further research in multilingual DocVQA.",,"Jonathan Li, Zoltan Csaki, Nidhi Hiremath, Etash Guha, Fenglu Hong, Edward Ma, Urmish Thakker",2025-05-29T02:34:36Z,Synthetic Document Question Answering in Hungarian,Synthetische Dokument-Frage-Antworten auf Ungarisch,匈牙利语的合成文件问题解答,http://arxiv.org/abs/2505.23008v1
1770,"The advancement of Large Language Models (LLMs) has led to significant improvements in various service domains, including search, recommendation, and chatbot applications. However, applying state-of-the-art (SOTA) research to industrial settings presents challenges, as it requires maintaining flexible conversational abilities while also strictly complying with service-specific constraints. This can be seen as two conflicting requirements due to the probabilistic nature of LLMs. In this paper, we propose our approach to addressing this challenge and detail the strategies we employed to overcome their inherent limitations in real-world applications. We conduct a practical case study of a conversational agent designed for the e-commerce domain, detailing our implementation workflow and optimizations. Our findings provide insights into bridging the gap between academic research and real-world application, introducing a framework for developing scalable, controllable, and reliable AI-driven agents.",,"Chiwan Park, Wonjun Jang, Daeryong Kim, Aelim Ahn, Kichang Yang, Woosung Hwang, Jihyeon Roh, Hyerin Park, Hyosun Wang, Min Seok Kim, Jihoon Kang",2025-05-29T02:30:27Z,A Practical Approach for Building Production-Grade Conversational Agents   with Workflow Graphs,Ein praktischer Ansatz für Gebäudeproduktions-Grade Conversational Agents mit Workflow Graphen,建立具有工作流量图的生产---- 生产---- 生产---- 不同阶段交流的代理物的实用方法,http://arxiv.org/abs/2505.23006v1
1771,"The use of Large Language Models (LLMs) for code generation has gained significant attention in recent years. Existing methods often aim to improve the quality of generated code by incorporating additional contextual information or guidance into input prompts. Many of these approaches adopt sequential reasoning strategies, mimicking human-like step-by-step thinking. However, such strategies may constrain flexibility, as they do not always align with the structured characteristics of programming languages. This paper introduces the Chain of Grounded Objectives (CGO), a method that embeds functional objectives into input prompts to enhance code generation. By leveraging appropriately structured objectives as input and avoiding explicit sequential procedures, CGO adapts effectively to the structured nature of programming tasks. Empirical evaluations demonstrate that CGO effectively enhances code generation, addressing limitations of existing approaches.",,"Sangyeop Yeo, Seung-won Hwang, Yu-Seung Ma",2025-05-29T02:28:30Z,Chain of Grounded Objectives: Bridging Process and Goal-oriented   Prompting for Code Generation,Kette der geerdeten Ziele: Überbrückungsprozess und zielorientiertes Prompting für die Codegenerierung,基本目标链链:搭桥进程和以目标为导向的促进代码生成,http://arxiv.org/abs/2501.13978v2
1772,"The scientific literature's exponential growth makes it increasingly challenging to navigate and synthesize knowledge across disciplines. Large language models (LLMs) are powerful tools for understanding scientific text, but they fail to capture detailed relationships across large bodies of work. Unstructured approaches, like retrieval augmented generation, can sift through such corpora to recall relevant facts; however, when millions of facts influence the answer, unstructured approaches become cost prohibitive. Structured representations offer a natural complement -- enabling systematic analysis across the whole corpus. Recent work enhances LLMs with unstructured or semistructured representations of scientific concepts; to complement this, we try extracting structured representations using LLMs. By combining LLMs' semantic understanding with a schema of scientific concepts, we prototype a system that answers precise questions about the literature as a whole. Our schema applies across scientific fields and we extract concepts from it using only 20 manually annotated abstracts. To demonstrate the system, we extract concepts from 30,000 papers on arXiv spanning astrophysics, fluid dynamics, and evolutionary biology. The resulting database highlights emerging trends and, by visualizing the knowledge graph, offers new ways to explore the ever-growing landscape of scientific knowledge. Demo: abby101/surveyor-0 on HF Spaces. Code: https://github.com/chiral-carbon/kg-for-science.",,"Abhipsha Das, Nicholas Lourie, Siavash Golkar, Mariel Pettee",2025-05-29T02:22:31Z,What's In Your Field? Mapping Scientific Research with Knowledge Graphs   and Large Language Models,Was ist auf Ihrem Gebiet? Mapping Wissenschaftliche Forschung mit Wissensgraphen und großen Sprachmodellen,你的领域是什么?用知识图和大语言模型绘制科学研究图。,http://arxiv.org/abs/2503.09894v2
1773,"As large language models (LLMs) are increasingly deployed in high-stakes applications, robust uncertainty estimation is essential for ensuring the safe and trustworthy deployment of LLMs. We present the most comprehensive study to date of uncertainty estimation in LLMs, evaluating 80 models spanning open- and closed-source families, dense and Mixture-of-Experts (MoE) architectures, reasoning and non-reasoning modes, quantization variants and parameter scales from 0.6B to 671B. Focusing on three representative black-box single-pass methods, including token probability-based uncertainty (TPU), numerical verbal uncertainty (NVU), and linguistic verbal uncertainty (LVU), we systematically evaluate uncertainty calibration and selective classification using the challenging MMLU-Pro benchmark, which covers both reasoning-intensive and knowledge-based tasks. Our results show that LVU consistently outperforms TPU and NVU, offering stronger calibration and discrimination while being more interpretable. We also find that high accuracy does not imply reliable uncertainty, and that model scale, post-training, reasoning ability and quantization all influence estimation performance. Notably, LLMs exhibit better uncertainty estimates on reasoning tasks than on knowledge-heavy ones, and good calibration does not necessarily translate to effective error ranking. These findings highlight the need for multi-perspective evaluation and position LVU as a practical tool for improving the reliability of LLMs in real-world settings.",,"Linwei Tao, Yi-Fan Yeh, Minjing Dong, Tao Huang, Philip Torr, Chang Xu",2025-05-29T02:04:49Z,Revisiting Uncertainty Estimation and Calibration of Large Language   Models,Überprüfung der Ungewissheitsschätzung und Kalibrierung von großen Sprachmodellen,重新审视大语言模型的不确定性估计和校准,http://arxiv.org/abs/2505.23854v1
1774,"Claim verification is a long-standing and challenging task that demands not only high accuracy but also explainability of the verification process. This task becomes an emerging research issue in the era of large language models (LLMs) since real-world claims are often complex, featuring intricate semantic structures or obfuscated entities. Traditional approaches typically address this by decomposing claims into sub-claims and querying a knowledge base to resolve hidden or ambiguous entities. However, the absence of effective disambiguation strategies for these entities can compromise the entire verification process. To address these challenges, we propose Verify-in-the-Graph (VeGraph), a novel framework leveraging the reasoning and comprehension abilities of LLM agents. VeGraph operates in three phases: (1) Graph Representation - an input claim is decomposed into structured triplets, forming a graph-based representation that integrates both structured and unstructured information; (2) Entity Disambiguation -VeGraph iteratively interacts with the knowledge base to resolve ambiguous entities within the graph for deeper sub-claim verification; and (3) Verification - remaining triplets are verified to complete the fact-checking process. Experiments using Meta-Llama-3-70B (instruct version) show that VeGraph achieves competitive performance compared to baselines on two benchmarks HoVer and FEVEROUS, effectively addressing claim verification challenges. Our source code and data are available for further exploitation.",,"Hoang Pham, Thanh-Do Nguyen, Khac-Hoai Nam Bui",2025-05-29T02:02:55Z,Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim   Verification with Interactive Graph Representation,Prüfen Sie in der Grafik: Entity Disambiguation Enhancement für komplexe Claim-Verifikation mit interaktiver Graphendarstellung,校验格中:实体对复杂索赔核实与交互式图表代表的分歧增强,http://arxiv.org/abs/2505.22993v1
1775,"This work presents Pangu Embedded, an efficient Large Language Model (LLM) reasoner developed on Ascend Neural Processing Units (NPUs), featuring flexible fast and slow thinking capabilities. Pangu Embedded addresses the significant computational costs and inference latency challenges prevalent in existing reasoning-optimized LLMs. We propose a two-stage training framework for its construction. In Stage 1, the model is finetuned via an iterative distillation process, incorporating inter-iteration model merging to effectively aggregate complementary knowledge. This is followed by reinforcement learning on Ascend clusters, optimized by a latency-tolerant scheduler that combines stale synchronous parallelism with prioritized data queues. The RL process is guided by a Multi-source Adaptive Reward System (MARS), which generates dynamic, task-specific reward signals using deterministic metrics and lightweight LLM evaluators for mathematics, coding, and general problem-solving tasks. Stage 2 introduces a dual-system framework, endowing Pangu Embedded with a ""fast"" mode for routine queries and a deeper ""slow"" mode for complex inference. This framework offers both manual mode switching for user control and an automatic, complexity-aware mode selection mechanism that dynamically allocates computational resources to balance latency and reasoning depth. Experimental results on benchmarks including AIME 2024, GPQA, and LiveCodeBench demonstrate that Pangu Embedded with 7B parameters, outperforms similar-size models like Qwen3-8B and GLM4-9B. It delivers rapid responses and state-of-the-art reasoning quality within a single, unified model architecture, highlighting a promising direction for developing powerful yet practically deployable LLM reasoners.",,"Hanting Chen, Yasheng Wang, Kai Han, Dong Li, Lin Li, Zhenni Bi, Jinpeng Li, Haoyu Wang, Fei Mi, Mingjian Zhu, Bin Wang, Kaikai Song, Yifei Fu, Xu He, Yu Luo, Chong Zhu, Quan He, Xueyu Wu, Wei He, Hailin Hu, Yehui Tang, Dacheng Tao, Xinghao Chen, Yunhe Wang",2025-05-29T01:59:00Z,Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition,Pangu Embedded: Effizienter Dual-System LLM Reasoner mit Metakognition,Pangu 嵌入式:高效的双系统LLM,http://arxiv.org/abs/2505.22375v2
1776,"We introduce Frankentexts, a new type of long-form narratives produced by LLMs under the extreme constraint that most tokens (e.g., 90%) must be copied verbatim from human writings. This task presents a challenging test of controllable generation, requiring models to satisfy a writing prompt, integrate disparate text fragments, and still produce a coherent narrative. To generate Frankentexts, we instruct the model to produce a draft by selecting and combining human-written passages, then iteratively revise the draft while maintaining a user-specified copy ratio. We evaluate the resulting Frankentexts along three axes: writing quality, instruction adherence, and detectability. Gemini-2.5-Pro performs surprisingly well on this task: 81% of its Frankentexts are coherent and 100% relevant to the prompt. Notably, up to 59% of these outputs are misclassified as human-written by detectors like Pangram, revealing limitations in AI text detectors. Human annotators can sometimes identify Frankentexts through their abrupt tone shifts and inconsistent grammar between segments, especially in longer generations. Beyond presenting a challenging generation task, Frankentexts invite discussion on building effective detectors for this new grey zone of authorship, provide training data for mixed authorship detection, and serve as a sandbox for studying human-AI co-writing processes.",,"Chau Minh Pham, Jenna Russell, Dzung Pham, Mohit Iyyer",2025-05-29T01:43:46Z,Frankentext: Stitching random text fragments into long-form narratives,Frankentext: Zufällige Textfragmente zu langformigen Erzählungen heften,Frankentext: 将随机文本片断成长式叙述,http://arxiv.org/abs/2505.18128v2
1777,"Objective: To demonstrate the capabilities of Large Language Models (LLMs) as autonomous agents to reproduce findings of published research studies using the same or similar dataset.   Materials and Methods: We used the ""Quick Access"" dataset of the National Alzheimer's Coordinating Center (NACC). We identified highly cited published research manuscripts using NACC data and selected five studies that appeared reproducible using this dataset alone. Using GPT-4o, we created a simulated research team of LLM-based autonomous agents tasked with writing and executing code to dynamically reproduce the findings of each study, given only study Abstracts, Methods sections, and data dictionary descriptions of the dataset.   Results: We extracted 35 key findings described in the Abstracts across 5 Alzheimer's studies. On average, LLM agents approximately reproduced 53.2% of findings per study. Numeric values and range-based findings often differed between studies and agents. The agents also applied statistical methods or parameters that varied from the originals, though overall trends and significance were sometimes similar.   Discussion: In some cases, LLM-based agents replicated research techniques and findings. In others, they failed due to implementation flaws or missing methodological detail. These discrepancies show the current limits of LLMs in fully automating reproducibility assessments. Still, this early investigation highlights the potential of structured agent-based systems to provide scalable evaluation of scientific rigor.   Conclusion: This exploratory work illustrates both the promise and limitations of LLMs as autonomous agents for automating reproducibility in biomedical research.",,"Nic Dobbins, Christelle Xiong, Kristine Lan, Meliha Yetisgen",2025-05-29T01:31:55Z,Large Language Model-Based Agents for Automated Research   Reproducibility: An Exploratory Study in Alzheimer's Disease,Large Language model-based agents for Automated Research Reproduzierbarkeit: Eine explorative Studie zur Alzheimer-Krankheit,自动化研究可复制性:阿尔茨海默氏病探索性研究,http://arxiv.org/abs/2505.23852v1
1778,"A simple and effective method for the inference-time alignment and scaling test-time compute of generative models is best-of-$n$ sampling, where $n$ samples are drawn from a reference policy, ranked based on a reward function, and the highest ranking one is selected. A commonly used analytical expression in the literature claims that the KL divergence between the best-of-$n$ policy and the reference policy is equal to $\log (n) - (n-1)/n.$ We disprove the validity of this claim, and show that it is an upper bound on the actual KL divergence. We also explore the tightness of this upper bound in different regimes, and propose a new estimator for the KL divergence and empirically show that it provides a tight approximation. We also show that the win rate of the best-of-$n$ policy against the reference policy is upper bounded by $n/(n+1)$ and derive bounds on the tightness of this characterization. We conclude with analyzing the tradeoffs between win rate and KL divergence of the best-of-$n$ alignment policy, which demonstrate that very good tradeoffs are achievable with $n < 1000$.",,"Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander D'Amour, Jacob Eisenstein, Chirag Nagpal, Ananda Theertha Suresh",2025-05-29T01:30:18Z,Theoretical guarantees on the best-of-n alignment policy,Theoretische Garantien für die optimale Ausrichtungspolitik,关于最佳协调政策理论保障,http://arxiv.org/abs/2401.01879v3
1779,"Process mining aims to discover, monitor and optimize the actual behaviors of real processes. While prior work has mainly focused on extracting procedural action flows from instructional texts, rule flows embedded in business documents remain underexplored. To this end, we introduce a novel annotated Chinese dataset, BPRF, which contains 50 business process documents with 326 explicitly labeled business rules across multiple domains. Each rule is represented as a <Condition, Action> pair, and we annotate logical dependencies between rules (sequential, conditional, or parallel). We also propose ExIde, a framework for automatic business rule extraction and dependency relationship identification using large language models (LLMs). We evaluate ExIde using 12 state-of-the-art (SOTA) LLMs on the BPRF dataset, benchmarking performance on both rule extraction and dependency classification tasks of current LLMs. Our results demonstrate the effectiveness of ExIde in extracting structured business rules and analyzing their interdependencies for current SOTA LLMs, paving the way for more automated and interpretable business process automation.",,"Chen Yang, Ruping Xu, Ruizhe Li, Bin Cao, Jing Fan",2025-05-29T01:22:02Z,Business as Rulesual: A Benchmark and Framework for Business Rule Flow   Modeling with LLMs,Business as Rulesual: Benchmark und Rahmen für Business Rule Flow Modellierung mit LLMs,业务作为规则:与LLMs建立商业规则流动模式的基准和框架,http://arxiv.org/abs/2505.18542v2
1780,"The emergence of scaling laws has profoundly shaped the development of large language models (LLMs), enabling predictable performance gains through systematic increases in model size, dataset volume, and compute. Yet, these principles remain largely unexplored in the context of electronic health records (EHRs) -- a rich, sequential, and globally abundant data source that differs structurally from natural language. In this work, we present the first empirical investigation of scaling laws for EHR foundation models. By training transformer architectures on patient timeline data from the MIMIC-IV database across varying model sizes and compute budgets, we identify consistent scaling patterns, including parabolic IsoFLOPs curves and power-law relationships between compute, model parameters, data size, and clinical utility. These findings demonstrate that EHR models exhibit scaling behavior analogous to LLMs, offering predictive insights into resource-efficient training strategies. Our results lay the groundwork for developing powerful EHR foundation models capable of transforming clinical prediction tasks and advancing personalized healthcare.",,"Sheng Zhang, Qin Liu, Naoto Usuyama, Cliff Wong, Tristan Naumann, Hoifung Poon",2025-05-29T01:05:11Z,Exploring Scaling Laws for EHR Foundation Models,Erforschung von Skalierungsgesetzen für EHR-Stiftungsmodelle,探索EHR基金会模式的扩展法律,http://arxiv.org/abs/2505.22964v1
1781,"Large language models (LLMs) have shown promising potential in persuasion, but existing works on training LLM persuaders are still preliminary. Notably, while humans are skilled in modeling their opponent's thoughts and opinions proactively and dynamically, current LLMs struggle with such Theory of Mind (ToM) reasoning, resulting in limited diversity and opponent awareness. To address this limitation, we introduce Theory of Mind Augmented Persuader (ToMAP), a novel approach for building more flexible persuader agents by incorporating two theory of mind modules that enhance the persuader's awareness and analysis of the opponent's mental state. Specifically, we begin by prompting the persuader to consider possible objections to the target central claim, and then use a text encoder paired with a trained MLP classifier to predict the opponent's current stance on these counterclaims. Our carefully designed reinforcement learning schema enables the persuader learns how to analyze opponent-related information and utilize it to generate more effective arguments. Experiments show that the ToMAP persuader, while containing only 3B parameters, outperforms much larger baselines, like GPT-4o, with a relative gain of 39.4% across multiple persuadee models and diverse corpora. Notably, ToMAP exhibits complex reasoning chains and reduced repetition during training, which leads to more diverse and effective arguments. The opponent-aware feature of ToMAP also makes it suitable for long conversations and enables it to employ more logical and opponent-aware strategies. These results underscore our method's effectiveness and highlight its potential for developing more persuasive language agents. Code is available at: https://github.com/ulab-uiuc/ToMAP.",,"Peixuan Han, Zijia Liu, Jiaxuan You",2025-05-29T01:03:41Z,ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind,ToMAP: Training Gegner-Bewusst LLM überzeugt mit Theorie des Geistes,ToMAP:培训有思想理论的对抗者软件软件LLM,http://arxiv.org/abs/2505.22961v1
1782,"Health, Safety, and Environment (HSE) compliance assessment demands dynamic real-time decision-making under complicated regulations and complex human-machine-environment interactions. While large language models (LLMs) hold significant potential for decision intelligence and contextual dialogue, their capacity for domain-specific knowledge in HSE and structured legal reasoning remains underexplored. We introduce HSE-Bench, the first benchmark dataset designed to evaluate the HSE compliance assessment capabilities of LLM. HSE-Bench comprises over 1,000 manually curated questions drawn from regulations, court cases, safety exams, and fieldwork videos, and integrates a reasoning flow based on Issue spotting, rule Recall, rule Application, and rule Conclusion (IRAC) to assess the holistic reasoning pipeline. We conduct extensive evaluations on different prompting strategies and more than 10 LLMs, including foundation models, reasoning models and multimodal vision models. The results show that, although current LLMs achieve good performance, their capabilities largely rely on semantic matching rather than principled reasoning grounded in the underlying HSE compliance context. Moreover, their native reasoning trace lacks the systematic legal reasoning required for rigorous HSE compliance assessment. To alleviate these, we propose a new prompting technique, Reasoning of Expert (RoE), which guides LLMs to simulate the reasoning process of different experts for compliance assessment and reach a more accurate unified decision. We hope our study highlights reasoning gaps in LLMs for HSE compliance and inspires further research on related tasks.",,"Jianwei Wang, Mengqi Wang, Yinsi Zhou, Zhenchang Xing, Qing Liu, Xiwei Xu, Wenjie Zhang, Liming Zhu",2025-05-29T01:02:53Z,"LLM-based HSE Compliance Assessment: Benchmark, Performance, and   Advancements","LLM-basierte HSE Compliance Assessment: Benchmark, Performance und Advancements",基于LLM的HSE合规评估:基准、业绩和进步,http://arxiv.org/abs/2505.22959v1
1783,"Large language models (LLMs) offer powerful capabilities but come with significant environmental impact, particularly in carbon emissions. Existing studies benchmark carbon emissions but lack a standardized basis for comparison across different model configurations. To address this, we introduce the concept of functional unit (FU) as a standardized basis and develop FUEL, the first FU-based framework for evaluating LLM serving's environmental impact. Through three case studies, we uncover key insights and trade-offs in reducing carbon emissions by optimizing model size, quantization strategy, and hardware choice, paving the way for more sustainable LLM serving. The code is available at https://github.com/jojacola/FUEL.",,"Yanran Wu, Inez Hua, Yi Ding",2025-05-29T00:42:50Z,Unveiling Environmental Impacts of Large Language Model Serving: A   Functional Unit View,Enthüllen von Umweltauswirkungen von großsprachigen Modellen: Eine funktionale Einheitsansicht,大型语文服务模式的不懈环境影响:职能单位观点,http://arxiv.org/abs/2502.11256v2
1784,"Existing methods fail to effectively steer Large Language Models (LLMs) between textual reasoning and code generation, leaving symbolic computing capabilities underutilized. We introduce CodeSteer, an effective method for guiding LLM code/text generation. We construct a comprehensive benchmark SymBench comprising 37 symbolic tasks with adjustable complexity and also synthesize datasets of 12k multi-turn guidance/generation trajectories and 5.5k guidance comparison pairs. We fine-tune the Llama-3-8B model with a newly designed multi-turn supervised fine-tuning (SFT) and direct preference optimization (DPO). The resulting model, CodeSteerLLM, augmented with the proposed symbolic and self-answer checkers, effectively guides the code/text generation of larger models. Augmenting GPT-4o with CodeSteer raises its average performance score from 53.3 to 86.4, even outperforming the existing best LLM OpenAI o1 (82.7), o1-preview (74.8), and DeepSeek R1 (76.8) across all 37 tasks (28 seen, 9 unseen). Trained for GPT-4o, CodeSteer demonstrates superior generalizability, providing an average 41.8 performance boost on Claude, Mistral, and GPT-3.5. CodeSteer-guided LLMs fully harness symbolic computing to maintain strong performance on highly complex tasks. Models, Datasets, and Codes are available at https://github.com/yongchao98/CodeSteer-v1.0 and https://huggingface.co/yongchao98.",,"Yongchao Chen, Yilun Hao, Yueying Liu, Yang Zhang, Chuchu Fan",2025-05-29T00:38:10Z,CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance,CodeSteer: Symbolisch-Augmentierte Sprachmodelle über Code/Text Anleitung,代码器:通过编码/文本指导的代码/文本指导的代码器:代号辅助语言模式,http://arxiv.org/abs/2502.04350v2
1785,"Automated large-scale analysis of public discussions around contested issues like abortion requires detecting and understanding the use of arguments. While Large Language Models (LLMs) have shown promise in language processing tasks, their performance in mining topic-specific, pre-defined arguments in online comments remains underexplored. We evaluate four state-of-the-art LLMs on three argument mining tasks using datasets comprising over 2,000 opinion comments across six polarizing topics. Quantitative evaluation suggests an overall strong performance across the three tasks, especially for large and fine-tuned LLMs, albeit at a significant environmental cost. However, a detailed error analysis revealed systematic shortcomings on long and nuanced comments and emotionally charged language, raising concerns for downstream applications like content moderation or opinion analysis. Our results highlight both the promise and current limitations of LLMs for automated argument analysis in online comments.",,"Matteo Guida, Yulia Otmakhova, Eduard Hovy, Lea Frermann",2025-05-29T00:29:51Z,"LLMs for Argument Mining: Detection, Extraction, and Relationship   Classification of pre-defined Arguments in Online Comments","LLMs for argument Mining: Detection, Extraction, and Relationship Classification of pre-defined argumentments in Online Kommentare",辩论采矿的LLMs:在线评论中预先界定的论据的探测、提取和关系分类,http://arxiv.org/abs/2505.22956v1
1786,"Large Language Models $($LLMs$)$ solve complex problems using training-free methods like prompt engineering and in-context learning, yet ensuring reasoning correctness remains challenging. While self-correction methods such as self-consistency and self-refinement aim to improve reliability, they often reinforce biases due to the lack of effective feedback mechanisms. Multi-Agent Debate $($MAD$)$ has emerged as an alternative, but we identify two key limitations: bias reinforcement, where debate amplifies model biases instead of correcting them, and lack of perspective diversity, as all agents share the same model and reasoning patterns, limiting true debate effectiveness. To systematically evaluate these issues, we introduce $\textit{MetaNIM Arena}$, a benchmark designed to assess LLMs in adversarial strategic decision-making, where dynamic interactions influence optimal decisions. To overcome MAD's limitations, we propose $\textbf{DReaMAD}$ $($$\textbf{D}$iverse $\textbf{Rea}$soning via $\textbf{M}$ulti-$\textbf{A}$gent $\textbf{D}$ebate with Refined Prompt$)$, a novel framework that $(1)$ refines LLM's strategic prior knowledge to improve reasoning quality and $(2)$ promotes diverse viewpoints within a single model by systematically modifying prompts, reducing bias. Empirical results show that $\textbf{DReaMAD}$ significantly improves decision accuracy, reasoning diversity, and bias mitigation across multiple strategic tasks, establishing it as a more effective approach for LLM-based decision-making.",,"Jihwan Oh, Minchan Jeong, Jongwoo Ko, Se-Young Yun",2025-05-29T00:25:23Z,Understanding Bias Reinforcement in LLM Agents Debate,Verständnis der Bias-Verstärkung in LLM-Agenten-Debatte,了解LLLM代理商的强化申请,http://arxiv.org/abs/2503.16814v2
1787,"Large language models (LLMs) have shown strong performance in zero-shot summarization, but often struggle to model document structure and identify salient information in long texts. In this work, we introduce StrucSum, a training-free prompting framework that enhances LLM reasoning through sentence-level graph structures. StrucSum injects structural signals into prompts via three targeted strategies: Neighbor-Aware Prompting (NAP) for local context, Centrality-Aware Prompting (CAP) for importance estimation, and Centrality-Guided Masking (CGM) for efficient input reduction. Experiments on ArXiv, PubMed, and Multi-News demonstrate that StrucSum consistently improves both summary quality and factual consistency over unsupervised baselines and vanilla prompting. Notably, on ArXiv, it boosts FactCC and SummaC by 19.2 and 9.7 points, indicating stronger alignment between summaries and source content. These findings suggest that structure-aware prompting is a simple yet effective approach for zero-shot extractive summarization with LLMs, without any training or task-specific tuning.",,"Haohan Yuan, Sukhwa Hong, Haopeng Zhang",2025-05-29T00:10:23Z,StrucSum: Graph-Structured Reasoning for Long Document Extractive   Summarization with LLMs,StrucSum: Graph-strukturierte Begründung für lange Dokumentextraktionszusammenfassung mit LLMs,StrucSum: 长文件提取摘要的图表结构化原因与LLMs,http://arxiv.org/abs/2505.22950v1
