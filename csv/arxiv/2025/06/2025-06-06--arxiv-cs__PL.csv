,abstract,abstract-zh,authors,date,title,title-de,title-zh,url
0,"In the spirit of the Curry-Howard correspondence between proofs and programs, we define and study a syntax and semantics for classical logic equipped with a computationally involutive negation, using a polarised effect calculus. A main challenge in designing a denotational semantics is to accommodate both call-by-value and call-by-name evaluation strategies, which leads to a failure of associativity of composition. Building on the work of the third author, we devise the notion of dialogue duploid, which provides a non-associative and effectful counterpart to the notion of dialogue category introduced by the second author in his 2-categorical account, based on adjunctions, of logical polarities and continuations. We show that the syntax of the polarised calculus can be interpreted in any dialogue duploid, and that it defines in fact a syntactic dialogue duploid. As an application, we establish, by semantic as well as syntactic means, the Hasegawa-Thielecke theorem, which states that the notions of central map and of thunkable map coincide in any dialogue duploid (in particular, for any double negation monad on a symmetric monoidal category).","本着证据与程序之间的咖哩-霍华德对应关系的精神,我们定义并研究一种古典逻辑的语法和语义学,该语法配有计算上不易演化的否定,使用两极分化效果的微积分。设计分解语义学的主要挑战是如何兼顾按个价值和按个名称的评价战略,这导致组合的关联性失败。我们根据第三作者的工作,设计了对话符号的概念,它为第二作者在二类账户中基于逻辑极化和延续的辅助性、逻辑极化和延续性引入的对话类别概念提供了一种非关联性和效果的对应。我们表明,任何对话符号都可以解释极化微积分的共性,事实上它定义了一个合成对话符号。作为应用,我们通过语义和同理手段,建立了与第二作者在二类对话账户中引入的对话类别概念的非关联性和有效对应性对应性。 我们展示了在任何对话的图类中,任何可同时标定的中间地图和任何定型地图的正标定的正式概念。","Éléonore Mangel, Paul-André Melliès, Guillaume Munch-Maccagnoni",2025-06-05T17:49:54Z,Classical notions of computation and the Hasegawa-Thielecke theorem,Klassische Begriffe der Berechnung und das Hasegawa-Thielecke-Theorem,经典的计算概念和长谷川-长谷川-希列克定理,http://arxiv.org/abs/2502.13033v2
1,"Quantum error correction (QEC) is fundamental for suppressing noise in quantum hardware and enabling fault-tolerant quantum computation. In this paper, we propose an efficient verification framework for QEC programs. We define an assertion logic and a program logic specifically crafted for QEC programs and establish a sound proof system. We then develop an efficient method for handling verification conditions (VCs) of QEC programs: for Pauli errors, the VCs are reduced to classical assertions that can be solved by SMT solvers, and for non-Pauli errors, we provide a heuristic algorithm. We formalize the proposed program logic in Coq proof assistant, making it a verified QEC verifier. Additionally, we implement an automated QEC verifier, Veri-QEC, for verifying various fault-tolerant scenarios. We demonstrate the efficiency and broad functionality of the framework by performing different verification tasks across various scenarios. Finally, we present a benchmark of 14 verified stabilizer codes.","量子错误校正( QEC) 是抑制量子硬件噪音和促成容错量量计算的基础。 在本文中, 我们建议了QEC程序的有效核查框架 。 我们定义了专门为 QEC 程序设计的断言逻辑和程序逻辑, 并建立了一个健全的验证系统 。 然后我们开发了一种处理 QEC 程序核查条件( VCs) 的有效方法 : 对于 保利 错误, VC 将简化为由 SMT 解答器解答的经典说法, 对于非 Pauli 错误, 我们提供了一种超常算法 。 我们在 Coq 验证助理中正式确定了拟议的程序逻辑, 使之成为一个经核实的 QEC 验证器 。 此外, 我们用一个自动的 QEC 校准器, Veri- QEC 来验证各种容错漏情况 。 我们通过在不同情况下执行不同的核查任务来展示框架的效率和广泛功能 。 最后, 我们提出了一个由 14 个经核实的稳定器代码基准 。","Qifan Huang, Li Zhou, Wang Fang, Mengyu Zhao, Mingsheng Ying",2025-06-05T02:20:09Z,Efficient Formal Verification of Quantum Error Correcting Programs,Effiziente formale Überprüfung von Quantenfehler-Korrekturprogrammen,量化错误纠正程序的有效正式核实,http://arxiv.org/abs/2504.07732v2
2,"Large language models (LLMs) are playing an increasingly large role in domains such as code generation, including hardware code generation, where Verilog is the key language. However, the amount of publicly available Verilog code pales in comparison to the amount of code available for software languages like Python. In this work, we present hdl2v (""HDL-to-Verilog""), a dataset which seeks to increase the amount of available human-written Verilog data by translating or compiling three other hardware description languages - VHDL, Chisel, and PyMTL3 - to Verilog. Furthermore, we demonstrate the value of hdl2v in enhancing LLM Verilog generation by improving performance of a 32 billion-parameter open-weight model by up to 23% (pass@10) in VerilogEvalV2, without utilizing any data augmentation or knowledge distillation from larger models. We also show hdl2v's ability to boost the performance of a data augmentation-based fine-tuning approach by 63%. Finally, we characterize and analyze our dataset to better understand which characteristics of HDL-to-Verilog datasets can be expanded upon in future work for even better performance.","大型语言模型(LLMS)在诸如代码生成(包括硬件代码生成)等领域发挥着越来越重要的作用, 包括硬件代码生成( Verilog 是 Verilog 的关键语言 ) 。 然而, 公开提供的 Verilog 代码数量与 Python 等软件语言可用的代码数量相比, 与可用代码数量相比, Vython 等软件语言的代码数量是苍白的。 在这项工作中, 我们提供了 hdl2v (“ HDL- 到 Verilog ” ) , 这个数据集试图通过翻译或汇编其他三种硬件描述语言( VHDL、 Chisel 和 PyMTL3 - 至 Verilog ) 来增加现有的人文版 Verilog 数据数量。 此外, 我们用 HDL- VerivalV 2 改进了320亿 参数开放度模型的性能模型的性能( passel@10) 。 我们还展示了 hdl2 能力, 63% 来提升基于数据增强基于 微调方法的性能的性能。 最后, 我们分析和分析了我们的数据数据集, 以便更好地了解未来数据系统如何改进了HDL- 。","Charles Hong, Brendan Roberts, Huijae An, Alex Um, Advay Ratan, Yakun Sophia Shao",2025-06-05T01:29:18Z,hdl2v: A Code Translation Dataset for Enhanced LLM Verilog Generation,hdl2v: Ein Code-Übersetzungsdatensatz für verbesserte LLM Verilog-Generierung,hdl2v: 用于强化LLM Verilog 生成的代码翻译数据集,http://arxiv.org/abs/2506.04544v1
3,"LLMs have been extensively used for the task of automated code generation. In this work, we examine the applicability of LLMs for the related but relatively unexplored task of code-equivalence checking, i.e., given two programs, whether they are functionally equivalent or not. This is an important problem since benchmarking code equivalence can play a critical role in evaluating LLM capabilities for tasks such as code re-writing and code translation. Towards this end, we present CETBench - Code Equivalence with Transformations Benchmark, constructed via a repository of programs, where two programs in the repository may be solving the same or different tasks. Each instance in our dataset is obtained by taking a pair of programs in the repository and applying a random series of pre-defined code transformations, resulting in (non-)equivalent pairs. Our analysis on this dataset reveals a surprising finding that very simple code transformations in the underlying pair of programs can result in a significant drop in performance of SOTA LLMs for the task of code-equivalence checking. To remedy this, we present a simple fine-tuning-based approach to boost LLM performance on the transformed pairs of programs. Our approach for dataset generation is generic, and can be used with repositories with varying program difficulty levels and allows for applying varying numbers as well as kinds of transformations. In our experiments, we perform ablations over the difficulty level of original programs, as well as the kind of transformations used in generating pairs for equivalence checking. Our analysis presents deep insights into the working of LLMs for the task of code-equivalence, and points to the fact that they may still be far from what could be termed as a semantic understanding of the underlying code.","在这项工作中,我们检查了LLMS对相关但相对未探索的代码等效检查任务的适用性,即,给两个程序,不管它们是否在功能上等同。这是一个重要的问题,因为基准代码等同在评估代码重写和代码翻译等任务LLM能力方面可以发挥关键作用。为此,我们展示了CETBench - Coc Equvalence with Transformations Birectors Birectors,它通过程序库的两个程序库可以解决相同或不同的任务。我们的数据集中的每个实例都是通过在存储库中采取一对程序来获取的,并应用一系列预定义的代码转换,从而导致(非)等同对等。我们对这个数据集的分析显示了一个非常简单的发现,基本程序中的代码转换可以导致SOTA LLMS的运行率大幅下降,用于代码等同检查任务。为了补救这一点,我们用一个简单的基于精细度的方法来提升LM的直径直径直径直到数据转换过程的操作水平,而我们用一个用于生成程序的运行过程的精度分析过程的精度水平。","Neeva Oza, Ishaan Govil, Parul Gupta, Dinesh Khandelwal, Dinesh Garg, Parag Singla",2025-06-04T14:47:14Z,CETBench: A Novel Dataset constructed via Transformations over Programs   for Benchmarking LLMs for Code-Equivalence Checking,"CETBench: Ein neuartiger Datensatz, der über Transformationen über Programme zum Benchmarking von LLMs für Code-Equivalenz-Checking erstellt wurde",CETBennch:通过对代码等效检查LMLM基准测试方案进行转换而构建的新数据集,http://arxiv.org/abs/2506.04019v1
4,"Safe memory reclamation techniques that utilize per read reservations, such as hazard pointers, often cause significant overhead in traversals of linked concurrent data structures. This is primarily due to the need to announce a reservation, and fence to enforce appropriate ordering, before each read. In read-intensive workloads, this overhead is amplified because, even if relatively little memory reclamation actually occurs, the full overhead of reserving records is still incurred while traversing data structures.   In this paper, we propose a novel memory reclamation technique by combining POSIX signals and delayed reclamation, introducing a publish-on-ping approach. This method eliminates the need to make reservations globally visible before use. Instead, threads privately track which records they are accessing, and share this information on demand with threads that intend to reclaim memory. The approach can serve as a drop-in replacement for hazard pointers and hazard eras. Furthermore, the capability to retain reservations during traversals in data structure operations and publish them on demand facilitates the construction of a variant of hazard pointers (EpochPOP). This variant uses epochs to approach the performance of epoch-based reclamation in the common case where threads are not frequently delayed (while retaining the robustness of hazard pointers).   Our publish-on-ping implementations based on hazard pointers (HP) and hazard eras, when applied to various data structures, exhibit significant performance improvements. The improvements across various workloads and data structures range from 1.2X to 4X over the original HP, up to 20% compared to a heavily optimized HP implementation similar to the one in the Folly open-source library, and up to 3X faster than hazard eras. EpochPOP delivers performance similar to epoch-based reclamation while providing stronger guarantees.","安全存储回收技术使用每读保留物,例如危险指示器,常常在连接的并行数据结构的穿梭过程中造成巨大的间接成本。这主要是由于需要在每次阅读之前宣布保留物和栅栏,以便执行适当的订购。在阅读密集的工作量中,这种间接成本被放大,因为即使实际发生相对较少的记忆回收,保留记录的全部间接成本仍然在数据结构穿行过程中发生。在本文件中,我们建议一种新型的记忆回收技术,将POSIX信号和延迟回收结合起来,采用一种出版即时的方法。这种方法消除了使全球在使用之前能够看到相关数据结构的隐性能。相反,需要将它们访问的私人记录线条连接起来,用希望恢复记忆的线条分享这种需求信息。这个方法可以用来取代危险点和危险时间。此外,在数据结构穿行期间保留保留保留记录物的能力有助于构建一个以POSIX为主的开放点(EpochPOPP),采用这种方法在使用之前就消除了全球范围的保留物。相反的私隐隐隐性记录器,在运行过程中,在运行过程中经常使用一种动态的精确的危害等级数据,在运行期间提供一种危险等级的数据,在使用。","Ajay Singh, Trevor Brown",2025-06-04T11:59:33Z,Publish on Ping: A Better Way to Publish Reservations in Memory   Reclamation for Concurrent Data Structures,Publishing on Ping: Ein besserer Weg zur Veröffentlichung von Reservierungen in Speicherreklamation für parallele Datenstrukturen,出版《Ping:同时数据结构内存检索保留书出版的更好方法》,http://arxiv.org/abs/2501.04250v2
5,"Software development is shifting from traditional logical programming to model-integrated applications that leverage generative AI and large language models (LLMs) during runtime. However, integrating LLMs remains complex, requiring developers to manually craft prompts and process outputs. Existing tools attempt to assist with prompt engineering, but often introduce additional complexity.   This paper presents Meaning-Typed Programming (MTP) model, a novel paradigm that abstracts LLM integration through intuitive language-level constructs. By leveraging the inherent semantic richness of code, MTP automates prompt generation and response handling without additional developer effort. We introduce the by operator for seamless LLM invocation, MT-IR, a meaning-based intermediate representation for semantic extraction, and MT-Runtime, an automated system for managing LLM interactions. We implement MTP in Jac, a Python superset language and find that MTP significantly reduces coding complexity while maintaining accuracy and efficiency. Our evaluation across diverse benchmarks and user studies demonstrates that MTP outperforms existing frameworks such as DSPy and LMQL by reducing lines of code by factors of 2.3-7.5X and 1.3-10.7X respectively. For math problems from the GSM8k dataset, MTP achieves accuracy rates approaching 90%, while reducing token usage in 10 out of 13 benchmarks. This leads to cost savings up to 4.5X and runtime speedups as high as 4.75X. Additionally, MTP demonstrates resilience even when 50% of naming conventions are suboptimal, establishing it as a practical, efficient solution for streamlining model-integrated application development.","软件开发正在从传统的逻辑编程转向模型集成应用程序,这些应用程序在运行期间利用基因性AI和大语言模型(LLMS),但整合LLMS仍然很复杂,需要开发者手动手动工艺提示和流程输出。现有工具试图协助快速工程,但往往会引入更多复杂性。本文展示了意义式编程(MTP)模式,这是一个通过直观语言层面的构造将LLMM整合起来的新范例。通过利用内在的语义丰富代码,MTP自动创建和反应处理迅速,而没有额外的开发者努力。我们引入了无缝LLM职业操作操作操作者、MT-IR(MT-Runtime)和MT-Runtime(一个管理LMMM互动的自动化系统)的中间代号。我们在Jac(一种Python 超级设置语言)中实施MTP,发现MTP在保持准确性和效率的同时大大降低了连锁复杂性。我们在不同基准和用户研究中进行的评估显示,MTP(甚至)快速生成和LMQL(L)等现有解决方案解决方案的解决方案,我们通过降低了现有框架,通过减少了50-7.X)的代码代码代码的代码的代码,同时将SMTP应用了13-7.x的精确度的缩缩缩缩缩缩缩缩缩缩缩缩缩缩算取了Mx的缩缩缩缩缩缩缩缩算。","Jayanaka L. Dantanarayana, Yiping Kang, Kugesan Sivasothynathan, Christopher Clarke, Baichuan Li, Savini Kashmira, Krisztian Flautner, Lingjia Tang, Jason Mars",2025-06-04T00:51:13Z,Meaning-Typed Programming: Language Abstraction and Runtime for   Model-Integrated Applications,Meaning-Typed Programmierung: Sprachabstraktion und Laufzeit für modellintegrierte Anwendungen,语言摘要和模式集成应用的运行时间,http://arxiv.org/abs/2405.08965v4
6,"We introduce an imperative, stack-based, and reversible computational model that characterizes Two-way Bijections both implicitly, concerning their computational complexity, and with zero-garbage.","我们引入了一种基于堆叠和可逆的强制性计算模式,这种模式可以隐含地描述双向双向截面的特点,既涉及其计算的复杂性,又带有零垃圾。","Matteo Palazzo, Luca Roversi",2025-06-03T20:40:45Z,Towards a Characterization of Two-way Bijections in a Reversible   Computational Model,Hin zu einer Charakterisierung von Zwei-Wege-Bijektionen in einem reversiblen Computermodell,争取在可逆计算模型中确定双向双向分轨的特征,http://arxiv.org/abs/2506.03382v1
7,"Does the choice of programming language affect energy consumption? Previous highly visible studies have established associations between certain programming languages and energy consumption. A causal misinterpretation of this work has led academics and industry leaders to use or support certain languages based on their claimed impact on energy consumption. This paper tackles this causal question directly. It first corrects and improves the measurement methodology used by prior work. It then develops a detailed causal model capturing the complex relationship between programming language choice and energy consumption. This model identifies and incorporates several critical but previously overlooked factors that affect energy usage. These factors, such as distinguishing programming languages from their implementations, the impact of the application implementations themselves, the number of active cores, and memory activity, can significantly skew energy consumption measurements if not accounted for. We show -- via empirical experiments, improved methodology, and careful examination of anomalies -- that when these factors are controlled for, notable discrepancies in prior work vanish. Our analysis suggests that the choice of programming language implementation has no significant impact on energy consumption beyond execution time.","编程语言的选择是否影响能源消耗? 以前的高可见度研究已经在某些编程语言和能源消耗之间建立了联系。对这项工作的因果误解导致学术界和行业领导人根据声称的对能源消耗的影响使用或支持某些语言。本文件直接处理这一因果问题。首先纠正并改进先前工作中使用的计量方法。然后开发一个详细的因果模型,记录编程语言选择与能源消耗之间的复杂关系。这一模型确定并纳入了影响能源使用的若干重要但先前被忽视的因素。这些因素,例如将编程语言与其实施区分开来、应用实施本身的影响、活动核心的数量和记忆活动,如果不计及,则会大大扭曲能源消耗的计量。我们通过实验、改进的方法和仔细检查异常现象,表明这些因素在控制这些因素时,先前工作中明显的差异就会消失。我们的分析表明,编程语言实施的选择不会对执行时间以外的能源消耗产生重大影响。","Nicolas van Kempen, Hyuk-Je Kwon, Dung Tuan Nguyen, Emery D. Berger",2025-06-03T18:06:44Z,It's Not Easy Being Green: On the Energy Efficiency of Programming   Languages,"Es ist nicht einfach, grün zu sein: Über die Energieeffizienz von Programmiersprachen",绿色不是件容易的事:关于编程语言的能源效率,http://arxiv.org/abs/2410.05460v3
8,"Efficiency is essential to support ever-growing datasets, especially for Deep Learning (DL) systems. DL frameworks have traditionally embraced deferred execution-style DL code -- supporting symbolic, graph-based Deep Neural Network (DNN) computation. While scalable, such development is error-prone, non-intuitive, and difficult to debug. Consequently, more natural, imperative DL frameworks encouraging eager execution have emerged but at the expense of run-time performance. Though hybrid approaches aim for the ""best of both worlds,"" using them effectively requires subtle considerations. Our key insight is that, while DL programs typically execute sequentially, hybridizing imperative DL code resembles parallelizing sequential code in traditional systems. Inspired by this, we present an automated refactoring approach that assists developers in determining which otherwise eagerly-executed imperative DL functions could be effectively and efficiently executed as graphs. The approach features novel static imperative tensor and side-effect analyses for Python. Due to its inherent dynamism, analyzing Python may be unsound; however, the conservative approach leverages a speculative (keyword-based) analysis for resolving difficult cases that informs developers of any assumptions made. The approach is: (i) implemented as a plug-in to the PyDev Eclipse IDE that integrates the WALA Ariadne analysis framework and (ii) evaluated on nineteen DL projects consisting of 132 KLOC. The results show that 326 of 766 candidate functions (42.56%) were refactorable, and an average relative speedup of 2.16 on performance tests was observed with negligible differences in model accuracy. The results indicate that the approach is useful in optimizing imperative DL code to its full potential.","效率对于支持不断增长的数据集至关重要, 特别是深海学习( DL) 系统。 DL 框架传统上采用推迟执行式 DL 代码, 支持符号性、 基于图形的深神经网络( DNN) 计算。 虽然可以缩放, 但这种开发是容易出错的, 非直观的, 并且难以调试。 因此, 更自然的、 必要的 DL 框架鼓励急迫执行, 但却牺牲了运行时的性能。 虽然混合方法旨在“ 最佳世界” , 有效地使用它们需要微妙的考虑。 我们的关键洞察力是, DL 程序通常按顺序执行, 混合的 DL 代码类似于传统系统中的平行代码。 受此启发, 我们提出了一个自动重构的重构方法, 帮助开发者确定哪些否则急迫地执行的 DL 功能可以有效和高效地作为图表执行。 这种方法以新颖的静态 高压和副效果分析为Python 。 由于它固有的活力, 分析Python 可能是不准确的; 但是, 保守方法将一个可调的 Ral- dal- dal- dal- disal- disal 函数法的精准的精准的精准的精准的精度函数在传统方法在传统的精准的精准的精准的精准的精准的精准的精准的精准的精度函数( , 。","Raffi Khatchadourian, Tatiana Castro Vélez, Mehdi Bagherzadeh, Nan Jia, Anita Raja",2025-06-03T15:01:43Z,Speculative Automated Refactoring of Imperative Deep Learning Programs   to Graph Execution,Spekulative Automatisierte Refaktorisierung imperativer Deep Learning-Programme zur Graphen-Execution,用于图表执行的势必深深学习方案的投机性自动重组,http://arxiv.org/abs/2504.05424v2
9,"In modern computing systems, compilation employs numerous optimization techniques to enhance code performance. Source-to-source code transformations, which include control flow and datapath transformations, have been widely used in High-Level Synthesis (HLS) and compiler optimization.   While researchers actively investigate methods to improve performance with source-to-source code transformations, they often overlook the significance of verifying their correctness. Current tools cannot provide a holistic verification of these transformations. This paper introduces HEC, a framework for equivalence checking that leverages the e-graph data structure to comprehensively verify functional equivalence between programs. HEC utilizes the MLIR as its frontend and integrates MLIR into the e-graph framework. Through the combination of dynamic and static e-graph rewriting, HEC facilitates the validation of comprehensive code transformations.   We demonstrate effectiveness of HEC on PolyBenchC benchmarks, successfully verifying loop unrolling, tiling, and fusion transformations. HEC processes over 100,000 lines of MLIR code in 40 minutes with predictable runtime scaling. Importantly, HEC identified two critical compilation errors in mlir-opt: loop boundary check errors causing unintended executions during unrolling, and memory read-after-write violations in loop fusion that alter program semantics. These findings demonstrate HEC practical value in detecting real-world compiler bugs and highlight the importance of formal verification in optimization pipelines.","在现代计算系统中,编译采用多种优化技术来提高代码性能。源到源代码转换,包括控制流程和数据路径转换,在高级合成(HLS)和编译器优化中被广泛使用。研究人员积极调查改进源到源代码转换的性能的方法,但往往忽视了核实其正确性的重要性。当前工具无法对这些转换进行整体核查。本文件介绍了HEC,这是一个对等检查框架,它利用电子制图数据结构全面核查程序之间的功能等同性。 HEC利用MLIR作为其前端,并将MLIR纳入电子版图框架。通过动态和静态电子制图的结合,HEC为全面代码转换的验证提供了便利。我们展示了HC在多边代码转换基准上的有效性,成功地核查了循环松动、节动和聚变。HEC在40分钟内处理超过100 000行的MLIR代码,并有可预测的运行时间缩放量。YIC在 ml-opt中发现了两个关键的编译错误:通过动态和静态电子文字重写来验证导致循环处决的准确性程序,这些修正的校正校验的校正的校验结果。","Jiaqi Yin, Zhan Song, Nicolas Bohm Agostini, Antonino Tumeo, Cunxi Yu",2025-06-02T21:59:17Z,HEC: Equivalence Verification Checking for Code Transformation via   Equality Saturation,HEC: Überprüfung der Gleichwertigkeit auf Code-Transformation durch Gleichstellungssättigung,EEC: 通过平等饱和对代码转换进行等同核查,http://arxiv.org/abs/2506.02290v1
10,"The wider adoption of tightly coupled core-adjacent accelerators, such as Arm Scalable Matrix Extension (SME), hinges on lowering software programming complexity. In this paper, we focus on enabling the use of SME architecture in Streaming Scalable Vector Extension (SSVE) mode for workloads written in C/C++. While current compilers optimize loops for all types of SIMD instructions, these techniques primarily target vector units within the core and falter when applied to disaggregated, core-adjacent SIMD accelerators. Our goal is to enable the compiler to automatically generate code for such accelerators only when profitable.   To this end, we investigate a path towards performant, precise, and repeatable computation offloading through two compiler ecosystems. We revisit LLVM compiler passes, MLIR transforms and their associated cost models, and heuristics. We hope that these insights can provide directions for evolving compiler capabilities towards automatic code generation for this next-generation vector processing paradigm.","较广泛采用紧凑相近的核心加速器,如可缩缩缩式矩阵扩展(SME),取决于软件编程复杂性的降低。在本文中,我们注重在C/C+++中为工作量使用流缩可缩放矢量扩展(SSVE)模式中,使中小企业结构能够用于流动可缩放矢量扩展(SSVE)模式。虽然目前的编译者优化了所有类型的SIMMD指令的循环,但这些技术主要针对核心内的矢量单位,在应用到分解、核心相近的 SIMD加速器时则会摇摆不定。我们的目标是使编译者只有在有利的情况下才能自动生成这种加速器的代码。为此,我们研究如何通过两个编译者生态系统实现性能、精确和可重复的计算。我们重新审视LLVM编译器通过、MLIR变换及其相关成本模型,以及超值模型。我们希望这些洞察能为编译者能力演变出向这种下一代矢量处理模式的自动代码生成提供方向。","Mohamed Husain Noor Mohamed, Adarsh Patil, Latchesar Ionkov, Eric Van Hensbergen",2025-06-02T20:24:32Z,Improving compiler support for SIMD offload using Arm Streaming SVE,Verbesserung der Compiler-Unterstützung für SIMD-Offload mit Arm Streaming SVE,利用武器流流 SVE改进对SIMD 卸载的编译器支持,http://arxiv.org/abs/2506.02233v1
11,"Region based memory management is a powerful tool designed with the goal of ensuring memory safety statically. The region calculus of Tofte and Talpin is a well known example of a region based system, which uses regions to manage memory in a stack-like fashion. However, the region calculus is lexically scoped and requires explicit annotation of memory regions, which can be cumbersome for the programmer. Other systems have addressed non-lexical regions, but these approaches typically require the use of a substructural type system to track the lifetimes of regions. We present Spegion, a language with implicit non-lexical regions, which provides these same memory safety guarantees for programs that go beyond using memory allocation in a stack-like manner. We are able to achieve this with a concise syntax, and without the use of substructural types, relying instead on an effect system to enforce constraints on region allocation and deallocation. These regions may be divided into sub-regions, i.e., Splittable rEgions, allowing fine grained control over memory allocation. Furthermore, Spegion permits sized allocations, where each value has an associated size which is used to ensure that regions are not over-allocated into. We present a type system for Spegion and prove it is type safe with respect to a small-step operational semantics.","基于区域的记忆管理是一个强有力的工具,其目的在于确保记忆安全静态。托夫特和塔尔平的区域缩微数是一个以区域为基础的系统的一个众所周知的例子,该系统使用区域来管理记忆,以类似堆叠的方式管理记忆。然而,该区域的缩微数具有法律范围,要求对记忆区域进行明确的注释,这对程序员来说可能很麻烦。其他系统针对的是非传统区域,但这些方法通常需要使用一个亚结构型系统来跟踪各区域的寿命。我们介绍Spegition,一种含有隐含非传统区域的语言,为超出以堆叠方式使用记忆分配的程序提供同样的记忆安全保障。我们可以用简洁的合成法和不使用亚结构型的记忆区域来实现这一目标,而依赖一种影响系统来强制限制区域分配和交易。这些区域可能分为一个子区域,即Slittable rEgion,允许对记忆分配进行精细的谷类控制。此外,Spegition允许对超出存储分配范围的程序提供同样的记忆安全性保障,而每个系统使用的是小型号,而我们使用的是Speticle sium destration strat","Jack Hughes, Michael Vollmer, Mark Batty",2025-06-02T19:11:46Z,Spegion: Implicit and Non-Lexical Regions with Sized Allocations,Spegion: Implizite und nicht-lexikalische Regionen mit vergrößerten Zuweisungen,特征:具有分层分配的隐性和非致命性区域,http://arxiv.org/abs/2506.02182v1
12,"Compilers are crucial in optimizing programs and accelerating their execution. However, optimizing programs automatically using compilers is not trivial. Recent work has attempted to use reinforcement learning (RL) to solve this problem. It has limitations though. Current methods either do not support the optimization of general loop nests or can only be used to optimize loop nests seen during training. In this paper, we propose Pearl, a novel framework that uses deep reinforcement learning to automate compiler code optimization. It uses an RL agent to select the sequence of code optimizations a compiler should apply to make the input code run faster. This agent can optimize general loop nests and can generalize to programs unseen during training. To enable the optimization of general loop nests, we propose a novel representation of the action space that allows the RL agent to select on which part of the loop nest a given code optimization should be applied. Training RL agents for loop nest optimization is slow and data-intensive. We accelerate this process by caching results and pre-training the agent. Integrated with the Tiramisu compiler, our approach streamlines optimization and outperforms existing methods. To the best of our knowledge, Pearl is the first RL-based system to support general programs composed of loop nests manipulating tensors while still being able to generalize to programs unseen during training. It is also the first to support the class of polyhedral optimizations, a class of advanced loop nest optimizations. We evaluate Pearl on a set of benchmarks, and demonstrate competitive performance improvements over state-of-the-art compilers. Notably, Pearl achieves a geometric mean speedup of 2.02x compared to Tiramisu and 3.36x compared to Pluto.","编译器对于优化编程和加速执行程序至关重要。 但是, 自动优化编译器的程序并非微不足道。 最近的工作试图使用强化学习 (RL) 来解决这个问题。 虽然它也有局限性 。 目前的方法要么不支持优化一般环状巢, 要么只能用于优化在训练期间看到的环状巢。 在本文件中, 我们提议 Pearl , 这是一个新颖的框架, 将深度强化学习用于自动编译器代码优化。 它使用 RL 代理器来选择代码优化序列, 一个编译器应该应用来加快输入速度的改进。 这个代理器可以优化一般环状巢巢巢, 并且可以在训练期间将程序推广到普通编译器 。 相对于一般环状环状巢, 我们提出行动空间的新缩略图显示空间的缩略图 3 , 相对于我们的知识, 将普通的轨迹状螺旋缩图比 。 在一般的轨迹中, 普通的轨迹到一般的精度的精度, 我们通过累积结果和预训练来加快这一过程。 与Tiramisu 的精度的精度的精度的精度, 我们的精度比到高级的精度的精度的精度的精度的精度, 。","Djamel Rassem Lamouri, Iheb Nassim Aouadj, Smail Kourta, Riyadh Baghdadi",2025-06-02T17:09:59Z,Pearl: Automatic Code Optimization Using Deep Reinforcement Learning,Perle: Automatische Code-Optimierung mit Deep Reinforcement Learning,珍珠:利用深强化学习实现自动代码优化,http://arxiv.org/abs/2506.01880v1
13,"In many application domains, domain-specific languages can allow domain experts to contribute to collaborative projects more correctly and efficiently. To do so, they must be able to understand program structure from reading existing source code. With high-quality data becoming an increasingly important resource, the creation of data pipelines is an important application domain for domain-specific languages. We execute a mixed-method study consisting of a controlled experiment and a follow-up descriptive survey among the participants to understand the effects of a domain-specific language on bottom-up program understanding and generate hypotheses for future research. During the experiment, participants need the same time to solve program structure comprehension tasks, but are significantly more correct when using the domain-specific language. In the descriptive survey, participants describe reasons related to the programming language itself, such as a better pipeline overview, more enforced code structure, and a closer alignment to the mental model of a data pipeline. In addition, human factors such as less required programming experience and the ability to reuse experience from other data engineering tools are discussed. Based on these results, domain-specific languages are a promising tool for creating data pipelines that can increase correct understanding of program structure and lower barriers to entry for domain experts. Open questions exist to make more informed implementation decisions for domain-specific languages for data pipelines in the future.","在许多应用领域,具体领域的语言可以使具体领域的专家能够更正确、更高效地为协作项目作出贡献,为此,他们必须能够从阅读现有源代码中理解方案结构。随着高质量数据成为日益重要的资源,数据管道的创建是特定领域语言的一个重要应用领域。我们开展了一种混合方法研究,其中包括一项受控实验和后续描述性调查,让参与者了解特定领域语言对自下而上方案理解的影响,并为未来研究提出假设。在试验期间,参与者需要同时解决方案结构理解任务,但在使用特定领域语言时则要更加正确。在描述性调查中,参与者说明了与方案语言本身有关的原因,例如更好的管道概览、更强制性的代码结构,以及更密切地与数据管道的精神模式挂钩。此外,还讨论了诸如不需要多少规划经验和其他数据工程工具的经验再利用能力等人类因素。根据这些结果,具体领域语言是创建数据管道的有希望的工具,可以提高准确理解方案结构,而使用特定领域决定的难度更低。在进入领域时,专家可以提出更明确领域,以便作出更明确的数据决定。","Philip Heltweg, Georg-Daniel Schwarz, Dirk Riehle",2025-06-02T15:03:31Z,Can a domain-specific language improve program structure comprehension   of data pipelines? A mixed-methods study,Kann eine domänenspezifische Sprache das Verständnis von Datenpipelines durch die Programmstruktur verbessern?,具体领域的语言能否改进对数据管道的方案结构理解?,http://arxiv.org/abs/2505.16764v2
14,"Policies are designed to distinguish between correct and incorrect actions; they are types. But badly typed actions may cause not compile errors, but financial and reputational harm We demonstrate how even the most complex ABAC policies can be expressed as types in dependently typed languages such as Agda and Lean, providing a single framework to express, analyze, and implement policies. We then go head-to-head with Rego, the popular and powerful open-source ABAC policy language. We show the superior safety that comes with a powerful type system and built-in proof assistant. In passing, we discuss various access control models, sketch how to integrate in a future when attributes are distributed and signed (as discussed at the W3C), and show how policies can be communicated using just the syntax of the language. Our examples are in Agda.","政策旨在区分正确和不正确的行动;它们是类型的。但打字不当的行动可能不会导致错误的汇编,而是财务和声誉的损害。我们展示了即使最复杂的ABAC政策也可以以依附性格式的语言,如Agda和Lean, 表达、分析和执行政策的单一框架。然后我们与热门和强大的开放源代码ABAC政策语言Rego, 即流行和强大的开放源代码ABAC政策语言, 进行头对头的对头的比较。 我们展示了超级安全, 包括强大的型号系统和内置验证助理。 顺便说一句, 我们讨论了各种访问控制模式, 勾画如何在未来分配和签署属性时整合(W3C讨论过), 并展示如何使用语言的语法来传达政策。 我们的例子在 Agda 中。",Matthew D. Fuchs,2025-06-02T09:04:48Z,"Policy as Code, Policy as Type","Politik als Code, Politik als Typ","政策作为守则,政策作为类型",http://arxiv.org/abs/2506.01446v1
15,"While model serving has unlocked unprecedented capabilities, the high cost of serving large-scale models continues to be a significant barrier to widespread accessibility and rapid innovation. Compiler optimizations have long driven substantial performance improvements, but existing compilers struggle with neural workloads due to the exponentially large and highly interdependent space of possible transformations. Although existing stochastic search techniques can be effective, they are often sample-inefficient and fail to leverage the structural context underlying compilation decisions. We set out to investigate the research question of whether reasoning with large language models (LLMs), without any retraining, can leverage the context-aware decision space of compiler optimization to significantly improve sample efficiency. To that end, we introduce a novel compilation framework (dubbed REASONING COMPILER) that formulates optimization as a sequential, context-aware decision process, guided by a large language model and structured Monte Carlo tree search (MCTS). The LLM acts as a proposal mechanism, suggesting hardware-aware transformations that reflect the current program state and accumulated performance feedback. Monte Carlo tree search (MCTS) incorporates the LLM-generated proposals to balance exploration and exploitation, facilitating structured, context-sensitive traversal of the expansive compiler optimization space. By achieving substantial speedups with markedly fewer samples than leading neural compilers, our approach demonstrates the potential of LLM-guided reasoning to transform the landscape of compiler optimization.","虽然模型服务已经释放出前所未有的能力,但为大型模型服务的高昂成本仍然是妨碍广泛获取和快速创新的重大障碍。编译优化长期以来推动了大量绩效改进,但现有编译者由于可能转型的空间巨大和高度相互依存而面临神经工作量。虽然现有的随机搜索技术可能有效,但它们往往缺乏样本效率,无法利用汇编决定所依据的结构背景。我们着手调查研究问题,即与大型语言模型(LLLMs)进行推理,而不进行任何再培训,是否能够利用编译者优化的环境认知决策空间,显著提高样本效率。为此,我们引入了一个创新的汇编框架(底盘REASONING COMILER),将优化发展成一个有序的、符合背景的、符合背景的决策过程,以大型语言模型和结构化的蒙特卡洛树搜索(MCTS)为指导。 LLMMM作为一个建议机制,提出反映当前方案状态和累积的业绩反馈的硬件觉悟转换。Monte Carlo树搜索(MCTS)将LM生成的建议纳入LMM公司提出的旨在平衡探索和探索,并显著地提高采样效率的建议,通过结构化的深层次的深层方法,便利地实现空间优化的深层优化,从而实现空间优化的深层优化的深层方法。","Sujun Tang, Christopher Priebe, Rohan Mahapatra, Lianhui Qin, Hadi Esmaeilzadeh",2025-06-02T07:02:46Z,Compiler Optimization via LLM Reasoning for Efficient Model Serving,Compiler-Optimierung über LLM-Reasoning für effizientes Modell Servieren,通过LLM优化高效示范服务理由,http://arxiv.org/abs/2506.01374v1
16,"Several studies have explored the mechanisms of large language models (LLMs) in coding tasks, but most have focused on programming languages (PLs) in a monolingual setting. In this paper, we investigate the relationship between multiple PLs and English in the concept space of LLMs. We perform a few-shot translation task on 21 PL pairs using two Llama-based models. By decoding the embeddings of intermediate layers during this task, we observe that the concept space is closer to English (including PL keywords) and assigns high probabilities to English tokens in the second half of the intermediate layers. We analyze neuron activations for 11 PLs and English, finding that while language-specific neurons are primarily concentrated in the bottom layers, those exclusive to each PL tend to appear in the top layers. For PLs that are highly aligned with multiple other PLs, identifying language-specific neurons is not feasible. These PLs also tend to have a larger keyword set than other PLs and are closer to the model's concept space regardless of the input/output PL in the translation task. Our findings provide insights into how LLMs internally represent PLs, revealing structural patterns in the model's concept space. Code is available at https://github.com/cisnlp/code-specific-neurons.","一些研究探索了大语言模型(LLMs)在编码任务方面的机制,但大多数研究侧重于单一语言环境下的英语(PLs)编程机制。在本文中,我们研究了LLMM概念空间中多个PL和英语之间的关系。我们使用两个Llama模型对21个PL配对进行了几分翻译任务。通过在任务期间解码中间层嵌入的中间层,我们观察到概念空间更接近英语(包括PL关键字),在中间层的后半层中为英语标牌分配高概率。我们分析了11个PLS和英语的神经激活,发现语言特有神经元主要集中在底层,而每个PLMS的特有神经元往往出现在顶层。对于与其他多PLlama模型高度一致的PLPS来说,确定语言特有神经元是不可行的。这些PLPS往往有一个比其他PL关键字更大的关键字组,并且更接近模型的概念空间,而不论翻译任务中的输入/输出PLPLS和英文,我们发现LMS-CMs的内部结构代码是如何体现的。 AS-cLGLCS-cis/exemex/ex","Amir Hossein Kargaran, Yihong Liu, François Yvon, Hinrich Schütze",2025-06-01T16:24:13Z,How Programming Concepts and Neurons Are Shared in Code Language Models,Wie Programmierkonzepte und Neuronen in Code Language Models geteilt werden,如何在代码语言模式中共享编程概念和新内容,http://arxiv.org/abs/2506.01074v1
17,"Grammar plays a critical role in natural language processing and text/code generation by enabling the definition of syntax, the creation of parsers, and guiding structured outputs. Although large language models (LLMs) demonstrate impressive capabilities across domains, their ability to infer and generate grammars has not yet been thoroughly explored. In this paper, we aim to study and improve the ability of LLMs for few-shot grammar generation, where grammars are inferred from sets of a small number of positive and negative examples and generated in Backus-Naur Form. To explore this, we introduced a novel dataset comprising 540 structured grammar generation challenges, devised 6 metrics, and evaluated 8 various LLMs against it. Our findings reveal that existing LLMs perform sub-optimally in grammar generation. To address this, we propose an LLM-driven hybrid genetic algorithm, namely HyGenar, to optimize grammar generation. HyGenar achieves substantial improvements in both the syntactic and semantic correctness of generated grammars across LLMs.","语法模型在自然语言处理和文本/代码生成中发挥着关键作用,它能够定义语法、创建剖析员和指导结构化产出。虽然大型语言模型(LLMs)显示了跨领域令人印象深刻的能力,但尚未彻底探索其推算和生成语法模型的能力。在本文中,我们的目标是研究和提高LLMs在几发语法生成中的能力,从少量正反例子中推断出语法,并在Backus-Naur格式中生成。为了探索这一点,我们引入了一个由540个结构化语法生成挑战组成的新数据集,设计了6公吨,并评估了8个不同的LLMs 。我们的研究结果表明,现有的LLMs在语法生成中表现了亚光性。为了解决这一问题,我们建议使用LLMM驱动的混合遗传算法,即HyGenar,以优化语法生成。HyGenar在LMs生成的语法和语法的合成和语法正确性两方面都取得了重大改进。","Weizhi Tang, Yixuan Li, Chris Sypherd, Elizabeth Polgreen, Vaishak Belle",2025-06-01T12:49:41Z,HyGenar: An LLM-Driven Hybrid Genetic Algorithm for Few-Shot Grammar   Generation,HyGenar: Ein LLM-getriebener Hybridgenetischer Algorithmus für die wenige scharfe Grammar-Generation,HyGenar: 鲜热语法一代的LLM-Driven混合遗传等级,http://arxiv.org/abs/2505.16978v2
18,"In software development, technical debt (TD) refers to suboptimal implementation choices made by the developers to meet urgent deadlines and limited resources, posing challenges for future maintenance. Self-Admitted Technical Debt (SATD) is a sub-type of TD, representing specific TD instances ``openly admitted'' by the developers and often expressed through source code comments. Previous research on SATD has focused predominantly on the Java programming language, revealing a significant gap in cross-language SATD. Such a narrow focus limits the generalizability of existing findings as well as SATD detection techniques across multiple programming languages. Our work addresses such limitation by introducing CppSATD, a dedicated C++ SATD dataset, comprising over 531,000 annotated comments and their source code contexts. Our dataset can serve as a foundation for future studies that aim to develop SATD detection methods in C++, generalize the existing findings to other languages, or contribute novel insights to cross-language SATD research.","在软件开发方面,技术债务(TD)是指开发商为满足紧迫的最后期限和有限资源而做出的执行选择不够理想,对今后的维护构成挑战。自认技术债务(SATD)是一种次级TD类型,代表开发商“公开承认”的具体TD案例,并经常通过源代码评论表达。以前关于技术债务的研究主要侧重于爪哇语编程语言,揭示了跨语言SATD的巨大差距。这种狭隘的焦点限制了现有发现以及多种编程语言的SATD探测技术的普遍性。我们的工作通过引入CppSATD(C++ SATD)这一专门的C+数据集来解决这种局限性,该数据集由531,000多份附加说明的评论及其源代码背景组成。我们的数据集可以作为未来研究的基础,目的是开发C++的SATD探测方法,将现有发现结果推广到其他语言,或者为跨语言的SATD研究提供新的见解。","Phuoc Pham, Murali Sridharan, Matteo Esposito, Valentina Lenarduzzi",2025-06-01T12:49:01Z,Descriptor: C++ Self-Admitted Technical Debt Dataset (CppSATD),Deskriptor: C++ Selbstzugelassener technischer Schuldendatensatz (CppSATD),说明:C++自发技术债务数据集(CppSATD),http://arxiv.org/abs/2505.01136v2
19,"This paper introduces SGL, a graphics language that is aesthetically similar to SQL. SGL is based on traditional grammars of graphics, as well as Vega-Lite's composition algebra. SGL demonstrates that the grammatical approach to graphics lends itself naturally to a SQL-like language. As a graphical counterpart to SQL, SGL facilitates the addition of visualization capabilities to SQL query interfaces. This paper presents components of the SGL language alongside examples. Comparisons to SQL and existing grammars are made throughout to provide further clarity.","本文件介绍SGL,这是一种与SQL相近的图形语言。SGL基于传统的图形语法以及Vega-Lite的构成代数。SGL表明,对图形的语法方法自然适合SQL类语言。SGL作为SQL的图形对应方,为SQL查询界面增加可视化能力提供了便利。本文提供SGL语言的构成部分以及示例。与SQL和现有语法的比较贯穿始终,以进一步清晰。",Jon Chapman,2025-06-01T00:00:00Z,SGL: A Structured Graphics Language,SGL: Eine strukturierte Grafiksprache,SGL:结构化图形语言,http://arxiv.org/abs/2505.14690v2
20,"The C++ programming language provides classes and structs as fundamental modeling entities. Consequently, C++ code tends to favour array-of-structs (AoS) for encoding data sequences, even though structure-of-arrays (SoA) yields better performance for some calculations. We propose a C++ language extension based on attributes that allows developers to guide the compiler in selecting memory arrangements, i.e.~to select the optimal choice between AoS and SoA dynamically depending on both the execution context and algorithm step. The compiler can then automatically convert data into the preferred format prior to the calculations and convert results back afterward. The compiler handles all the complexity of determining which data to convert and how to manage data transformations. Our implementation realises the compiler-extension for the new annotations in Clang and demonstrates their effectiveness through a smoothed particle hydrodynamics (SPH) code, which we evaluate on an Intel CPU, an ARM CPU, and a Grace-Hopper GPU. While the separation of concerns between data structure and operators is elegant and provides performance improvements, the new annotations do not eliminate the need for performance engineering. Instead, they challenge conventional performance wisdom and necessitate rethinking approaches how to write efficient implementations.","C++ 编程语言提供类和支架,作为基本的建模实体。 因此, C++ 代码往往偏向于编码数据序列的数组阵列( AoS) , 尽管数组结构( SoA) 产生更好的性能可以进行某些计算。 我们提出C++ 语言扩展, 其属性使开发者能够指导编译者选择内存安排, 即根据执行背景和算法步骤动态地选择 AoS 和 SoA 的最佳选择。 编译者随后可以自动将数据转换为首选格式, 并在计算和转换结果后再转换结果。 编译者处理确定哪些数据转换以及如何管理数据转换的所有复杂性。 我们的实施工作实现了Clanng 中新说明的编程扩展, 并通过一个通畅的粒子流体( SHPH) 代码来显示其有效性, 我们用该代码来评价一个 Intel CPU、 一个 ARM CPU 和一个 Grace- Hoper GPUP 。 虽然数据结构与操作者之间的关切问题分离是优雅化的, 并且提供改进性改进性操作的方法。","Pawel K. Radtke, Tobias Weinzierl",2025-05-31T17:28:04Z,Annotation-guided AoS-to-SoA conversions and GPU offloading with data   views in C++,Annotationsgeführte AoS-to-SoA-Konvertierungen und GPU-Offloading mit Datenansichten in C++,C++中带有数据视图的批注-制导 AosS-to-SoA转换和GPU卸载,http://arxiv.org/abs/2502.16517v2
21,"Quantum computing, with its vast potential, is fundamentally shaped by the intricacies of quantum mechanics, which both empower and constrain its capabilities. The development of a universal, robust quantum programming language has emerged as a key research focus in this rapidly evolving field. This paper explores Silq, a recent high-level quantum programming language, highlighting its strengths and unique features. We aim to share our insights on designing and implementing high-level quantum algorithms using Silq, demonstrating its practical applications and advantages for quantum programming.","量子计算具有巨大的潜力,其根本决定因素是量子力学的复杂,既赋予其能力,又限制其能力。在这个迅速变化的领域,发展普遍、稳健的量子编程语言已成为一个关键的研究焦点。本文探讨Silq,这是最近一种高水平量子编程语言,突出其优势和独特性。我们的目标是分享我们对利用Silq设计和实施高量子算法的见解,展示其实际应用和量子编程优势。","Viktorija Bezganovic, Marco Lewis, Sadegh Soudjani, Paolo Zuliani",2025-05-31T14:14:50Z,High-level quantum algorithm programming using Silq,High-Level-Quantenalgorithmus-Programmierung mit Silq,使用 Silq 的高级量子算法编程,http://arxiv.org/abs/2409.10231v2
22,"Coding is a fundamental skill required in the engineering discipline, and much work exists exploring better ways of teaching coding in the higher education context. In particular, Code Snippets (CSs) are approved to be an effective way of introducing programming language units to students. CSs are portions of source code of varying size and content. They can be used in a myriad of ways, one of which is to teach the code they contain as well as its function. To further explore the use of CSs, a pedagogical summer internship project was set up at the Warwick Manufacturing Group (WMG). The scope of the considerations for the study derives from an educational standpoint. Within the evaluations made, the focus was primarily given to pieces of information which proved to provide evidence pertaining to the methodology involved in either teaching or developing teaching materials. By taking the results produced into account from a pedagogical perspective, it was found that several qualities of popular code snippet tutorials which benefit or hinder the learning process, including code length, interactivity, further support, and quality of explanation. These qualities are then combined and used to present a plan for the design of an effective learning resource which makes use of code snippets.","在工程学科中,编码是一项基本技能,许多工作正在探索高等教育中更好的教学编码方法,特别是,守则片段被批准为向学生介绍编程语言单元的有效方式。守则片段是源代码的一部分,其大小和内容不同。守则代码可以多种方式使用,其中之一是教授其包含的守则及其功能。为了进一步探索CS的使用,沃里克制造集团(WMG)制定了一个暑期教学实习项目。研究的考虑范围来自教育观点。在评价中,重点主要放在证明为教学或开发教材所用方法提供证据的信息片段。通过从教学角度考虑产生的结果,发现流行代码片段的几种质量有利于或阻碍学习过程,包括代码长度、互动性、进一步支持和解释质量。这些品质随后被组合起来,用来提出设计有效学习资源的计划,从而使用代码片段。","Joshua Akingbade, Jianhua Yang, Mir Seyedebrahimi",2025-05-31T05:51:32Z,Using Code Snippets to Teach Programming Languages,Verwendung von Code-Snippets zum Lehren von Programmiersprachen,使用代码片段教授编程语言,http://arxiv.org/abs/2506.00404v1
23,"This paper presents a novel symbolic algorithm for the Maximal End Component (MEC) decomposition of a Markov Decision Process (MDP). The key idea behind our algorithm INTERLEAVE is to interleave the computation of Strongly Connected Components (SCCs) with eager elimination of redundant state-action pairs, rather than performing these computations sequentially as done by existing state-of-the-art algorithms. Even though our approach has the same complexity as prior works, an empirical evaluation of INTERLEAVE on the standardized Quantitative Verification Benchmark Set demonstrates that it solves 19 more benchmarks (out of 379) than the closest previous algorithm. On the 149 benchmarks that prior approaches can solve, we demonstrate a 3.81x average speedup in runtime.","本文为马克夫决定程序(MDP)最大端组件分解提供了一个新型的象征性算法。 我们的INTELEAVE算法背后的关键思想是,将“连接强的部件”的计算与热切地消除多余的州-州对对,而不是像现有最新算法那样按顺序进行这些计算。 尽管我们的方法与先前的工程一样复杂,但对INTERLEAVE在标准化量化核查基准集上的实证评估表明,它解决的基准(在379项中)比以前的最接近的算法多19项。 关于以前方法可以解决的149项基准,我们展示了运行时平均速度3.81x。","Suguman Bansal, Ramneet Singh",2025-05-30T20:56:09Z,INTERLEAVE: A Faster Symbolic Algorithm for Maximal End Component   Decomposition,INTERLEAVE: Ein schnellerer symbolischer Algorithmus für maximale Endkomponentenzersetzung,最大末端组件分解的更快的符号性算法,http://arxiv.org/abs/2505.20748v2
24,"While AI programming tools hold the promise of increasing programmers' capabilities and productivity to a remarkable degree, they often exclude users from essential decision-making processes, causing many to effectively ""turn off their brains"" and over-rely on solutions provided by these systems. These behaviors can have severe consequences in critical domains, like software security. We propose Human-in-the-loop Decoding, a novel interaction technique that allows users to observe and directly influence LLM decisions during code generation, in order to align the model's output with their personal requirements. We implement this technique in HiLDe, a code completion assistant that highlights critical decisions made by the LLM and provides local alternatives for the user to explore. In a within-subjects study (N=18) on security-related tasks, we found that HiLDe led participants to generate significantly fewer vulnerabilities and better align code generation with their goals compared to a traditional code completion assistant.","虽然AI编程工具有望提高程序员的能力和生产力,但往往将用户排除在基本决策进程之外,导致许多用户有效“转变大脑”和过度依赖这些系统提供的解决办法。这些行为可能在软件安全等关键领域产生严重后果。我们提出“人与人互换脱钩”这一新型互动技术,使用户能够在代码生成过程中观察并直接影响LLM决定,以便使模型的产出与其个人需求相一致。我们在HiLDE实施这一技术,这是一个代码完成助理,它突出LLM做出的关键决定,并为用户提供可供探索的本地替代方法。在一项关于与安全有关的任务的内题研究(N=18)中,我们发现HILDE引导参与者比传统的代码完成助理少得多的脆弱性,更好地将代码生成与其目标相匹配。","Emmanuel Anaya González, Raven Rothkopf, Sorin Lerner, Nadia Polikarpova",2025-05-30T18:45:49Z,HiLDe: Intentional Code Generation via Human-in-the-Loop Decoding,HiLDe: Intentionale Code-Generierung über Human-in-the-Loop-Dekodierung,"HILDe:通过 "" 人类在圈套解码 "" 进行有意代码生成",http://arxiv.org/abs/2505.22906v2
25,"With the explosion in popularity of the Rust programming language, a wealth of tools have recently been developed to analyze, verify, and test Rust programs. Alas, the Rust ecosystem remains relatively young, meaning that every one of these tools has had to re-implement difficult, time-consuming machinery to interface with the Rust compiler and its cargo build system, to hook into the Rust compiler's internal representation, and to expose an abstract syntax tree (AST) that is suitable for analysis rather than optimized for efficiency. We address this missing building block of the Rust ecosystem, and propose Charon, an analysis framework for Rust. Charon acts as a swiss-army knife for analyzing Rust programs, and deals with all of the tedium above, providing clients with a clean, stable AST that can serve as the foundation of many analyses. We demonstrate the usefulness of Charon through a series of case studies, ranging from a Rust verification framework (Aeneas), a compiler from Rust to C (Eurydice), and a novel taint-checker for cryptographic code. To drive the point home, we also re-implement a popular existing analysis (Rudra), and show that it can be replicated by leveraging the Charon framework.","随着拉斯特编程语言的流行,最近开发了大量工具来分析、核查和测试拉斯特程序。唉,拉斯特生态系统仍然相对年轻,这意味着这些工具中的每一个工具都不得不重新实施困难、耗时的机器,以便与拉斯特编程者及其货物建造系统进行互动,连接到拉斯特编程者的内部代表,并暴露出适合分析而不是优化效率的抽象语法树(AST ) 。 我们处理鲁斯特生态系统这一缺失的建筑块,并提议沙隆(Rust. Charon) 是一个分析拉斯特程序的分析框架,作为分析鲁斯特程序的刀子,并处理上面的所有品,向客户提供清洁、稳定的AST,作为许多分析的基础。我们通过一系列案例研究,从鲁斯特核查框架(Aeneas)到C(Eurydice),以及一个新的加密代码的固定检查者(Chartoptical- checker) 。我们还可以通过一个大众化框架(Rustrical ) 来推动家分析。","Son Ho, Guillaume Boisseau, Lucas Franceschino, Yoann Prak, Aymeric Fromherz, Jonathan Protzenko",2025-05-30T18:35:08Z,Charon: An Analysis Framework for Rust,Charon: Ein Analyserahmen für Rust,Charon: 鲁斯特分析框架,http://arxiv.org/abs/2410.18042v3
26,"Automatic program generation has long been a fundamental challenge in computer science. Recent benchmarks have shown that large language models (LLMs) can effectively generate code at the function level, make code edits, and solve algorithmic coding tasks. However, to achieve full automation, LLMs should be able to generate production-quality, self-contained application modules. To evaluate the capabilities of LLMs in solving this challenge, we introduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for the generation of backend applications. We focus on backends for three critical reasons: (i) they are practically relevant, building the core components of most modern web and cloud software, (ii) they are difficult to get right, requiring multiple functions and files to achieve the desired functionality, and (iii) they are security-critical, as they are exposed to untrusted third-parties, making secure solutions that prevent deployment-time attacks an imperative. BaxBench validates the functionality of the generated applications with comprehensive test cases, and assesses their security exposure by executing end-to-end exploits. Our experiments reveal key limitations of current LLMs in both functionality and security: (i) even the best model, OpenAI o1, achieves a mere 62% on code correctness; (ii) on average, we could successfully execute security exploits on around half of the correct programs generated by each LLM; and (iii) in less popular backend frameworks, models further struggle to generate correct and secure applications. Progress on BaxBench signifies important steps towards autonomous and secure software development with LLMs.","长期以来,自动程序生成一直是计算机科学的一个根本性挑战。最近的基准显示,大型语言模型(LLMS)能够有效地生成功能层面的代码,进行代码编辑,并解决算法编码任务。然而,为了实现完全自动化,LLMS应该能够生成生产质量和自足的应用模块。为了评估LLMS在应对这一挑战方面的能力,我们引入了BaxBench,这是一个新的评价基准,由生成后端应用程序的392项任务组成。我们侧重于后端,有三个关键原因:(一)它们实际上具有相关性,可以建立最现代网络和云层软件的核心组件;(二)它们很难正确获取,需要多个功能和文件才能实现理想功能,以及(三)它们应当具有安全性,因为它们暴露在不受信任的第三方面前,从而能够找到安全解决方案,防止部署-时间袭击的必然需要。BaxBench 将生成的应用程序的功能与全面测试案例相匹配,并通过实施端对端-端利用来评估其安全风险暴露。我们的实验揭示了当前LMS的核心组件在功能和安全性与安全性两个方面的关键限制:(一)在功能和安全性与安全性测试中,我们可以成功的模型上,可以使用B正正正正正正的模型,可以成功。","Mark Vero, Niels Mündler, Victor Chibotaru, Veselin Raychev, Maximilian Baader, Nikola Jovanović, Jingxuan He, Martin Vechev",2025-05-30T13:01:16Z,BaxBench: Can LLMs Generate Correct and Secure Backends?,BaxBench: Können LLMs korrekte und sichere Backends erzeugen?,BaxBench:LLMs能够产生正确和安全的后端吗?,http://arxiv.org/abs/2502.11844v3
27,"In recent years, large language models (LLMs) have showcased significant advancements in code generation. However, most evaluation benchmarks are primarily oriented towards Python, making it difficult to evaluate other programming languages, such as Swift, with high quality. By examining widely established multilingual benchmarks like HumanEval-XL and MultiPL-E, we identified critical issues specific to their Swift components, making them insufficient or even irrelevant for assessing LLM coding capabilities on Swift. Unlike these existing approaches, which prioritize rapid scaling and generalization by automatically translating Python-centric benchmarks with LLMs, we adopt a quality-over-quantity methodology. We present SwiftEval, the first Swift-oriented benchmark consisting of 28 carefully hand-crafted problems, and evaluate 44 popular Code LLMs on it. Our results show significant LLM scores drop for problems requiring language-specific features, most noticeable in the models of smaller sizes.","近年来,大型语言模型(LLMS)展示了在代码生成方面取得的重大进步,然而,大多数评价基准主要面向Python, 因而难以对诸如Swift等高品质的其他编程语言进行评价。我们通过审查诸如HumanEval-XL和MultiPL-E等广泛确立的多语种基准,发现了其Swift组成部分特有的关键问题,使其不足以或甚至与评估Swift的LLM编码能力无关。与这些现有方法不同,这些方法通过将Python中心基准与LLMs自动翻译为Python中心基准,将快速扩展和普及列为优先事项,我们采用了一种质量超量的方法。我们介绍了SwiftEval,这是第一个面向Swift的基准,由28个仔细的手工操作问题组成,并评价了44个通用代码LMs。我们的结果显示,LMM在需要语言特点的问题上成绩显著下降,最明显的是较小规模的模型。","Ivan Petrukha, Yana Kurliak, Nataliia Stulova",2025-05-30T08:06:30Z,SwiftEval: Developing a Language-Specific Benchmark for LLM-generated   Code Evaluation,SwiftEval: Entwicklung eines sprachspezifischen Benchmarks für die LLM-generierte Code-Bewertung,SwiftEval:为LLM产生的守则评价制定语言特定基准,http://arxiv.org/abs/2505.24324v1
28,"Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop a rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code-NL-code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage ""distill-then-RL"" training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12~20%, while matching or even exceeding the performance of 671B DeepSeek-R1. We will release our model, training pipeline, and dataset to facilitate research in EDA and LLM communities.","通过可核实的奖励(RLVR)强化学习培训的大型语言模型(LLM)在明确、可自动化的核查任务(如软件编程和数学问题)方面取得了突破。将RLVR扩大到电子设计自动化(EDA),特别是自动生成硬件描述语言(HDLs),如来自自然语言(NL)规格的Verilog(HDLL),然而,这带来了三大挑战:缺乏自动和准确的核查环境,缺少高质量的NLVR码配对,以及RLVR的计算成本过高。为此,我们引入了代码V-R1,一个用于培训VRR1的RV-7框架,用于培训Verilog的LLLMM。首先,我们开发了一个基于规则的测试引擎,对金参考进行强有力的等值检查。第二,我们提出了一个圆柱数据合成方法,将开放源的 Verilog布片与LM生成的NLLS描述配对,通过生成的测试对代码-NLF20码一致性进行校准,并通过等示例进行过滤, 提供高品质数据集。第三,我们开始两个阶段的“DLVR-R-R-R-R-R-R-R-R-R-d-d-d-d-drodustris-d-d-d-d-dal”的升级的测试,然后进行成本的升级的升级的校程的校程的校程的校程的校程的校制算算法。","Yaoyu Zhu, Di Huang, Hanqi Lyu, Xiaoyun Zhang, Chongxiao Li, Wenxuan Shi, Yutong Wu, Jianan Mu, Jinghua Wang, Yang Zhao, Pengwei Jin, Shuyao Cheng, Shengwen Liang, Xishan Zhang, Rui Zhang, Zidong Du, Qi Guo, Xing Hu, Yunji Chen",2025-05-30T03:51:06Z,CodeV-R1: Reasoning-Enhanced Verilog Generation,CodeV-R1: Grundlegende Verilog-Generierung,代码V-R1:有理性的增强性性性性性性性生殖器生成,http://arxiv.org/abs/2505.24183v1
29,"Device driver bugs are the leading cause of OS compromises, and their formal verification is therefore highly desirable. To the best of our knowledge, no realistic and performant driver has been verified for a non-trivial device. We propose Pancake, an imperative language for systems programming that features a well-defined and verification-friendly semantics. Leveraging the verified compiler backend of the CakeML functional language, we develop a compiler for Pancake that guarantees that the binary retains the semantics of the source code. Usng automatic translation of Pancake to the Viper SMT front-end, we verify a performant driver for an Ethernet NIC.","设备驱动器错误是OS 折中的主要原因, 因此它们的正式核查是十分可取的。 据我们所知, 没有现实和性能的驱动器被核实为非三边设备。 我们提议使用Pancake, 这是系统编程的必备语言, 包含定义明确且对核查友好的语义。 我们利用已核实的 CakeML 功能语言的编译器后端, 我们为 Pancake开发一个编译器, 保证二进制保留源代码的语义 。 将 Pancake 自动翻译为Viper SMT 前端, 我们验证Ethernet NIC 的演算器驱动器 。","Junming Zhao, Miki Tanaka, Johannes Åman Pohjola, Alessandro Legnani, Tiana Tsang Ung, H. Truong, Tsun Wang Sau, Thomas Sewell, Rob Sison, Hira Syeda, Magnus Myreen, Michael Norrish, Gernot Heiser",2025-05-30T00:48:15Z,Verifying Device Drivers with Pancake,Gerätetreiber mit Pfannkuchen überprüfen,使用 Pancake 校验设备驱动程序,http://arxiv.org/abs/2501.08249v2
30,"Code auditing is the process of reviewing code with the aim of identifying bugs. Large Language Models (LLMs) have demonstrated promising capabilities for this task without requiring compilation, while also supporting user-friendly customization. However, auditing a code repository with LLMs poses significant challenges: limited context windows and hallucinations can degrade the quality of bug reports, and analyzing large-scale repositories incurs substantial time and token costs, hindering efficiency and scalability.   This work introduces an LLM-based agent, RepoAudit, designed to perform autonomous repository-level code auditing. Equipped with agent memory, RepoAudit explores the codebase on demand by analyzing data-flow facts along feasible program paths within individual functions. It further incorporates a validator module to mitigate hallucinations by verifying data-flow facts and checking the satisfiability of path conditions associated with potential bugs, thereby reducing false positives. RepoAudit detects 40 true bugs across 15 real-world benchmark projects with a precision of 78.43%, requiring on average only 0.44 hours and $2.54 per project. Also, it detects 185 new bugs in high-profile projects, among which 174 have been confirmed or fixed. We have open-sourced RepoAudit at https://github.com/PurCL/RepoAudit.","大型语言模型(LLMS)在不要求编纂的同时,也支持方便用户的定制;然而,用LLMS审计一个代码存储库带来了重大挑战:有限的背景窗口和幻觉可以降低错误报告的质量,分析大型存储库会产生大量的时间和象征性成本,从而妨碍效率和可缩放性。这项工作引入了一个基于LLM的代理机构RepoAudit, 目的是进行自主存储库级代码审计。用代理存储器进行配置,RepoAudit根据需求对代码库进行探索,按照单个功能中可行的程序路径分析数据流事实。它还包含一个验证模块,通过核查数据流事实和检查与潜在错误相关的路径条件的可视性来减轻幻觉,从而降低错误的阳性。RepoAudit在15个真实世界基准项目中检测40个真正的错误,精确度为78.43%,平均只需要0.44小时,每个项目2.54。此外,它还检测了高清晰度A/Recom项目中的185个新错误,其中174个已经确认。","Jinyao Guo, Chengpeng Wang, Xiangzhe Xu, Zian Su, Xiangyu Zhang",2025-05-29T22:08:26Z,RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing,RepoAudit: Ein autonomer LLM-Agent für Repository-Level Code Auditing,重审:主管仓库一级守则审计的自治LLM-Agent,http://arxiv.org/abs/2501.18160v3
31,"Following Milner's seminal paper, the representation of functions as processes has received considerable attention. For pure $\lambda$-calculus, the process representations yield (at best) non-extensional $\lambda $-theories (i.e., $\beta$ rule holds, whereas $\eta$ does not).   In the paper, we study how to obtain extensional representations, and how to move between extensional and non-extensional representations. Using Internal $\pi$, $\mathrm{I}\pi$ (a subset of the $\pi$-calculus in which all outputs are bound), we develop a refinement of Milner's original encoding of functions as processes that is parametric on certain abstract components called wires. These are, intuitively, processes whose task is to connect two end-point channels. We show that when a few algebraic properties of wires hold, the encoding yields a $\lambda$-theory. Exploiting the symmetries and dualities of $\mathrm{I}\pi$, we isolate three main classes of wires. The first two have a sequential behaviour and are dual of each other; the third has a parallel behaviour and is the dual of itself. We show the adoption of the parallel wires yields an extensional $\lambda$-theory; in fact, it yields an equality that coincides with that of B\""ohm trees with infinite $\eta$. In contrast, the other two classes of wires yield non-extensional $\lambda$-theories whose equalities are those of the L\'evy-Longo and B\""ohm trees.","遵循 Milner 的原始文件, 函数作为进程表达得到相当的注意 。 对于纯 $\ lambda $- calcululs 的计算, 进程表达方式产生( 最多) 非扩展 $\ lambda $ 美元理论( 即 $\ beta$ 规则持有, 而不是 $\ a $ 美元 ) 。 在文件中, 我们研究如何获得扩展表达方式, 以及如何在扩展和非扩展表达方式之间移动 。 使用 $\ pi, $\ materm { Ipi $ ( $- 美元- calcallulus 的分类 ) 。 我们开发了 Milner 最初的元编码( $- lambda $ 美元 ) ( 美元 美元- 美元 ) ( 美元- 美元 ) 的计算方式, 以 美元- 美元 美元 的双线表达方式 。","Ken Sakayori, Davide Sangiorgi",2025-05-29T13:36:48Z,Extensional and Non-extensional Functions as Processes,Erweiterungs- und Nichterweiterungsfunktionen als Prozesse,作为处理过程的扩展和非扩展函数,http://arxiv.org/abs/2405.03536v2
32,"We present a data-driven approach to the quantitative verification of probabilistic programs and stochastic dynamical models. Our approach leverages neural networks to compute tight and sound bounds for the probability that a stochastic process hits a target condition within finite time. This problem subsumes a variety of quantitative verification questions, from the reachability and safety analysis of discrete-time stochastic dynamical models, to the study of assertion-violation and termination analysis of probabilistic programs. We rely on neural networks to represent supermartingale certificates that yield such probability bounds, which we compute using a counterexample-guided inductive synthesis loop: we train the neural certificate while tightening the probability bound over samples of the state space using stochastic optimisation, and then we formally check the certificate's validity over every possible state using satisfiability modulo theories; if we receive a counterexample, we add it to our set of samples and repeat the loop until validity is confirmed. We demonstrate on a diverse set of benchmarks that, thanks to the expressive power of neural networks, our method yields smaller or comparable probability bounds than existing symbolic methods in all cases, and that our approach succeeds on models that are entirely beyond the reach of such alternative techniques.","我们提出了一种数据驱动的方法,用于对概率程序进行定量核查,以及随机动态模型。我们的方法利用神经网络对神经网络进行精确和稳健的界限进行计算,以确定随机过程在有限时间内击中目标条件的可能性。这个问题包含各种定量核查问题,从离散随机动态模型的可及性和安全分析,到对概率程序侵犯和终止性分析的研究。我们依靠神经网络来代表产生这种概率界限的超测证书,我们利用反比制导导导导导导导导引导合成环路进行计算:我们培训神经证书,同时使用随机优选法收紧对州空间样本的概率,然后我们利用可互观性模拟模型的可及性分析,正式检查证书对每个可能州的有效性;如果我们得到反比反比分析,我们把它添加到我们的样本中,并在确认有效性之前重复循环。我们用一套不同的基准来进行计算,由于神经测试方法的显性能力,因此在模拟模型的绝对成功性模型上,我们现有的方法压得更小。","Alessandro Abate, Alec Edwards, Mirco Giacobbe, Hashan Punchihewa, Diptarko Roy",2025-05-29T10:15:46Z,Quantitative Verification with Neural Networks,Quantitative Überprüfung mit neuralen Netzen,与神经网络进行定量核查,http://arxiv.org/abs/2301.06136v5
33,"Selecting a subset of cells is a common task in data engineering, for example, to remove errors or select only specific parts of a table. Multiple approaches to express this selection exist. One option is numeric indexing, commonly found in general programming languages, where a tuple of numbers identifies the cell. Alternatively, the separate dimensions can be referred to using different enumeration schemes like ""A1"" for the first cell, commonly found in software such as spreadsheet systems.   In a large-scale controlled experiment with student participants as proxy for data practitioners, we compare the two options with respect to speed and correctness of reading and writing code.   The results show that, when reading code, participants make less mistakes using spreadsheet-style syntax. Additionally, when writing code, they make fewer mistakes and are faster when using spreadsheet syntax compared to numeric syntax.   From this, a domain-specific syntax, such as spreadsheet syntax for data engineering, appears to be a promising alternative to explore in future tools to support practitioners without a software engineering background.","选择一组单元格是数据工程中的一项共同任务,例如,清除错误或只选择表格的特定部分。存在表达此选择的多种方法。一个选项是数字索引化,通常在一般编程语言中找到,其中数字图示可以识别单元格。或者,可以将不同的维度提到使用不同的查点方案,例如“A1”等第一个单元格的查点方案,通常在电子表格系统等软件中找到。在与学生参与者作为数据实践者代理的大规模控制实验中,我们比较了关于读写代码速度和正确性的两个选项。结果显示,在阅读代码时,参与者使用电子表格风格语法的误差较少。此外,在写入代码时,他们会减少错误,使用电子表格语法比数字语法更快。从这一点看,一个特定域的语法,例如数据工程电子表格语法,似乎是今后在没有软件工程背景的情况下探索支持从业者的工具的一个很有希望的替代方法。","Philip Heltweg, Dirk Riehle, Georg-Daniel Schwarz",2025-05-29T09:49:14Z,Is spreadsheet syntax better than numeric indexing for cell selection?,Ist die Tabellensyntax besser als die numerische Indexierung für die Zellenauswahl?,选择单元格的电子表格语法是否比数字索引更好?,http://arxiv.org/abs/2505.23296v1
34,"Server-side request forgery (SSRF) vulnerabilities are inevitable in PHP web applications. Existing static tools in detecting vulnerabilities in PHP web applications neither contain SSRF-related features to enhance detection accuracy nor consider PHP's dynamic type features. In this paper, we present Artemis, a static taint analysis tool for detecting SSRF vulnerabilities in PHP web applications. First, Artemis extracts both PHP built-in and third-party functions as candidate source and sink functions. Second, Artemis constructs both explicit and implicit call graphs to infer functions' relationships. Third, Artemis performs taint analysis based on a set of rules that prevent over-tainting and pauses when SSRF exploitation is impossible. Fourth, Artemis analyzes the compatibility of path conditions to prune false positives. We have implemented a prototype of Artemis and evaluated it on 250 PHP web applications. Artemis reports 207 true vulnerable paths (106 true SSRFs) with 15 false positives. Of the 106 detected SSRFs, 35 are newly found and reported to developers, with 24 confirmed and assigned CVE IDs.","在PHP网络应用中,发现PHP网络应用中的弱点的现有静态工具既不含SERF相关特性,也非用于提高检测准确性,也非考虑到PHP动态类型特征。在本文件中,我们介绍了Artemis,这是一个用于检测PHP网络应用中SERF脆弱性的静态污点分析工具。首先,Artemis提取了PHP内在和第三方功能,作为候选源和汇的功能。第二,Artemis构建了明确和隐含的调用图,以推断功能的关系。第三,Artemis根据一套规则进行了污点分析,以防止在SSRF无法开发时过度拉扯和暂停。第四,Artemis分析了用于提取假阳点的路径条件的兼容性。我们实施了Artemis原型,并对250 PHP网络应用进行了评估。Artemis报告207条真实的脆弱路径(106个真正的SSRF),有15个假阳点。在106个测得的SSRF中,新发现并向开发商报告了35条,有24个确认和指定的CVEID。","Yuchen Ji, Ting Dai, Zhichao Zhou, Yutian Tang, Jingzhu He",2025-05-29T07:34:13Z,Artemis: Toward Accurate Detection of Server-Side Request Forgeries   through LLM-Assisted Inter-Procedural Path-Sensitive Taint Analysis,Artemis: Auf dem Weg zur genauen Erkennung von Server-Side Request Forgeries durch LLM-Assisted Inter-Procedural Path-Sensitive Taint Analysis,"人工制品:通过LLM协助的跨程序间路由感知性图解分析,力求准确探测服务器-Side请求的伪造情况",http://arxiv.org/abs/2502.21026v3
35,"Large language models (LLMs) are increasingly integrated in software development, but ensuring correctness in LLM-generated code remains challenging and often requires costly manual review. Verifiable code generation -- jointly generating code, specifications, and proofs of code-specification alignment -- offers a promising path to address this limitation and further unleash LLMs' benefits in coding. Yet, there exists a significant gap in evaluation: current benchmarks often lack support for end-to-end verifiable code generation. In this paper, we introduce Verina (Verifiable Code Generation Arena), a high-quality benchmark enabling a comprehensive and modular evaluation of code, specification, and proof generation as well as their compositions. Verina consists of 189 manually curated coding tasks in Lean, with detailed problem descriptions, reference implementations, formal specifications, and extensive test suites. Our extensive evaluation of state-of-the-art LLMs reveals significant challenges in verifiable code generation, especially in proof generation, underscoring the need for improving LLM-based theorem provers in verification domains. The best model, OpenAI o4-mini, generates only 61.4% correct code, 51.0% sound and complete specifications, and 3.6% successful proofs, with one trial per task. We hope Verina will catalyze progress in verifiable code generation by providing a rigorous and comprehensive benchmark. We release our dataset on https://huggingface.co/datasets/sunblaze-ucb/verina and our evaluation code on https://github.com/sunblaze-ucb/verina.","大型语言模型(LLMS)日益融入软件开发,但确保LLM生成的代码的正确性仍具有挑战性,而且往往需要花费昂贵的人工审查。可验证代码的生成 -- -- 共同生成代码、规格和具体编码协调的证明 -- -- 为解决这一限制和进一步释放LLMS的编码好处提供了一条充满希望的道路。然而,在评价方面存在着巨大的差距:目前的基准往往缺乏对端至端可核查代码生成的支持。在本文件中,我们引入了一个高质量基准,从而能够对代码、规格和证据生成及其构成进行全面和模块化评价。Verina由189个手工拼凑的编码任务组成,其中有详细的问题描述、参考执行、正式规格和广泛的测试套件。我们对目前最先进的LLMSM(可验证代码生成的DLMSUCSDS/Arencrearetures)的生成存在重大挑战。我们的最佳模型(OO4minirea)只能生成61.4%的代码,51.0%的硬度和3.6%的精确度的代码,我们将提供我们精确度数据生成的进度和3.6%的数据。","Zhe Ye, Zhengxu Yan, Jingxuan He, Timothe Kasriel, Kaiyu Yang, Dawn Song",2025-05-29T06:12:52Z,VERINA: Benchmarking Verifiable Code Generation,VERINA: Benchmarking der überprüfbaren Code-Generierung,VERINA:可核实代码生成基准,http://arxiv.org/abs/2505.23135v1
36,"Large Language Models (LLMs) are increasingly being used to automate programming tasks. Yet, LLMs' capabilities in reasoning about program semantics are still inadequately studied, leaving significant potential for further exploration. This paper introduces FormalBench, a comprehensive benchmark designed to evaluate LLMs' reasoning abilities on program semantics, particularly via the task of synthesizing formal program specifications to assist verifying program correctness. This task requires both comprehensive reasoning over all possible program executions and the generation of precise, syntactically correct expressions that adhere to formal syntax and semantics. Using this benchmark, we evaluated the ability of LLMs in synthesizing consistent and complete specifications. Our findings show that LLMs perform well with simple control flows but struggle with more complex structures, especially loops, even with advanced prompting. Additionally, LLMs exhibit limited robustness against semantic-preserving transformations. We also highlight common failure patterns and design self-repair prompts, improving success rates by 25%.","大型语言模型(LLMs)正越来越多地被用于使程序设计任务自动化。然而,LLMs在程序语义学的推理能力仍然没有得到充分的研究,从而留下了进一步探索的巨大潜力。本文介绍了旨在评估LLMs在程序语义学上的推理能力的全面基准“正式Bench”,尤其是通过综合正式程序规格协助核实程序正确性的任务。这项任务要求对所有可能的方案执行进行综合推理,并生成精确、统一和正确的表达方式,以坚持正式的语义学和语义学。我们利用这一基准,评估LLMs在综合一致和完整的规格方面的能力。我们的研究结果显示LMs在简单控制流程方面表现良好,但与更复杂的结构,特别是圆环进行斗争,即使有先进的提示。此外,LLMs在反对语义-保留转换方面表现出有限的强健性。我们还强调了常见的失败模式和设计自我修复的提示,使成功率提高了25%。","Thanh Le-Cong, Bach Le, Toby Murray",2025-05-29T06:07:32Z,Can LLMs Reason About Program Semantics? A Comprehensive Evaluation of   LLMs on Formal Specification Inference,Kann LLMs Grund über Programm Semantik? Eine umfassende Bewertung von LLMs auf formale Spezifikation Inferenz,CLLMs 方案语义学理由:全面评价关于正式具体推断的LLMs,http://arxiv.org/abs/2503.04779v4
37,"We introduce CASS, the first large-scale dataset and model suite for cross-architecture GPU code transpilation, targeting both source-level (CUDA <--> HIP) and assembly-level (Nvidia SASS <--> AMD RDNA3) translation. The dataset comprises 70k verified code pairs across host and device, addressing a critical gap in low-level GPU code portability. Leveraging this resource, we train the CASS family of domain-specific language models, achieving 95% source translation accuracy and 37.5% assembly translation accuracy, substantially outperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our generated code matches native performance in over 85% of test cases, preserving runtime and memory behavior. To support rigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16 GPU domains with ground-truth execution. All data, models, and evaluation tools are released as open source to foster progress in GPU compiler tooling, binary compatibility, and LLM-guided hardware translation.","我们引入了CASS, 这是首个用于跨建筑化 GPU 代码转换的大型数据集和模型套件, 针对源级( CUDA < - > HIP) 和组装级( Nvidia SASSS < - > AMD RDNA3) 翻译。 该数据集由70k经核实的对数组成, 跨越主机和装置, 解决低级别 GPU 代码可移植性的重大差距。 利用此资源, 我们培训 CASS 群域域语言模型, 实现95% 源翻译准确性和37.5% 组装翻译准确性, 大大超过 GPT-4o、 Claude 和 Hipifify等商业基线。 我们生成的代码匹配了85%以上测试案例的本地性能, 保存运行时间和记忆行为。 为了支持严格的评估, 我们引入了 CASS- Bench, 一个覆盖16 GPU 域域的曲线基准, 并带有地标执行。 所有的数据、 模型和评价工具都作为公开来源发布, 以促进 GPUPU 工具的编译、 和 LLM 制硬件翻译的进展 。","Ahmed Heakl, Sarim Hashmi, Gustavo Bertolo Stahl, Seung Hun Eddie Han, Salman Khan, Abdulrahman Mahmoud",2025-05-29T05:44:32Z,"CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark","CASS: Nvidia zu AMD Transpilation mit Daten, Modellen und Benchmark",CASS: Nvidia 到AMD 传输数据、模型和基准,http://arxiv.org/abs/2505.16968v3
38,"Diffusion LLMs have emerged as a promising alternative to conventional autoregressive LLMs, offering significant potential for improved runtime efficiency. However, existing diffusion models lack the ability to provably enforce user-specified formal constraints, such as regular expressions, which makes them unreliable for tasks that require structured outputs, such as fixed-schema JSON generation. Unlike autoregressive models that generate tokens sequentially, diffusion LLMs predict a block of tokens in parallel. This parallelism makes traditional constrained decoding algorithms, which are designed for sequential token prediction, ineffective at preserving the true output distribution. To address this limitation, we propose DINGO, a dynamic programming-based constrained decoding strategy that is both efficient and provably distribution-preserving. DINGO enables sampling of output strings with the highest probability under the model's predicted distribution, while strictly satisfying any user-specified regular expression. On standard symbolic math and JSON generation benchmarks, DINGO achieves up to a 68 percentage point improvement over unconstrained inference","与传统自动递减的LMS相比,LMS已成为一种大有希望的替代传统自动递减的LMS,它为提高运行时间效率提供了巨大的潜力;然而,现有的推广模式缺乏能力,无法对用户指定的正式限制,例如常规表达方式,使其不适于执行需要结构化产出的任务,例如固定的JSON 生成。与自动递减模式不同,扩散LMS同时预测一系列象征性。这种平行使得传统的受限制解码算法(这些算法是为按顺序进行象征性预测而设计的,在保存真正的产出分布方面无效)。为了应对这一限制,我们建议DINGO,这是一个动态的、基于程序化的受限解码战略,既高效又可可移动的分布保存。DINGO能够根据模型预测的分布,在严格满足用户指定的任何常规表达方式时,以最高概率取样产出字符。关于标准的象征性数学和JSONS生成基准,DINGO在未受限制的推算之外,实现了68个百分点的改进。","Tarun Suresh, Debangshu Banerjee, Shubham Ugare, Sasa Misailovic, Gagandeep Singh",2025-05-29T04:04:54Z,DINGO: Constrained Inference for Diffusion LLMs,DINGO: Beschränkte Schlussfolgerung für Diffusion LLMs,DINGO: 扩散长效LMM的连续推论,http://arxiv.org/abs/2505.23061v1
